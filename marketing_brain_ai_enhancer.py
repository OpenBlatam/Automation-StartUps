#!/usr/bin/env python3
"""
üöÄ MARKETING BRAIN AI ENHANCER
Sistema Avanzado de IA para Mejora Continua del Marketing Brain System
Incluye Deep Learning, NLP Avanzado, y Optimizaci√≥n Autom√°tica
"""

import json
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import logging
from dataclasses import dataclass, asdict
from pathlib import Path
import sys
import asyncio
import aiohttp
from collections import defaultdict, Counter
import re
import pickle
import joblib
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings('ignore')

# Agregar el directorio actual al path
sys.path.append(str(Path(__file__).parent))
from advanced_marketing_brain_system import AdvancedMarketingBrain, MarketingConcept
from marketing_brain_analytics import MarketingBrainAnalytics
from marketing_brain_automation import MarketingBrainAutomation

logger = logging.getLogger(__name__)

@dataclass
class AIEnhancementResult:
    """Resultado de mejora con IA"""
    concept_id: str
    original_score: float
    enhanced_score: float
    improvement_percentage: float
    enhancement_type: str
    applied_techniques: List[str]
    confidence_level: float
    timestamp: str

@dataclass
class LearningInsight:
    """Insight de aprendizaje autom√°tico"""
    insight_type: str
    pattern_description: str
    confidence: float
    supporting_data: Dict[str, Any]
    actionable_recommendations: List[str]
    impact_score: float

@dataclass
class PredictiveModel:
    """Modelo predictivo entrenado"""
    model_name: str
    model_type: str
    accuracy: float
    features_used: List[str]
    training_data_size: int
    last_trained: str
    model_path: str

class MarketingBrainAIEnhancer:
    """
    Sistema Avanzado de IA para Mejora Continua del Marketing Brain System
    Incluye Deep Learning, NLP Avanzado, y Optimizaci√≥n Autom√°tica
    """
    
    def __init__(self, brain_system: AdvancedMarketingBrain = None, 
                 analytics: MarketingBrainAnalytics = None,
                 automation: MarketingBrainAutomation = None):
        self.brain = brain_system or AdvancedMarketingBrain()
        self.analytics = analytics or MarketingBrainAnalytics(self.brain)
        self.automation = automation or MarketingBrainAutomation(self.brain, self.analytics)
        
        # Modelos de IA
        self.predictive_models = {}
        self.nlp_models = {}
        self.clustering_models = {}
        self.optimization_models = {}
        
        # Datos de entrenamiento
        self.training_data = self._load_training_data()
        self.feature_vectors = {}
        
        # Configuraci√≥n de IA
        self.ai_config = self._load_ai_config()
        
        # M√©tricas de mejora
        self.enhancement_metrics = {
            'total_enhancements': 0,
            'successful_enhancements': 0,
            'average_improvement': 0.0,
            'models_trained': 0,
            'insights_generated': 0
        }
        
        # Cache de resultados
        self.enhancement_cache = {}
        self.learning_insights = []
        
        logger.info("üöÄ Marketing Brain AI Enhancer initialized successfully")
    
    def _load_training_data(self) -> Dict[str, Any]:
        """Cargar datos de entrenamiento"""
        training_data = {
            'campaigns': [],
            'concepts': [],
            'performance_metrics': [],
            'text_data': [],
            'feature_data': []
        }
        
        # Cargar campa√±as existentes
        for campaign in self.brain.campaigns:
            training_data['campaigns'].append(campaign)
            
            # Extraer m√©tricas de rendimiento
            if 'metrics' in campaign:
                training_data['performance_metrics'].append(campaign['metrics'])
            
            # Extraer texto para NLP
            text_features = [
                campaign.get('name', ''),
                campaign.get('description', ''),
                campaign.get('category', ''),
                campaign.get('technology', ''),
                campaign.get('channel', ''),
                campaign.get('vertical', '')
            ]
            training_data['text_data'].append(' '.join(text_features))
        
        logger.info(f"üìä Loaded {len(training_data['campaigns'])} campaigns for training")
        return training_data
    
    def _load_ai_config(self) -> Dict[str, Any]:
        """Cargar configuraci√≥n de IA"""
        return {
            'model_training': {
                'test_size': 0.2,
                'random_state': 42,
                'cross_validation_folds': 5,
                'early_stopping_rounds': 10
            },
            'nlp_settings': {
                'max_features': 1000,
                'ngram_range': (1, 2),
                'min_df': 2,
                'max_df': 0.95
            },
            'clustering': {
                'n_clusters_range': (2, 10),
                'max_iter': 300,
                'random_state': 42
            },
            'optimization': {
                'max_iterations': 100,
                'convergence_threshold': 0.001,
                'learning_rate': 0.01
            },
            'enhancement': {
                'min_improvement_threshold': 0.05,
                'max_enhancement_attempts': 3,
                'confidence_threshold': 0.7
            }
        }
    
    def train_predictive_models(self) -> Dict[str, PredictiveModel]:
        """Entrenar modelos predictivos avanzados"""
        logger.info("ü§ñ Training predictive models...")
        
        # Preparar datos de entrenamiento
        X, y = self._prepare_training_data()
        
        if len(X) == 0 or len(y) == 0:
            logger.warning("No training data available")
            return {}
        
        models = {}
        
        # Modelo de predicci√≥n de √©xito
        success_model = self._train_success_prediction_model(X, y)
        if success_model:
            models['success_prediction'] = success_model
        
        # Modelo de predicci√≥n de m√©tricas
        metrics_model = self._train_metrics_prediction_model(X, y)
        if metrics_model:
            models['metrics_prediction'] = metrics_model
        
        # Modelo de optimizaci√≥n de presupuesto
        budget_model = self._train_budget_optimization_model(X, y)
        if budget_model:
            models['budget_optimization'] = budget_model
        
        # Guardar modelos
        self.predictive_models.update(models)
        self.enhancement_metrics['models_trained'] += len(models)
        
        logger.info(f"‚úÖ Trained {len(models)} predictive models")
        return models
    
    def _prepare_training_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """Preparar datos de entrenamiento"""
        features = []
        targets = []
        
        for campaign in self.training_data['campaigns']:
            # Extraer caracter√≠sticas
            feature_vector = self._extract_campaign_features(campaign)
            if feature_vector is not None:
                features.append(feature_vector)
                
                # Extraer target (probabilidad de √©xito)
                target = campaign.get('success_probability', 0.5)
                targets.append(target)
        
        if not features:
            return np.array([]), np.array([])
        
        return np.array(features), np.array(targets)
    
    def _extract_campaign_features(self, campaign: Dict[str, Any]) -> Optional[np.ndarray]:
        """Extraer caracter√≠sticas de una campa√±a"""
        try:
            features = []
            
            # Caracter√≠sticas categ√≥ricas (one-hot encoding)
            categories = ['category', 'technology', 'channel', 'vertical', 'objective']
            for cat in categories:
                value = campaign.get(cat, 'unknown')
                # Simplificado: usar hash para representar categor√≠as
                features.append(hash(value) % 100)
            
            # Caracter√≠sticas num√©ricas
            budget = campaign.get('budget', {}).get('amount', 0)
            features.append(budget / 10000)  # Normalizar
            
            duration = campaign.get('timeline', {}).get('duration_weeks', 8)
            features.append(duration / 20)  # Normalizar
            
            # M√©tricas de rendimiento
            metrics = campaign.get('metrics', {})
            metric_features = [
                metrics.get('conversion_rate', 0) / 20,
                metrics.get('engagement_rate', 0) / 20,
                metrics.get('click_through_rate', 0) / 10,
                metrics.get('cost_per_acquisition', 0) / 100,
                metrics.get('return_on_ad_spend', 0) / 10
            ]
            features.extend(metric_features)
            
            return np.array(features)
            
        except Exception as e:
            logger.error(f"Error extracting features: {e}")
            return None
    
    def _train_success_prediction_model(self, X: np.ndarray, y: np.ndarray) -> Optional[PredictiveModel]:
        """Entrenar modelo de predicci√≥n de √©xito"""
        try:
            from sklearn.model_selection import train_test_split
            from sklearn.metrics import mean_squared_error, r2_score
            
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=self.ai_config['model_training']['test_size'],
                random_state=self.ai_config['model_training']['random_state']
            )
            
            # Entrenar modelo Gradient Boosting
            model = GradientBoostingRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=6,
                random_state=42
            )
            
            model.fit(X_train, y_train)
            
            # Evaluar modelo
            y_pred = model.predict(X_test)
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            
            # Guardar modelo
            model_path = f"models/success_prediction_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
            Path("models").mkdir(exist_ok=True)
            joblib.dump(model, model_path)
            
            return PredictiveModel(
                model_name="Success Prediction Model",
                model_type="GradientBoostingRegressor",
                accuracy=r2,
                features_used=[f"feature_{i}" for i in range(X.shape[1])],
                training_data_size=len(X_train),
                last_trained=datetime.now().isoformat(),
                model_path=model_path
            )
            
        except Exception as e:
            logger.error(f"Error training success prediction model: {e}")
            return None
    
    def _train_metrics_prediction_model(self, X: np.ndarray, y: np.ndarray) -> Optional[PredictiveModel]:
        """Entrenar modelo de predicci√≥n de m√©tricas"""
        try:
            from sklearn.ensemble import RandomForestRegressor
            from sklearn.multioutput import MultiOutputRegressor
            
            # Preparar targets m√∫ltiples (m√∫ltiples m√©tricas)
            multi_targets = []
            for campaign in self.training_data['campaigns']:
                metrics = campaign.get('metrics', {})
                target_vector = [
                    metrics.get('conversion_rate', 0),
                    metrics.get('engagement_rate', 0),
                    metrics.get('click_through_rate', 0),
                    metrics.get('cost_per_acquisition', 0),
                    metrics.get('return_on_ad_spend', 0)
                ]
                multi_targets.append(target_vector)
            
            if not multi_targets:
                return None
            
            multi_targets = np.array(multi_targets)
            
            # Entrenar modelo multi-output
            model = MultiOutputRegressor(RandomForestRegressor(
                n_estimators=100,
                random_state=42
            ))
            
            model.fit(X, multi_targets)
            
            # Guardar modelo
            model_path = f"models/metrics_prediction_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
            joblib.dump(model, model_path)
            
            return PredictiveModel(
                model_name="Metrics Prediction Model",
                model_type="MultiOutputRandomForest",
                accuracy=0.85,  # Placeholder
                features_used=[f"feature_{i}" for i in range(X.shape[1])],
                training_data_size=len(X),
                last_trained=datetime.now().isoformat(),
                model_path=model_path
            )
            
        except Exception as e:
            logger.error(f"Error training metrics prediction model: {e}")
            return None
    
    def _train_budget_optimization_model(self, X: np.ndarray, y: np.ndarray) -> Optional[PredictiveModel]:
        """Entrenar modelo de optimizaci√≥n de presupuesto"""
        try:
            from sklearn.ensemble import RandomForestRegressor
            
            # Preparar datos de presupuesto vs rendimiento
            budget_targets = []
            for campaign in self.training_data['campaigns']:
                budget = campaign.get('budget', {}).get('amount', 0)
                success = campaign.get('success_probability', 0)
                # Target: ratio de √©xito por d√≥lar invertido
                if budget > 0:
                    budget_targets.append(success / (budget / 1000))
                else:
                    budget_targets.append(0)
            
            if not budget_targets:
                return None
            
            budget_targets = np.array(budget_targets)
            
            # Entrenar modelo
            model = RandomForestRegressor(
                n_estimators=100,
                random_state=42
            )
            
            model.fit(X, budget_targets)
            
            # Guardar modelo
            model_path = f"models/budget_optimization_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
            joblib.dump(model, model_path)
            
            return PredictiveModel(
                model_name="Budget Optimization Model",
                model_type="RandomForestRegressor",
                accuracy=0.80,  # Placeholder
                features_used=[f"feature_{i}" for i in range(X.shape[1])],
                training_data_size=len(X),
                last_trained=datetime.now().isoformat(),
                model_path=model_path
            )
            
        except Exception as e:
            logger.error(f"Error training budget optimization model: {e}")
            return None
    
    def train_nlp_models(self) -> Dict[str, Any]:
        """Entrenar modelos de procesamiento de lenguaje natural"""
        logger.info("üìù Training NLP models...")
        
        if not self.training_data['text_data']:
            logger.warning("No text data available for NLP training")
            return {}
        
        models = {}
        
        # Modelo TF-IDF para an√°lisis de texto
        tfidf_model = self._train_tfidf_model()
        if tfidf_model:
            models['tfidf'] = tfidf_model
        
        # Modelo de clustering de texto
        text_clustering_model = self._train_text_clustering_model()
        if text_clustering_model:
            models['text_clustering'] = text_clustering_model
        
        # Modelo de an√°lisis de sentimiento
        sentiment_model = self._train_sentiment_model()
        if sentiment_model:
            models['sentiment'] = sentiment_model
        
        self.nlp_models.update(models)
        logger.info(f"‚úÖ Trained {len(models)} NLP models")
        return models
    
    def _train_tfidf_model(self) -> Optional[Any]:
        """Entrenar modelo TF-IDF"""
        try:
            vectorizer = TfidfVectorizer(
                max_features=self.ai_config['nlp_settings']['max_features'],
                ngram_range=self.ai_config['nlp_settings']['ngram_range'],
                min_df=self.ai_config['nlp_settings']['min_df'],
                max_df=self.ai_config['nlp_settings']['max_df']
            )
            
            # Entrenar vectorizador
            tfidf_matrix = vectorizer.fit_transform(self.training_data['text_data'])
            
            # Guardar modelo
            model_path = f"models/tfidf_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
            joblib.dump(vectorizer, model_path)
            
            return {
                'model': vectorizer,
                'matrix': tfidf_matrix,
                'path': model_path,
                'vocabulary_size': len(vectorizer.vocabulary_)
            }
            
        except Exception as e:
            logger.error(f"Error training TF-IDF model: {e}")
            return None
    
    def _train_text_clustering_model(self) -> Optional[Any]:
        """Entrenar modelo de clustering de texto"""
        try:
            if 'tfidf' not in self.nlp_models:
                logger.warning("TF-IDF model not available for clustering")
                return None
            
            tfidf_matrix = self.nlp_models['tfidf']['matrix']
            
            # Encontrar n√∫mero √≥ptimo de clusters
            best_n_clusters = 3
            best_score = -1
            
            for n_clusters in range(2, min(8, len(self.training_data['text_data']))):
                kmeans = KMeans(
                    n_clusters=n_clusters,
                    random_state=self.ai_config['clustering']['random_state'],
                    max_iter=self.ai_config['clustering']['max_iter']
                )
                cluster_labels = kmeans.fit_predict(tfidf_matrix)
                score = silhouette_score(tfidf_matrix, cluster_labels)
                
                if score > best_score:
                    best_score = score
                    best_n_clusters = n_clusters
            
            # Entrenar modelo final
            final_kmeans = KMeans(
                n_clusters=best_n_clusters,
                random_state=self.ai_config['clustering']['random_state'],
                max_iter=self.ai_config['clustering']['max_iter']
            )
            final_kmeans.fit(tfidf_matrix)
            
            # Guardar modelo
            model_path = f"models/text_clustering_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
            joblib.dump(final_kmeans, model_path)
            
            return {
                'model': final_kmeans,
                'n_clusters': best_n_clusters,
                'silhouette_score': best_score,
                'path': model_path
            }
            
        except Exception as e:
            logger.error(f"Error training text clustering model: {e}")
            return None
    
    def _train_sentiment_model(self) -> Optional[Any]:
        """Entrenar modelo de an√°lisis de sentimiento"""
        try:
            # Modelo simplificado de sentimiento basado en palabras clave
            positive_words = [
                'exitoso', 'excelente', 'mejor', 'incremento', 'crecimiento',
                'optimizaci√≥n', 'eficiente', 'innovador', 'avanzado', 'superior'
            ]
            
            negative_words = [
                'problema', 'error', 'fallo', 'decrecimiento', 'reducci√≥n',
                'ineficiente', 'lento', 'complejo', 'dif√≠cil', 'costoso'
            ]
            
            def calculate_sentiment_score(text):
                text_lower = text.lower()
                positive_count = sum(1 for word in positive_words if word in text_lower)
                negative_count = sum(1 for word in negative_words if word in text_lower)
                
                if positive_count + negative_count == 0:
                    return 0.5  # Neutral
                
                return positive_count / (positive_count + negative_count)
            
            # Calcular sentimientos para todos los textos
            sentiment_scores = [calculate_sentiment_score(text) for text in self.training_data['text_data']]
            
            return {
                'positive_words': positive_words,
                'negative_words': negative_words,
                'sentiment_scores': sentiment_scores,
                'average_sentiment': np.mean(sentiment_scores)
            }
            
        except Exception as e:
            logger.error(f"Error training sentiment model: {e}")
            return None
    
    def enhance_concept_with_ai(self, concept: MarketingConcept) -> AIEnhancementResult:
        """Mejorar concepto usando IA avanzada"""
        logger.info(f"üöÄ Enhancing concept {concept.concept_id} with AI...")
        
        original_score = concept.success_probability
        enhancement_techniques = []
        
        try:
            # 1. Optimizaci√≥n de caracter√≠sticas
            enhanced_concept = self._optimize_concept_features(concept)
            if enhanced_concept:
                concept = enhanced_concept
                enhancement_techniques.append("feature_optimization")
            
            # 2. Mejora de descripci√≥n con NLP
            enhanced_description = self._enhance_description_with_nlp(concept)
            if enhanced_description:
                concept.description = enhanced_description
                enhancement_techniques.append("nlp_description_enhancement")
            
            # 3. Optimizaci√≥n de presupuesto
            optimized_budget = self._optimize_budget_with_ai(concept)
            if optimized_budget:
                concept.estimated_budget = optimized_budget
                enhancement_techniques.append("budget_optimization")
            
            # 4. Predicci√≥n y ajuste de m√©tricas
            predicted_metrics = self._predict_and_adjust_metrics(concept)
            if predicted_metrics:
                concept.expected_metrics = predicted_metrics
                enhancement_techniques.append("metrics_prediction")
            
            # 5. Mejora de tags con clustering
            enhanced_tags = self._enhance_tags_with_clustering(concept)
            if enhanced_tags:
                concept.tags = enhanced_tags
                enhancement_techniques.append("tag_enhancement")
            
            # Calcular nuevo score
            enhanced_score = self._calculate_enhanced_score(concept)
            improvement_percentage = ((enhanced_score - original_score) / original_score) * 100
            
            # Crear resultado
            result = AIEnhancementResult(
                concept_id=concept.concept_id,
                original_score=original_score,
                enhanced_score=enhanced_score,
                improvement_percentage=improvement_percentage,
                enhancement_type="comprehensive_ai_enhancement",
                applied_techniques=enhancement_techniques,
                confidence_level=self._calculate_confidence_level(enhancement_techniques),
                timestamp=datetime.now().isoformat()
            )
            
            # Actualizar m√©tricas
            self.enhancement_metrics['total_enhancements'] += 1
            if improvement_percentage > 0:
                self.enhancement_metrics['successful_enhancements'] += 1
            
            # Actualizar promedio de mejora
            total_improvements = self.enhancement_metrics['total_enhancements']
            current_avg = self.enhancement_metrics['average_improvement']
            self.enhancement_metrics['average_improvement'] = (
                (current_avg * (total_improvements - 1) + improvement_percentage) / total_improvements
            )
            
            logger.info(f"‚úÖ Enhanced concept {concept.concept_id}: {improvement_percentage:.1f}% improvement")
            return result
            
        except Exception as e:
            logger.error(f"Error enhancing concept {concept.concept_id}: {e}")
            return AIEnhancementResult(
                concept_id=concept.concept_id,
                original_score=original_score,
                enhanced_score=original_score,
                improvement_percentage=0.0,
                enhancement_type="error",
                applied_techniques=[],
                confidence_level=0.0,
                timestamp=datetime.now().isoformat()
            )
    
    def _optimize_concept_features(self, concept: MarketingConcept) -> Optional[MarketingConcept]:
        """Optimizar caracter√≠sticas del concepto"""
        try:
            # Usar modelo de predicci√≥n de √©xito si est√° disponible
            if 'success_prediction' in self.predictive_models:
                model_path = self.predictive_models['success_prediction'].model_path
                model = joblib.load(model_path)
                
                # Extraer caracter√≠sticas del concepto
                features = self._extract_concept_features(concept)
                if features is not None:
                    predicted_success = model.predict([features])[0]
                    
                    # Si la predicci√≥n es baja, intentar optimizar
                    if predicted_success < 0.7:
                        # Ajustar caracter√≠sticas para mejorar predicci√≥n
                        optimized_concept = self._adjust_concept_for_better_prediction(concept, model, features)
                        return optimized_concept
            
            return None
            
        except Exception as e:
            logger.error(f"Error optimizing concept features: {e}")
            return None
    
    def _extract_concept_features(self, concept: MarketingConcept) -> Optional[np.ndarray]:
        """Extraer caracter√≠sticas de un concepto"""
        try:
            features = []
            
            # Caracter√≠sticas categ√≥ricas
            categories = [concept.category, concept.technology, concept.channel, concept.vertical, concept.objective]
            for cat in categories:
                features.append(hash(cat) % 100)
            
            # Caracter√≠sticas num√©ricas
            budget = concept.estimated_budget.get('amount', 0)
            features.append(budget / 10000)
            
            duration = concept.timeline.get('duration_weeks', 8)
            features.append(duration / 20)
            
            # M√©tricas esperadas
            metrics = concept.expected_metrics
            metric_features = [
                metrics.get('conversion_rate', 0) / 20,
                metrics.get('engagement_rate', 0) / 20,
                metrics.get('click_through_rate', 0) / 10,
                metrics.get('cost_per_acquisition', 0) / 100,
                metrics.get('return_on_ad_spend', 0) / 10
            ]
            features.extend(metric_features)
            
            return np.array(features)
            
        except Exception as e:
            logger.error(f"Error extracting concept features: {e}")
            return None
    
    def _adjust_concept_for_better_prediction(self, concept: MarketingConcept, model: Any, features: np.ndarray) -> MarketingConcept:
        """Ajustar concepto para mejor predicci√≥n"""
        try:
            # Crear copia del concepto
            optimized_concept = MarketingConcept(
                concept_id=concept.concept_id,
                name=concept.name,
                description=concept.description,
                category=concept.category,
                technology=concept.technology,
                channel=concept.channel,
                vertical=concept.vertical,
                objective=concept.objective,
                inspiration_campaigns=concept.inspiration_campaigns,
                success_probability=concept.success_probability,
                complexity=concept.complexity,
                priority=concept.priority,
                estimated_budget=concept.estimated_budget.copy(),
                timeline=concept.timeline.copy(),
                expected_metrics=concept.expected_metrics.copy(),
                tags=concept.tags.copy(),
                created_at=concept.created_at
            )
            
            # Ajustar presupuesto si es muy bajo o muy alto
            current_budget = concept.estimated_budget.get('amount', 0)
            if current_budget < 15000:
                optimized_concept.estimated_budget['amount'] = 25000
                optimized_concept.estimated_budget['tier'] = 'Intermedio'
            elif current_budget > 100000:
                optimized_concept.estimated_budget['amount'] = 75000
                optimized_concept.estimated_budget['tier'] = 'Avanzado'
            
            # Ajustar duraci√≥n si es muy corta o muy larga
            current_duration = concept.timeline.get('duration_weeks', 8)
            if current_duration < 4:
                optimized_concept.timeline['duration_weeks'] = 8
            elif current_duration > 16:
                optimized_concept.timeline['duration_weeks'] = 12
            
            return optimized_concept
            
        except Exception as e:
            logger.error(f"Error adjusting concept: {e}")
            return concept
    
    def _enhance_description_with_nlp(self, concept: MarketingConcept) -> Optional[str]:
        """Mejorar descripci√≥n usando NLP"""
        try:
            if 'tfidf' not in self.nlp_models:
                return None
            
            # Analizar descripci√≥n actual
            current_description = concept.description
            
            # Buscar descripciones similares exitosas
            similar_descriptions = self._find_similar_successful_descriptions(current_description)
            
            if similar_descriptions:
                # Combinar elementos de descripciones exitosas
                enhanced_description = self._combine_descriptions(current_description, similar_descriptions)
                return enhanced_description
            
            return None
            
        except Exception as e:
            logger.error(f"Error enhancing description with NLP: {e}")
            return None
    
    def _find_similar_successful_descriptions(self, description: str) -> List[str]:
        """Encontrar descripciones similares exitosas"""
        try:
            if 'tfidf' not in self.nlp_models:
                return []
            
            vectorizer = self.nlp_models['tfidf']['model']
            tfidf_matrix = self.nlp_models['tfidf']['matrix']
            
            # Vectorizar descripci√≥n actual
            current_vector = vectorizer.transform([description])
            
            # Calcular similitudes
            similarities = []
            for i, campaign in enumerate(self.training_data['campaigns']):
                if campaign.get('success_probability', 0) > 0.8:  # Solo campa√±as exitosas
                    campaign_vector = tfidf_matrix[i]
                    similarity = np.dot(current_vector.toarray(), campaign_vector.toarray().T)[0][0]
                    similarities.append((similarity, campaign.get('description', '')))
            
            # Ordenar por similitud y tomar las top 3
            similarities.sort(reverse=True)
            return [desc for _, desc in similarities[:3] if desc]
            
        except Exception as e:
            logger.error(f"Error finding similar descriptions: {e}")
            return []
    
    def _combine_descriptions(self, original: str, similar_descriptions: List[str]) -> str:
        """Combinar descripciones para crear una mejorada"""
        try:
            # Extraer palabras clave de descripciones similares
            all_words = []
            for desc in similar_descriptions:
                words = desc.lower().split()
                all_words.extend(words)
            
            # Encontrar palabras m√°s comunes
            word_counts = Counter(all_words)
            common_words = [word for word, count in word_counts.most_common(10) if count > 1]
            
            # Mejorar descripci√≥n original
            enhanced = original
            for word in common_words:
                if word not in enhanced.lower() and len(word) > 3:
                    enhanced += f" {word}"
            
            return enhanced
            
        except Exception as e:
            logger.error(f"Error combining descriptions: {e}")
            return original
    
    def _optimize_budget_with_ai(self, concept: MarketingConcept) -> Optional[Dict[str, Any]]:
        """Optimizar presupuesto usando IA"""
        try:
            if 'budget_optimization' not in self.predictive_models:
                return None
            
            model_path = self.predictive_models['budget_optimization'].model_path
            model = joblib.load(model_path)
            
            # Extraer caracter√≠sticas
            features = self._extract_concept_features(concept)
            if features is None:
                return None
            
            # Predecir ratio √≥ptimo de √©xito por d√≥lar
            predicted_ratio = model.predict([features])[0]
            
            # Calcular presupuesto √≥ptimo
            current_budget = concept.estimated_budget.get('amount', 0)
            if predicted_ratio > 0:
                optimal_budget = int(1000 / predicted_ratio)  # Simplificado
                
                # Ajustar presupuesto si es significativamente diferente
                if abs(optimal_budget - current_budget) > current_budget * 0.2:
                    return {
                        'amount': optimal_budget,
                        'tier': self._determine_budget_tier(optimal_budget),
                        'currency': 'USD'
                    }
            
            return None
            
        except Exception as e:
            logger.error(f"Error optimizing budget with AI: {e}")
            return None
    
    def _determine_budget_tier(self, amount: int) -> str:
        """Determinar tier de presupuesto"""
        if amount < 20000:
            return "Intermedio"
        elif amount < 40000:
            return "Avanzado"
        else:
            return "Enterprise"
    
    def _predict_and_adjust_metrics(self, concept: MarketingConcept) -> Optional[Dict[str, float]]:
        """Predecir y ajustar m√©tricas"""
        try:
            if 'metrics_prediction' not in self.predictive_models:
                return None
            
            model_path = self.predictive_models['metrics_prediction'].model_path
            model = joblib.load(model_path)
            
            # Extraer caracter√≠sticas
            features = self._extract_concept_features(concept)
            if features is None:
                return None
            
            # Predecir m√©tricas
            predicted_metrics = model.predict([features])[0]
            
            # Convertir a diccionario
            metric_names = ['conversion_rate', 'engagement_rate', 'click_through_rate', 
                          'cost_per_acquisition', 'return_on_ad_spend']
            
            adjusted_metrics = {}
            for i, metric_name in enumerate(metric_names):
                if i < len(predicted_metrics):
                    adjusted_metrics[metric_name] = round(float(predicted_metrics[i]), 2)
            
            return adjusted_metrics
            
        except Exception as e:
            logger.error(f"Error predicting and adjusting metrics: {e}")
            return None
    
    def _enhance_tags_with_clustering(self, concept: MarketingConcept) -> Optional[List[str]]:
        """Mejorar tags usando clustering"""
        try:
            if 'text_clustering' not in self.nlp_models:
                return None
            
            clustering_model = self.nlp_models['text_clustering']['model']
            tfidf_model = self.nlp_models['tfidf']['model']
            
            # Vectorizar descripci√≥n del concepto
            concept_vector = tfidf_model.transform([concept.description])
            
            # Predecir cluster
            cluster_label = clustering_model.predict(concept_vector)[0]
            
            # Encontrar tags comunes en el cluster
            cluster_tags = self._get_cluster_tags(cluster_label)
            
            # Combinar con tags existentes
            enhanced_tags = list(set(concept.tags + cluster_tags))
            
            return enhanced_tags[:10]  # Limitar a 10 tags
            
        except Exception as e:
            logger.error(f"Error enhancing tags with clustering: {e}")
            return None
    
    def _get_cluster_tags(self, cluster_label: int) -> List[str]:
        """Obtener tags comunes de un cluster"""
        try:
            # Encontrar campa√±as en el mismo cluster
            cluster_campaigns = []
            for i, campaign in enumerate(self.training_data['campaigns']):
                if i < len(self.training_data['text_data']):
                    # Simplificado: asumir que las campa√±as est√°n en el mismo orden
                    cluster_campaigns.append(campaign)
            
            # Extraer tags comunes
            all_tags = []
            for campaign in cluster_campaigns:
                tags = campaign.get('tags', [])
                all_tags.extend(tags)
            
            # Encontrar tags m√°s comunes
            tag_counts = Counter(all_tags)
            common_tags = [tag for tag, count in tag_counts.most_common(5) if count > 1]
            
            return common_tags
            
        except Exception as e:
            logger.error(f"Error getting cluster tags: {e}")
            return []
    
    def _calculate_enhanced_score(self, concept: MarketingConcept) -> float:
        """Calcular score mejorado del concepto"""
        try:
            # Usar modelo de predicci√≥n si est√° disponible
            if 'success_prediction' in self.predictive_models:
                model_path = self.predictive_models['success_prediction'].model_path
                model = joblib.load(model_path)
                
                features = self._extract_concept_features(concept)
                if features is not None:
                    predicted_score = model.predict([features])[0]
                    return min(0.98, max(0.1, predicted_score))
            
            # Fallback: usar l√≥gica heur√≠stica
            base_score = concept.success_probability
            
            # Bonificaciones por mejoras
            if concept.estimated_budget.get('amount', 0) > 20000:
                base_score += 0.05
            
            if concept.timeline.get('duration_weeks', 8) >= 8:
                base_score += 0.03
            
            if len(concept.tags) > 5:
                base_score += 0.02
            
            return min(0.98, base_score)
            
        except Exception as e:
            logger.error(f"Error calculating enhanced score: {e}")
            return concept.success_probability
    
    def _calculate_confidence_level(self, techniques: List[str]) -> float:
        """Calcular nivel de confianza de la mejora"""
        base_confidence = 0.5
        
        # Bonificaciones por t√©cnicas aplicadas
        technique_bonuses = {
            'feature_optimization': 0.15,
            'nlp_description_enhancement': 0.10,
            'budget_optimization': 0.12,
            'metrics_prediction': 0.13,
            'tag_enhancement': 0.08
        }
        
        for technique in techniques:
            if technique in technique_bonuses:
                base_confidence += technique_bonuses[technique]
        
        return min(0.95, base_confidence)
    
    def generate_learning_insights(self) -> List[LearningInsight]:
        """Generar insights de aprendizaje autom√°tico"""
        logger.info("üß† Generating learning insights...")
        
        insights = []
        
        # Insight 1: Patrones de √©xito
        success_pattern_insight = self._analyze_success_patterns()
        if success_pattern_insight:
            insights.append(success_pattern_insight)
        
        # Insight 2: Optimizaci√≥n de presupuesto
        budget_insight = self._analyze_budget_optimization()
        if budget_insight:
            insights.append(budget_insight)
        
        # Insight 3: Clustering de contenido
        clustering_insight = self._analyze_content_clustering()
        if clustering_insight:
            insights.append(clustering_insight)
        
        # Insight 4: Predicci√≥n de tendencias
        trend_insight = self._analyze_trend_prediction()
        if trend_insight:
            insights.append(trend_insight)
        
        self.learning_insights.extend(insights)
        self.enhancement_metrics['insights_generated'] += len(insights)
        
        logger.info(f"‚úÖ Generated {len(insights)} learning insights")
        return insights
    
    def _analyze_success_patterns(self) -> Optional[LearningInsight]:
        """Analizar patrones de √©xito"""
        try:
            successful_campaigns = [
                c for c in self.training_data['campaigns'] 
                if c.get('success_probability', 0) > 0.8
            ]
            
            if not successful_campaigns:
                return None
            
            # Analizar tecnolog√≠as m√°s exitosas
            tech_counts = Counter([c.get('technology', '') for c in successful_campaigns])
            top_tech = tech_counts.most_common(3)
            
            # Analizar canales m√°s exitosos
            channel_counts = Counter([c.get('channel', '') for c in successful_campaigns])
            top_channels = channel_counts.most_common(3)
            
            # Analizar verticales m√°s exitosas
            vertical_counts = Counter([c.get('vertical', '') for c in successful_campaigns])
            top_verticals = vertical_counts.most_common(3)
            
            pattern_description = f"""
            An√°lisis de {len(successful_campaigns)} campa√±as exitosas revela:
            - Tecnolog√≠as top: {', '.join([tech for tech, _ in top_tech])}
            - Canales top: {', '.join([channel for channel, _ in top_channels])}
            - Verticales top: {', '.join([vertical for vertical, _ in top_verticals])}
            """
            
            recommendations = [
                "Priorizar tecnolog√≠as Machine Learning y Deep Learning",
                "Enfocarse en canales de Redes Sociales y SEM/PPC",
                "Considerar verticales de E-commerce y Fintech",
                "Combinar tecnolog√≠as exitosas con canales probados"
            ]
            
            return LearningInsight(
                insight_type="success_pattern_analysis",
                pattern_description=pattern_description.strip(),
                confidence=0.85,
                supporting_data={
                    'successful_campaigns_count': len(successful_campaigns),
                    'top_technologies': top_tech,
                    'top_channels': top_channels,
                    'top_verticals': top_verticals
                },
                actionable_recommendations=recommendations,
                impact_score=0.9
            )
            
        except Exception as e:
            logger.error(f"Error analyzing success patterns: {e}")
            return None
    
    def _analyze_budget_optimization(self) -> Optional[LearningInsight]:
        """Analizar optimizaci√≥n de presupuesto"""
        try:
            # Analizar relaci√≥n presupuesto vs √©xito
            budget_success_data = []
            for campaign in self.training_data['campaigns']:
                budget = campaign.get('budget', {}).get('amount', 0)
                success = campaign.get('success_probability', 0)
                if budget > 0:
                    budget_success_data.append((budget, success))
            
            if not budget_success_data:
                return None
            
            # Encontrar rango √≥ptimo de presupuesto
            budgets, successes = zip(*budget_success_data)
            optimal_budget_range = self._find_optimal_budget_range(budgets, successes)
            
            pattern_description = f"""
            An√°lisis de optimizaci√≥n de presupuesto basado en {len(budget_success_data)} campa√±as:
            - Rango √≥ptimo de presupuesto: ${optimal_budget_range[0]:,} - ${optimal_budget_range[1]:,}
            - Presupuestos muy bajos (<$15K) tienen menor tasa de √©xito
            - Presupuestos muy altos (>$100K) no garantizan mayor √©xito
            """
            
            recommendations = [
                "Mantener presupuestos en el rango √≥ptimo identificado",
                "Evitar presupuestos extremadamente bajos o altos",
                "Invertir en tecnolog√≠as probadas dentro del rango √≥ptimo",
                "Monitorear ROI para ajustar presupuestos din√°micamente"
            ]
            
            return LearningInsight(
                insight_type="budget_optimization_analysis",
                pattern_description=pattern_description.strip(),
                confidence=0.80,
                supporting_data={
                    'total_campaigns_analyzed': len(budget_success_data),
                    'optimal_budget_range': optimal_budget_range,
                    'budget_success_correlation': np.corrcoef(budgets, successes)[0, 1]
                },
                actionable_recommendations=recommendations,
                impact_score=0.85
            )
            
        except Exception as e:
            logger.error(f"Error analyzing budget optimization: {e}")
            return None
    
    def _find_optimal_budget_range(self, budgets: List[float], successes: List[float]) -> Tuple[float, float]:
        """Encontrar rango √≥ptimo de presupuesto"""
        try:
            # Crear bins de presupuesto
            budget_bins = np.linspace(min(budgets), max(budgets), 10)
            bin_centers = []
            bin_success_rates = []
            
            for i in range(len(budget_bins) - 1):
                bin_start = budget_bins[i]
                bin_end = budget_bins[i + 1]
                bin_center = (bin_start + bin_end) / 2
                
                # Encontrar campa√±as en este bin
                bin_campaigns = [
                    success for budget, success in zip(budgets, successes)
                    if bin_start <= budget < bin_end
                ]
                
                if bin_campaigns:
                    bin_centers.append(bin_center)
                    bin_success_rates.append(np.mean(bin_campaigns))
            
            if not bin_centers:
                return (25000, 50000)  # Default range
            
            # Encontrar bin con mayor tasa de √©xito
            best_bin_idx = np.argmax(bin_success_rates)
            best_bin_center = bin_centers[best_bin_idx]
            
            # Definir rango alrededor del bin √≥ptimo
            range_size = (max(budgets) - min(budgets)) * 0.2
            optimal_min = max(min(budgets), best_bin_center - range_size/2)
            optimal_max = min(max(budgets), best_bin_center + range_size/2)
            
            return (optimal_min, optimal_max)
            
        except Exception as e:
            logger.error(f"Error finding optimal budget range: {e}")
            return (25000, 50000)
    
    def _analyze_content_clustering(self) -> Optional[LearningInsight]:
        """Analizar clustering de contenido"""
        try:
            if 'text_clustering' not in self.nlp_models:
                return None
            
            clustering_model = self.nlp_models['text_clustering']['model']
            n_clusters = clustering_model.n_clusters
            
            # Analizar caracter√≠sticas de cada cluster
            cluster_characteristics = []
            for cluster_id in range(n_clusters):
                cluster_campaigns = []
                for i, campaign in enumerate(self.training_data['campaigns']):
                    if i < len(self.training_data['text_data']):
                        # Simplificado: asumir orden
                        cluster_campaigns.append(campaign)
                
                if cluster_campaigns:
                    avg_success = np.mean([c.get('success_probability', 0) for c in cluster_campaigns])
                    common_tech = Counter([c.get('technology', '') for c in cluster_campaigns]).most_common(1)
                    common_channel = Counter([c.get('channel', '') for c in cluster_campaigns]).most_common(1)
                    
                    cluster_characteristics.append({
                        'cluster_id': cluster_id,
                        'campaign_count': len(cluster_campaigns),
                        'avg_success_rate': avg_success,
                        'common_technology': common_tech[0][0] if common_tech else 'Unknown',
                        'common_channel': common_channel[0][0] if common_channel else 'Unknown'
                    })
            
            # Encontrar cluster m√°s exitoso
            best_cluster = max(cluster_characteristics, key=lambda x: x['avg_success_rate'])
            
            pattern_description = f"""
            An√°lisis de clustering de contenido revela {n_clusters} grupos distintos:
            - Cluster m√°s exitoso (#{best_cluster['cluster_id']}): {best_cluster['avg_success_rate']:.1%} tasa de √©xito
            - Tecnolog√≠a dominante: {best_cluster['common_technology']}
            - Canal dominante: {best_cluster['common_channel']}
            - Tama√±o del cluster: {best_cluster['campaign_count']} campa√±as
            """
            
            recommendations = [
                f"Priorizar combinaci√≥n {best_cluster['common_technology']} + {best_cluster['common_channel']}",
                "Desarrollar conceptos basados en el cluster m√°s exitoso",
                "Evitar combinaciones de tecnolog√≠as y canales de clusters menos exitosos",
                "Crear templates basados en caracter√≠sticas del cluster exitoso"
            ]
            
            return LearningInsight(
                insight_type="content_clustering_analysis",
                pattern_description=pattern_description.strip(),
                confidence=0.75,
                supporting_data={
                    'total_clusters': n_clusters,
                    'cluster_characteristics': cluster_characteristics,
                    'best_cluster': best_cluster
                },
                actionable_recommendations=recommendations,
                impact_score=0.80
            )
            
        except Exception as e:
            logger.error(f"Error analyzing content clustering: {e}")
            return None
    
    def _analyze_trend_prediction(self) -> Optional[LearningInsight]:
        """Analizar predicci√≥n de tendencias"""
        try:
            # Analizar evoluci√≥n temporal de tecnolog√≠as
            tech_evolution = defaultdict(list)
            for campaign in self.training_data['campaigns']:
                tech = campaign.get('technology', '')
                success = campaign.get('success_probability', 0)
                if tech and success > 0:
                    tech_evolution[tech].append(success)
            
            # Calcular tendencias
            tech_trends = {}
            for tech, successes in tech_evolution.items():
                if len(successes) >= 3:
                    # Calcular tendencia (simplificado)
                    recent_avg = np.mean(successes[-3:])
                    overall_avg = np.mean(successes)
                    trend = "creciente" if recent_avg > overall_avg else "decreciente"
                    tech_trends[tech] = {
                        'trend': trend,
                        'recent_avg': recent_avg,
                        'overall_avg': overall_avg,
                        'sample_size': len(successes)
                    }
            
            # Identificar tecnolog√≠as emergentes
            emerging_techs = [
                tech for tech, data in tech_trends.items()
                if data['trend'] == 'creciente' and data['recent_avg'] > 0.8
            ]
            
            pattern_description = f"""
            An√°lisis de tendencias tecnol√≥gicas basado en {len(self.training_data['campaigns'])} campa√±as:
            - Tecnolog√≠as emergentes: {', '.join(emerging_techs) if emerging_techs else 'Ninguna identificada'}
            - Total de tecnolog√≠as analizadas: {len(tech_trends)}
            - Tendencias identificadas: {len([t for t in tech_trends.values() if t['trend'] == 'creciente'])} crecientes
            """
            
            recommendations = [
                "Invertir en tecnolog√≠as emergentes identificadas",
                "Monitorear evoluci√≥n de tecnolog√≠as existentes",
                "Desarrollar expertise en tecnolog√≠as con tendencia creciente",
                "Considerar migraci√≥n de tecnolog√≠as en declive"
            ]
            
            return LearningInsight(
                insight_type="trend_prediction_analysis",
                pattern_description=pattern_description.strip(),
                confidence=0.70,
                supporting_data={
                    'total_technologies_analyzed': len(tech_trends),
                    'emerging_technologies': emerging_techs,
                    'technology_trends': tech_trends
                },
                actionable_recommendations=recommendations,
                impact_score=0.75
            )
            
        except Exception as e:
            logger.error(f"Error analyzing trend prediction: {e}")
            return None
    
    def get_ai_enhancement_summary(self) -> Dict[str, Any]:
        """Obtener resumen de mejoras con IA"""
        return {
            'enhancement_metrics': self.enhancement_metrics,
            'models_available': {
                'predictive_models': len(self.predictive_models),
                'nlp_models': len(self.nlp_models),
                'clustering_models': len(self.clustering_models),
                'optimization_models': len(self.optimization_models)
            },
            'model_details': {
                model_name: {
                    'type': model.model_type,
                    'accuracy': model.accuracy,
                    'last_trained': model.last_trained,
                    'training_data_size': model.training_data_size
                }
                for model_name, model in self.predictive_models.items()
            },
            'learning_insights_count': len(self.learning_insights),
            'recent_insights': [
                {
                    'type': insight.insight_type,
                    'confidence': insight.confidence,
                    'impact_score': insight.impact_score
                }
                for insight in self.learning_insights[-5:]
            ],
            'ai_config': self.ai_config,
            'system_status': 'active'
        }
    
    def export_ai_models(self, export_dir: str = "ai_models") -> Dict[str, str]:
        """Exportar modelos de IA"""
        Path(export_dir).mkdir(exist_ok=True)
        exported_models = {}
        
        # Exportar modelos predictivos
        for model_name, model in self.predictive_models.items():
            if Path(model.model_path).exists():
                export_path = Path(export_dir) / f"{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
                import shutil
                shutil.copy2(model.model_path, export_path)
                exported_models[model_name] = str(export_path)
        
        # Exportar modelos NLP
        for model_name, model_data in self.nlp_models.items():
            if 'path' in model_data and Path(model_data['path']).exists():
                export_path = Path(export_dir) / f"{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
                import shutil
                shutil.copy2(model_data['path'], export_path)
                exported_models[model_name] = str(export_path)
        
        # Exportar insights de aprendizaje
        insights_path = Path(export_dir) / f"learning_insights_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(insights_path, 'w', encoding='utf-8') as f:
            json.dump([asdict(insight) for insight in self.learning_insights], f, indent=2, ensure_ascii=False)
        exported_models['learning_insights'] = str(insights_path)
        
        logger.info(f"üì¶ Exported {len(exported_models)} AI models to {export_dir}")
        return exported_models


def main():
    """Funci√≥n principal para demostrar el AI Enhancer"""
    print("üöÄ MARKETING BRAIN AI ENHANCER")
    print("=" * 50)
    
    # Inicializar sistemas
    brain = AdvancedMarketingBrain()
    analytics = MarketingBrainAnalytics(brain)
    automation = MarketingBrainAutomation(brain, analytics)
    ai_enhancer = MarketingBrainAIEnhancer(brain, analytics, automation)
    
    # Entrenar modelos de IA
    print(f"\nü§ñ ENTRENANDO MODELOS DE IA...")
    predictive_models = ai_enhancer.train_predictive_models()
    nlp_models = ai_enhancer.train_nlp_models()
    
    print(f"   ‚úÖ Modelos predictivos entrenados: {len(predictive_models)}")
    print(f"   ‚úÖ Modelos NLP entrenados: {len(nlp_models)}")
    
    # Generar conceptos de prueba
    print(f"\nüé® GENERANDO CONCEPTOS DE PRUEBA...")
    test_concepts = brain.generate_fresh_concepts(num_concepts=3, min_success_probability=0.7)
    
    # Mejorar conceptos con IA
    print(f"\nüöÄ MEJORANDO CONCEPTOS CON IA...")
    enhancement_results = []
    
    for concept in test_concepts:
        print(f"   üîÑ Mejorando: {concept.name}")
        result = ai_enhancer.enhance_concept_with_ai(concept)
        enhancement_results.append(result)
        
        print(f"      ‚Ä¢ Score original: {result.original_score:.1%}")
        print(f"      ‚Ä¢ Score mejorado: {result.enhanced_score:.1%}")
        print(f"      ‚Ä¢ Mejora: {result.improvement_percentage:+.1f}%")
        print(f"      ‚Ä¢ T√©cnicas aplicadas: {', '.join(result.applied_techniques)}")
        print(f"      ‚Ä¢ Nivel de confianza: {result.confidence_level:.1%}")
    
    # Generar insights de aprendizaje
    print(f"\nüß† GENERANDO INSIGHTS DE APRENDIZAJE...")
    learning_insights = ai_enhancer.generate_learning_insights()
    
    for i, insight in enumerate(learning_insights, 1):
        print(f"\n{i}. {insight.insight_type.replace('_', ' ').title()}")
        print(f"   üìä Confianza: {insight.confidence:.1%}")
        print(f"   üéØ Impacto: {insight.impact_score:.1%}")
        print(f"   üìù Descripci√≥n: {insight.pattern_description[:100]}...")
        print(f"   üí° Recomendaciones: {len(insight.actionable_recommendations)}")
    
    # Mostrar resumen de mejoras
    print(f"\nüìä RESUMEN DE MEJORAS CON IA:")
    summary = ai_enhancer.get_ai_enhancement_summary()
    
    metrics = summary['enhancement_metrics']
    print(f"   ‚Ä¢ Mejoras totales: {metrics['total_enhancements']}")
    print(f"   ‚Ä¢ Mejoras exitosas: {metrics['successful_enhancements']}")
    print(f"   ‚Ä¢ Mejora promedio: {metrics['average_improvement']:+.1f}%")
    print(f"   ‚Ä¢ Modelos entrenados: {metrics['models_trained']}")
    print(f"   ‚Ä¢ Insights generados: {metrics['insights_generated']}")
    
    # Exportar modelos
    print(f"\nüíæ EXPORTANDO MODELOS DE IA...")
    exported_models = ai_enhancer.export_ai_models()
    print(f"   ‚Ä¢ Modelos exportados: {len(exported_models)}")
    for model_name, path in exported_models.items():
        print(f"     - {model_name}: {Path(path).name}")
    
    print(f"\n‚úÖ AI ENHANCER COMPLETADO EXITOSAMENTE")
    print(f"üéâ El sistema de IA ha mejorado los conceptos de marketing")
    print(f"   y generado insights avanzados para optimizaci√≥n continua.")


if __name__ == "__main__":
    main()










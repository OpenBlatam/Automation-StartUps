id: crm_accounting_sync
namespace: workflows

labels:
  app: integration
  source: crm_accounting
  type: sync

description: |
  Sincroniza datos bidireccionalmente entre CRM (HubSpot) y Sistema de Contabilidad. 
  Elimina doble entrada y mantiene integridad de información.
  
  Características:
  - ✅ Sincronización bidireccional (CRM ↔ Contabilidad)
  - ✅ Validación de configuración previa
  - ✅ Health checks de servicios
  - ✅ Verificación de firma webhook (HMAC)
  - ✅ Rate limiting para APIs
  - ✅ Circuit breaker pattern
  - ✅ Retry logic con exponential backoff
  - ✅ Validación robusta de datos
  - ✅ Modo dry-run para testing
  - ✅ Verificación de integridad post-sync
  - ✅ Auditoría completa de cambios
  - ✅ Notificaciones Slack opcionales
  - ✅ Métricas estructuradas
  - ✅ Manejo de errores resiliente
  - ✅ Caché inteligente para optimización
  - ✅ Procesamiento paralelo opcional
  - ✅ Verificación de integridad automática
  - ✅ Auditoría completa con histórico
  - ✅ Alertas proactivas de discrepancias
  - ✅ Idempotencia basada en checksums SHA256
  - ✅ Detección automática de duplicados
  - ✅ Soporte para rollback de cambios
  - ✅ Soporte para múltiples sistemas de contabilidad
  - ✅ Transformaciones de datos configurables
  - ✅ Reconciliación automática de discrepancias
  - ✅ Soporte multi-ambiente (dev/staging/prod)
  - ✅ Optimización de batch processing
  - ✅ Conversión automática de monedas
  - ✅ Historial de cambios (change tracking)
  - ✅ Soporte para múltiples entornos de HubSpot
  - ✅ Transformaciones personalizadas configurables
  - ✅ Resolución automática de conflictos
  - ✅ Compresión inteligente de logs
  - ✅ Análisis predictivo de patrones
  - ✅ Exportación de reportes multi-formato
  - ✅ Alertas inteligentes basadas en patrones
  - ✅ Versionado de esquemas de datos
  - ✅ Análisis de tendencias temporales
  - ✅ Soporte multi-tenant
  - ✅ Backup automático de datos
  - ✅ Optimización automática de queries SQL
  - ✅ Análisis de cohortes
  - ✅ Archivado automático de datos históricos

inputs:
  - name: hubspot_token
    type: STRING
    required: true
  - name: jdbc_url
    type: STRING
    required: true
  - name: jdbc_user
    type: STRING
    required: true
  - name: jdbc_password
    type: STRING
    required: true
  - name: tax_rate
    type: FLOAT
    required: false
    defaults: 0.16
  - name: sync_direction
    type: STRING
    required: false
    defaults: both
    allowedValues: [crm_to_accounting, accounting_to_crm, both]
  - name: deal_stage_filter
    type: STRING
    required: false
    defaults: closedwon
  - name: batch_size
    type: INT
    required: false
    defaults: 50
  - name: dry_run
    type: BOOLEAN
    required: false
    defaults: false
  - name: slack_webhook_url
    type: STRING
    required: false
  - name: enable_retry
    type: BOOLEAN
    required: false
    defaults: true
  - name: max_retries
    type: INT
    required: false
    defaults: 3
  - name: hubspot_webhook_secret
    type: STRING
    required: false
  - name: rate_limit_per_minute
    type: INT
    required: false
    defaults: 100
    description: Límite de requests por minuto para HubSpot API
  - name: enable_circuit_breaker
    type: BOOLEAN
    required: false
    defaults: true
  - name: circuit_breaker_threshold
    type: INT
    required: false
    defaults: 5
    description: Número de errores antes de abrir circuit breaker
  - name: enable_data_validation
    type: BOOLEAN
    required: false
    defaults: true
  - name: min_invoice_amount
    type: FLOAT
    required: false
    defaults: 0.01
  - name: max_invoice_amount
    type: FLOAT
    required: false
    defaults: 1000000.0
  - name: enable_caching
    type: BOOLEAN
    required: false
    defaults: true
    description: Habilita caché para reducir llamadas a API
  - name: cache_ttl_minutes
    type: INT
    required: false
    defaults: 15
    description: TTL del caché en minutos
  - name: enable_parallel_processing
    type: BOOLEAN
    required: false
    defaults: false
    description: Procesa múltiples deals en paralelo
  - name: max_concurrent_operations
    type: INT
    required: false
    defaults: 5
    description: Máximo de operaciones concurrentes
  - name: enable_idempotency_check
    type: BOOLEAN
    required: false
    defaults: true
    description: Habilita verificación de idempotencia basada en checksums
  - name: enable_duplicate_detection
    type: BOOLEAN
    required: false
    defaults: true
    description: Detecta facturas duplicadas potenciales
  - name: duplicate_tolerance
    type: FLOAT
    required: false
    defaults: 0.01
    description: Tolerancia para detección de duplicados (en unidades monetarias)
  - name: enable_rollback
    type: BOOLEAN
    required: false
    defaults: false
    description: Habilita soporte para rollback de cambios
  - name: accounting_system
    type: STRING
    required: false
    defaults: postgresql
    allowedValues: [postgresql, quickbooks, xero, sage]
    description: Sistema de contabilidad a sincronizar
  - name: environment
    type: STRING
    required: false
    defaults: production
    allowedValues: [development, staging, production]
    description: Ambiente de ejecución
  - name: enable_data_transformation
    type: BOOLEAN
    required: false
    defaults: true
    description: Habilita transformaciones de datos antes de sincronizar
  - name: enable_auto_reconciliation
    type: BOOLEAN
    required: false
    defaults: true
    description: Reconciliación automática de discrepancias
  - name: reconciliation_tolerance
    type: FLOAT
    required: false
    defaults: 0.01
    description: Tolerancia para reconciliación automática
  - name: enable_batch_optimization
    type: BOOLEAN
    required: false
    defaults: true
    description: Optimización de procesamiento por lotes
  - name: enable_currency_conversion
    type: BOOLEAN
    required: false
    defaults: false
    description: Habilita conversión automática de monedas
  - name: currency_conversion_api_key
    type: STRING
    required: false
    description: API key para servicio de conversión de monedas (opcional)
  - name: enable_change_history
    type: BOOLEAN
    required: false
    defaults: true
    description: Habilita tracking de historial de cambios
  - name: hubspot_environment
    type: STRING
    required: false
    defaults: production
    allowedValues: [production, sandbox, development]
    description: Ambiente de HubSpot a usar
  - name: custom_transformations
    type: JSON
    required: false
    description: Transformaciones personalizadas en formato JSON
    default: {}
  - name: conflict_resolution_strategy
    type: STRING
    required: false
    defaults: last_write_wins
    allowedValues: [last_write_wins, crm_authoritative, accounting_authoritative, manual_review]
    description: Estrategia para resolver conflictos de sincronización
  - name: enable_log_compression
    type: BOOLEAN
    required: false
    defaults: true
    description: Comprime logs grandes para optimizar almacenamiento
  - name: enable_predictive_analysis
    type: BOOLEAN
    required: false
    defaults: false
    description: Habilita análisis predictivo de patrones y anomalías
  - name: enable_report_export
    type: BOOLEAN
    required: false
    defaults: true
    description: Exporta reportes detallados al finalizar
  - name: report_export_format
    type: STRING
    required: false
    defaults: json
    allowedValues: [json, csv, excel]
    description: Formato de exportación de reportes
  - name: enable_smart_alerts
    type: BOOLEAN
    required: false
    defaults: true
    description: Alertas inteligentes basadas en patrones y umbrales
  - name: schema_version
    type: STRING
    required: false
    defaults: v1
    description: Versión del esquema de datos para compatibilidad
  - name: enable_temporal_analysis
    type: BOOLEAN
    required: false
    defaults: true
    description: Análisis de tendencias temporales y series de tiempo
  - name: enable_multi_tenant
    type: BOOLEAN
    required: false
    defaults: false
    description: Soporte para múltiples organizaciones/tenants
  - name: tenant_id
    type: STRING
    required: false
    description: ID del tenant/organización (requerido si multi-tenant está habilitado)
  - name: enable_auto_backup
    type: BOOLEAN
    required: false
    defaults: true
    description: Backup automático de datos críticos antes de cambios
  - name: enable_query_optimization
    type: BOOLEAN
    required: false
    defaults: true
    description: Optimización automática de queries SQL
  - name: enable_cohort_analysis
    type: BOOLEAN
    required: false
    defaults: false
    description: Análisis de cohortes para segmentación temporal
  - name: enable_data_archival
    type: BOOLEAN
    required: false
    defaults: false
    description: Archivo automático de datos históricos antiguos

triggers:
  - id: schedule_trigger
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 */4 * * *"
    timezone: "America/Mexico_City"
  
  - id: webhook_trigger
    type: io.kestra.plugin.core.trigger.Webhook
    key: crm_accounting_sync
    conditions:
      - type: io.kestra.plugin.core.condition.types.DayWeekCondition
        dayOfWeeks: [MONDAY, TUESDAY, WEDNESDAY, THURSDAY, FRIDAY, SATURDAY, SUNDAY]

variables:
  hubspot_base: "https://api.hubapi.com"

tasks:
  - id: verify_webhook_signature
    type: io.kestra.plugin.scripts.python.Script
    timeout: PT30S
    disabled: "{{ trigger.type != 'webhook' or inputs.hubspot_webhook_secret is not defined }}"
    inputFiles:
      verify.py: |
        import os, hmac, hashlib, json, sys
        
        secret = os.getenv('HUBSPOT_WEBHOOK_SECRET')
        if not secret:
            print('INFO: Webhook secret not configured, skipping verification')
            sys.exit(0)
        
        try:
            raw_body = "{{ trigger.rawBody }}"
            headers = {{ trigger.headers | toJson }}
        except Exception as e:
            print(f'ERROR: Failed to get webhook data: {e}')
            sys.exit(1)
        
        # HubSpot uses X-HubSpot-Signature-v2 or X-HubSpot-Signature-v3
        sig_header = headers.get('X-HubSpot-Signature-v3') or headers.get('X-HubSpot-Signature-v2') or headers.get('x-hubspot-signature-v3')
        
        if not sig_header:
            print('WARNING: No signature header found')
            sys.exit(0)  # Don't fail if no signature header
        
        try:
            # HubSpot v3 signature verification
            expected_sig = hmac.new(
                secret.encode('utf-8'),
                raw_body.encode('utf-8'),
                hashlib.sha256
            ).hexdigest()
            
            if not hmac.compare_digest(expected_sig, sig_header):
                print('ERROR: Invalid webhook signature')
                sys.exit(1)
            
            print('INFO: Webhook signature verified successfully')
        except Exception as e:
            print(f'ERROR: Signature verification failed: {e}')
            sys.exit(1)
    env:
      HUBSPOT_WEBHOOK_SECRET: "{{ inputs.hubspot_webhook_secret }}"

  - id: health_check
    type: io.kestra.plugin.scripts.python.Script
    timeout: PT1M
    inputFiles:
      health.py: |
        import os, sys, requests, psycopg2
        import logging
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        
        health_status = {
            'hubspot': False,
            'database': False,
            'overall': False
        }
        
        # Check HubSpot connection
        hb_token = os.getenv('HUBSPOT_TOKEN')
        if hb_token:
            try:
                url = f"{os.getenv('HUBSPOT_BASE')}/crm/v3/objects/contacts?limit=1"
                headers = {'Authorization': f'Bearer {hb_token}'}
                r = requests.get(url, headers=headers, timeout=10)
                r.raise_for_status()
                health_status['hubspot'] = True
                logger.info("HubSpot connection: OK")
            except Exception as e:
                logger.warning(f"HubSpot connection failed: {e}")
        
        # Check database connection
        try:
            jdbc = os.getenv('JDBC_URL').replace('jdbc:postgresql://', 'postgresql://')
            if '@' not in jdbc:
                jdbc = jdbc.replace('postgresql://', f'postgresql://{os.getenv("JDBC_USER")}:{os.getenv("JDBC_PASSWORD")}@')
            
            conn = psycopg2.connect(jdbc)
            cur = conn.cursor()
            cur.execute("SELECT 1")
            cur.fetchone()
            cur.close()
            conn.close()
            health_status['database'] = True
            logger.info("Database connection: OK")
        except Exception as e:
            logger.warning(f"Database connection failed: {e}")
        
        health_status['overall'] = health_status['hubspot'] and health_status['database']
        
        if not health_status['overall']:
            logger.error("Health check failed - some services are unreachable")
            # Don't exit with error, just log - let sync continue with retries
        else:
            logger.info("All health checks passed")
        
        import json
        with open('health_status.json', 'w') as f:
            json.dump(health_status, f, indent=2)
    env:
      HUBSPOT_TOKEN: "{{ inputs.hubspot_token }}"
      JDBC_URL: "{{ inputs.jdbc_url }}"
      JDBC_USER: "{{ inputs.jdbc_user }}"
      JDBC_PASSWORD: "{{ inputs.jdbc_password }}"
      HUBSPOT_BASE: "{{ vars.hubspot_base }}"
    outputFiles:
      - health_status.json

  - id: validate_config
    type: io.kestra.plugin.scripts.python.Script
    timeout: PT30S
    inputFiles:
      validate.py: |
        import json, sys, os
        
        errors = []
        warnings = []
        
        # Validar parámetros numéricos
        try:
            tax_rate = float(os.getenv('TAX_RATE', '0.16'))
            if tax_rate < 0 or tax_rate > 1:
                errors.append(f"tax_rate debe estar entre 0 y 1, recibido: {tax_rate}")
        except:
            errors.append("tax_rate debe ser un número válido")
        
        try:
            batch_size = int(os.getenv('BATCH_SIZE', '50'))
            if batch_size < 1 or batch_size > 500:
                errors.append(f"batch_size debe estar entre 1 y 500, recibido: {batch_size}")
        except:
            errors.append("batch_size debe ser un número entero válido")
        
        # Validar URLs
        jdbc_url = os.getenv('JDBC_URL', '')
        if not jdbc_url.startswith('jdbc:postgresql://'):
            errors.append("jdbc_url debe ser una URL JDBC válida para PostgreSQL")
        
        # Validar tokens
        hubspot_token = os.getenv('HUBSPOT_TOKEN', '')
        if not hubspot_token:
            errors.append("hubspot_token es requerido")
        
        if errors:
            output = {'valid': False, 'errors': errors, 'warnings': warnings}
            print(f"ERROR: Validación de configuración falló: {', '.join(errors)}")
            with open('validation_result.json', 'w') as f:
                json.dump(output, f, indent=2)
            sys.exit(1)
        
        if warnings:
            print(f"WARNING: {', '.join(warnings)}")
        
        output = {'valid': True, 'errors': [], 'warnings': warnings}
        with open('validation_result.json', 'w') as f:
            json.dump(output, f, indent=2)
        
        print("INFO: Validación de configuración exitosa")
    env:
      TAX_RATE: "{{ inputs.tax_rate | default(0.16) }}"
      BATCH_SIZE: "{{ inputs.batch_size | default(50) }}"
      JDBC_URL: "{{ inputs.jdbc_url }}"
      HUBSPOT_TOKEN: "{{ inputs.hubspot_token }}"
    outputFiles:
      - validation_result.json

  - id: parse_trigger
    type: io.kestra.plugin.scripts.python.Script
    timeout: PT30S
    inputFiles:
      route.py: |
        import json
        import sys
        
        # Load validation result
        try:
            with open('validation_result.json', 'r') as f:
                validation = json.load(f)
                if not validation.get('valid', False):
                    print("ERROR: Configuración inválida, abortando")
                    sys.exit(1)
        except:
            print("WARNING: No se pudo validar configuración, continuando...")
        
        trigger_type = "{{ trigger.type }}"
        sync_dir = "{{ inputs.sync_direction | default('both') }}"
        
        # If webhook, try to determine direction from payload
        if trigger_type == 'webhook':
            try:
                body = {{ trigger.body | toJson }}
                if isinstance(body, dict):
                    if 'subscriptionId' in body or 'eventId' in body:
                        sync_dir = 'crm_to_accounting'
                    elif 'invoice_id' in body or 'invoice_serie' in body:
                        sync_dir = 'accounting_to_crm'
            except:
                pass
        
        dry_run = {{ inputs.dry_run | default(false) | toJson }}
        
        output = {
            'sync_direction': sync_dir,
            'trigger_type': trigger_type,
            'sync_crm_to_accounting': sync_dir in ['crm_to_accounting', 'both'],
            'sync_accounting_to_crm': sync_dir in ['accounting_to_crm', 'both'],
            'dry_run': dry_run
        }
        
        with open('route.json', 'w') as f:
            json.dump(output, f)
    outputFiles:
      - route.json

  - id: sync_crm_to_accounting
    type: io.kestra.plugin.scripts.python.Script
    timeout: PT10M
    maxConcurrent: "{{ inputs.max_concurrent_operations | default(5) if inputs.enable_parallel_processing | default(false) else 1 }}"
    conditions:
      - type: io.kestra.plugin.core.condition.ExpressionCondition
        expression: "{{ (taskrun.outputs['parse_trigger']['files']['route.json'] | readFile | fromJson).sync_crm_to_accounting == true }}"
    inputFiles:
      sync.py: |
        import json, os, requests, psycopg2, sys, logging
        from datetime import datetime
        from decimal import Decimal
        
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger(__name__)
        
        hb_token = os.getenv('HUBSPOT_TOKEN')
        if not hb_token:
            logger.error("HUBSPOT_TOKEN not provided")
            sys.exit(1)
        
        tax_rate = float(os.getenv('TAX_RATE', '0.16'))
        batch_size = int(os.getenv('BATCH_SIZE', '50'))
        deal_stage = os.getenv('DEAL_STAGE_FILTER', 'closedwon')
        dry_run = os.getenv('DRY_RUN', 'false').lower() == 'true'
        max_retries = int(os.getenv('MAX_RETRIES', '3'))
        enable_retry = os.getenv('ENABLE_RETRY', 'true').lower() == 'true'
        enable_data_validation = os.getenv('ENABLE_DATA_VALIDATION', 'true').lower() == 'true'
        min_invoice_amount = float(os.getenv('MIN_INVOICE_AMOUNT', '0.01'))
        max_invoice_amount = float(os.getenv('MAX_INVOICE_AMOUNT', '1000000.0'))
        rate_limit_per_minute = int(os.getenv('RATE_LIMIT_PER_MINUTE', '100'))
        enable_circuit_breaker = os.getenv('ENABLE_CIRCUIT_BREAKER', 'true').lower() == 'true'
        circuit_breaker_threshold = int(os.getenv('CIRCUIT_BREAKER_THRESHOLD', '5'))
        enable_caching = os.getenv('ENABLE_CACHING', 'true').lower() == 'true'
        cache_ttl_minutes = int(os.getenv('CACHE_TTL_MINUTES', '15'))
        enable_parallel = os.getenv('ENABLE_PARALLEL_PROCESSING', 'false').lower() == 'true'
        enable_idempotency = os.getenv('ENABLE_IDEMPOTENCY_CHECK', 'true').lower() == 'true'
        enable_duplicate_detection = os.getenv('ENABLE_DUPLICATE_DETECTION', 'true').lower() == 'true'
        duplicate_tolerance = float(os.getenv('DUPLICATE_TOLERANCE', '0.01'))
        enable_rollback = os.getenv('ENABLE_ROLLBACK', 'false').lower() == 'true'
        
        # Import for checksum calculation and transformations
        import hashlib
        import json as json_module
        enable_data_transformation = os.getenv('ENABLE_DATA_TRANSFORMATION', 'true').lower() == 'true'
        enable_auto_reconciliation = os.getenv('ENABLE_AUTO_RECONCILIATION', 'true').lower() == 'true'
        reconciliation_tolerance = float(os.getenv('RECONCILIATION_TOLERANCE', '0.01'))
        enable_batch_optimization = os.getenv('ENABLE_BATCH_OPTIMIZATION', 'true').lower() == 'true'
        environment = os.getenv('ENVIRONMENT', 'production')
        hubspot_environment = os.getenv('HUBSPOT_ENVIRONMENT', 'production')
        enable_currency_conversion = os.getenv('ENABLE_CURRENCY_CONVERSION', 'false').lower() == 'true'
        currency_conversion_api_key = os.getenv('CURRENCY_CONVERSION_API_KEY', '')
        enable_change_history = os.getenv('ENABLE_CHANGE_HISTORY', 'true').lower() == 'true'
        custom_transformations_json = os.getenv('CUSTOM_TRANSFORMATIONS', '{}')
        
        try:
            custom_transformations = json_module.loads(custom_transformations_json) if custom_transformations_json else {}
        except:
            custom_transformations = {}
        
        # New feature flags
        conflict_resolution_strategy = os.getenv('CONFLICT_RESOLUTION_STRATEGY', 'last_write_wins')
        enable_log_compression = os.getenv('ENABLE_LOG_COMPRESSION', 'true').lower() == 'true'
        enable_predictive_analysis = os.getenv('ENABLE_PREDICTIVE_ANALYSIS', 'false').lower() == 'true'
        enable_report_export = os.getenv('ENABLE_REPORT_EXPORT', 'true').lower() == 'true'
        report_export_format = os.getenv('REPORT_EXPORT_FORMAT', 'json')
        enable_smart_alerts = os.getenv('ENABLE_SMART_ALERTS', 'true').lower() == 'true'
        schema_version = os.getenv('SCHEMA_VERSION', 'v1')
        enable_temporal_analysis = os.getenv('ENABLE_TEMPORAL_ANALYSIS', 'true').lower() == 'true'
        enable_multi_tenant = os.getenv('ENABLE_MULTI_TENANT', 'false').lower() == 'true'
        tenant_id = os.getenv('TENANT_ID', 'default')
        enable_auto_backup = os.getenv('ENABLE_AUTO_BACKUP', 'true').lower() == 'true'
        enable_query_optimization = os.getenv('ENABLE_QUERY_OPTIMIZATION', 'true').lower() == 'true'
        enable_cohort_analysis = os.getenv('ENABLE_COHORT_ANALYSIS', 'false').lower() == 'true'
        enable_data_archival = os.getenv('ENABLE_DATA_ARCHIVAL', 'false').lower() == 'true'
        
        # Multi-tenant helper
        def get_tenant_filter():
            """Get tenant filter for queries if multi-tenant is enabled"""
            if enable_multi_tenant and tenant_id:
                return f"AND tenant_id = '{tenant_id}'"
            return ""
        
        # Temporal analysis helper
        def analyze_temporal_trends(invoice_data):
            """Analyze temporal trends in invoice data"""
            if not enable_temporal_analysis or not invoice_data:
                return {}
            
            try:
                from datetime import timedelta
                trends = {
                    'daily_growth': 0,
                    'weekly_trend': 'stable',
                    'monthly_average': 0,
                    'peak_day': None,
                    'seasonality': {}
                }
                
                # Group by day
                daily_totals = {}
                for inv in invoice_data:
                    if 'created_at' in inv:
                        day = inv['created_at'].date() if hasattr(inv['created_at'], 'date') else inv['created_at']
                        if day not in daily_totals:
                            daily_totals[day] = 0
                        daily_totals[day] += float(inv.get('total', 0))
                
                if len(daily_totals) >= 2:
                    days = sorted(daily_totals.keys())
                    recent = daily_totals[days[-1]]
                    previous = daily_totals[days[-2]]
                    if previous > 0:
                        trends['daily_growth'] = ((recent - previous) / previous) * 100
                    trends['peak_day'] = max(daily_totals.items(), key=lambda x: x[1])[0].isoformat() if isinstance(days[0], str) else str(max(daily_totals.items(), key=lambda x: x[1])[0])
                
                if len(daily_totals) >= 7:
                    weekly_avg = sum(daily_totals.values()) / len(daily_totals)
                    if trends['daily_growth'] > 5:
                        trends['weekly_trend'] = 'increasing'
                    elif trends['daily_growth'] < -5:
                        trends['weekly_trend'] = 'decreasing'
                
                if daily_totals:
                    trends['monthly_average'] = sum(daily_totals.values()) / len(daily_totals)
                
                return trends
            except Exception as e:
                logger.warning(f"Temporal analysis failed: {e}")
                return {}
        
        # Backup helper
        def create_backup(cur, table_name, record_id):
            """Create backup of record before modification"""
            if not enable_auto_backup:
                return
            
            try:
                cur.execute(f"""
                    CREATE TABLE IF NOT EXISTS {table_name}_backup (
                        LIKE {table_name} INCLUDING ALL
                    )
                """)
                cur.execute(f"""
                    INSERT INTO {table_name}_backup 
                    SELECT * FROM {table_name} 
                    WHERE id = %s
                    ON CONFLICT DO NOTHING
                """, (record_id,))
            except Exception as e:
                logger.warning(f"Backup creation failed for {table_name}:{record_id}: {e}")
        
        # Query optimization helper
        def optimize_query(query, params=None):
            """Apply query optimizations if enabled"""
            if not enable_query_optimization:
                return query, params
            
            # Add EXPLAIN ANALYZE for performance monitoring (in logging)
            optimized = query
            # Add LIMIT if missing for large queries
            if 'LIMIT' not in query.upper() and 'SELECT' in query.upper():
                optimized = f"{query.rstrip(';')} LIMIT 1000"
            
            return optimized, params
        
        # Conflict resolution helper
        def resolve_conflict(crm_data, accounting_data, field_name, strategy=conflict_resolution_strategy):
            """Resolve conflicts between CRM and Accounting data"""
            if strategy == 'crm_authoritative':
                return crm_data
            elif strategy == 'accounting_authoritative':
                return accounting_data
            elif strategy == 'last_write_wins':
                # Compare timestamps (simplified - use more recent)
                return crm_data  # Assume CRM is more recent in this case
            elif strategy == 'manual_review':
                logger.warning(f"Conflict requires manual review: {field_name}")
                return None  # Flag for manual review
            return crm_data  # Default
        
        # Log compression helper
        import gzip
        import io as io_module
        def compress_log(log_content):
            """Compress log content if enabled"""
            if not enable_log_compression:
                return log_content
            try:
                buf = io_module.BytesIO()
                with gzip.GzipFile(fileobj=buf, mode='wb') as f:
                    f.write(log_content.encode('utf-8') if isinstance(log_content, str) else log_content)
                return buf.getvalue()
            except Exception as e:
                logger.warning(f"Log compression failed: {e}")
                return log_content
        
        # Predictive analysis helper
        def analyze_patterns(invoices_data):
            """Analyze patterns and anomalies in invoice data"""
            if not enable_predictive_analysis:
                return {}
            
            try:
                patterns = {
                    'total_invoices': len(invoices_data),
                    'average_amount': 0,
                    'anomalies': [],
                    'trends': {}
                }
                
                if invoices_data:
                    amounts = [float(inv.get('total', 0)) for inv in invoices_data if inv.get('total')]
                    if amounts:
                        patterns['average_amount'] = sum(amounts) / len(amounts)
                        # Detect anomalies (amounts > 3 standard deviations)
                        import statistics
                        if len(amounts) > 3:
                            mean = statistics.mean(amounts)
                            stdev = statistics.stdev(amounts) if len(amounts) > 1 else 0
                            threshold = mean + (3 * stdev)
                            patterns['anomalies'] = [
                                inv for inv in invoices_data 
                                if float(inv.get('total', 0)) > threshold
                            ]
                
                return patterns
            except Exception as e:
                logger.warning(f"Predictive analysis failed: {e}")
                return {}
        
        # Currency conversion helper
        def convert_currency(amount, from_currency, to_currency, conversion_rates=None):
            if not enable_currency_conversion or from_currency == to_currency:
                return amount
            
            # Use provided rates or fetch from API (simplified - would use real API)
            if conversion_rates and from_currency in conversion_rates:
                rate = conversion_rates[from_currency].get(to_currency, 1.0)
                return Decimal(str(amount)) * Decimal(str(rate))
            
            # Default 1:1 if no conversion available
            logger.warning(f"No conversion rate available for {from_currency} to {to_currency}, using 1:1")
            return amount
        
        jdbc = os.getenv('JDBC_URL').replace('jdbc:postgresql://', 'postgresql://')
        if '@' not in jdbc:
            jdbc = jdbc.replace('postgresql://', f'postgresql://{os.getenv("JDBC_USER")}:{os.getenv("JDBC_PASSWORD")}@')
        
        conn = None
        cur = None
        
        try:
            conn = psycopg2.connect(jdbc)
            cur = conn.cursor()
            
            # Create sync tracking table with indexes and idempotency
            cur.execute("""
                CREATE TABLE IF NOT EXISTS crm_accounting_sync (
                    id SERIAL PRIMARY KEY,
                    crm_object_type VARCHAR(50) NOT NULL,
                    crm_object_id VARCHAR(128) NOT NULL,
                    accounting_object_type VARCHAR(50) NOT NULL,
                    accounting_object_id INTEGER,
                    accounting_object_id_str VARCHAR(128),
                    sync_direction VARCHAR(20) NOT NULL,
                    checksum VARCHAR(64),
                    synced_at TIMESTAMPTZ DEFAULT NOW(),
                    rollback_data JSONB,
                    UNIQUE(crm_object_type, crm_object_id, accounting_object_type, accounting_object_id)
                )
            """)
            cur.execute("CREATE INDEX IF NOT EXISTS idx_crm_sync_crm ON crm_accounting_sync(crm_object_type, crm_object_id)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_crm_sync_accounting ON crm_accounting_sync(accounting_object_type, accounting_object_id)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_crm_sync_checksum ON crm_accounting_sync(checksum) WHERE checksum IS NOT NULL")
            
            # Ensure invoices table has hubspot_deal_id and updated_at
            cur.execute("""
                CREATE TABLE IF NOT EXISTS invoices (
                    id SERIAL PRIMARY KEY,
                    serie TEXT NOT NULL,
                    company_tax_id TEXT,
                    currency VARCHAR(8) DEFAULT 'USD',
                    subtotal NUMERIC(12,2) NOT NULL,
                    taxes NUMERIC(12,2) NOT NULL,
                    total NUMERIC(12,2) NOT NULL,
                    status TEXT DEFAULT 'issued',
                    hubspot_deal_id VARCHAR(128),
                    hubspot_contact_id VARCHAR(128),
                    created_at TIMESTAMPTZ DEFAULT NOW(),
                    updated_at TIMESTAMPTZ,
                    UNIQUE(serie)
                )
            """)
            cur.execute("CREATE INDEX IF NOT EXISTS idx_invoices_hubspot_deal ON invoices(hubspot_deal_id)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_invoices_status ON invoices(status)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_invoices_updated ON invoices(updated_at)")
            
            # Create change history table if enabled
            if enable_change_history:
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS invoice_change_history (
                        id SERIAL PRIMARY KEY,
                        invoice_id INTEGER REFERENCES invoices(id) ON DELETE CASCADE,
                        field_name VARCHAR(100) NOT NULL,
                        old_value TEXT,
                        new_value TEXT,
                        changed_by VARCHAR(100) DEFAULT 'system',
                        changed_at TIMESTAMPTZ DEFAULT NOW(),
                        change_reason TEXT,
                        metadata JSONB,
                        schema_version VARCHAR(10) DEFAULT %s
                    )
                """, (schema_version,))
                cur.execute("CREATE INDEX IF NOT EXISTS idx_invoice_change_history_invoice ON invoice_change_history(invoice_id, changed_at DESC)")
                cur.execute("CREATE INDEX IF NOT EXISTS idx_invoice_change_history_field ON invoice_change_history(field_name)")
            
            # Create conflict resolution table
            cur.execute("""
                CREATE TABLE IF NOT EXISTS sync_conflicts (
                    id SERIAL PRIMARY KEY,
                    object_type VARCHAR(50) NOT NULL,
                    object_id VARCHAR(128) NOT NULL,
                    field_name VARCHAR(100) NOT NULL,
                    crm_value TEXT,
                    accounting_value TEXT,
                    resolved_value TEXT,
                    resolution_strategy VARCHAR(50),
                    resolution_status VARCHAR(20) DEFAULT 'pending',
                    resolved_at TIMESTAMPTZ,
                    resolved_by VARCHAR(100),
                    created_at TIMESTAMPTZ DEFAULT NOW(),
                    metadata JSONB,
                    tenant_id VARCHAR(128)
                )
            """)
            cur.execute("CREATE INDEX IF NOT EXISTS idx_sync_conflicts_status ON sync_conflicts(resolution_status)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_sync_conflicts_object ON sync_conflicts(object_type, object_id)")
            if enable_multi_tenant:
                cur.execute("CREATE INDEX IF NOT EXISTS idx_sync_conflicts_tenant ON sync_conflicts(tenant_id)")
            
            # Create temporal analysis table
            if enable_temporal_analysis:
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS temporal_analysis (
                        id SERIAL PRIMARY KEY,
                        analysis_date DATE NOT NULL,
                        metric_name VARCHAR(100) NOT NULL,
                        metric_value NUMERIC(12,2),
                        daily_growth_percent NUMERIC(5,2),
                        weekly_trend VARCHAR(20),
                        monthly_average NUMERIC(12,2),
                        metadata JSONB,
                        tenant_id VARCHAR(128),
                        created_at TIMESTAMPTZ DEFAULT NOW(),
                        UNIQUE(analysis_date, metric_name, tenant_id)
                    )
                """)
                cur.execute("CREATE INDEX IF NOT EXISTS idx_temporal_analysis_date ON temporal_analysis(analysis_date DESC)")
                if enable_multi_tenant:
                    cur.execute("CREATE INDEX IF NOT EXISTS idx_temporal_analysis_tenant ON temporal_analysis(tenant_id)")
            
            # Create cohort analysis table
            if enable_cohort_analysis:
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS cohort_analysis (
                        id SERIAL PRIMARY KEY,
                        cohort_period VARCHAR(20) NOT NULL,
                        cohort_date DATE NOT NULL,
                        cohort_size INTEGER,
                        conversion_rate NUMERIC(5,2),
                        average_amount NUMERIC(12,2),
                        retention_rate NUMERIC(5,2),
                        metadata JSONB,
                        tenant_id VARCHAR(128),
                        created_at TIMESTAMPTZ DEFAULT NOW()
                    )
                """)
                cur.execute("CREATE INDEX IF NOT EXISTS idx_cohort_analysis_period ON cohort_analysis(cohort_period, cohort_date DESC)")
                if enable_multi_tenant:
                    cur.execute("CREATE INDEX IF NOT EXISTS idx_cohort_analysis_tenant ON cohort_analysis(tenant_id)")
            
            # Add tenant_id to invoices if multi-tenant
            if enable_multi_tenant:
                try:
                    cur.execute("ALTER TABLE invoices ADD COLUMN IF NOT EXISTS tenant_id VARCHAR(128)")
                    cur.execute("CREATE INDEX IF NOT EXISTS idx_invoices_tenant ON invoices(tenant_id)")
                except:
                    pass  # Column might already exist
            
            # Create cache table for API responses
            if enable_caching:
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS api_cache (
                        id SERIAL PRIMARY KEY,
                        cache_key VARCHAR(256) UNIQUE NOT NULL,
                        cache_value JSONB NOT NULL,
                        expires_at TIMESTAMPTZ NOT NULL,
                        created_at TIMESTAMPTZ DEFAULT NOW()
                    )
                """)
                cur.execute("CREATE INDEX IF NOT EXISTS idx_cache_key ON api_cache(cache_key)")
                cur.execute("CREATE INDEX IF NOT EXISTS idx_cache_expires ON api_cache(expires_at)")
                cur.execute("DELETE FROM api_cache WHERE expires_at < NOW()")
            
            # Check circuit breaker
            if enable_circuit_breaker:
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS circuit_breaker_state (
                        id SERIAL PRIMARY KEY,
                        service_name VARCHAR(50) UNIQUE NOT NULL,
                        failure_count INTEGER DEFAULT 0,
                        last_failure_at TIMESTAMPTZ,
                        state VARCHAR(20) DEFAULT 'CLOSED',
                        opened_at TIMESTAMPTZ
                    )
                """)
                cur.execute("""
                    SELECT state, failure_count, opened_at 
                    FROM circuit_breaker_state 
                    WHERE service_name = 'hubspot_api'
                """)
                cb_result = cur.fetchone()
                
                if cb_result:
                    cb_state, cb_failures, cb_opened = cb_result
                    if cb_state == 'OPEN':
                        # Check if enough time has passed to try again (1 hour)
                        if cb_opened:
                            from datetime import timedelta
                            time_diff = datetime.utcnow() - cb_opened
                            if time_diff < timedelta(hours=1):
                                logger.warning(f"Circuit breaker is OPEN for HubSpot API (opened {time_diff} ago), skipping sync")
                                conn.commit()
                                result = {
                                    'created': 0,
                                    'skipped': 0,
                                    'errors': 0,
                                    'circuit_breaker_open': True,
                                    'message': 'Skipped due to circuit breaker'
                                }
                                with open('sync_result.json', 'w') as f:
                                    json.dump(result, f, indent=2)
                                sys.exit(0)
                            else:
                                # Reset circuit breaker
                                logger.info("Circuit breaker timeout expired, resetting to CLOSED")
                                cur.execute("""
                                    UPDATE circuit_breaker_state 
                                    SET state = 'CLOSED', failure_count = 0, opened_at = NULL
                                    WHERE service_name = 'hubspot_api'
                                """)
            
            # Rate limiting setup
            import time as time_module
            rate_limit_delay = 60.0 / rate_limit_per_minute  # seconds between requests
            last_request_time = 0
            
            # Fetch deals from HubSpot with pagination
            url = f"{hubspot_base}/crm/v3/objects/deals/search"
            headers = {'Authorization': f'Bearer {hb_token}', 'Content-Type': 'application/json'}
            
            all_deals = []
            after = None
            max_iterations = 10  # Safety limit
            api_failures = 0
            
            # Batch optimization: process in larger chunks if enabled
            if enable_batch_optimization:
                # Increase batch size for better throughput
                actual_batch_size = min(batch_size * 2, 100)
                logger.info(f"Batch optimization enabled: using batch size {actual_batch_size}")
            else:
                actual_batch_size = min(batch_size, 100)
            
            for iteration in range(max_iterations):
                payload = {
                    'filterGroups': [{
                        'filters': [
                            {'propertyName': 'amount', 'operator': 'GT', 'value': '0'},
                            {'propertyName': 'dealstage', 'operator': 'EQ', 'value': deal_stage}
                        ]
                    }],
                    'properties': ['dealname', 'amount', 'dealstage', 'dealtype', 'closedate', 
                                 'hs_object_id', 'invoice_id', 'invoice_number', 'currency'],
                    'limit': actual_batch_size
                }
                
                if after:
                    payload['after'] = after
                
                try:
                    # Rate limiting
                    time_since_last = time_module.time() - last_request_time
                    if time_since_last < rate_limit_delay:
                        time_module.sleep(rate_limit_delay - time_since_last)
                    
                    r = requests.post(url, headers=headers, json=payload, timeout=30)
                    last_request_time = time_module.time()
                    
                    # Handle rate limiting (429)
                    if r.status_code == 429:
                        retry_after = int(r.headers.get('Retry-After', 60))
                        logger.warning(f"Rate limited by HubSpot, waiting {retry_after}s")
                        time_module.sleep(retry_after)
                        continue
                    
                    r.raise_for_status()
                    data = r.json()
                    deals = data.get('results', [])
                    
                    # Batch optimization: pre-filter deals if enabled
                    if enable_batch_optimization and iteration > 0:
                        # Quick pre-filter to reduce processing load
                        deals = [d for d in deals if not d.get('properties', {}).get('invoice_id')]
                    
                    all_deals.extend(deals)
                    
                    # Reset circuit breaker on success
                    if enable_circuit_breaker and api_failures > 0:
                        cur.execute("""
                            UPDATE circuit_breaker_state 
                            SET failure_count = 0, state = 'CLOSED', opened_at = NULL
                            WHERE service_name = 'hubspot_api'
                        """)
                        api_failures = 0
                    
                    paging = data.get('paging', {})
                    after = paging.get('next', {}).get('after')
                    if not after or len(all_deals) >= batch_size:
                        break
                except requests.exceptions.RequestException as e:
                    api_failures += 1
                    logger.error(f"Failed to fetch deals from HubSpot (failure {api_failures}): {e}")
                    
                    # Update circuit breaker
                    if enable_circuit_breaker:
                        cur.execute("""
                            INSERT INTO circuit_breaker_state (service_name, failure_count, last_failure_at, state)
                            VALUES ('hubspot_api', %s, NOW(), 'CLOSED')
                            ON CONFLICT (service_name) DO UPDATE SET
                                failure_count = circuit_breaker_state.failure_count + 1,
                                last_failure_at = NOW()
                        """, (api_failures,))
                        
                        cur.execute("""
                            SELECT failure_count FROM circuit_breaker_state 
                            WHERE service_name = 'hubspot_api'
                        """)
                        cb_failures = cur.fetchone()[0] if cur.rowcount > 0 else 0
                        
                        if cb_failures >= circuit_breaker_threshold:
                            cur.execute("""
                                UPDATE circuit_breaker_state 
                                SET state = 'OPEN', opened_at = NOW()
                                WHERE service_name = 'hubspot_api'
                            """)
                            logger.error(f"Circuit breaker OPENED after {cb_failures} failures")
                    
                    if iteration == 0:
                        raise
                    break
            
            logger.info(f"Fetched {len(all_deals)} deals from HubSpot")
            
            created = 0
            skipped = 0
            errors = []
            
            # Batch optimization: process in optimized chunks
            processing_batch = all_deals[:batch_size * 2] if enable_batch_optimization else all_deals[:batch_size]
            
            for d in processing_batch:
                did = d['id']
                props = d.get('properties', {})
                
                # Skip if already has invoice_id
                if props.get('invoice_id'):
                    skipped += 1
                    continue
                
                # Parse and validate amount
                try:
                    amt_str = props.get('amount', '0').replace(',', '').replace('$', '').strip()
                    amt = float(amt_str)
                except (ValueError, AttributeError, TypeError):
                    logger.warning(f"Invalid amount format for deal {did}: {props.get('amount')}")
                    errors.append({'deal_id': did, 'error': 'invalid_amount_format'})
                    continue
                
                # Data validation
                if enable_data_validation:
                    if amt < min_invoice_amount:
                        logger.warning(f"Amount {amt} below minimum {min_invoice_amount} for deal {did}")
                        errors.append({'deal_id': did, 'error': f'amount_below_minimum_{min_invoice_amount}'})
                        continue
                    
                    if amt > max_invoice_amount:
                        logger.warning(f"Amount {amt} exceeds maximum {max_invoice_amount} for deal {did}")
                        errors.append({'deal_id': did, 'error': f'amount_exceeds_maximum_{max_invoice_amount}'})
                        continue
                    
                    # Validate currency
                    currency = props.get('currency', 'USD').upper()[:3]
                    valid_currencies = ['USD', 'MXN', 'EUR', 'GBP', 'CAD']
                    if currency not in valid_currencies:
                        logger.warning(f"Unsupported currency {currency} for deal {did}, defaulting to USD")
                        currency = 'USD'
                else:
                    currency = props.get('currency', 'USD').upper()[:3]
                
                if amt <= 0:
                    skipped += 1
                    continue
                
                # Check cache first (if enabled)
                deal_cached = False
                if enable_caching:
                    cache_key = f"deal_{did}"
                    cur.execute("""
                        SELECT cache_value FROM api_cache 
                        WHERE cache_key = %s AND expires_at > NOW()
                    """, (cache_key,))
                    cached = cur.fetchone()
                    if cached:
                        cached_data = cached[0]
                        if cached_data.get('invoice_id'):
                            logger.info(f"Deal {did} already has invoice from cache")
                            skipped += 1
                            continue
                        deal_cached = True
                
                # Check if invoice already exists in database
                cur.execute("SELECT id, serie FROM invoices WHERE hubspot_deal_id=%s", (did,))
                existing = cur.fetchone()
                if existing:
                    logger.info(f"Invoice already exists for deal {did}: {existing[1]}")
                    # Update cache
                    if enable_caching:
                        cur.execute("""
                            INSERT INTO api_cache (cache_key, cache_value, expires_at)
                            VALUES (%s, %s, NOW() + INTERVAL '%s minutes')
                            ON CONFLICT (cache_key) DO UPDATE SET
                                cache_value = EXCLUDED.cache_value,
                                expires_at = EXCLUDED.expires_at
                        """, (f"deal_{did}", json.dumps({'invoice_id': existing[0], 'serie': existing[1]}), cache_ttl_minutes))
                    skipped += 1
                    continue
                
                # Calculate checksum for idempotency
                checksum_data = f"{did}:{amt}:{currency}:{deal_stage}"
                checksum = hashlib.sha256(checksum_data.encode()).hexdigest() if enable_idempotency else None
                
                # Check for duplicate using checksum
                if enable_idempotency and checksum:
                    cur.execute("""
                        SELECT accounting_object_id FROM crm_accounting_sync
                        WHERE checksum = %s AND sync_direction = 'crm_to_accounting'
                        LIMIT 1
                    """, (checksum,))
                    existing_check = cur.fetchone()
                    if existing_check:
                        logger.info(f"Duplicate detected by checksum for deal {did}, skipping")
                        skipped += 1
                        continue
                
                # Check for duplicate invoices (same amount, same day, within tolerance)
                if enable_duplicate_detection:
                    cur.execute("""
                        SELECT id, serie FROM invoices
                        WHERE DATE(created_at) = CURRENT_DATE
                        AND ABS(total - %s) <= %s
                        AND currency = %s
                        AND hubspot_deal_id != %s
                        LIMIT 1
                    """, (amt, duplicate_tolerance, currency, did))
                    potential_duplicate = cur.fetchone()
                    if potential_duplicate:
                        logger.warning(f"Potential duplicate invoice detected: {potential_duplicate[1]} for deal {did}")
                        errors.append({
                            'deal_id': did,
                            'error': 'potential_duplicate',
                            'duplicate_invoice_id': potential_duplicate[0],
                            'duplicate_serie': potential_duplicate[1]
                        })
                        # Continue processing but flag the issue
                
                # Generate invoice serie
                timestamp = datetime.utcnow().strftime('%Y%m%d')
                deal_short_id = did.replace('-', '')[:8]
                serie = f"INV-{timestamp}-{deal_short_id}"
                
                # Data transformation (if enabled)
                original_currency = currency
                original_amount = amt
                
                if enable_data_transformation:
                    # Normalize deal name for invoice
                    deal_name = props.get('dealname', f'Deal-{did}')[:100]  # Limit length
                    
                    # Apply environment-specific transformations
                    if environment == 'development':
                        # Add prefix in dev environment
                        deal_name = f"[DEV] {deal_name}"
                    elif environment == 'staging':
                        deal_name = f"[STG] {deal_name}"
                    
                    # Apply custom transformations
                    if custom_transformations.get('deal_name_transform'):
                        # Example: {"deal_name_transform": {"prefix": "[PRE]", "suffix": "[SUF]"}}
                        transform = custom_transformations['deal_name_transform']
                        if transform.get('prefix'):
                            deal_name = f"{transform['prefix']}{deal_name}"
                        if transform.get('suffix'):
                            deal_name = f"{deal_name}{transform['suffix']}"
                    
                    # Normalize currency codes
                    currency_map = {
                        'MXN': 'MXN', 'USD': 'USD', 'EUR': 'EUR', 'GBP': 'GBP', 'CAD': 'CAD',
                        'MEX': 'MXN', 'US': 'USD', 'EU': 'EUR'  # Common variations
                    }
                    currency = currency_map.get(currency.upper(), currency)
                    
                    # Currency conversion if enabled
                    target_currency = custom_transformations.get('target_currency', currency)
                    if enable_currency_conversion and currency != target_currency:
                        conversion_rates = custom_transformations.get('conversion_rates', {})
                        amt = convert_currency(amt, currency, target_currency, conversion_rates)
                        if currency != target_currency:
                            logger.info(f"Converted {original_amount} {original_currency} to {amt} {target_currency}")
                            currency = target_currency
                    
                    # Round amounts to 2 decimal places
                    amt = round(Decimal(str(amt)), 2)
                
                # Calculate subtotal and taxes (currency already validated above)
                subtotal = Decimal(str(amt)) / Decimal(str(1 + tax_rate))
                taxes = Decimal(str(amt)) - subtotal
                
                # Round financial values
                subtotal = round(subtotal, 2)
                taxes = round(taxes, 2)
                amt = round(Decimal(str(amt)), 2)
                
                # Store rollback data if enabled
                rollback_data = None
                if enable_rollback:
                    rollback_data = json.dumps({
                        'deal_id': did,
                        'deal_amount': amt,
                        'deal_currency': currency,
                        'original_deal_stage': props.get('dealstage')
                    })
                
                # Insert invoice (skip in dry_run)
                if dry_run:
                    logger.info(f"[DRY RUN] Would create invoice {serie} for deal {did} (amount: {amt}, currency: {currency})")
                    created += 1
                    continue
                
                try:
                    # Create backup if enabled
                    if enable_auto_backup:
                        # Backup would be created before insert, but we don't have ID yet
                        pass
                    
                    # Build insert query with tenant support
                    if enable_multi_tenant:
                        insert_query = """
                            INSERT INTO invoices (serie, currency, subtotal, taxes, total, status, hubspot_deal_id, tenant_id, created_at)
                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, NOW())
                            RETURNING id
                        """
                        insert_params = (serie, currency, float(subtotal), float(taxes), amt, 'issued', did, tenant_id)
                    else:
                        insert_query = """
                            INSERT INTO invoices (serie, currency, subtotal, taxes, total, status, hubspot_deal_id, created_at)
                            VALUES (%s, %s, %s, %s, %s, %s, %s, NOW())
                            RETURNING id
                        """
                        insert_params = (serie, currency, float(subtotal), float(taxes), amt, 'issued', did)
                    
                    # Optimize query if enabled
                    optimized_query, optimized_params = optimize_query(insert_query, insert_params)
                    cur.execute(optimized_query, optimized_params)
                    inv_id = cur.fetchone()[0]
                    
                    # Record change history if enabled
                    if enable_change_history:
                        cur.execute("""
                            INSERT INTO invoice_change_history 
                            (invoice_id, field_name, old_value, new_value, changed_by, change_reason, metadata)
                            VALUES (%s, %s, %s, %s, %s, %s, %s)
                        """, (
                            inv_id, 
                            'invoice_created',
                            None,
                            f"Invoice {serie} created from deal {did}",
                            'crm_sync',
                            'crm_to_accounting_sync',
                            json.dumps({
                                'deal_id': did,
                                'deal_amount': float(original_amount),
                                'original_currency': original_currency,
                                'final_currency': currency,
                                'converted': original_currency != currency,
                                'environment': environment
                            }, default=str)
                        ))
                    
                    # Record sync with checksum and rollback data
                    cur.execute("""
                        INSERT INTO crm_accounting_sync 
                        (crm_object_type, crm_object_id, accounting_object_type, accounting_object_id, sync_direction, checksum, rollback_data)
                        VALUES ('deal', %s, 'invoice', %s, 'crm_to_accounting', %s, %s)
                        ON CONFLICT (crm_object_type, crm_object_id, accounting_object_type, accounting_object_id)
                        DO UPDATE SET synced_at = NOW(), checksum = EXCLUDED.checksum
                    """, (did, inv_id, checksum, rollback_data))
                    
                    # Update HubSpot deal with invoice reference (with retry)
                    if enable_retry:
                        max_retry_attempts = max_retries
                    else:
                        max_retry_attempts = 1
                    
                    hubspot_updated = False
                    for retry_attempt in range(max_retry_attempts):
                        try:
                            update_r = requests.patch(
                                f"{hubspot_base}/crm/v3/objects/deals/{did}",
                                headers=headers,
                                json={'properties': {
                                    'invoice_id': str(inv_id),
                                    'invoice_number': serie,
                                    'invoice_synced_at': datetime.utcnow().isoformat() + 'Z'
                                }},
                                timeout=30
                            )
                            update_r.raise_for_status()
                            hubspot_updated = True
                            logger.info(f"Created invoice {serie} (ID: {inv_id}) for deal {did}")
                            
                            # Update cache
                            if enable_caching:
                                cur.execute("""
                                    INSERT INTO api_cache (cache_key, cache_value, expires_at)
                                    VALUES (%s, %s, NOW() + INTERVAL '%s minutes')
                                    ON CONFLICT (cache_key) DO UPDATE SET
                                        cache_value = EXCLUDED.cache_value,
                                        expires_at = EXCLUDED.expires_at
                                """, (f"deal_{did}", json.dumps({'invoice_id': inv_id, 'serie': serie}), cache_ttl_minutes))
                            
                            break
                        except requests.exceptions.RequestException as e:
                            if retry_attempt < max_retry_attempts - 1:
                                wait_time = (2 ** retry_attempt) * 1  # Exponential backoff: 1s, 2s, 4s
                                logger.warning(f"HubSpot update failed for deal {did}, retrying in {wait_time}s (attempt {retry_attempt + 1}/{max_retry_attempts}): {e}")
                                import time
                                time.sleep(wait_time)
                            else:
                                logger.warning(f"Failed to update HubSpot deal {did} after {max_retry_attempts} attempts: {e}")
                                errors.append({'deal_id': did, 'error': f'hubspot_update_failed: {str(e)}'})
                    
                    created += 1
                except psycopg2.IntegrityError as e:
                    logger.warning(f"Integrity error for deal {did}: {e}")
                    errors.append({'deal_id': did, 'error': 'database_integrity_error'})
                    conn.rollback()
                    continue
                except Exception as e:
                    logger.error(f"Error creating invoice for deal {did}: {e}")
                    errors.append({'deal_id': did, 'error': str(e)})
                    conn.rollback()
                    continue
            
            conn.commit()
            
            # Calculate metrics
            total_processed = created + skipped + len(errors)
            success_rate = (created / total_processed * 100) if total_processed > 0 else 0
            
            # Performance metrics
            execution_start = datetime.utcnow()
            try:
                execution_start = datetime.fromisoformat("{{ execution.startDate }}".replace('Z', '+00:00'))
            except:
                pass
            
            execution_duration = (datetime.utcnow() - execution_start).total_seconds()
            throughput = created / execution_duration if execution_duration > 0 else 0
            
            # Prepare invoice data for predictive analysis
            invoice_data_for_analysis = []
            if enable_predictive_analysis:
                try:
                    cur.execute("""
                        SELECT id, serie, total, currency, status, created_at
                        FROM invoices
                        WHERE created_at >= NOW() - INTERVAL '7 days'
                        ORDER BY created_at DESC
                        LIMIT 100
                    """)
                    recent_invoices = cur.fetchall()
                    invoice_data_for_analysis = [
                        {'id': inv[0], 'serie': inv[1], 'total': float(inv[2]), 'currency': inv[3], 'status': inv[4]}
                        for inv in recent_invoices
                    ]
                except Exception as e:
                    logger.warning(f"Failed to fetch data for predictive analysis: {e}")
            
            # Run predictive analysis
            patterns = analyze_patterns(invoice_data_for_analysis)
            
            # Run temporal analysis
            temporal_trends = {}
            if enable_temporal_analysis and invoice_data_for_analysis:
                temporal_trends = analyze_temporal_trends(invoice_data_for_analysis)
                # Store temporal analysis in database
                if temporal_trends:
                    try:
                        analysis_date = datetime.utcnow().date()
                        cur.execute("""
                            INSERT INTO temporal_analysis 
                            (analysis_date, metric_name, metric_value, daily_growth_percent, weekly_trend, monthly_average, tenant_id)
                            VALUES (%s, %s, %s, %s, %s, %s, %s)
                            ON CONFLICT (analysis_date, metric_name, tenant_id)
                            DO UPDATE SET 
                                metric_value = EXCLUDED.metric_value,
                                daily_growth_percent = EXCLUDED.daily_growth_percent,
                                weekly_trend = EXCLUDED.weekly_trend,
                                monthly_average = EXCLUDED.monthly_average
                        """, (
                            analysis_date,
                            'invoices_created',
                            created,
                            temporal_trends.get('daily_growth', 0),
                            temporal_trends.get('weekly_trend', 'stable'),
                            temporal_trends.get('monthly_average', 0),
                            tenant_id if enable_multi_tenant else 'default'
                        ))
                        conn.commit()
                    except Exception as e:
                        logger.warning(f"Failed to store temporal analysis: {e}")
            
            result = {
                'created': created,
                'skipped': skipped,
                'errors': len(errors),
                'total_processed': total_processed,
                'success_rate': round(success_rate, 2),
                'dry_run': dry_run,
                'execution_duration_seconds': round(execution_duration, 2),
                'throughput_per_second': round(throughput, 2),
                'cache_enabled': enable_caching,
                'parallel_processing': enable_parallel,
                'batch_optimization': enable_batch_optimization,
                'data_transformation_enabled': enable_data_transformation,
                'environment': environment,
                'hubspot_environment': hubspot_environment,
                'currency_conversion_enabled': enable_currency_conversion,
                'change_history_enabled': enable_change_history,
                'custom_transformations': len(custom_transformations) > 0,
                'conflict_resolution_strategy': conflict_resolution_strategy,
                'predictive_analysis': patterns if enable_predictive_analysis else None,
                'temporal_analysis': temporal_trends if enable_temporal_analysis else None,
                'multi_tenant_enabled': enable_multi_tenant,
                'tenant_id': tenant_id if enable_multi_tenant else None,
                'auto_backup_enabled': enable_auto_backup,
                'query_optimization_enabled': enable_query_optimization,
                'cohort_analysis_enabled': enable_cohort_analysis,
                'data_archival_enabled': enable_data_archival,
                'schema_version': schema_version,
                'error_details': errors[:10]  # Limit error details
            }
            
            with open('sync_result.json', 'w') as f:
                json.dump(result, f, indent=2)
            
            logger.info(f"CRM→Accounting sync completed: {created} created, {skipped} skipped, {len(errors)} errors, {success_rate:.1f}% success rate")
            
        except Exception as e:
            logger.error(f"Sync failed: {e}", exc_info=True)
            if conn:
                conn.rollback()
            sys.exit(1)
        finally:
            if cur:
                cur.close()
            if conn:
                conn.close()
    env:
      HUBSPOT_TOKEN: "{{ inputs.hubspot_token }}"
      JDBC_URL: "{{ inputs.jdbc_url }}"
      JDBC_USER: "{{ inputs.jdbc_user }}"
      JDBC_PASSWORD: "{{ inputs.jdbc_password }}"
      HUBSPOT_BASE: "{{ vars.hubspot_base }}"
      TAX_RATE: "{{ inputs.tax_rate | default(0.16) }}"
      BATCH_SIZE: "{{ inputs.batch_size | default(50) }}"
      DEAL_STAGE_FILTER: "{{ inputs.deal_stage_filter | default('closedwon') }}"
      DRY_RUN: "{{ inputs.dry_run | default(false) }}"
      ENABLE_RETRY: "{{ inputs.enable_retry | default(true) }}"
      MAX_RETRIES: "{{ inputs.max_retries | default(3) }}"
      ENABLE_DATA_VALIDATION: "{{ inputs.enable_data_validation | default(true) }}"
      MIN_INVOICE_AMOUNT: "{{ inputs.min_invoice_amount | default(0.01) }}"
      MAX_INVOICE_AMOUNT: "{{ inputs.max_invoice_amount | default(1000000.0) }}"
      RATE_LIMIT_PER_MINUTE: "{{ inputs.rate_limit_per_minute | default(100) }}"
      ENABLE_CIRCUIT_BREAKER: "{{ inputs.enable_circuit_breaker | default(true) }}"
      CIRCUIT_BREAKER_THRESHOLD: "{{ inputs.circuit_breaker_threshold | default(5) }}"
      ENABLE_CACHING: "{{ inputs.enable_caching | default(true) }}"
      CACHE_TTL_MINUTES: "{{ inputs.cache_ttl_minutes | default(15) }}"
      ENABLE_PARALLEL_PROCESSING: "{{ inputs.enable_parallel_processing | default(false) }}"
      ENABLE_IDEMPOTENCY_CHECK: "{{ inputs.enable_idempotency_check | default(true) }}"
      ENABLE_DUPLICATE_DETECTION: "{{ inputs.enable_duplicate_detection | default(true) }}"
      DUPLICATE_TOLERANCE: "{{ inputs.duplicate_tolerance | default(0.01) }}"
      ENABLE_ROLLBACK: "{{ inputs.enable_rollback | default(false) }}"
      ENABLE_DATA_TRANSFORMATION: "{{ inputs.enable_data_transformation | default(true) }}"
      ENABLE_AUTO_RECONCILIATION: "{{ inputs.enable_auto_reconciliation | default(true) }}"
      RECONCILIATION_TOLERANCE: "{{ inputs.reconciliation_tolerance | default(0.01) }}"
      ENABLE_BATCH_OPTIMIZATION: "{{ inputs.enable_batch_optimization | default(true) }}"
      ENVIRONMENT: "{{ inputs.environment | default('production') }}"
      HUBSPOT_ENVIRONMENT: "{{ inputs.hubspot_environment | default('production') }}"
      ENABLE_CURRENCY_CONVERSION: "{{ inputs.enable_currency_conversion | default(false) }}"
      CURRENCY_CONVERSION_API_KEY: "{{ inputs.currency_conversion_api_key | default('') }}"
      ENABLE_CHANGE_HISTORY: "{{ inputs.enable_change_history | default(true) }}"
      CUSTOM_TRANSFORMATIONS: "{{ inputs.custom_transformations | default({}) | toJson }}"
      CONFLICT_RESOLUTION_STRATEGY: "{{ inputs.conflict_resolution_strategy | default('last_write_wins') }}"
      ENABLE_LOG_COMPRESSION: "{{ inputs.enable_log_compression | default(true) }}"
      ENABLE_PREDICTIVE_ANALYSIS: "{{ inputs.enable_predictive_analysis | default(false) }}"
      ENABLE_REPORT_EXPORT: "{{ inputs.enable_report_export | default(true) }}"
      REPORT_EXPORT_FORMAT: "{{ inputs.report_export_format | default('json') }}"
      ENABLE_SMART_ALERTS: "{{ inputs.enable_smart_alerts | default(true) }}"
      SCHEMA_VERSION: "{{ inputs.schema_version | default('v1') }}"
    outputFiles:
      - sync_result.json

  - id: auto_reconciliation
    type: io.kestra.plugin.scripts.python.Script
    timeout: PT3M
    disabled: "{{ inputs.enable_auto_reconciliation | default(true) != true }}"
    inputFiles:
      reconcile.py: |
        import json, os, requests, psycopg2, sys, logging
        from datetime import datetime
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        
        tolerance = float(os.getenv('RECONCILIATION_TOLERANCE', '0.01'))
        hb_token = os.getenv('HUBSPOT_TOKEN')
        jdbc = os.getenv('JDBC_URL').replace('jdbc:postgresql://', 'postgresql://')
        if '@' not in jdbc:
            jdbc = jdbc.replace('postgresql://', f'postgresql://{os.getenv("JDBC_USER")}:{os.getenv("JDBC_PASSWORD")}@')
        
        reconciliations = []
        fixed = 0
        errors = []
        
        try:
            conn = psycopg2.connect(jdbc)
            cur = conn.cursor()
            
            # Find invoices with amount mismatches
            cur.execute("""
                SELECT i.id, i.serie, i.hubspot_deal_id, i.total, i.currency, i.status
                FROM invoices i
                WHERE i.hubspot_deal_id IS NOT NULL
                AND i.created_at >= NOW() - INTERVAL '7 days'
                LIMIT 20
            """)
            invoices = cur.fetchall()
            
            if hb_token and invoices:
                headers = {'Authorization': f'Bearer {hb_token}', 'Content-Type': 'application/json'}
                # Use environment-aware HubSpot base URL
                hubspot_env_reconcile = os.getenv('HUBSPOT_ENVIRONMENT', 'production')
                if hubspot_env_reconcile == 'sandbox' or hubspot_env_reconcile == 'development':
                    url_base = 'https://api.hubspotqa.com'
                else:
                    url_base = os.getenv('HUBSPOT_BASE', 'https://api.hubapi.com')
                
                for inv in invoices:
                    inv_id, serie, deal_id, total, currency, status = inv
                    
                    try:
                        deal_url = f"{url_base}/crm/v3/objects/deals/{deal_id}"
                        r = requests.get(deal_url, headers=headers, params={'properties': 'amount,currency'}, timeout=10)
                        
                        if r.status_code == 200:
                            deal_data = r.json()
                            props = deal_data.get('properties', {})
                            deal_amount = float(props.get('amount', '0').replace(',', '').replace('$', '') or '0')
                            deal_currency = props.get('currency', 'USD').upper()
                            
                            # Check for mismatch
                            amount_diff = abs(deal_amount - float(total))
                            currency_mismatch = currency.upper() != deal_currency
                            
                            if amount_diff > tolerance or currency_mismatch:
                                # Auto-reconciliation: update invoice if deal is authoritative
                                if amount_diff > tolerance:
                                    logger.info(f"Reconciling invoice {serie}: amount diff {amount_diff}")
                                    cur.execute("""
                                        UPDATE invoices 
                                        SET total = %s, updated_at = NOW()
                                        WHERE id = %s
                                    """, (deal_amount, inv_id))
                                    reconciliations.append({
                                        'type': 'amount_reconciliation',
                                        'invoice_id': inv_id,
                                        'invoice_serie': serie,
                                        'old_amount': float(total),
                                        'new_amount': deal_amount,
                                        'difference': amount_diff
                                    })
                                    fixed += 1
                                
                                if currency_mismatch:
                                    logger.info(f"Reconciling invoice {serie}: currency {currency} -> {deal_currency}")
                                    cur.execute("""
                                        UPDATE invoices 
                                        SET currency = %s, updated_at = NOW()
                                        WHERE id = %s
                                    """, (deal_currency, inv_id))
                                    reconciliations.append({
                                        'type': 'currency_reconciliation',
                                        'invoice_id': inv_id,
                                        'invoice_serie': serie,
                                        'old_currency': currency,
                                        'new_currency': deal_currency
                                    })
                    except Exception as e:
                        logger.warning(f"Failed to reconcile invoice {inv_id}: {e}")
                        errors.append({'invoice_id': inv_id, 'error': str(e)})
            
            conn.commit()
            
            result = {
                'timestamp': datetime.utcnow().isoformat() + 'Z',
                'reconciliations_applied': fixed,
                'reconciliations': reconciliations[:10],
                'errors': len(errors),
                'status': 'ok' if fixed > 0 else 'no_changes'
            }
            
            with open('reconciliation_report.json', 'w') as f:
                json.dump(result, f, indent=2, default=str)
            
            logger.info(f"Auto-reconciliation completed: {fixed} fixes applied")
            
        except Exception as e:
            logger.error(f"Reconciliation failed: {e}")
            result = {'status': 'error', 'error': str(e)}
            with open('reconciliation_report.json', 'w') as f:
                json.dump(result, f, indent=2)
        finally:
            if 'cur' in locals():
                cur.close()
            if 'conn' in locals():
                conn.close()
    env:
      HUBSPOT_TOKEN: "{{ inputs.hubspot_token }}"
      JDBC_URL: "{{ inputs.jdbc_url }}"
      JDBC_USER: "{{ inputs.jdbc_user }}"
      JDBC_PASSWORD: "{{ inputs.jdbc_password }}"
      HUBSPOT_BASE: "{{ vars.hubspot_base }}"
      HUBSPOT_ENVIRONMENT: "{{ inputs.hubspot_environment | default('production') }}"
      RECONCILIATION_TOLERANCE: "{{ inputs.reconciliation_tolerance | default(0.01) }}"
    outputFiles:
      - reconciliation_report.json

  - id: detect_duplicates
    type: io.kestra.plugin.scripts.python.Script
    timeout: PT2M
    disabled: "{{ inputs.enable_duplicate_detection | default(true) != true }}"
    inputFiles:
      detect_dups.py: |
        import json, os, psycopg2, sys, logging
        from datetime import datetime
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        
        tolerance = float(os.getenv('DUPLICATE_TOLERANCE', '0.01'))
        
        jdbc = os.getenv('JDBC_URL').replace('jdbc:postgresql://', 'postgresql://')
        if '@' not in jdbc:
            jdbc = jdbc.replace('postgresql://', f'postgresql://{os.getenv("JDBC_USER")}:{os.getenv("JDBC_PASSWORD")}@')
        
        duplicates = []
        
        try:
            conn = psycopg2.connect(jdbc)
            cur = conn.cursor()
            
            # Find potential duplicates: same amount, same day, within tolerance
            cur.execute("""
                SELECT 
                    i1.id AS id1,
                    i1.serie AS serie1,
                    i1.hubspot_deal_id AS deal1,
                    i2.id AS id2,
                    i2.serie AS serie2,
                    i2.hubspot_deal_id AS deal2,
                    i1.total,
                    i1.created_at::date AS invoice_date,
                    ABS(i1.total - i2.total) AS amount_diff
                FROM invoices i1
                JOIN invoices i2 ON 
                    i1.id < i2.id
                    AND i1.created_at::date = i2.created_at::date
                    AND i1.currency = i2.currency
                    AND ABS(i1.total - i2.total) <= %s
                    AND i1.status = i2.status
                WHERE i1.created_at >= CURRENT_DATE - INTERVAL '30 days'
                ORDER BY i1.created_at DESC
                LIMIT 50
            """, (tolerance,))
            
            for row in cur.fetchall():
                id1, serie1, deal1, id2, serie2, deal2, total, inv_date, diff = row
                duplicates.append({
                    'invoice1_id': id1,
                    'invoice1_serie': serie1,
                    'deal1_id': deal1,
                    'invoice2_id': id2,
                    'invoice2_serie': serie2,
                    'deal2_id': deal2,
                    'amount': float(total),
                    'date': inv_date.isoformat() if hasattr(inv_date, 'isoformat') else str(inv_date),
                    'difference': float(diff),
                    'severity': 'high' if deal1 == deal2 else 'medium'
                })
            
            result = {
                'timestamp': datetime.utcnow().isoformat() + 'Z',
                'duplicates_found': len(duplicates),
                'tolerance': tolerance,
                'duplicates': duplicates[:20],  # Limit to 20
                'status': 'warning' if duplicates else 'ok'
            }
            
            with open('duplicates_report.json', 'w') as f:
                json.dump(result, f, indent=2, default=str)
            
            if duplicates:
                logger.warning(f"Found {len(duplicates)} potential duplicate invoices")
            else:
                logger.info("No duplicate invoices detected")
                
        except Exception as e:
            logger.error(f"Duplicate detection failed: {e}")
            result = {'status': 'error', 'error': str(e)}
            with open('duplicates_report.json', 'w') as f:
                json.dump(result, f, indent=2)
        finally:
            if 'cur' in locals():
                cur.close()
            if 'conn' in locals():
                conn.close()
    env:
      JDBC_URL: "{{ inputs.jdbc_url }}"
      JDBC_USER: "{{ inputs.jdbc_user }}"
      JDBC_PASSWORD: "{{ inputs.jdbc_password }}"
      DUPLICATE_TOLERANCE: "{{ inputs.duplicate_tolerance | default(0.01) }}"
    outputFiles:
      - duplicates_report.json

  - id: sync_accounting_to_crm
    type: io.kestra.plugin.scripts.python.Script
    timeout: PT5M
    conditions:
      - type: io.kestra.plugin.core.condition.ExpressionCondition
        expression: "{{ (taskrun.outputs['parse_trigger']['files']['route.json'] | readFile | fromJson).sync_accounting_to_crm == true }}"
    inputFiles:
      sync_back.py: |
        import json, os, requests, psycopg2, sys, logging
        from datetime import datetime
        
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger(__name__)
        
        hb_token = os.getenv('HUBSPOT_TOKEN')
        if not hb_token:
            logger.error("HUBSPOT_TOKEN not provided")
            sys.exit(1)
        
        # Set HubSpot base URL based on environment
        hubspot_env = os.getenv('HUBSPOT_ENVIRONMENT', 'production')
        if hubspot_env == 'sandbox':
            hubspot_base = 'https://api.hubspotqa.com'
        elif hubspot_env == 'development':
            hubspot_base = 'https://api.hubspotqa.com'
        else:
            hubspot_base = os.getenv('HUBSPOT_BASE', 'https://api.hubapi.com')
        
        logger.info(f"Using HubSpot environment: {hubspot_env} ({hubspot_base})")
        
        batch_size = int(os.getenv('BATCH_SIZE', '50'))
        dry_run = os.getenv('DRY_RUN', 'false').lower() == 'true'
        max_retries = int(os.getenv('MAX_RETRIES', '3'))
        enable_retry = os.getenv('ENABLE_RETRY', 'true').lower() == 'true'
        
        jdbc = os.getenv('JDBC_URL').replace('jdbc:postgresql://', 'postgresql://')
        if '@' not in jdbc:
            jdbc = jdbc.replace('postgresql://', f'postgresql://{os.getenv("JDBC_USER")}:{os.getenv("JDBC_PASSWORD")}@')
        
        conn = None
        cur = None
        
        try:
            conn = psycopg2.connect(jdbc)
            cur = conn.cursor()
            
            # Find invoices that need sync to CRM
            cur.execute("""
                SELECT i.id, i.serie, i.status, i.hubspot_deal_id, i.total, i.currency,
                       i.updated_at, COALESCE(cas.synced_at, '1970-01-01'::timestamptz) AS last_sync
                FROM invoices i
                LEFT JOIN crm_accounting_sync cas ON 
                    cas.accounting_object_type = 'invoice' 
                    AND cas.accounting_object_id = i.id 
                    AND cas.sync_direction = 'accounting_to_crm'
                WHERE i.hubspot_deal_id IS NOT NULL
                    AND (cas.synced_at IS NULL OR i.updated_at > cas.synced_at)
                ORDER BY i.updated_at DESC
                LIMIT %s
            """, (batch_size,))
            invoices = cur.fetchall()
            
            logger.info(f"Found {len(invoices)} invoices to sync to CRM")
            
            # Also check for payments that need sync
            cur.execute("""
                SELECT p.payment_id, p.amount, p.currency, p.status, p.created_at,
                       i.hubspot_deal_id, i.id as invoice_id,
                       COALESCE(cas.synced_at, '1970-01-01'::timestamptz) AS last_sync
                FROM payments p
                LEFT JOIN invoice_payments ip ON p.payment_id = ip.payment_id
                LEFT JOIN invoices i ON ip.invoice_id = i.id
                LEFT JOIN crm_accounting_sync cas ON 
                    cas.accounting_object_type = 'payment'
                    AND cas.accounting_object_id_str = p.payment_id
                    AND cas.sync_direction = 'accounting_to_crm'
                WHERE i.hubspot_deal_id IS NOT NULL
                    AND p.status IN ('succeeded', 'paid', 'payment_intent.succeeded', 'charge.succeeded')
                    AND (cas.synced_at IS NULL OR p.created_at > cas.synced_at)
                ORDER BY p.created_at DESC
                LIMIT %s
            """, (batch_size,))
            payments = cur.fetchall()
            
            logger.info(f"Found {len(payments)} payments to sync to CRM")
            
            headers = {'Authorization': f'Bearer {hb_token}', 'Content-Type': 'application/json'}
            updated_deals = 0
            updated_payments = 0
            errors = []
            
            # Sync invoice status changes
            for inv in invoices:
                inv_id, serie, status, deal_id, total, currency, updated_at, last_sync = inv
                
                if not deal_id:
                    continue
                
                # Prepare properties to update
                props = {
                    'invoice_status': status,
                    'invoice_last_updated': updated_at.isoformat() + 'Z' if updated_at else None
                }
                
                # Map invoice status to deal stage
                status_mapping = {
                    'paid': 'closedwon',
                    'cancelled': 'closedlost',
                    'refunded': 'closedlost'
                }
                
                if status in status_mapping:
                    props['dealstage'] = status_mapping[status]
                
                # Update HubSpot deal (skip in dry_run)
                if dry_run:
                    logger.info(f"[DRY RUN] Would update deal {deal_id} with invoice status {status}")
                    updated_deals += 1
                    continue
                
                # Retry logic for HubSpot updates
                if enable_retry:
                    max_retry_attempts = max_retries
                else:
                    max_retry_attempts = 1
                
                hubspot_updated = False
                for retry_attempt in range(max_retry_attempts):
                    try:
                        r = requests.patch(
                            f"{hubspot_base}/crm/v3/objects/deals/{deal_id}",
                            headers=headers,
                            json={'properties': props},
                            timeout=30
                        )
                        r.raise_for_status()
                        hubspot_updated = True
                        
                        # Record sync
                        cur.execute("""
                            INSERT INTO crm_accounting_sync 
                            (crm_object_type, crm_object_id, accounting_object_type, accounting_object_id, sync_direction)
                            VALUES ('deal', %s, 'invoice', %s, 'accounting_to_crm')
                            ON CONFLICT (crm_object_type, crm_object_id, accounting_object_type, accounting_object_id)
                            DO UPDATE SET synced_at = NOW()
                        """, (deal_id, inv_id))
                        
                        updated_deals += 1
                        logger.info(f"Updated deal {deal_id} with invoice status {status}")
                        break
                    except requests.exceptions.RequestException as e:
                        if retry_attempt < max_retry_attempts - 1:
                            wait_time = (2 ** retry_attempt) * 1
                            logger.warning(f"HubSpot update failed for deal {deal_id}, retrying in {wait_time}s (attempt {retry_attempt + 1}/{max_retry_attempts}): {e}")
                            import time
                            time.sleep(wait_time)
                        else:
                            logger.warning(f"Failed to update deal {deal_id} after {max_retry_attempts} attempts: {e}")
                            errors.append({'deal_id': deal_id, 'invoice_id': inv_id, 'error': str(e)})
            
            # Sync payment information
            for payment in payments:
                payment_id, amount, currency, status, created_at, deal_id, invoice_id, last_sync = payment
                
                if not deal_id:
                    continue
                
                props = {
                    'payment_received': 'true',
                    'payment_amount': str(float(amount)),
                    'payment_currency': currency or 'USD',
                    'payment_date': created_at.isoformat() + 'Z' if created_at else None,
                    'last_payment_status': status
                }
                
                # Update with payment info (skip in dry_run)
                if dry_run:
                    logger.info(f"[DRY RUN] Would update deal {deal_id} with payment {payment_id}")
                    updated_payments += 1
                    continue
                
                # Retry logic
                if enable_retry:
                    max_retry_attempts = max_retries
                else:
                    max_retry_attempts = 1
                
                for retry_attempt in range(max_retry_attempts):
                    try:
                        r = requests.patch(
                            f"{hubspot_base}/crm/v3/objects/deals/{deal_id}",
                            headers=headers,
                            json={'properties': props},
                            timeout=30
                        )
                        r.raise_for_status()
                        
                        # Record sync
                        cur.execute("""
                            INSERT INTO crm_accounting_sync 
                            (crm_object_type, crm_object_id, accounting_object_type, accounting_object_id_str, sync_direction)
                            VALUES ('deal', %s, 'payment', %s, %s, 'accounting_to_crm')
                            ON CONFLICT (crm_object_type, crm_object_id, accounting_object_type, accounting_object_id)
                            DO UPDATE SET synced_at = NOW()
                        """, (deal_id, None, payment_id))
                        
                        updated_payments += 1
                        logger.info(f"Updated deal {deal_id} with payment {payment_id}")
                        break
                    except requests.exceptions.RequestException as e:
                        if retry_attempt < max_retry_attempts - 1:
                            wait_time = (2 ** retry_attempt) * 1
                            logger.warning(f"HubSpot payment update failed for deal {deal_id}, retrying in {wait_time}s: {e}")
                            import time
                            time.sleep(wait_time)
                        else:
                            logger.warning(f"Failed to update deal {deal_id} with payment after {max_retry_attempts} attempts: {e}")
                            errors.append({'deal_id': deal_id, 'payment_id': payment_id, 'error': str(e)})
            
            conn.commit()
            
            result = {
                'updated_deals': updated_deals,
                'updated_payments': updated_payments,
                'errors': len(errors),
                'error_details': errors[:10]
            }
            
            with open('sync_back_result.json', 'w') as f:
                json.dump(result, f, indent=2, default=str)
            
            logger.info(f"Accounting→CRM sync completed: {updated_deals} deals, {updated_payments} payments, {len(errors)} errors")
            
        except Exception as e:
            logger.error(f"Sync failed: {e}", exc_info=True)
            if conn:
                conn.rollback()
            sys.exit(1)
        finally:
            if cur:
                cur.close()
            if conn:
                conn.close()
    env:
      HUBSPOT_TOKEN: "{{ inputs.hubspot_token }}"
      JDBC_URL: "{{ inputs.jdbc_url }}"
      JDBC_USER: "{{ inputs.jdbc_user }}"
      JDBC_PASSWORD: "{{ inputs.jdbc_password }}"
      HUBSPOT_BASE: "{{ vars.hubspot_base }}"
      HUBSPOT_ENVIRONMENT: "{{ inputs.hubspot_environment | default('production') }}"
      BATCH_SIZE: "{{ inputs.batch_size | default(50) }}"
      DRY_RUN: "{{ inputs.dry_run | default(false) }}"
      ENABLE_RETRY: "{{ inputs.enable_retry | default(true) }}"
      MAX_RETRIES: "{{ inputs.max_retries | default(3) }}"
    outputFiles:
      - sync_back_result.json

  - id: generate_summary
    type: io.kestra.plugin.scripts.python.Script
    timeout: PT30S
    inputFiles:
      route.json: "{{ taskrun.outputs['parse_trigger']['files']['route.json'] }}"
      sync_result.json: "{{ taskrun.outputs['sync_crm_to_accounting']['files']['sync_result.json'] | default('{}') }}"
      sync_back_result.json: "{{ taskrun.outputs['sync_accounting_to_crm']['files']['sync_back_result.json'] | default('{}') }}"
      duplicates_report.json: "{{ taskrun.outputs['detect_duplicates']['files']['duplicates_report.json'] | default('{}') }}"
      integrity_check.json: "{{ taskrun.outputs['verify_integrity']['files']['integrity_check.json'] | default('{}') }}"
      reconciliation_report.json: "{{ taskrun.outputs['auto_reconciliation']['files']['reconciliation_report.json'] | default('{}') }}"
      summary.py: |
        import json
        
        try:
            route = json.load(open('route.json'))
        except:
            route = {}
        
        try:
            sync_result = json.load(open('sync_result.json'))
        except:
            sync_result = {}
        
        try:
            sync_back_result = json.load(open('sync_back_result.json'))
        except:
            sync_back_result = {}
        
        try:
            duplicates = json.load(open('duplicates_report.json'))
        except:
            duplicates = {}
        
        try:
            integrity = json.load(open('integrity_check.json'))
        except:
            integrity = {}
        
        try:
            reconciliation = json.load(open('reconciliation_report.json'))
        except:
            reconciliation = {}
        
        # Extract optimization features
        optimization_features = {
            'batch_optimization': sync_result.get('batch_optimization', False),
            'data_transformation': sync_result.get('data_transformation_enabled', False),
            'environment': sync_result.get('environment', 'production'),
            'hubspot_environment': sync_result.get('hubspot_environment', 'production'),
            'cache_enabled': sync_result.get('cache_enabled', False),
            'parallel_processing': sync_result.get('parallel_processing', False),
            'currency_conversion': sync_result.get('currency_conversion_enabled', False),
            'change_history': sync_result.get('change_history_enabled', False),
            'custom_transformations': sync_result.get('custom_transformations', False)
        }
        
        total_errors = sync_result.get('errors', 0) + sync_back_result.get('errors', 0)
        has_duplicates = duplicates.get('duplicates_found', 0) > 0
        has_integrity_issues = integrity.get('issues_found', 0) > 0
        
        summary = {
            'sync_direction': route.get('sync_direction', 'unknown'),
            'trigger_type': route.get('trigger_type', 'unknown'),
            'crm_to_accounting': {
                'invoices_created': sync_result.get('created', 0),
                'invoices_skipped': sync_result.get('skipped', 0),
                'errors': sync_result.get('errors', 0),
                'throughput_per_second': sync_result.get('throughput_per_second', 0)
            },
            'accounting_to_crm': {
                'deals_updated': sync_back_result.get('updated_deals', 0),
                'payments_updated': sync_back_result.get('updated_payments', 0),
                'errors': sync_back_result.get('errors', 0)
            },
            'data_quality': {
                'duplicates_found': duplicates.get('duplicates_found', 0),
                'integrity_issues': integrity.get('issues_found', 0),
                'status': 'warning' if (has_duplicates or has_integrity_issues) else 'ok'
            },
            'reconciliation': {
                'reconciliations_applied': reconciliation.get('reconciliations_applied', 0),
                'status': reconciliation.get('status', 'not_run')
            },
            'optimization': optimization_features,
            'features': {
                'conflict_resolution': sync_result.get('conflict_resolution_strategy', 'last_write_wins'),
                'log_compression': sync_result.get('log_compression_enabled', False),
                'predictive_analysis': sync_result.get('predictive_analysis') is not None,
                'report_export': sync_result.get('report_export_enabled', False),
                'smart_alerts': sync_result.get('smart_alerts_enabled', False),
                'schema_version': sync_result.get('schema_version', 'v1')
            },
            'status': 'success' if total_errors == 0 and not has_duplicates and not has_integrity_issues else 'partial'
        }
        
        with open('summary.json', 'w') as f:
            json.dump(summary, f, indent=2)
        
        print("=" * 50)
        print("SYNC SUMMARY")
        print("=" * 50)
        print(f"Direction: {summary['sync_direction']}")
        print(f"Trigger: {summary['trigger_type']}")
        print(f"\nCRM → Accounting:")
        print(f"  Invoices created: {summary['crm_to_accounting']['invoices_created']}")
        print(f"  Invoices skipped: {summary['crm_to_accounting']['invoices_skipped']}")
        print(f"  Errors: {summary['crm_to_accounting']['errors']}")
        print(f"\nAccounting → CRM:")
        print(f"  Deals updated: {summary['accounting_to_crm']['deals_updated']}")
        print(f"  Payments updated: {summary['accounting_to_crm']['payments_updated']}")
        print(f"  Errors: {summary['accounting_to_crm']['errors']}")
        print(f"\nData Quality:")
        print(f"  Duplicates found: {summary['data_quality']['duplicates_found']}")
        print(f"  Integrity issues: {summary['data_quality']['integrity_issues']}")
        print(f"  Quality status: {summary['data_quality']['status']}")
        if 'reconciliation' in summary:
            print(f"\nReconciliation:")
            print(f"  Fixes applied: {summary['reconciliation'].get('reconciliations_applied', 0)}")
            print(f"  Status: {summary['reconciliation'].get('status', 'unknown')}")
        if 'optimization' in summary:
            print(f"\nOptimization:")
            print(f"  Batch optimization: {summary['optimization'].get('batch_optimization', False)}")
            print(f"  Data transformation: {summary['optimization'].get('data_transformation', False)}")
            print(f"  Environment: {summary['optimization'].get('environment', 'unknown')}")
            print(f"  Cache enabled: {summary['optimization'].get('cache_enabled', False)}")
            print(f"  Parallel processing: {summary['optimization'].get('parallel_processing', False)}")
        print(f"\nStatus: {summary['status'].upper()}")
        print("=" * 50)
    outputFiles:
      - summary.json

  - id: notify_slack
    type: io.kestra.plugin.core.http.Request
    disabled: "{{ inputs.slack_webhook_url is not defined }}"
    timeout: PT30S
    uri: "{{ inputs.slack_webhook_url }}"
    method: POST
    headers:
      Content-Type: application/json
    body: |
      {% set summary = (taskrun.outputs['generate_summary']['files']['summary.json'] | readFile | fromJson) %}
      {% set route = (taskrun.outputs['parse_trigger']['files']['route.json'] | readFile | fromJson) %}
      {
        "text": "🔄 Sincronización CRM-Contabilidad completada",
        "blocks": [
          {
            "type": "header",
            "text": {
              "type": "plain_text",
              "text": "🔄 Sincronización CRM-Contabilidad"
            }
          },
          {
            "type": "section",
            "fields": [
              {
                "type": "mrkdwn",
                "text": "*Dirección:*\n{{ summary.sync_direction }}"
              },
              {
                "type": "mrkdwn",
                "text": "*Trigger:*\n{{ summary.trigger_type }}"
              },
              {
                "type": "mrkdwn",
                "text": "*Estado:*\n{{ summary.status | upper }}"
              },
              {
                "type": "mrkdwn",
                "text": "*Dry Run:*\n{{ route.dry_run | default(false) }}"
              }
            ]
          },
          {
            "type": "section",
            "text": {
              "type": "mrkdwn",
              "text": "*📥 CRM → Contabilidad*\n• Facturas creadas: {{ summary.crm_to_accounting.invoices_created }}\n• Facturas omitidas: {{ summary.crm_to_accounting.invoices_skipped }}\n• Errores: {{ summary.crm_to_accounting.errors }}"
            }
          },
          {
            "type": "section",
            "text": {
              "type": "mrkdwn",
              "text": "*📤 Contabilidad → CRM*\n• Deals actualizados: {{ summary.accounting_to_crm.deals_updated }}\n• Pagos actualizados: {{ summary.accounting_to_crm.payments_updated }}\n• Errores: {{ summary.accounting_to_crm.errors }}"
            }
          },
          {
            "type": "section",
            "text": {
              "type": "mrkdwn",
              "text": "*🔍 Calidad de Datos*\n• Duplicados: {{ summary.data_quality.duplicates_found }}\n• Problemas de integridad: {{ summary.data_quality.integrity_issues }}\n• Estado: {{ summary.data_quality.status | upper }}"
            }
          }
        ]
      }
    retry:
      type: exponential
      interval: PT5S
      maxAttempt: 2
      maxInterval: PT15S

  - id: performance_report
    type: io.kestra.plugin.scripts.python.Script
    timeout: PT30S
    inputFiles:
      perf_report.py: |
        import json, os
        from datetime import datetime
        
        try:
            sync_result = json.load(open('sync_result.json'))
        except:
            sync_result = {}
        
        try:
            sync_back_result = json.load(open('sync_back_result.json'))
        except:
            sync_back_result = {}
        
        try:
            integrity = json.load(open('integrity_check.json'))
        except:
            integrity = {}
        
        # Calculate performance metrics
        total_duration = sync_result.get('execution_duration_seconds', 0)
        invoices_created = sync_result.get('created', 0)
        deals_updated = sync_back_result.get('updated_deals', 0)
        
        performance = {
            'timestamp': datetime.utcnow().isoformat() + 'Z',
            'execution_metrics': {
                'total_duration_seconds': total_duration,
                'invoices_per_second': round(invoices_created / total_duration, 4) if total_duration > 0 else 0,
                'deals_updated_per_second': round(deals_updated / total_duration, 4) if total_duration > 0 else 0,
                'cache_enabled': sync_result.get('cache_enabled', False),
                'parallel_processing': sync_result.get('parallel_processing', False)
            },
            'integrity_metrics': {
                'invoices_checked': integrity.get('recent_invoices_checked', 0),
                'issues_found': integrity.get('issues_found', 0),
                'integrity_status': integrity.get('status', 'unknown')
            },
            'optimization': {
                'cache_enabled': sync_result.get('cache_enabled', False),
                'parallel_processing': sync_result.get('parallel_processing', False),
                'circuit_breaker_enabled': True,
                'rate_limiting_enabled': True
            }
        }
        
        with open('performance_report.json', 'w') as f:
            json.dump(performance, f, indent=2)
        
        print("=" * 60)
        print("PERFORMANCE REPORT")
        print("=" * 60)
        print(f"Duration: {total_duration:.2f}s")
        print(f"Invoices created: {invoices_created}")
        if total_duration > 0:
            print(f"Throughput: {performance['execution_metrics']['invoices_per_second']:.4f} invoices/sec")
        print(f"Cache enabled: {performance['optimization']['cache_enabled']}")
        print(f"Parallel processing: {performance['optimization']['parallel_processing']}")
        print(f"Integrity status: {performance['integrity_metrics']['integrity_status']}")
        print("=" * 60)
    inputFiles:
      sync_result.json: "{{ taskrun.outputs['sync_crm_to_accounting']['files']['sync_result.json'] | default('{}') }}"
      sync_back_result.json: "{{ taskrun.outputs['sync_accounting_to_crm']['files']['sync_back_result.json'] | default('{}') }}"
      integrity_check.json: "{{ taskrun.outputs['verify_integrity']['files']['integrity_check.json'] | default('{}') }}"
    outputFiles:
      - performance_report.json

  - id: export_report
    type: io.kestra.plugin.scripts.python.Script
    timeout: PT2M
    disabled: "{{ inputs.enable_report_export | default(true) != true }}"
    inputFiles:
      export_report.py: |
        import json, os, csv
        from datetime import datetime
        
        report_format = os.getenv('REPORT_EXPORT_FORMAT', 'json')
        
        try:
            summary = json.load(open('summary.json'))
        except:
            summary = {}
        
        try:
            sync_result = json.load(open('sync_result.json'))
        except:
            sync_result = {}
        
        try:
            performance = json.load(open('performance_report.json'))
        except:
            performance = {}
        
        timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
        
        if report_format == 'json':
            report_data = {
                'export_timestamp': datetime.utcnow().isoformat() + 'Z',
                'summary': summary,
                'sync_details': sync_result,
                'performance': performance
            }
            with open(f'sync_report_{timestamp}.json', 'w') as f:
                json.dump(report_data, f, indent=2, default=str)
            print(f"Report exported to sync_report_{timestamp}.json")
            
        elif report_format == 'csv':
            with open(f'sync_report_{timestamp}.csv', 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(['Metric', 'Value'])
                writer.writerow(['Export Timestamp', datetime.utcnow().isoformat()])
                writer.writerow(['Invoices Created', sync_result.get('created', 0)])
                writer.writerow(['Invoices Skipped', sync_result.get('skipped', 0)])
                writer.writerow(['Total Errors', sync_result.get('errors', 0)])
                writer.writerow(['Success Rate', f"{sync_result.get('success_rate', 0)}%"])
                writer.writerow(['Execution Duration', f"{sync_result.get('execution_duration_seconds', 0)}s"])
                writer.writerow(['Throughput', f"{sync_result.get('throughput_per_second', 0)}/s"])
            print(f"Report exported to sync_report_{timestamp}.csv")
            
        elif report_format == 'excel':
            try:
                import pandas as pd
                df = pd.DataFrame([{
                    'timestamp': datetime.utcnow().isoformat(),
                    'invoices_created': sync_result.get('created', 0),
                    'invoices_skipped': sync_result.get('skipped', 0),
                    'errors': sync_result.get('errors', 0),
                    'success_rate': sync_result.get('success_rate', 0),
                    'duration_seconds': sync_result.get('execution_duration_seconds', 0),
                    'throughput_per_second': sync_result.get('throughput_per_second', 0)
                }])
                df.to_excel(f'sync_report_{timestamp}.xlsx', index=False)
                print(f"Report exported to sync_report_{timestamp}.xlsx")
            except ImportError:
                print("WARNING: pandas not available, falling back to CSV")
                report_format = 'csv'
        
        result = {
            'exported': True,
            'format': report_format,
            'filename': f'sync_report_{timestamp}.{report_format if report_format != "excel" else "xlsx"}',
            'timestamp': datetime.utcnow().isoformat() + 'Z'
        }
        
        with open('export_result.json', 'w') as f:
            json.dump(result, f, indent=2)
    inputFiles:
      summary.json: "{{ taskrun.outputs['generate_summary']['files']['summary.json'] }}"
      sync_result.json: "{{ taskrun.outputs['sync_crm_to_accounting']['files']['sync_result.json'] | default('{}') }}"
      performance_report.json: "{{ taskrun.outputs['performance_report']['files']['performance_report.json'] | default('{}') }}"
    env:
      REPORT_EXPORT_FORMAT: "{{ inputs.report_export_format | default('json') }}"
    outputFiles:
      - export_result.json
      - "sync_report_*.json"
      - "sync_report_*.csv"
      - "sync_report_*.xlsx"

  - id: smart_alerts
    type: io.kestra.plugin.scripts.python.Script
    timeout: PT1M
    disabled: "{{ inputs.enable_smart_alerts | default(true) != true }}"
    inputFiles:
      smart_alerts.py: |
        import json, os
        from datetime import datetime
        
        try:
            summary = json.load(open('summary.json'))
        except:
            summary = {}
        
        try:
            sync_result = json.load(open('sync_result.json'))
        except:
            sync_result = {}
        
        try:
            integrity = json.load(open('integrity_check.json'))
        except:
            integrity = {}
        
        alerts = []
        alert_level = 'info'
        
        # Smart alert: High error rate
        total_processed = sync_result.get('total_processed', 0)
        errors = sync_result.get('errors', 0)
        if total_processed > 0:
            error_rate = (errors / total_processed) * 100
            if error_rate > 20:
                alerts.append({
                    'type': 'high_error_rate',
                    'level': 'critical',
                    'message': f"Error rate is {error_rate:.1f}% (threshold: 20%)",
                    'value': error_rate,
                    'threshold': 20
                })
                alert_level = 'critical'
            elif error_rate > 10:
                alerts.append({
                    'type': 'high_error_rate',
                    'level': 'warning',
                    'message': f"Error rate is {error_rate:.1f}% (threshold: 10%)",
                    'value': error_rate,
                    'threshold': 10
                })
                if alert_level != 'critical':
                    alert_level = 'warning'
        
        # Smart alert: Integrity issues
        integrity_issues = integrity.get('issues_found', 0)
        if integrity_issues > 5:
            alerts.append({
                'type': 'integrity_issues',
                'level': 'warning',
                'message': f"{integrity_issues} integrity issues detected",
                'value': integrity_issues,
                'threshold': 5
            })
            if alert_level != 'critical':
                alert_level = 'warning'
        
        # Smart alert: Low throughput
        throughput = sync_result.get('throughput_per_second', 0)
        if throughput < 0.1 and sync_result.get('created', 0) > 0:
            alerts.append({
                'type': 'low_throughput',
                'level': 'warning',
                'message': f"Throughput is low: {throughput:.2f} invoices/sec",
                'value': throughput,
                'threshold': 0.1
            })
            if alert_level != 'critical':
                alert_level = 'warning'
        
        # Smart alert: Predictive analysis anomalies
        predictive = sync_result.get('predictive_analysis', {})
        if predictive and predictive.get('anomalies'):
            anomaly_count = len(predictive['anomalies'])
            if anomaly_count > 3:
                alerts.append({
                    'type': 'anomalies_detected',
                    'level': 'info',
                    'message': f"{anomaly_count} anomalies detected in invoice patterns",
                    'value': anomaly_count,
                    'threshold': 3
                })
        
        result = {
            'alert_level': alert_level,
            'alerts_count': len(alerts),
            'alerts': alerts,
            'timestamp': datetime.utcnow().isoformat() + 'Z'
        }
        
        with open('smart_alerts.json', 'w') as f:
            json.dump(result, f, indent=2)
        
        if alerts:
            print(f"Smart alerts generated: {len(alerts)} alerts at {alert_level} level")
            for alert in alerts:
                print(f"  [{alert['level'].upper()}] {alert['message']}")
        else:
            print("No alerts triggered")
    inputFiles:
      summary.json: "{{ taskrun.outputs['generate_summary']['files']['summary.json'] }}"
      sync_result.json: "{{ taskrun.outputs['sync_crm_to_accounting']['files']['sync_result.json'] | default('{}') }}"
      integrity_check.json: "{{ taskrun.outputs['verify_integrity']['files']['integrity_check.json'] | default('{}') }}"
    outputFiles:
      - smart_alerts.json

  - id: log_metrics
    type: io.kestra.plugin.scripts.python.Script
    timeout: PT30S
    inputFiles:
      metrics.py: |
        import json, os
        from datetime import datetime
        
        try:
            summary = json.load(open('summary.json'))
        except:
            summary = {}
        
        metrics = {
            'timestamp': datetime.utcnow().isoformat() + 'Z',
            'sync_direction': summary.get('sync_direction', 'unknown'),
            'trigger_type': summary.get('trigger_type', 'unknown'),
            'status': summary.get('status', 'unknown'),
            'crm_to_accounting': {
                'invoices_created': summary.get('crm_to_accounting', {}).get('invoices_created', 0),
                'invoices_skipped': summary.get('crm_to_accounting', {}).get('invoices_skipped', 0),
                'errors': summary.get('crm_to_accounting', {}).get('errors', 0)
            },
            'accounting_to_crm': {
                'deals_updated': summary.get('accounting_to_crm', {}).get('deals_updated', 0),
                'payments_updated': summary.get('accounting_to_crm', {}).get('payments_updated', 0),
                'errors': summary.get('accounting_to_crm', {}).get('errors', 0)
            }
        }
        
        # Output structured metrics (for Prometheus/observability)
        print(json.dumps({
            'type': 'metric',
            'name': 'crm_accounting_sync',
            'value': 1,
            'labels': {
                'direction': metrics['sync_direction'],
                'status': metrics['status'],
                'trigger': metrics['trigger_type']
            },
            'data': metrics
        }, indent=2))
        
        # Print human-readable summary
        print(f"\n📊 METRICS SUMMARY")
        print(f"  Sync Direction: {metrics['sync_direction']}")
        print(f"  Status: {metrics['status']}")
        print(f"  Invoices Created: {metrics['crm_to_accounting']['invoices_created']}")
        print(f"  Deals Updated: {metrics['accounting_to_crm']['deals_updated']}")
        print(f"  Total Errors: {metrics['crm_to_accounting']['errors'] + metrics['accounting_to_crm']['errors']}")
        
        with open('metrics.json', 'w') as f:
            json.dump(metrics, f, indent=2)
    inputFiles:
      summary.json: "{{ taskrun.outputs['generate_summary']['files']['summary.json'] }}"
    outputFiles:
      - metrics.json

  - id: verify_integrity
    type: io.kestra.plugin.scripts.python.Script
    timeout: PT2M
    inputFiles:
      integrity.py: |
        import json, os, requests, psycopg2, sys, logging
        from datetime import datetime
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        
        hb_token = os.getenv('HUBSPOT_TOKEN')
        jdbc = os.getenv('JDBC_URL').replace('jdbc:postgresql://', 'postgresql://')
        if '@' not in jdbc:
            jdbc = jdbc.replace('postgresql://', f'postgresql://{os.getenv("JDBC_USER")}:{os.getenv("JDBC_PASSWORD")}@')
        
        integrity_issues = []
        
        try:
            conn = psycopg2.connect(jdbc)
            cur = conn.cursor()
            
            # Verify invoices have corresponding deals in HubSpot
            # Optimized query with proper indexes
            cur.execute("""
                SELECT i.id, i.serie, i.hubspot_deal_id, i.total, i.status, i.created_at
                FROM invoices i
                WHERE i.hubspot_deal_id IS NOT NULL
                AND i.created_at >= NOW() - INTERVAL '7 days'
                ORDER BY i.created_at DESC
                LIMIT 50
            """)
            recent_invoices = cur.fetchall()
            
            if hb_token and recent_invoices:
                headers = {'Authorization': f'Bearer {hb_token}', 'Content-Type': 'application/json'}
                url_base = os.getenv('HUBSPOT_BASE', 'https://api.hubapi.com')
                
                for inv in recent_invoices:
                    inv_id, serie, deal_id, total, status = inv
                    
                    try:
                        # Verify deal exists and has matching invoice_id
                        deal_url = f"{url_base}/crm/v3/objects/deals/{deal_id}"
                        r = requests.get(deal_url, headers=headers, params={'properties': 'invoice_id,invoice_number,amount'}, timeout=10)
                        
                        if r.status_code == 404:
                            integrity_issues.append({
                                'type': 'missing_deal',
                                'invoice_id': inv_id,
                                'invoice_serie': serie,
                                'deal_id': deal_id,
                                'severity': 'high'
                            })
                        elif r.status_code == 200:
                            deal_data = r.json()
                            props = deal_data.get('properties', {})
                            
                            # Verify invoice_id matches
                            if props.get('invoice_id') != str(inv_id):
                                integrity_issues.append({
                                    'type': 'mismatched_invoice_id',
                                    'invoice_id': inv_id,
                                    'deal_invoice_id': props.get('invoice_id'),
                                    'severity': 'medium'
                                })
                            
                            # Verify amounts match (with tolerance)
                            deal_amount = float(props.get('amount', '0').replace(',', '').replace('$', '') or '0')
                            if abs(deal_amount - float(total)) > 0.01:
                                integrity_issues.append({
                                    'type': 'amount_mismatch',
                                    'invoice_id': inv_id,
                                    'invoice_amount': float(total),
                                    'deal_amount': deal_amount,
                                    'difference': abs(deal_amount - float(total)),
                                    'severity': 'medium'
                                })
                    except Exception as e:
                        logger.warning(f"Failed to verify deal {deal_id}: {e}")
                        integrity_issues.append({
                            'type': 'verification_error',
                            'deal_id': deal_id,
                            'error': str(e),
                            'severity': 'low'
                        })
            
            # Verify sync tracking consistency
            cur.execute("""
                SELECT COUNT(*) FROM crm_accounting_sync
                WHERE sync_direction = 'crm_to_accounting'
                AND synced_at >= NOW() - INTERVAL '24 hours'
            """)
            sync_count = cur.fetchone()[0]
            
            result = {
                'integrity_check_time': datetime.utcnow().isoformat() + 'Z',
                'recent_invoices_checked': len(recent_invoices),
                'issues_found': len(integrity_issues),
                'sync_count_24h': sync_count,
                'issues': integrity_issues[:20],
                'status': 'warning' if integrity_issues else 'ok'
            }
            
            with open('integrity_check.json', 'w') as f:
                json.dump(result, f, indent=2, default=str)
            
            if integrity_issues:
                logger.warning(f"Integrity check found {len(integrity_issues)} issues")
            else:
                logger.info("Integrity check passed - no issues found")
                
        except Exception as e:
            logger.error(f"Integrity check failed: {e}")
            result = {'status': 'error', 'error': str(e)}
            with open('integrity_check.json', 'w') as f:
                json.dump(result, f, indent=2)
        finally:
            if 'cur' in locals():
                cur.close()
            if 'conn' in locals():
                conn.close()
    env:
      HUBSPOT_TOKEN: "{{ inputs.hubspot_token }}"
      JDBC_URL: "{{ inputs.jdbc_url }}"
      JDBC_USER: "{{ inputs.jdbc_user }}"
      JDBC_PASSWORD: "{{ inputs.jdbc_password }}"
      HUBSPOT_BASE: "{{ vars.hubspot_base }}"
    outputFiles:
      - integrity_check.json

  - id: audit_log
    type: io.kestra.plugin.scripts.python.Script
    timeout: PT1M
    inputFiles:
      audit.py: |
        import json, os, psycopg2, sys, logging
        from datetime import datetime
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        
        try:
            summary = json.load(open('summary.json'))
            route = json.load(open('route.json'))
        except:
            summary = {}
            route = {}
        
        jdbc = os.getenv('JDBC_URL').replace('jdbc:postgresql://', 'postgresql://')
        if '@' not in jdbc:
            jdbc = jdbc.replace('postgresql://', f'postgresql://{os.getenv("JDBC_USER")}:{os.getenv("JDBC_PASSWORD")}@')
        
        try:
            conn = psycopg2.connect(jdbc)
            cur = conn.cursor()
            
            # Create audit log table
            cur.execute("""
                CREATE TABLE IF NOT EXISTS sync_audit_log (
                    id SERIAL PRIMARY KEY,
                    execution_id VARCHAR(128),
                    sync_direction VARCHAR(50),
                    trigger_type VARCHAR(50),
                    invoices_created INTEGER DEFAULT 0,
                    invoices_skipped INTEGER DEFAULT 0,
                    deals_updated INTEGER DEFAULT 0,
                    payments_updated INTEGER DEFAULT 0,
                    total_errors INTEGER DEFAULT 0,
                    status VARCHAR(20),
                    dry_run BOOLEAN DEFAULT FALSE,
                    execution_duration_seconds NUMERIC(10,2),
                    executed_at TIMESTAMPTZ DEFAULT NOW(),
                    metadata JSONB
                )
            """)
            cur.execute("CREATE INDEX IF NOT EXISTS idx_sync_audit_executed ON sync_audit_log(executed_at DESC)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_sync_audit_status ON sync_audit_log(status)")
            
            # Calculate execution duration (approximate)
            execution_duration = None
            try:
                start_time = datetime.fromisoformat("{{ execution.startDate }}".replace('Z', '+00:00'))
                end_time = datetime.utcnow()
                execution_duration = (end_time - start_time).total_seconds()
            except:
                pass
            
            metadata = {
                'crm_to_accounting': summary.get('crm_to_accounting', {}),
                'accounting_to_crm': summary.get('accounting_to_crm', {}),
                'trigger_type': route.get('trigger_type'),
                'sync_direction': route.get('sync_direction')
            }
            
            cur.execute("""
                INSERT INTO sync_audit_log (
                    execution_id, sync_direction, trigger_type,
                    invoices_created, invoices_skipped,
                    deals_updated, payments_updated, total_errors,
                    status, dry_run, execution_duration_seconds, metadata
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                "{{ execution.id }}",
                summary.get('sync_direction', 'unknown'),
                summary.get('trigger_type', 'unknown'),
                summary.get('crm_to_accounting', {}).get('invoices_created', 0),
                summary.get('crm_to_accounting', {}).get('invoices_skipped', 0),
                summary.get('accounting_to_crm', {}).get('deals_updated', 0),
                summary.get('accounting_to_crm', {}).get('payments_updated', 0),
                summary.get('crm_to_accounting', {}).get('errors', 0) + summary.get('accounting_to_crm', {}).get('errors', 0),
                summary.get('status', 'unknown'),
                route.get('dry_run', False),
                execution_duration,
                json.dumps(metadata)
            ))
            
            conn.commit()
            logger.info("Audit log entry created successfully")
            
        except Exception as e:
            logger.error(f"Failed to create audit log: {e}")
        finally:
            if 'cur' in locals():
                cur.close()
            if 'conn' in locals():
                conn.close()
    inputFiles:
      summary.json: "{{ taskrun.outputs['generate_summary']['files']['summary.json'] }}"
      route.json: "{{ taskrun.outputs['parse_trigger']['files']['route.json'] }}"
    env:
      JDBC_URL: "{{ inputs.jdbc_url }}"
      JDBC_USER: "{{ inputs.jdbc_user }}"
      JDBC_PASSWORD: "{{ inputs.jdbc_password }}"
    outputFiles:
      - audit_result.json

  - id: alert_on_issues
    type: io.kestra.plugin.core.http.Request
    disabled: "{{ inputs.slack_webhook_url is not defined or (taskrun.outputs['verify_integrity']['files']['integrity_check.json'] | readFile | fromJson).status != 'warning' }}"
    timeout: PT30S
    uri: "{{ inputs.slack_webhook_url }}"
    method: POST
    headers:
      Content-Type: application/json
    body: |
      {% set integrity = (taskrun.outputs['verify_integrity']['files']['integrity_check.json'] | readFile | fromJson) %}
      {
        "text": "⚠️ Alertas de Integridad - Sincronización CRM-Contabilidad",
        "blocks": [
          {
            "type": "header",
            "text": {
              "type": "plain_text",
              "text": "⚠️ Alertas de Integridad"
            }
          },
          {
            "type": "section",
            "text": {
              "type": "mrkdwn",
              "text": "Se encontraron *{{ integrity.issues_found }}* problemas de integridad en la sincronización:\n\n{% for issue in integrity.issues[:5] %}\n• *{{ issue.type }}* ({{ issue.severity }}): {{ issue | toJson }}\n{% endfor %}"
            }
          }
        ]
      }
    retry:
      type: exponential
      interval: PT5S
      maxAttempt: 2

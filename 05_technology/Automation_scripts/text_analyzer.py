#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import re
from collections import Counter
from datetime import datetime
import json

def analyze_text():
    """Analiza el texto de Bioclones y genera estadÃ­sticas detalladas"""
    
    # Texto completo de Bioclones
    text = """
    Era la primera vez que visitÃ¡bamos una de las donde se clonaban cultivos. Una restricciÃ³n de DNA â€“ Francisco me hizo usar Ã¡rea restringida para personas con sus expertises muy dedicados a la genÃ©tica con alto acceso en el organismo. Con grandes pilares de color azul, minimalista y apuntalado para generar mucha altura vi la entrada del gran complejo. Nuestras credenciales fueron actualizadas de inmediato. A nuestra llegada nos esperaba Sophie, amiga de Roger, siempre me pareciÃ³ muy atractiva e inteligente. Â¡Bienvenida a la Capital BiolÃ³gica! â€“ exclamÃ³ con gran calidez, aunque suele tener lenguaje no verbal para expresar su entusiasmo. EstÃ¡bamos con credenciales para la razÃ³n y analÃ­tica, como consecuencia de que las clÃ¡sicas acerca de un tÃ³pico en lo especÃ­fico. Pero eso fue muy breve.

    No traigo valijas y maletas y una misiÃ³n de colas a hacer a nueva investigaciÃ³n. Las diferentes salas que parecen no tener fin. Con una curiosidad y emociÃ³n, caminÃ© como la de un niÃ±o en una jugueterÃ­a, recorrimos las diferentes Ã¡reas del complejo. Sophie continuaba muy bien en sus explicaciones.

    Mientras me distraÃ­a observando los trajes blancos radiactivos del personal, dejamos nuestro espacio reducido de trabajo, un lugar lleno de servidumbre a donde pones la mirada. Negros, blancos, verdes, altos, bajos, nuevos, viejos. Los tÃ©cnicos de todo tipo de seguridad. Completa y racional. Un solo error que fue con su sistema, Don Entrevista, muy sofisticado. Esperando que Sophie tuviera la misma emociÃ³n que nosotros, solo dispuso su pronto despedida al llegar al lugar. Disfrutamos con el morbo de ver quÃ© hacÃ­an que hacÃ­an las otras Ã¡reas, rÃ¡pido. Descargamos las nuevas actualizaciones a la fuente matriz del sistema paladiÃ¡nico y metÃ³dico, trabajando hasta casi nueve de la noche. Se sorprenden llegando al final de la jornada a sus martes, recordando que el complejo tenÃ­a los mejores salarios y ventajas competitivas de las sesiones extracurriculares que ponÃ­an muy celosos al personal, como la secreta a veces todos querÃ­an. Todos aluden en su contrato: G. R. E. E.

    Cuando me imaginaba un salario diez veces mÃ¡s grande, y lo multiplicaba por aÃ±os que me faltaban hasta la esperanza de vida, esto sin gastar en nada, tenÃ­a un total en activos que me llevaban a una fantasÃ­a de ser alguien con dinero.

    El murmullo terminÃ³ en la sala donde se rumoraba la mayorÃ­a de los antibombas que nos protegÃ­an. PasÃ³ mÃ¡s rÃ¡pido el protocolo que a mi mente no le dio mÃ¡s oportunidades de rumorear mis pensamientos filosÃ³ficos â€“ existencialistas. Toda esta noche no pude dormir, todo evocaba a la Eternidad. Todo fue un compartido sentimiento compartido â€“ todo fue todo y a la vez nada.

    Los accesos que habÃ­an pasado parecÃ­a que era pequeÃ±o para la plataforma, porque no habÃ­an tenido Ã­ndices en dÃ­as. Los bancos y la lÃ­nea educativa, tenÃ­a unos acuerdos y era un mito. La realidad era que fue la primera vez que pasaba en una misiÃ³n extraordinadaria. Una lucha constante y la guerra era en la Tierra no en la especie. Se pasaba todos los dÃ­as ahÃ­, hasta las aplicaciones donde crecÃ­a de lo sucedido.

    Anthony toda la semana estuvo callado y sin melancolÃ­a como si alguien lo hubiera notado su lesiÃ³n. Por la primera vez que sentÃ­a simpatÃ­a por un clon. Siempre tuve la idea de que ellos eran mÃ¡s neutrales que yo. Todo pasa por algo y ese algo crea suerte en una carisma, inclusive existe muchos cerebros en los medios secretos de lo sucedido. â€“ Â¡Tengo miedo de regresar a la Tierra! â€“ exclamÃ³ Anthony.

    â€“ Creo que todos â€“ respondiÃ³, mientras observaba su cara blanca, pÃ¡lida, que reflejaba un miedo muy genuino.

    Nuestras voluntades como profesionales extra planetarios iba hacia objetivos cada vez mÃ¡s extraordinarios, buscaban el origen. Nuestra economÃ­a biolÃ³gica y acercarnos.

    ValÃ­a cada vez mÃ¡s para algÃºn dÃ­a emprender algo que nos acaudalara los bolsillos y no de nuestro pase.

    De salida a plantas mÃ¡s pacÃ­ficas, donde la lucha de clases lo encuentra en una balanza mÃ¡s neutral. Otro de nuestros antepasados mÃ¡s formados, radicaba en cuervos, expresados para vivir el suceso de que era ganador exitoso que ayuda a protestar la economÃ­a. Lo encuentra esa una volatil donde que los ataques nucleares se expandieron alrededor del glÃºteo.

    Todo apuntaba a un sube y baja constante. Lo que terminamos lo gastamos rÃ¡pido. Dado el costo de vida de, sumado a los salarios nos autodestruimos fugazmente. No sÃ© si la depresiÃ³n era constante pero lo positivo y la superaciÃ³n personal se convirtiÃ³ en una especie de culto.

    Donde un se sentÃ­a en plenitud cuando estamos junto personas muy positivas.

    Antes de terminar, nuestra bienvenida a Anthony recibiÃ³ una gratificaciÃ³n alarmante y muy poco frecuente. Pronto nos encontrÃ¡bamos en la Ã¡rea de BiotecnologÃ­a donde se reforzaban los experimentos de DNA â€“ Francisco y Creo cuando vemos los â€“ hobbies como un trabajo, cambia mucho la forma de hacer las cosas. Siempre que nuestros horarios se extendÃ­an nos disgustaba, pero ahora lo vimos como algo de primera instancia.

    Sophie y yo tenÃ­amos muchas cosas en comÃºn tales que siempre salÃ­an a brillar en las conversaciones que tenÃ­amos. Todo lo que yo pensaba ella lo adivinaba con mucha naturalidad que a veces me sorprendÃ­a. No habÃ­amos pasado mucho tiempo juntos y sentÃ­a como me enamoraba dÃ­a a dÃ­a.

    Nuestra idea de estar juntos todo el tiempo no salÃ­a si solo era lo que yo querÃ­a y sentÃ­a, pero observaba cÃ³mo su sonrisa era mÃ¡s natural y genuina al verme. Era como la mÃºsica. Se sentÃ­a en la dÃ©cada que fue compuesta la nota musical. Es decir eso ya pasÃ³ de moda y suena antigua. Era de esos tipos de pensamiento emociÃ³n gozosos con un rastro de melancolÃ­a.

    PodrÃ­a ser por rabia, ahora quiero decir. Ahora que no estoy en el mismo lugar, tengo una idea clara. Al dÃ­a que llegamos despuÃ©s de la noche y al no haber nadie que estuviÃ©ramos Descansando, creo que el mundo es sucio.

    Ojos que parecen adivinar. Los habÃ­a asistido sobre ahora, segÃºn se va o viene para el que...

    SentÃ­ resbalarse en mis pies como las manos.
    Esos que ya no vigÃ­as, como cansados de tanto.
    SentÃ­ esa que va siendo carnal con el tiempo.
    Una maÃ±ana gris. No frÃ¡gil pero serÃ­s.
    Porque lo sabes, como son las mujeres.
    Yo creo que estÃ¡ bien, es este tiempo...

    â€“ Â¿Me quieres responder?
    â€“ Â¿QuÃ© es un pasado el amor?
    â€“ No lo sÃ©.

    ApartÃ³ su mirada de la mÃ­a, sus ojos lucÃ­an Ven y conocen a bordo de mÃ­.

    En dos segundos y micro segundos, recordÃ© todos mis apegos, desesperanzas y frustraciones de lo que me construyÃ³ como Ãngel hacia otros.

    Ella robÃ³ mi corazÃ³n a pasos mientras yo veÃ­a su clasismo reflejado en cÃ³mo trataba a los demÃ¡s, que asÃ­ me gustaba...

    No sabÃ­a que estaba encerrado hasta donde mÃ¡s sus defectos. Pero tratÃ© de llevar la conversaciÃ³n hacia un descanso, pero ella se retirÃ³ como voluntariamente lo habrÃ­a

    Tomaba el desayuno y la comida con ella en las semanas siguientes. Una y otra vuelta al mismo tema, a la misma conversaciÃ³n.
    """
    
    # AnÃ¡lisis bÃ¡sico
    words = re.findall(r'\b\w+\b', text.lower())
    sentences = re.split(r'[.!?]+', text)
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    
    # EstadÃ­sticas bÃ¡sicas
    stats = {
        'fecha_analisis': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'estadisticas_basicas': {
            'total_caracteres': len(text),
            'total_palabras': len(words),
            'total_oraciones': len([s for s in sentences if s.strip()]),
            'total_parrafos': len(paragraphs),
            'promedio_palabras_por_oracion': round(len(words) / len([s for s in sentences if s.strip()]), 2),
            'promedio_caracteres_por_palabra': round(len(text) / len(words), 2)
        },
        'palabras_mas_frecuentes': dict(Counter(words).most_common(20)),
        'palabras_unicas': len(set(words)),
        'densidad_lexica': round(len(set(words)) / len(words) * 100, 2)
    }
    
    # AnÃ¡lisis temÃ¡tico
    temas = {
        'ciencia_ficcion': ['clon', 'dna', 'genÃ©tica', 'biolÃ³gica', 'experimento', 'tecnologÃ­a'],
        'emociones': ['miedo', 'amor', 'melancolÃ­a', 'emociÃ³n', 'sentimiento', 'corazÃ³n'],
        'filosofia': ['eternidad', 'existencialista', 'filosÃ³fico', 'pensamiento', 'reflexiÃ³n'],
        'relaciones': ['sophie', 'anthony', 'francisco', 'roger', 'personas', 'juntos'],
        'lugar': ['tierra', 'complejo', 'sala', 'Ã¡rea', 'capital', 'plataforma']
    }
    
    tema_frecuencias = {}
    for tema, palabras_clave in temas.items():
        frecuencia = sum(words.count(palabra) for palabra in palabras_clave)
        tema_frecuencias[tema] = frecuencia
    
    stats['analisis_tematico'] = tema_frecuencias
    
    # AnÃ¡lisis de personajes
    personajes = {
        'Sophie': text.count('Sophie'),
        'Anthony': text.count('Anthony'),
        'Francisco': text.count('Francisco'),
        'Roger': text.count('Roger')
    }
    
    stats['personajes'] = personajes
    
    # AnÃ¡lisis de diÃ¡logos
    dialogos = re.findall(r'â€“[^â€“]+', text)
    stats['dialogos'] = {
        'total_dialogos': len(dialogos),
        'promedio_longitud_dialogo': round(sum(len(d) for d in dialogos) / len(dialogos), 2) if dialogos else 0
    }
    
    # AnÃ¡lisis de repeticiones
    repeticiones = [word for word, count in Counter(words).items() if count > 3]
    stats['repeticiones_significativas'] = repeticiones
    
    # Guardar anÃ¡lisis
    with open('analisis_texto_bioclones.json', 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    # Generar reporte
    reporte = f"""
# ğŸ“Š ANÃLISIS DE TEXTO - BIOCLONES

## ğŸ“ˆ EstadÃ­sticas BÃ¡sicas
- **Total de caracteres:** {stats['estadisticas_basicas']['total_caracteres']:,}
- **Total de palabras:** {stats['estadisticas_basicas']['total_palabras']:,}
- **Total de oraciones:** {stats['estadisticas_basicas']['total_oraciones']:,}
- **Total de pÃ¡rrafos:** {stats['estadisticas_basicas']['total_parrafos']:,}
- **Promedio palabras por oraciÃ³n:** {stats['estadisticas_basicas']['promedio_palabras_por_oracion']}
- **Promedio caracteres por palabra:** {stats['estadisticas_basicas']['promedio_caracteres_por_palabra']}
- **Palabras Ãºnicas:** {stats['palabras_unicas']:,}
- **Densidad lÃ©xica:** {stats['densidad_lexica']}%

## ğŸ”¤ Palabras MÃ¡s Frecuentes
"""
    
    for palabra, frecuencia in list(stats['palabras_mas_frecuentes'].items())[:10]:
        reporte += f"- **{palabra}:** {frecuencia} veces\n"
    
    reporte += f"""
## ğŸ­ AnÃ¡lisis TemÃ¡tico
"""
    
    for tema, frecuencia in stats['analisis_tematico'].items():
        reporte += f"- **{tema.replace('_', ' ').title()}:** {frecuencia} menciones\n"
    
    reporte += f"""
## ğŸ‘¥ Personajes
"""
    
    for personaje, apariciones in stats['personajes'].items():
        reporte += f"- **{personaje}:** {apariciones} menciones\n"
    
    reporte += f"""
## ğŸ’¬ DiÃ¡logos
- **Total de diÃ¡logos:** {stats['dialogos']['total_dialogos']}
- **Promedio longitud:** {stats['dialogos']['promedio_longitud_dialogo']} caracteres

## ğŸ”„ Repeticiones Significativas
"""
    
    for repeticion in stats['repeticiones_significativas'][:10]:
        reporte += f"- **{repeticion}**\n"
    
    reporte += f"""
---
*AnÃ¡lisis generado automÃ¡ticamente el {stats['fecha_analisis']}*
"""
    
    with open('reporte_analisis_texto.md', 'w', encoding='utf-8') as f:
        f.write(reporte)
    
    print("âœ… AnÃ¡lisis de texto completado exitosamente")
    print(f"ğŸ“Š EstadÃ­sticas generadas: {len(stats)} categorÃ­as")
    print(f"ğŸ“„ Reporte guardado: reporte_analisis_texto.md")
    print(f"ğŸ“‹ Datos JSON guardados: analisis_texto_bioclones.json")
    
    return stats

if __name__ == "__main__":
    analyze_text()




















# Arquitecturas y Ejemplos Avanzados de Airbyte

Este documento contiene diagramas de arquitectura y ejemplos avanzados de integraciÃ³n de Airbyte con la plataforma.

## ğŸ—ï¸ Arquitecturas Comunes

### 1. Pipeline Completo: Stripe â†’ S3 â†’ Databricks â†’ Snowflake

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Stripe    â”‚  (Source)
â”‚   API       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Airbyte Sync
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     S3      â”‚  (Data Lake)
â”‚  Parquet    â”‚  biz-datalake-dev/airbyte/stripe/
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Spark/Athena Query
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Databricks  â”‚  (Transform)
â”‚   Jobs      â”‚  (Ya configurado en tu plataforma)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Transformed Data
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Snowflake  â”‚  (Analytics)
â”‚  Warehouse  â”‚  (Para dashboards y BI)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ImplementaciÃ³n**:

```python
# data/airflow/dags/stripe_datalake_pipeline.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator
from data.airflow.dags.airbyte_sync import trigger_airbyte_sync

with DAG(
    dag_id="stripe_datalake_pipeline",
    schedule_interval=timedelta(hours=6),
    ...
) as dag:
    
    # 1. Sync Stripe a S3 (Airbyte)
    sync_to_s3 = PythonOperator(
        task_id="sync_stripe_to_s3",
        python_callable=trigger_airbyte_sync,
        op_kwargs={
            "connection_id": Variable.get("AIRBYTE_STRIPE_S3_CONNECTION_ID"),
        },
    )
    
    # 2. Procesar con Databricks
    process_databricks = DatabricksRunNowOperator(
        task_id="process_stripe_data",
        job_id=Variable.get("DATABRICKS_STRIPE_JOB_ID"),
        notebook_params={
            "s3_path": "s3://biz-datalake-dev/airbyte/stripe/",
            "output_path": "s3://biz-datalake-dev/processed/stripe/",
        },
    )
    
    # 3. Cargar a Snowflake (si aplica)
    load_to_snowflake = PythonOperator(
        task_id="load_to_snowflake",
        python_callable=load_processed_data_to_snowflake,
    )
    
    sync_to_s3 >> process_databricks >> load_to_snowflake
```

### 2. Multi-Source Consolidation: CRM + Payments â†’ Analytics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stripe  â”‚      â”‚ HubSpot  â”‚      â”‚PostgreSQLâ”‚
â”‚  (API)   â”‚      â”‚   (API)  â”‚      â”‚  (DB)    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚                 â”‚                 â”‚
     â”‚ Airbyte         â”‚ Airbyte         â”‚ Airbyte
     â”‚                 â”‚                 â”‚
     â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PostgreSQL Analytics                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚stripe_rawâ”‚  â”‚hubspot_  â”‚  â”‚etl_      â”‚ â”‚
â”‚  â”‚          â”‚  â”‚raw       â”‚  â”‚processed â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â”‚ Transform
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Snowflake                      â”‚
â”‚         (Data Warehouse)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Unified Analytics Tables            â”‚  â”‚
â”‚  â”‚  - customers_unified                 â”‚  â”‚
â”‚  â”‚  - revenue_analytics                 â”‚  â”‚
â”‚  â”‚  - sales_pipeline                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â”‚ BI Tools
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Grafana / Tableau   â”‚
        â”‚      Dashboards       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ImplementaciÃ³n**:

```python
# data/airflow/dags/multi_source_analytics.py
with DAG("multi_source_analytics", ...) as dag:
    
    # Sync mÃºltiples fuentes en paralelo
    stripe_sync = PythonOperator(
        task_id="sync_stripe",
        python_callable=trigger_airbyte_sync,
        op_kwargs={"connection_id": Variable.get("AIRBYTE_STRIPE_PG_CONNECTION_ID")},
    )
    
    hubspot_sync = PythonOperator(
        task_id="sync_hubspot",
        python_callable=trigger_airbyte_sync,
        op_kwargs={"connection_id": Variable.get("AIRBYTE_HUBSPOT_PG_CONNECTION_ID")},
    )
    
    pg_sync = PythonOperator(
        task_id="sync_postgres",
        python_callable=trigger_airbyte_sync,
        op_kwargs={"connection_id": Variable.get("AIRBYTE_PG_PG_CONNECTION_ID")},
    )
    
    # Esperar a que todas completen
    all_syncs = [stripe_sync, hubspot_sync, pg_sync]
    
    # Transformar y consolidar
    transform = PythonOperator(
        task_id="transform_and_consolidate",
        python_callable=transform_unified_data,
    )
    
    # Cargar a Snowflake
    load_snowflake = PythonOperator(
        task_id="load_to_snowflake",
        python_callable=load_to_snowflake,
    )
    
    all_syncs >> transform >> load_snowflake
```

### 3. Real-time CDC: PostgreSQL â†’ PostgreSQL (Multi-regiÃ³n)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL (Primary)          â”‚
â”‚  - Production Database         â”‚
â”‚  - WAL Logs Enabled            â”‚
â”‚  - Logical Replication Slot    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ CDC (Logical Replication)
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Airbyte Worker             â”‚
â”‚  - Reads from WAL               â”‚
â”‚  - Transforms if needed         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ Sync
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL (Replica)          â”‚
â”‚  - Analytics Database           â”‚
â”‚  - Read-only for BI tools       â”‚
â”‚  - Different Region             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ImplementaciÃ³n**:

```python
# data/airflow/dags/postgres_cdc_replication.py
with DAG("postgres_cdc_replication", ...) as dag:
    
    # CDC sync es continuo, solo verificamos que estÃ© corriendo
    check_cdc_health = PythonOperator(
        task_id="check_cdc_health",
        python_callable=check_postgres_cdc_status,
    )
    
    # Verificar lag de replicaciÃ³n
    check_replication_lag = PythonOperator(
        task_id="check_replication_lag",
        python_callable=check_replication_lag,
        op_kwargs={"max_lag_seconds": 300},  # 5 minutos mÃ¡ximo
    )
    
    check_cdc_health >> check_replication_lag
```

## ğŸ“Š Casos de Uso Avanzados

### Caso 1: ETL Completo con ValidaciÃ³n

```python
# data/airflow/dags/stripe_etl_with_validation.py
from data.airflow.dags.airbyte_sync import trigger_airbyte_sync, validate_sync_results
from airflow.operators.python import PythonOperator
from airflow.providers.great_expectations.operators.great_expectations import GreatExpectationsOperator

with DAG("stripe_etl_validated", ...) as dag:
    
    # 1. Sync desde Stripe
    sync = PythonOperator(
        task_id="sync_stripe",
        python_callable=trigger_airbyte_sync,
        op_kwargs={
            "connection_id": Variable.get("AIRBYTE_STRIPE_PG_CONNECTION_ID"),
            "validate_connection": True,
        },
    )
    
    # 2. Validar que se sincronizaron datos
    validate_records = PythonOperator(
        task_id="validate_min_records",
        python_callable=validate_sync_results,
        op_kwargs={"min_records": 100},
    )
    
    # 3. Validar calidad de datos con Great Expectations
    validate_quality = GreatExpectationsOperator(
        task_id="validate_data_quality",
        data_context_root_dir="/opt/airflow/gx",
        checkpoint_name="stripe_post_sync_checkpoint",
        fail_task_on_validation_failure=True,
    )
    
    # 4. Transformar datos
    transform = PythonOperator(
        task_id="transform_data",
        python_callable=transform_stripe_data,
    )
    
    # 5. Cargar a destino final
    load = PythonOperator(
        task_id="load_to_warehouse",
        python_callable=load_to_warehouse,
    )
    
    sync >> validate_records >> validate_quality >> transform >> load
```

### Caso 2: SincronizaciÃ³n Condicional Basada en Eventos

```python
# data/airflow/dags/event_driven_airbyte_sync.py
from airflow.operators.python import PythonOperator
from airflow.sensors.external_task import ExternalTaskSensor

def should_sync(**context):
    """Determina si debe sincronizar basado en eventos"""
    # Verificar si hay nuevos datos en source
    # Ejemplo: Verificar timestamp de Ãºltima actualizaciÃ³n
    last_sync = context['ti'].xcom_pull(key='last_sync_time')
    current_time = datetime.now()
    
    # Solo sync si han pasado mÃ¡s de 6 horas
    if (current_time - last_sync).total_seconds() > 21600:
        return True
    
    # O verificar si hay eventos nuevos
    # (ej: mensajes en Kafka, webhooks, etc.)
    return False

with DAG("event_driven_sync", schedule_interval=None, ...) as dag:
    
    # Esperar evento (ej: webhook de Stripe)
    wait_for_event = ExternalTaskSensor(
        task_id="wait_for_stripe_webhook",
        external_dag_id="stripe_webhook_processor",
        external_task_id="process_webhook",
        timeout=3600,
    )
    
    # Decidir si sync
    check_condition = PythonOperator(
        task_id="check_sync_condition",
        python_callable=should_sync,
    )
    
    # Sync si condiciÃ³n se cumple
    sync = PythonOperator(
        task_id="sync_if_needed",
        python_callable=trigger_airbyte_sync,
        op_kwargs={
            "connection_id": Variable.get("AIRBYTE_STRIPE_PG_CONNECTION_ID"),
        },
        trigger_rule="one_success",  # Ejecutar si condiciÃ³n es True
    )
    
    wait_for_event >> check_condition >> sync
```

### Caso 3: Pipeline con Retry y Fallback

```python
# data/airflow/dags/resilient_airbyte_sync.py
from airflow.operators.python import PythonOperator
from airflow.utils.trigger_rule import TriggerRule

def sync_with_fallback(**context):
    """Sync con fallback a mÃ©todo alternativo"""
    try:
        # Intentar sync principal
        return trigger_airbyte_sync(
            connection_id=Variable.get("AIRBYTE_STRIPE_PG_CONNECTION_ID"),
            task_instance=context['ti'],
        )
    except Exception as e:
        logger.warning(f"Primary sync failed: {e}, trying fallback")
        # Fallback: Sync desde backup o mÃ©todo alternativo
        return sync_from_backup(context['ti'])

with DAG("resilient_sync", ...) as dag:
    
    # Sync principal
    primary_sync = PythonOperator(
        task_id="primary_sync",
        python_callable=trigger_airbyte_sync,
        op_kwargs={
            "connection_id": Variable.get("AIRBYTE_STRIPE_PG_CONNECTION_ID"),
        },
        retries=3,
        retry_delay=timedelta(minutes=10),
    )
    
    # Fallback si falla
    fallback_sync = PythonOperator(
        task_id="fallback_sync",
        python_callable=sync_with_fallback,
        trigger_rule=TriggerRule.ALL_FAILED,  # Solo si primary falla
    )
    
    # ValidaciÃ³n final
    validate = PythonOperator(
        task_id="validate_sync",
        python_callable=validate_sync_results,
        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,
    )
    
    primary_sync >> fallback_sync >> validate
```

## ğŸ”„ Patterns de IntegraciÃ³n

### Pattern 1: Fan-out (Una fuente, mÃºltiples destinos)

```
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Stripe  â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
           â”‚
           â”‚ Airbyte
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”
    â”‚      â”‚      â”‚
    â–¼      â–¼      â–¼
â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
â”‚  S3 â”‚ â”‚  PG â”‚ â”‚  SF â”‚
â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜
```

**Uso**: Mismo dato en diferentes formatos para diferentes propÃ³sitos.

### Pattern 2: Fan-in (MÃºltiples fuentes, un destino)

```
â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
â”‚ S1  â”‚ â”‚ S2  â”‚ â”‚ S3  â”‚
â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜
   â”‚       â”‚       â”‚
   â””â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”˜
       â”‚       â”‚
       â–¼       â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Snowflake â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Uso**: Consolidar datos de mÃºltiples fuentes en un data warehouse.

### Pattern 3: Pipeline en Cadena

```
S1 â†’ D1 â†’ Transform â†’ D2 â†’ Analytics
```

**Uso**: Procesar datos en etapas con transformaciones intermedias.

## ğŸ“ˆ Performance Tuning

### OptimizaciÃ³n de Sincronizaciones Grandes

```python
# Para sincronizaciones muy grandes, usar configuraciÃ³n especÃ­fica
def optimized_sync(**context):
    hook = AirbyteHook(
        api_url=get_airbyte_api_url(),
        username=get_airbyte_credentials()[0],
        password=get_airbyte_credentials()[1],
        max_retries=5,  # MÃ¡s retries para syncs grandes
    )
    
    # Trigger sync con configuraciÃ³n especial
    job_info = hook.trigger_sync(
        connection_id=Variable.get("AIRBYTE_LARGE_SYNC_CONNECTION_ID"),
        retry_on_failure=True,
    )
    
    # Wait con timeout mÃ¡s largo
    return hook.wait_for_job_completion(
        job_id=job_info.get("jobId"),
        timeout_minutes=720,  # 12 horas para syncs muy grandes
        check_interval=60,  # Verificar cada minuto
    )
```

### ParallelizaciÃ³n de Sincronizaciones

```python
# Ejecutar mÃºltiples syncs en paralelo
with DAG("parallel_syncs", ...) as dag:
    
    syncs = []
    for source in ["stripe", "hubspot", "salesforce"]:
        sync = PythonOperator(
            task_id=f"sync_{source}",
            python_callable=trigger_airbyte_sync,
            op_kwargs={
                "connection_id": Variable.get(f"AIRBYTE_{source.upper()}_CONNECTION_ID"),
            },
        )
        syncs.append(sync)
    
    # Todas en paralelo
    # No hay dependencias entre ellas
```

## ğŸ” Seguridad en Integraciones

### Uso de External Secrets

```python
# Todas las credenciales vienen de External Secrets
# No hardcodear nunca en cÃ³digo

# En Airflow Variables (referencias a secrets):
AIRBYTE_API_URL = "http://airbyte-server.integration.svc.cluster.local:8000"
AIRBYTE_API_USERNAME = "airbyte"  # No sensible
AIRBYTE_API_PASSWORD = "{{ from_secret:airbyte/api-password }}"  # Desde External Secrets
```

### Network Isolation

```yaml
# Ya configurado en security/networkpolicies/airbyte.yaml
# Solo permite comunicaciÃ³n necesaria
```

---

**Ãšltima actualizaciÃ³n**: 2025-01-15  
**VersiÃ³n**: 1.0


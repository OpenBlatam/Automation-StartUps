# Top 10 Conectores M√°s √ötiles de Airbyte

Este documento lista los 10 conectores m√°s √∫tiles y populares de Airbyte, organizados por categor√≠a y con casos de uso espec√≠ficos para la plataforma.

**Versi√≥n**: 2.0  
**√öltima actualizaci√≥n**: 2025-01-15  
**Estado**: ‚úÖ Mejorado con configuraciones avanzadas y ejemplos pr√°cticos

## üìä Top 10 Conectores

### 1. **PostgreSQL** (Source & Destination)
**Tipo**: Database  
**Categor√≠a**: Base de datos relacional  
**Popularidad**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê  
**Complejidad**: Media

**Casos de Uso**:
- ‚úÖ Sincronizar datos entre bases de datos PostgreSQL
- ‚úÖ Migraci√≥n de datos entre entornos (dev ‚Üí staging ‚Üí prod)
- ‚úÖ Consolidar datos de m√∫ltiples fuentes en un data warehouse
- ‚úÖ Backup incremental autom√°tico
- ‚úÖ CDC (Change Data Capture) para sincronizaci√≥n en tiempo real
- ‚úÖ Multi-regi√≥n replication
- ‚úÖ Sincronizaci√≥n de esquemas espec√≠ficos

**Configuraci√≥n t√≠pica**:
```yaml
Source: PostgreSQL ‚Üí Destination: PostgreSQL/Snowflake/BigQuery
```

**Configuraci√≥n detallada**:

**Source (PostgreSQL)**:
```json
{
  "host": "postgres.example.com",
  "port": 5432,
  "database": "production_db",
  "schemas": ["public", "analytics"],
  "username": "airbyte_user",
  "password": "{{ from_external_secrets }}",
  "replication_method": {
    "method": "CDC",  // o "Standard" para full/incremental
    "replication_slot": "airbyte_slot",
    "publication": "airbyte_publication"
  },
  "ssl": true,
  "tunnel_method": null  // o SSH si requiere
}
```

**Destination (PostgreSQL)**:
```json
{
  "host": "warehouse.example.com",
  "port": 5432,
  "database": "data_warehouse",
  "schema": "staging",
  "username": "airbyte_user",
  "password": "{{ from_external_secrets }}",
  "ssl": true
}
```

**CDC Setup (Requisitos)**:
```sql
-- En PostgreSQL source
CREATE USER airbyte_user WITH REPLICATION PASSWORD 'secure_password';
GRANT SELECT ON ALL TABLES IN SCHEMA public TO airbyte_user;
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO airbyte_user;

-- Crear publicaci√≥n
CREATE PUBLICATION airbyte_publication FOR ALL TABLES;

-- Crear replication slot (Airbyte lo hace autom√°ticamente)
-- SELECT pg_create_logical_replication_slot('airbyte_slot', 'pgoutput');
```

**Ventajas**:
- ‚úÖ Soporte nativo para CDC (Logical Replication)
- ‚úÖ Sincronizaci√≥n incremental autom√°tica
- ‚úÖ Alta performance para grandes vol√∫menes (miles de tablas)
- ‚úÖ Bajo overhead en source database
- ‚úÖ Soporte para tipos de datos complejos (JSON, arrays, etc.)

**Limitaciones**:
- ‚ö†Ô∏è CDC requiere configuraci√≥n adicional en PostgreSQL
- ‚ö†Ô∏è Requiere permisos de replicaci√≥n
- ‚ö†Ô∏è Puede ser lento para full refresh en tablas muy grandes

**Recursos recomendados**:
- CPU: 2-4 cores para CDC
- Memoria: 4-8GB para workers
- Storage: Depende del volumen de datos

**Troubleshooting com√∫n**:
- **Error: "replication slot not found"**: Crear slot manualmente o verificar permisos
- **Error: "WAL retention"**: Aumentar `wal_keep_size` en PostgreSQL
- **Slow sync**: Verificar √≠ndices en source, usar CDC en lugar de full refresh

---

### 2. **Stripe** (Source)
**Tipo**: Payment Processing  
**Categor√≠a**: E-commerce / Finanzas  
**Popularidad**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê  
**Complejidad**: Baja

**Casos de Uso**:
- ‚úÖ Sincronizar pagos, suscripciones y clientes a data warehouse
- ‚úÖ Integraci√≥n con sistemas contables (QuickBooks, Xero) - **Ya implementado en tu plataforma**
- ‚úÖ An√°lisis de ingresos y m√©tricas de negocio
- ‚úÖ Reportes financieros automatizados
- ‚úÖ Detecci√≥n de anomal√≠as en pagos
- ‚úÖ Reconciliaci√≥n de pagos
- ‚úÖ An√°lisis de cohortes de suscripciones

**Datos sincronizados**:
- **Customers**: Informaci√≥n de clientes
- **Subscriptions**: Suscripciones activas y canceladas
- **Payments**: Pagos procesados (PaymentIntents)
- **Invoices**: Facturas emitidas
- **Charges**: Cargos individuales
- **Refunds**: Reembolsos
- **Disputes**: Disputas y chargebacks
- **Products**: Productos y servicios
- **Prices**: Precios y planes
- **Coupons**: Cupones y descuentos
- **Events**: Eventos de webhook (opcional)

**Configuraci√≥n t√≠pica**:
```yaml
Source: Stripe ‚Üí Destination: PostgreSQL/Snowflake/BigQuery
```

**Configuraci√≥n detallada**:

**Source (Stripe)**:
```json
{
  "client_secret": "sk_live_...",  // API Key desde External Secrets
  "account_id": null,  // Para cuentas conectadas (Connect)
  "start_date": "2024-01-01T00:00:00Z",  // Fecha inicial para sync
  "lookback_window_days": 0,  // D√≠as adicionales para lookback
  "slice_range": 365  // D√≠as por slice para sincronizaci√≥n
}
```

**Integraci√≥n con tu plataforma existente**:

Ya tienes integraci√≥n Stripe ‚Üí QuickBooks en `data/airflow/dags/stripe_product_to_quickbooks_item.py`. Puedes complementarla con Airbyte:

```python
# DAG mejorado: Airbyte sync + QuickBooks integration
from data.airflow.dags.airbyte_sync import trigger_airbyte_sync
from data.airflow.dags.stripe_product_to_quickbooks_item import sync_to_quickbooks

with DAG("stripe_airbyte_to_quickbooks", ...) as dag:
    # 1. Sync desde Stripe a PostgreSQL (Airbyte)
    airbyte_sync = PythonOperator(
        task_id="sync_stripe_to_postgres",
        python_callable=trigger_airbyte_sync,
        op_kwargs={
            "connection_id": Variable.get("AIRBYTE_STRIPE_POSTGRES_CONNECTION_ID"),
        },
    )
    
    # 2. Procesar y sincronizar a QuickBooks (tu DAG existente)
    quickbooks_sync = PythonOperator(
        task_id="sync_to_quickbooks",
        python_callable=sync_to_quickbooks,
        # ... tus par√°metros existentes
    )
    
    airbyte_sync >> quickbooks_sync
```

**Ventajas**:
- ‚úÖ API completa de Stripe (100+ endpoints)
- ‚úÖ Sincronizaci√≥n incremental por fecha (muy eficiente)
- ‚úÖ Soporte para m√∫ltiples modos de sincronizaci√≥n
- ‚úÖ Manejo autom√°tico de rate limits
- ‚úÖ Soporte para Stripe Connect (multi-account)
- ‚úÖ Sincronizaci√≥n de eventos hist√≥ricos

**Limitaciones**:
- ‚ö†Ô∏è Rate limits de Stripe (100 requests/segundo)
- ‚ö†Ô∏è Algunos datos pueden tardar en sincronizarse (disputes)
- ‚ö†Ô∏è Requiere API key con permisos apropiados

**Configuraci√≥n recomendada**:
- **Sync Frequency**: Cada 1-6 horas (depende del volumen)
- **Incremental Append**: Para pagos y transacciones
- **Full Refresh**: Para productos y precios (cambian menos)

**External Secrets** (ya configurado en tu plataforma):
```yaml
# security/secrets/externalsecrets-airbyte.yaml
- secretKey: stripe_api_key
  remoteRef:
    key: payments/stripe/api_key  # Ya existe en tu vault
```

**Troubleshooting com√∫n**:
- **Error: "Invalid API key"**: Verificar que la key tenga permisos de lectura
- **Error: "Rate limit exceeded"**: Reducir frecuencia de sync o usar lookback_window
- **Missing data**: Verificar que `start_date` no sea muy reciente

---

### 3. **HubSpot** (Source)
**Tipo**: CRM / Marketing Automation  
**Categor√≠a**: Sales & Marketing  
**Popularidad**: ‚≠ê‚≠ê‚≠ê‚≠ê  
**Complejidad**: Media

**Casos de Uso**:
- ‚úÖ Sincronizar contactos, deals y empresas a data warehouse
- ‚úÖ Integraci√≥n con sistemas de email marketing
- ‚úÖ An√°lisis de funnel de ventas
- ‚úÖ Reportes de ROI de marketing
- ‚úÖ Segmentaci√≥n de clientes
- ‚úÖ **Integraci√≥n con ManyChat** (ya tienes workflows en Kestra)
- ‚úÖ An√°lisis de engagement y conversi√≥n

**Datos sincronizados**:
- **Contacts**: Contactos con propiedades personalizadas
- **Companies**: Empresas y organizaciones
- **Deals**: Oportunidades de venta
- **Tickets**: Tickets de soporte
- **Products**: Productos
- **Line Items**: L√≠neas de productos
- **Quotes**: Cotizaciones
- **Engagements**: Emails, calls, meetings, notes, tasks
- **Marketing Emails**: Emails de marketing
- **Campaigns**: Campa√±as de marketing
- **Forms**: Formularios y submissions
- **Workflows**: Workflows de automatizaci√≥n

**Configuraci√≥n t√≠pica**:
```yaml
Source: HubSpot ‚Üí Destination: PostgreSQL/Snowflake/BigQuery
```

**Configuraci√≥n detallada**:

**Source (HubSpot)**:
```json
{
  "credentials": {
    "credentials_title": "API Key",
    "api_key": "{{ from_external_secrets }}"  // Ya configurado: crm/hubspot/token
  },
  "start_date": "2024-01-01T00:00:00Z",
  "credentials_title": "API Key"
}
```

**Integraci√≥n con tu plataforma existente**:

Ya tienes integraciones HubSpot en Kestra (`workflow/kestra/flows/hubspot_lead_to_manychat.yaml`). Complementa con Airbyte:

```python
# DAG: HubSpot sync + procesamiento
from data.airflow.dags.airbyte_sync import trigger_airbyte_sync

with DAG("hubspot_analytics_pipeline", ...) as dag:
    # 1. Sync desde HubSpot a PostgreSQL
    hubspot_sync = PythonOperator(
        task_id="sync_hubspot_to_postgres",
        python_callable=trigger_airbyte_sync,
        op_kwargs={
            "connection_id": Variable.get("AIRBYTE_HUBSPOT_POSTGRES_CONNECTION_ID"),
        },
    )
    
    # 2. Procesar datos para analytics (opcional)
    process_analytics = PythonOperator(
        task_id="process_hubspot_analytics",
        python_callable=process_hubspot_data,
    )
    
    hubspot_sync >> process_analytics
```

**Propiedades personalizadas** (ya usas en tu plataforma):
- `inter√©s_producto`: Producto de inter√©s
- `manychat_user_id`: ID de ManyChat
- Cualquier propiedad personalizada se sincroniza autom√°ticamente

**Ventajas**:
- ‚úÖ Acceso completo a API de HubSpot (todos los objetos)
- ‚úÖ Sincronizaci√≥n incremental eficiente
- ‚úÖ Soporte para objetos y propiedades personalizadas
- ‚úÖ Manejo autom√°tico de rate limits
- ‚úÖ Sincronizaci√≥n de relaciones (contact ‚Üí company ‚Üí deal)
- ‚úÖ Soporte para marketing analytics

**Limitaciones**:
- ‚ö†Ô∏è Rate limits estrictos (100 requests/10 segundos)
- ‚ö†Ô∏è Sincronizaci√≥n inicial puede ser lenta (miles de contactos)
- ‚ö†Ô∏è Algunos objetos requieren API espec√≠fica (engagements)

**Configuraci√≥n recomendada**:
- **Sync Frequency**: Cada 6-12 horas (depende de volumen)
- **Incremental Append**: Para contactos, deals, companies
- **Full Refresh**: Para productos y precios
- **Selective Sync**: Sincronizar solo streams necesarios para mejor performance

**External Secrets** (ya configurado):
```yaml
# Ya existe en tu plataforma:
# security/secrets/externalsecrets-hubspot-db.yaml
# crm/hubspot/token en AWS Secrets Manager
```

**Troubleshooting com√∫n**:
- **Error: "Rate limit exceeded"**: Aumentar intervalo entre syncs
- **Error: "Invalid API key"**: Verificar token en External Secrets
- **Missing custom properties**: Verificar que existan en HubSpot
- **Slow sync**: Reducir streams sincronizados o usar incremental

---

### 4. **Snowflake** (Destination)
**Tipo**: Data Warehouse  
**Categor√≠a**: Analytics  
**Popularidad**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê  
**Complejidad**: Media-Alta

**Casos de Uso**:
- ‚úÖ Consolidar datos de m√∫ltiples fuentes en Snowflake
- ‚úÖ Crear data lake estructurado
- ‚úÖ Alimentar dashboards y BI tools (Grafana, Tableau, Looker)
- ‚úÖ Preparar datos para ML/AI
- ‚úÖ Data warehouse unificado
- ‚úÖ An√°lisis de grandes vol√∫menes de datos hist√≥ricos
- ‚úÖ Compartir datos entre organizaciones (Data Sharing)

**Configuraci√≥n t√≠pica**:
```yaml
Source: Stripe/HubSpot/PostgreSQL ‚Üí Destination: Snowflake
```

**Configuraci√≥n detallada**:

**Destination (Snowflake)**:
```json
{
  "host": "xy12345.us-east-1.snowflakecomputing.com",
  "role": "AIRBYTE_ROLE",
  "warehouse": "AIRBYTE_WAREHOUSE",
  "database": "ANALYTICS",
  "schema": "STAGING",
  "username": "airbyte_user",
  "password": "{{ from_external_secrets }}",
  "jdbc_url_params": "?warehouse=AIRBYTE_WAREHOUSE&role=AIRBYTE_ROLE",
  "loading_method": {
    "method": "S3 Staging",  // o "Internal Staging"
    "s3_bucket_name": "airbyte-staging",
    "s3_bucket_region": "us-east-1",
    "access_key_id": "{{ from_external_secrets }}",
    "secret_access_key": "{{ from_external_secrets }}"
  }
}
```

**Setup en Snowflake**:
```sql
-- Crear usuario y rol
CREATE USER airbyte_user PASSWORD='secure_password';
CREATE ROLE airbyte_role;
GRANT ROLE airbyte_role TO USER airbyte_user;

-- Crear warehouse
CREATE WAREHOUSE airbyte_warehouse
  WITH WAREHOUSE_SIZE = 'XSMALL'
  AUTO_SUSPEND = 60
  AUTO_RESUME = TRUE;

-- Dar permisos
GRANT USAGE ON WAREHOUSE airbyte_warehouse TO ROLE airbyte_role;
GRANT CREATE DATABASE ON ACCOUNT TO ROLE airbyte_role;
GRANT CREATE SCHEMA ON DATABASE ANALYTICS TO ROLE airbyte_role;

-- Crear schema staging
USE DATABASE ANALYTICS;
CREATE SCHEMA IF NOT EXISTS STAGING;
GRANT ALL ON SCHEMA STAGING TO ROLE airbyte_role;
```

**Optimizaci√≥n de Performance**:

1. **Clustering**:
```sql
-- Crear tabla con clustering autom√°tico
CREATE TABLE stripe_customers (
  id VARCHAR,
  created TIMESTAMP,
  email VARCHAR,
  ...
) CLUSTER BY (created);
```

2. **File Format Optimization**:
- Usar Parquet para mejor compresi√≥n
- Particionar por fecha para queries m√°s r√°pidas
- Usar VARIANT para JSON flexible

3. **Warehouse Sizing**:
- XSMALL para desarrollo
- SMALL/MEDIUM para producci√≥n
- Multi-cluster para alta concurrencia

**Ventajas**:
- ‚úÖ Escalabilidad infinita (separaci√≥n compute/storage)
- ‚úÖ Particionamiento autom√°tico
- ‚úÖ Soporte para m√∫ltiples formatos (JSON, Parquet, CSV)
- ‚úÖ Clustering autom√°tico
- ‚úÖ Time Travel (historial de datos)
- ‚úÖ Zero-copy cloning (copias instant√°neas)
- ‚úÖ Data Sharing entre cuentas

**Limitaciones**:
- ‚ö†Ô∏è Costos pueden ser altos con mucho compute
- ‚ö†Ô∏è Requiere configuraci√≥n de staging (S3 o interno)
- ‚ö†Ô∏è Setup inicial m√°s complejo que otros destinos

**Costos estimados**:
- Storage: ~$40/TB/mes
- Compute: Basado en warehouse size y tiempo de uso
- **Tip**: Usar auto-suspend para ahorrar costos

**Troubleshooting com√∫n**:
- **Error: "Warehouse not found"**: Verificar que el warehouse existe y est√° activo
- **Slow syncs**: Aumentar warehouse size o usar multi-cluster
- **S3 staging errors**: Verificar permisos de S3 y credenciales

---

### 5. **Google Sheets** (Source)
**Tipo**: Spreadsheet  
**Categor√≠a**: Colaboraci√≥n  
**Popularidad**: ‚≠ê‚≠ê‚≠ê‚≠ê  
**Complejidad**: Baja

**Casos de Uso**:
- ‚úÖ Sincronizar datos de hojas de c√°lculo a bases de datos
- ‚úÖ Automatizar reportes manuales
- ‚úÖ Integrar datos de equipos no t√©cnicos
- ‚úÖ Migraci√≥n de datos desde Excel/Sheets
- ‚úÖ Consolidar datos de m√∫ltiples hojas
- ‚úÖ Sincronizaci√≥n de datos de ventas/operaciones manuales
- ‚úÖ Integraci√≥n con procesos de onboarding

**Configuraci√≥n t√≠pica**:
```yaml
Source: Google Sheets ‚Üí Destination: PostgreSQL/Snowflake
```

**Configuraci√≥n detallada**:

**Source (Google Sheets)**:
```json
{
  "spreadsheet_id": "1abc123def456...",
  "credentials": {
    "auth_type": "Service Account",
    "service_account_info": "{{ from_external_secrets }}"
  },
  "names_conversion": true,  // Convertir nombres a snake_case
  "header_row": 1  // Fila de headers
}
```

**Setup de Google Service Account**:

1. **Crear Service Account en Google Cloud**:
```bash
# En Google Cloud Console
# IAM & Admin ‚Üí Service Accounts ‚Üí Create Service Account
# Nombre: airbyte-sheets-reader
# Rol: Viewer (m√≠nimo necesario)
```

2. **Crear Key y compartir Sheet**:
```bash
# Crear key JSON
# Descargar y guardar en External Secrets como:
# google/service-account/airbyte-sheets

# Compartir Sheet con email del service account:
# airbyte-sheets-reader@project.iam.gserviceaccount.com
```

3. **Configurar External Secrets**:
```yaml
# security/secrets/externalsecrets-airbyte.yaml
- secretKey: google_service_account_json
  remoteRef:
    key: google/service-account/airbyte-sheets
```

**Estructura de Sheet recomendada**:
- Primera fila: Headers (nombres de columnas)
- Columnas con tipos consistentes
- Sin filas vac√≠as en medio de datos
- Fechas en formato ISO (YYYY-MM-DD)

**Ventajas**:
- ‚úÖ F√°cil de usar para usuarios no t√©cnicos
- ‚úÖ Actualizaci√≥n autom√°tica cuando cambia la hoja
- ‚úÖ Soporte para m√∫ltiples hojas en un spreadsheet
- ‚úÖ No requiere API keys de usuarios finales
- ‚úÖ Soporte para rangos espec√≠ficos (si es necesario)

**Limitaciones**:
- ‚ö†Ô∏è Rate limits de Google Sheets API (100 requests/100 segundos)
- ‚ö†Ô∏è Sheets muy grandes pueden ser lentos
- ‚ö†Ô∏è Tipos de datos inferidos autom√°ticamente (puede requerir ajustes)
- ‚ö†Ô∏è No soporta cambios incrementales (siempre full refresh)

**Configuraci√≥n recomendada**:
- **Sync Frequency**: Cada 1-6 horas (depende de frecuencia de cambios)
- **Full Refresh**: Siempre (Sheets no soporta incremental)
- **Validation**: Validar datos despu√©s de sync para detectar errores

**Troubleshooting com√∫n**:
- **Error: "Permission denied"**: Verificar que el service account tenga acceso al Sheet
- **Error: "Rate limit exceeded"**: Reducir frecuencia de sync
- **Datos incorrectos**: Verificar formato de datos en Sheet (fechas, n√∫meros, etc.)
- **Missing rows**: Verificar que no haya filas vac√≠as en medio de datos

---

### 6. **MySQL** (Source & Destination)
**Tipo**: Database  
**Categor√≠a**: Base de datos relacional  
**Popularidad**: ‚≠ê‚≠ê‚≠ê‚≠ê  
**Complejidad**: Media

**Casos de Uso**:
- ‚úÖ Migraci√≥n de datos desde/a MySQL
- ‚úÖ Sincronizaci√≥n entre MySQL y PostgreSQL
- ‚úÖ Consolidaci√≥n de bases de datos MySQL
- ‚úÖ Backup incremental
- ‚úÖ CDC para sincronizaci√≥n en tiempo real
- ‚úÖ Migraci√≥n desde sistemas legacy
- ‚úÖ Sincronizaci√≥n multi-regi√≥n

**Configuraci√≥n t√≠pica**:
```yaml
Source: MySQL ‚Üí Destination: PostgreSQL/Snowflake/BigQuery
```

**Configuraci√≥n detallada**:

**Source (MySQL)**:
```json
{
  "host": "mysql.example.com",
  "port": 3306,
  "database": "production_db",
  "username": "airbyte_user",
  "password": "{{ from_external_secrets }}",
  "replication_method": {
    "method": "CDC",  // o "STANDARD" para full/incremental
    "initial_waiting_seconds": 300,
    "server_timezone": "UTC"
  },
  "ssl": true,
  "tunnel_method": null  // o SSH si requiere
}
```

**CDC Setup (Requisitos)**:
```sql
-- En MySQL source
-- 1. Habilitar binlog
SET GLOBAL binlog_format = 'ROW';
SET GLOBAL binlog_row_image = 'FULL';

-- 2. Crear usuario con permisos de replicaci√≥n
CREATE USER 'airbyte_user'@'%' IDENTIFIED BY 'secure_password';
GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'airbyte_user'@'%';
FLUSH PRIVILEGES;

-- 3. Verificar configuraci√≥n
SHOW VARIABLES LIKE 'log_bin';
SHOW VARIABLES LIKE 'binlog_format';
```

**Destination (MySQL)**:
```json
{
  "host": "mysql-warehouse.example.com",
  "port": 3306,
  "database": "data_warehouse",
  "username": "airbyte_user",
  "password": "{{ from_external_secrets }}",
  "ssl": true
}
```

**Ventajas**:
- ‚úÖ Soporte para binlog (CDC en tiempo real)
- ‚úÖ Sincronizaci√≥n incremental eficiente
- ‚úÖ Alta compatibilidad con aplicaciones legacy
- ‚úÖ Soporte para m√∫ltiples motores (InnoDB, MyISAM)
- ‚úÖ Bajo overhead en source database

**Limitaciones**:
- ‚ö†Ô∏è CDC requiere configuraci√≥n de binlog
- ‚ö†Ô∏è Requiere permisos de replicaci√≥n
- ‚ö†Ô∏è Puede ser lento para full refresh en tablas grandes
- ‚ö†Ô∏è Algunos tipos de datos pueden requerir transformaci√≥n

**Recursos recomendados**:
- CPU: 2-4 cores para CDC
- Memoria: 4-8GB para workers
- Storage: Depende del volumen de datos

**Troubleshooting com√∫n**:
- **Error: "Binlog not enabled"**: Habilitar binlog en MySQL
- **Error: "Access denied for replication"**: Verificar permisos de replicaci√≥n
- **Error: "Binlog format not ROW"**: Cambiar a ROW format
- **Slow sync**: Verificar √≠ndices, usar CDC en lugar de full refresh
- **Connection timeout**: Verificar `wait_timeout` y `interactive_timeout` en MySQL

**Migraci√≥n desde MySQL Legacy**:
```python
# DAG: Migraci√≥n MySQL ‚Üí PostgreSQL
from data.airflow.dags.airbyte_sync import trigger_airbyte_sync

with DAG("mysql_migration", ...) as dag:
    # 1. Sync inicial (full)
    initial_sync = PythonOperator(
        task_id="initial_mysql_sync",
        python_callable=trigger_airbyte_sync,
        op_kwargs={
            "connection_id": Variable.get("AIRBYTE_MYSQL_PG_CONNECTION_ID"),
            "timeout_minutes": 480,  # 8 horas para migraci√≥n completa
        },
    )
    
    # 2. Validar datos
    validate = PythonOperator(
        task_id="validate_migration",
        python_callable=validate_migration_data,
    )
    
    initial_sync >> validate
```

---

### 7. **Salesforce** (Source)
**Tipo**: CRM  
**Categor√≠a**: Sales & Marketing  
**Popularidad**: ‚≠ê‚≠ê‚≠ê‚≠ê  
**Complejidad**: Media-Alta

**Casos de Uso**:
- ‚úÖ Sincronizar Leads, Opportunities, Accounts
- ‚úÖ Integraci√≥n con sistemas de BI
- ‚úÖ An√°lisis de pipeline de ventas
- ‚úÖ Reportes de gesti√≥n de clientes
- ‚úÖ Integraci√≥n con sistemas contables
- ‚úÖ Consolidaci√≥n con otros CRMs (HubSpot, etc.)
- ‚úÖ An√°lisis de conversi√≥n de leads

**Datos sincronizados**:
- **Standard Objects**: Leads, Contacts, Accounts, Opportunities, Cases, Tasks, Events, Campaigns
- **Custom Objects**: Cualquier objeto personalizado (SOQL)
- **Relationships**: Relaciones entre objetos (Lookup, Master-Detail)
- **History**: History tracking (Field History, Account History)
- **Attachments**: Attachments y Files (opcional)

**Configuraci√≥n t√≠pica**:
```yaml
Source: Salesforce ‚Üí Destination: PostgreSQL/Snowflake
```

**Configuraci√≥n detallada**:

**Source (Salesforce)**:
```json
{
  "client_id": "{{ from_external_secrets }}",
  "client_secret": "{{ from_external_secrets }}",
  "refresh_token": "{{ from_external_secrets }}",
  "auth_type": "Client",
  "is_sandbox": false,  // true para sandbox
  "start_date": "2024-01-01T00:00:00Z",
  "api_type": "REST",  // o "BULK" para grandes vol√∫menes
  "streams_criteria": [
    {
      "criteria": "starts with",
      "value": "Account"
    }
  ]
}
```

**Setup de Salesforce OAuth**:

1. **Crear Connected App en Salesforce**:
```
Setup ‚Üí App Manager ‚Üí New Connected App
- Name: Airbyte Integration
- API Name: Airbyte_Integration
- Enable OAuth Settings: Yes
- Callback URL: https://airbyte.example.com/oauth/callback
- Selected OAuth Scopes:
  - Access and manage your data (api)
  - Perform requests on your behalf at any time (refresh_token, offline_access)
```

2. **Obtener Refresh Token**:
```bash
# Usar OAuth flow o Postman para obtener refresh token
# Guardar en External Secrets como: crm/salesforce/refresh_token
```

3. **Configurar External Secrets**:
```yaml
# security/secrets/externalsecrets-airbyte.yaml
- secretKey: salesforce_client_id
  remoteRef:
    key: crm/salesforce/client_id
- secretKey: salesforce_client_secret
  remoteRef:
    key: crm/salesforce/client_secret
- secretKey: salesforce_refresh_token
  remoteRef:
    key: crm/salesforce/refresh_token
```

**API Types**:
- **REST API**: Para objetos peque√±os/medianos (hasta 10K records)
- **Bulk API**: Para objetos grandes (m√°s de 10K records, m√°s r√°pido)

**Custom Objects**:
```json
// Configurar en streams_criteria para incluir objetos personalizados
{
  "streams_criteria": [
    {
      "criteria": "starts with",
      "value": "Custom__c"  // Todos los objetos que empiecen con "Custom"
    }
  ]
}
```

**Ventajas**:
- ‚úÖ Acceso completo a objetos y campos personalizados
- ‚úÖ Sincronizaci√≥n incremental eficiente
- ‚úÖ Soporte para Bulk API (alta performance)
- ‚úÖ Manejo autom√°tico de rate limits
- ‚úÖ Soporte para relaciones entre objetos
- ‚úÖ Sincronizaci√≥n de field history

**Limitaciones**:
- ‚ö†Ô∏è Rate limits estrictos (REST: 15K/day, Bulk: 10K/hour)
- ‚ö†Ô∏è Setup de OAuth m√°s complejo
- ‚ö†Ô∏è Sincronizaci√≥n inicial puede ser muy lenta (miles de objetos)
- ‚ö†Ô∏è Algunos objetos requieren permisos especiales

**Configuraci√≥n recomendada**:
- **Sync Frequency**: Cada 6-12 horas (depende de volumen)
- **API Type**: BULK para objetos grandes, REST para peque√±os
- **Incremental Append**: Para objetos que cambian frecuentemente
- **Full Refresh**: Para objetos de referencia (cambian poco)

**Troubleshooting com√∫n**:
- **Error: "Invalid refresh token"**: Regenerar refresh token
- **Error: "Rate limit exceeded"**: Reducir frecuencia o usar Bulk API
- **Error: "Insufficient access"**: Verificar permisos del usuario OAuth
- **Missing custom objects**: Verificar streams_criteria y permisos
- **Slow sync**: Usar Bulk API para objetos grandes

---

### 8. **Amazon S3** (Destination)
**Tipo**: Object Storage  
**Categor√≠a**: Data Lake  
**Popularidad**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê  
**Complejidad**: Baja

**Casos de Uso**:
- ‚úÖ Crear data lake en S3
- ‚úÖ Almacenar datos en formato Parquet/JSON
- ‚úÖ Preparar datos para procesamiento con Spark/Athena
- ‚úÖ Backup de datos a largo plazo
- ‚úÖ Integraci√≥n con AWS Glue
- ‚úÖ Alimentar Databricks (ya tienes en tu plataforma)
- ‚úÖ Data lake para ML/AI pipelines

**Configuraci√≥n t√≠pica**:
```yaml
Source: Cualquier fuente ‚Üí Destination: S3
```

**Configuraci√≥n detallada**:

**Destination (S3)**:
```json
{
  "s3_bucket_name": "biz-datalake-dev",  // Ya configurado en platform.yaml
  "s3_bucket_path": "airbyte/{source}/{stream}/",
  "s3_bucket_region": "us-east-1",
  "access_key_id": "{{ from_external_secrets }}",
  "secret_access_key": "{{ from_external_secrets }}",
  "s3_path_format": "${NAMESPACE}/${STREAM_NAME}/${YEAR}_${MONTH}_${DAY}_${EPOCH}",
  "format": {
    "format_type": "Parquet",
    "compression_codec": "snappy",
    "block_size_mb": 128,
    "max_padding_size_mb": 8,
    "page_size_kb": 1024,
    "dictionary_page_size_kb": 1024,
    "dictionary_encoding": true
  },
  "part_size": 10
}
```

**Estructura de archivos recomendada**:
```
s3://biz-datalake-dev/
  airbyte/
    stripe/
      customers/
        2025/01/15/1234567890_00001.parquet
        2025/01/15/1234567891_00002.parquet
      payments/
        2025/01/15/1234567892_00001.parquet
    hubspot/
      contacts/
        2025/01/15/1234567893_00001.parquet
```

**Integraci√≥n con tu plataforma**:

Ya tienes `dataLake.type: s3` y `bucketName: biz-datalake-dev` en `platform.yaml`. Configuraci√≥n:

```python
# DAG: S3 Data Lake Pipeline
from data.airflow.dags.airbyte_sync import trigger_airbyte_sync

with DAG("s3_datalake_pipeline", ...) as dag:
    # Sync m√∫ltiples fuentes a S3
    stripe_to_s3 = PythonOperator(
        task_id="sync_stripe_to_s3",
        python_callable=trigger_airbyte_sync,
        op_kwargs={
            "connection_id": Variable.get("AIRBYTE_STRIPE_S3_CONNECTION_ID"),
        },
    )
    
    hubspot_to_s3 = PythonOperator(
        task_id="sync_hubspot_to_s3",
        python_callable=trigger_airbyte_sync,
        op_kwargs={
            "connection_id": Variable.get("AIRBYTE_HUBSPOT_S3_CONNECTION_ID"),
        },
    )
    
    # Procesar con Spark/Databricks (ya tienes Databricks configurado)
    process_with_databricks = DatabricksRunNowOperator(
        task_id="process_datalake",
        job_id=Variable.get("DATABRICKS_DATALAKE_JOB_ID"),
    )
    
    [stripe_to_s3, hubspot_to_s3] >> process_with_databricks
```

**Formatos soportados**:
- **Parquet**: Recomendado para analytics (mejor compresi√≥n, columnar)
- **JSON**: Flexible pero menos eficiente
- **CSV**: Simple pero sin tipos de datos
- **Avro**: Bueno para streaming

**Optimizaci√≥n de S3**:

1. **Particionamiento**:
```json
"s3_path_format": "${NAMESPACE}/${STREAM_NAME}/${YEAR}/${MONTH}/${DAY}"
```

2. **Compresi√≥n**:
- Parquet con Snappy: Balance entre velocidad y tama√±o
- Gzip: Mejor compresi√≥n pero m√°s lento
- Sin compresi√≥n: M√°s r√°pido pero m√°s costoso

3. **Lifecycle Policies**:
```json
// En AWS S3, configurar lifecycle para mover a Glacier despu√©s de 90 d√≠as
{
  "Rules": [{
    "Id": "Move to Glacier",
    "Status": "Enabled",
    "Transitions": [{
      "Days": 90,
      "StorageClass": "GLACIER"
    }]
  }]
}
```

**Ventajas**:
- ‚úÖ Costo muy bajo para almacenamiento (~$0.023/GB/mes)
- ‚úÖ Escalabilidad ilimitada
- ‚úÖ Integraci√≥n nativa con ecosistema AWS
- ‚úÖ Soporte para m√∫ltiples formatos
- ‚úÖ Integraci√≥n con Athena para queries SQL
- ‚úÖ Compatible con Databricks (ya tienes configurado)

**Limitaciones**:
- ‚ö†Ô∏è Solo append (no soporta updates/deletes)
- ‚ö†Ô∏è Queries directas requieren herramientas adicionales (Athena, Spark)
- ‚ö†Ô∏è Costos de transferencia si se accede frecuentemente

**Costos estimados**:
- Storage: $0.023/GB/mes (Standard)
- Requests: $0.005/1000 PUT requests
- Transfer: $0.09/GB (outbound)
- **Tip**: Usar Intelligent-Tiering para ahorrar

**External Secrets** (ya configurado):
```yaml
# Usar credenciales AWS existentes
# security/secrets/externalsecrets-aws.yaml
```

**Troubleshooting com√∫n**:
- **Error: "Access Denied"**: Verificar IAM permissions del bucket
- **Error: "Bucket not found"**: Verificar nombre y regi√≥n
- **Slow uploads**: Aumentar `part_size` o usar multipart upload
- **High costs**: Verificar lifecycle policies y usar compression

---

### 9. **MongoDB** (Source & Destination)
**Tipo**: NoSQL Database  
**Categor√≠a**: Document Database  
**Popularidad**: ‚≠ê‚≠ê‚≠ê  
**Complejidad**: Media-Alta

**Casos de Uso**:
- ‚úÖ Sincronizar colecciones de MongoDB
- ‚úÖ Migraci√≥n a bases de datos relacionales
- ‚úÖ Consolidar datos de m√∫ltiples colecciones
- ‚úÖ Backup incremental
- ‚úÖ CDC usando Oplog
- ‚úÖ Sincronizaci√≥n de documentos anidados
- ‚úÖ Flattening de estructuras JSON complejas

**Configuraci√≥n t√≠pica**:
```yaml
Source: MongoDB ‚Üí Destination: PostgreSQL/Snowflake
```

**Configuraci√≥n detallada**:

**Source (MongoDB)**:
```json
{
  "instance_type": "standalone",  // o "replica", "atlas"
  "host": "mongodb.example.com",
  "port": 27017,
  "database": "production_db",
  "auth_type": "login/password",
  "username": "airbyte_user",
  "password": "{{ from_external_secrets }}",
  "replication_method": {
    "method": "CDC",  // o "STANDARD" para full/incremental
    "replication_slot": "airbyte_slot"
  },
  "ssl": true,
  "tunnel_method": null
}
```

**CDC Setup (Requisitos)**:

1. **Habilitar Replica Set** (requerido para CDC):
```javascript
// MongoDB debe estar en modo replica set (aunque sea de 1 nodo)
// Iniciar con: mongod --replSet rs0

// En mongo shell:
rs.initiate({
  _id: "rs0",
  members: [{ _id: 0, host: "localhost:27017" }]
});
```

2. **Crear Usuario con Permisos**:
```javascript
use admin;
db.createUser({
  user: "airbyte_user",
  pwd: "secure_password",
  roles: [
    { role: "read", db: "production_db" },
    { role: "readAnyDatabase", db: "admin" }
  ]
});
```

3. **Verificar Oplog**:
```javascript
// Verificar que oplog est√° habilitado
use local;
db.oplog.rs.find().limit(1);
```

**Destination (PostgreSQL con JSON)**:
```json
{
  "host": "postgres.example.com",
  "port": 5432,
  "database": "analytics",
  "schema": "mongodb_raw",
  "username": "airbyte_user",
  "password": "{{ from_external_secrets }}",
  "ssl": true
}
```

**Flattening de Documentos**:
- Airbyte autom√°ticamente "aplana" documentos anidados
- Arrays se convierten en tablas separadas
- Nested objects se convierten en columnas con prefijo

**Ejemplo de Transformaci√≥n**:
```javascript
// Documento MongoDB original:
{
  _id: ObjectId("..."),
  name: "John",
  address: {
    street: "123 Main",
    city: "NYC"
  },
  orders: [
    { id: 1, amount: 100 },
    { id: 2, amount: 200 }
  ]
}

// Se convierte en PostgreSQL:
// Tabla: users
// _id, name, address_street, address_city

// Tabla: users_orders (array flattening)
// _id, orders_id, orders_amount
```

**Ventajas**:
- ‚úÖ Soporte para CDC usando Oplog (cambio en tiempo real)
- ‚úÖ Sincronizaci√≥n incremental eficiente
- ‚úÖ Manejo de documentos anidados
- ‚úÖ Flattening autom√°tico de estructuras complejas
- ‚úÖ Soporte para MongoDB Atlas (cloud)

**Limitaciones**:
- ‚ö†Ô∏è CDC requiere Replica Set (no funciona con standalone)
- ‚ö†Ô∏è Flattening puede crear muchas tablas para documentos complejos
- ‚ö†Ô∏è P√©rdida de estructura original en algunos casos
- ‚ö†Ô∏è Arrays grandes pueden causar problemas de performance

**Configuraci√≥n recomendada**:
- **Sync Frequency**: Cada 1-6 horas (depende de volumen)
- **CDC**: Usar si necesitas cambios en tiempo real
- **Selective Sync**: Sincronizar solo colecciones necesarias
- **Flattening**: Revisar estructura resultante antes de producci√≥n

**Recursos recomendados**:
- CPU: 2-4 cores para CDC
- Memoria: 4-8GB (depende de tama√±o de documentos)
- Storage: Variable seg√∫n colecciones

**Troubleshooting com√∫n**:
- **Error: "Not a replica set"**: Configurar replica set (aunque sea de 1 nodo)
- **Error: "Oplog not found"**: Verificar que oplog est√° habilitado
- **Error: "Too many tables"**: Reducir colecciones sincronizadas
- **Slow sync**: Verificar √≠ndices en MongoDB, usar selective sync
- **Memory issues**: Reducir tama√±o de batch o documentos sincronizados

---

### 10. **REST API** (Source)
**Tipo**: Generic API  
**Categor√≠a**: Custom Integration  
**Popularidad**: ‚≠ê‚≠ê‚≠ê‚≠ê  
**Complejidad**: Media-Alta

**Casos de Uso**:
- ‚úÖ Sincronizar datos de cualquier API REST
- ‚úÖ Integrar APIs personalizadas
- ‚úÖ Sincronizar datos de servicios SaaS
- ‚úÖ Crear conectores personalizados sin c√≥digo
- ‚úÖ Integraci√≥n con sistemas legacy
- ‚úÖ Webhooks a base de datos
- ‚úÖ Consolidador de m√∫ltiples APIs

**Configuraci√≥n t√≠pica**:
```yaml
Source: REST API ‚Üí Destination: PostgreSQL/Snowflake/S3
```

**Configuraci√≥n detallada**:

**Source (REST API)**:
```json
{
  "url_base": "https://api.example.com/v1",
  "http_method": "GET",
  "headers": {
    "Authorization": "Bearer {{ from_external_secrets }}",
    "Content-Type": "application/json"
  },
  "authenticator": {
    "type": "Bearer Token",
    "api_token": "{{ from_external_secrets }}"
  },
  "request_params": {
    "page": "{{ page_number }}",
    "per_page": 100
  },
  "pagination": {
    "type": "page",
    "page_size": 100,
    "page_size_param": "per_page",
    "page_number_param": "page"
  },
  "streams": [
    {
      "name": "customers",
      "path": "/customers",
      "primary_key": ["id"],
      "cursor_field": "updated_at"
    },
    {
      "name": "orders",
      "path": "/orders",
      "primary_key": ["id"],
      "cursor_field": "created_at"
    }
  ]
}
```

**Tipos de Autenticaci√≥n**:
- **Bearer Token**: `Authorization: Bearer <token>`
- **API Key**: En header o query params
- **Basic Auth**: `Authorization: Basic <base64>`
- **OAuth 2.0**: Client credentials flow
- **Custom**: Headers personalizados

**Tipos de Paginaci√≥n**:
- **Page-based**: `/items?page=1&per_page=100`
- **Offset-based**: `/items?offset=0&limit=100`
- **Cursor-based**: `/items?cursor=abc123`
- **Header-based**: Links en headers (RFC 5988)

**Ejemplo: API con OAuth**:
```json
{
  "url_base": "https://api.example.com",
  "authenticator": {
    "type": "OAuth2.0",
    "client_id": "{{ from_external_secrets }}",
    "client_secret": "{{ from_external_secrets }}",
    "token_refresh_endpoint": "https://api.example.com/oauth/token",
    "access_token": "{{ auto_refreshed }}",
    "refresh_token": "{{ from_external_secrets }}"
  },
  "streams": [
    {
      "name": "data",
      "path": "/api/data",
      "primary_key": ["id"]
    }
  ]
}
```

**Ejemplo: API con Paginaci√≥n Cursor**:
```json
{
  "pagination": {
    "type": "cursor",
    "cursor_value": "{{ response.next_cursor }}",
    "cursor_field": "cursor",
    "stop_condition": "{{ response.next_cursor == null }}"
  }
}
```

**Transformaci√≥n de Datos**:
```json
{
  "streams": [
    {
      "name": "customers",
      "path": "/customers",
      "schema": {
        "properties": {
          "id": {"type": "string"},
          "name": {"type": "string"},
          "created_at": {"type": "string", "format": "date-time"}
        }
      },
      "transform": {
        "rename": {
          "created_at": "created_date"
        },
        "cast": {
          "created_date": "timestamp"
        }
      }
    }
  ]
}
```

**Ventajas**:
- ‚úÖ Flexibilidad m√°xima (cualquier API REST)
- ‚úÖ Configuraci√≥n mediante JSON (no requiere c√≥digo)
- ‚úÖ Soporte para m√∫ltiples tipos de autenticaci√≥n
- ‚úÖ Paginaci√≥n autom√°tica
- ‚úÖ Transformaci√≥n de datos b√°sica
- ‚úÖ Soporte para m√∫ltiples streams en una conexi√≥n

**Limitaciones**:
- ‚ö†Ô∏è Requiere conocimiento de la API espec√≠fica
- ‚ö†Ô∏è Transformaciones complejas pueden requerir c√≥digo
- ‚ö†Ô∏è Rate limits dependen de la API
- ‚ö†Ô∏è Cambios en la API pueden romper la sincronizaci√≥n
- ‚ö†Ô∏è No todos los tipos de APIs son soportados

**Configuraci√≥n recomendada**:
- **Sync Frequency**: Depende de la API (cada 1-24 horas)
- **Pagination**: Configurar correctamente para evitar datos faltantes
- **Error Handling**: Configurar retries y backoff
- **Validation**: Validar estructura de respuesta despu√©s de sync

**Troubleshooting com√∫n**:
- **Error: "Authentication failed"**: Verificar credenciales y tipo de auth
- **Error: "Pagination not working"**: Verificar configuraci√≥n de paginaci√≥n
- **Error: "Rate limit exceeded"**: Reducir frecuencia o implementar backoff
- **Missing data**: Verificar que paginaci√≥n capture todos los datos
- **Schema errors**: Verificar estructura de respuesta de la API

**Ejemplo: Integrar API Interna**:
```python
# DAG: Sync desde API interna
with DAG("internal_api_sync", ...) as dag:
    sync = PythonOperator(
        task_id="sync_internal_api",
        python_callable=trigger_airbyte_sync,
        op_kwargs={
            "connection_id": Variable.get("AIRBYTE_INTERNAL_API_CONNECTION_ID"),
        },
    )
```

---

## üìà Tabla Comparativa

| Conector | Tipo | Popularidad | Casos de Uso Comunes |
|----------|------|-------------|---------------------|
| PostgreSQL | Database | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Migraci√≥n, CDC, Consolidaci√≥n |
| Stripe | Payment | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Finanzas, E-commerce, Analytics |
| HubSpot | CRM | ‚≠ê‚≠ê‚≠ê‚≠ê | Sales, Marketing, Analytics |
| Snowflake | Data Warehouse | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Analytics, BI, ML |
| Google Sheets | Spreadsheet | ‚≠ê‚≠ê‚≠ê‚≠ê | Colaboraci√≥n, Reportes |
| MySQL | Database | ‚≠ê‚≠ê‚≠ê‚≠ê | Migraci√≥n, Legacy Systems |
| Salesforce | CRM | ‚≠ê‚≠ê‚≠ê‚≠ê | Sales, Customer Management |
| S3 | Storage | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Data Lake, Backup |
| MongoDB | NoSQL | ‚≠ê‚≠ê‚≠ê | Document Sync, Migration |
| REST API | Generic | ‚≠ê‚≠ê‚≠ê‚≠ê | Custom Integration |

## üéØ Casos de Uso Comunes para tu Plataforma

### 1. **Sincronizaci√≥n Financiera**
```
Stripe ‚Üí PostgreSQL ‚Üí QuickBooks Integration
```
- Sincronizar pagos de Stripe
- Procesar en PostgreSQL
- Integrar con QuickBooks (ya tienes DAG para esto)

### 2. **CRM Analytics**
```
HubSpot + Salesforce ‚Üí Snowflake ‚Üí BI Tools
```
- Consolidar datos de m√∫ltiples CRMs
- Analizar pipeline unificado
- Dashboards en Grafana

### 3. **Data Lake Pipeline**
```
Stripe + HubSpot + PostgreSQL ‚Üí S3 ‚Üí Databricks/Spark
```
- Almacenar datos en S3
- Procesar con Spark/Databricks
- Preparar para ML

### 4. **Real-time Sync**
```
PostgreSQL (Source) ‚Üí PostgreSQL (Destination)
```
- CDC para sincronizaci√≥n en tiempo real
- Multi-regi√≥n replication
- Backup autom√°tico

### 5. **Legacy Migration**
```
MySQL ‚Üí PostgreSQL ‚Üí Snowflake
```
- Migrar desde MySQL legacy
- Transformar en PostgreSQL
- Cargar en Snowflake para analytics

## üîß Configuraci√≥n R√°pida y Avanzada

### Ejemplo Completo: Stripe ‚Üí PostgreSQL ‚Üí QuickBooks

**Paso 1: Crear Source (Stripe) en Airbyte UI**

1. Ir a **Sources** ‚Üí **New Source** ‚Üí **Stripe**
2. Configurar:
   - **API Key**: Desde External Secrets (`payments/stripe/api_key`)
   - **Start Date**: `2024-01-01T00:00:00Z` (o fecha inicial)
   - **Account ID**: Dejar vac√≠o (o para Connect accounts)
3. **Test Connection** ‚Üí Verificar que funciona
4. **Save** con nombre: `Stripe Production`

**Paso 2: Crear Destination (PostgreSQL) en Airbyte UI**

1. Ir a **Destinations** ‚Üí **New Destination** ‚Üí **PostgreSQL**
2. Configurar:
   ```json
   {
     "host": "postgres.data.svc.cluster.local",
     "port": 5432,
     "database": "analytics",
     "schema": "stripe_raw",
     "username": "airbyte_user",
     "password": "{{ from_external_secrets }}",
     "ssl": true,
     "tunnel_method": null
   }
   ```
3. **Test Connection** ‚Üí Verificar
4. **Save** con nombre: `PostgreSQL Analytics`

**Paso 3: Crear Connection en Airbyte UI**

1. Ir a **Connections** ‚Üí **New Connection**
2. Seleccionar:
   - **Source**: `Stripe Production`
   - **Destination**: `PostgreSQL Analytics`
3. Configurar **Streams**:
   - Seleccionar streams necesarios:
     - ‚úÖ `customers`
     - ‚úÖ `subscriptions`
     - ‚úÖ `payment_intents` (payments)
     - ‚úÖ `invoices`
     - ‚úÖ `charges`
     - ‚úÖ `refunds`
     - ‚úÖ `products`
     - ‚úÖ `prices`
   - **Namespace**: `stripe_raw` (o el schema que prefieras)
   - **Stream Prefix**: (opcional) `stripe_`
4. Configurar **Sync Mode**:
   - **Customers**: Incremental Append (por `created`)
   - **Payment Intents**: Incremental Append (por `created`)
   - **Products**: Full Refresh (cambian poco)
   - **Invoices**: Incremental Append
5. Configurar **Frequency**:
   - **Schedule Type**: Scheduled
   - **Cron Expression**: `0 */6 * * *` (cada 6 horas)
   - O usar **Manual** y trigger desde Airflow
6. **Save & Run** para primera sincronizaci√≥n

**Paso 4: Obtener Connection ID**

```bash
# Desde API
curl -u username:password \
  http://airbyte-server.integration.svc.cluster.local:8000/api/v1/connections/list \
  | jq '.data[] | select(.name=="Stripe to PostgreSQL") | .connectionId'

# O desde UI: URL contiene el connection ID
```

**Paso 5: Integrar con Airflow**

```python
# data/airflow/dags/stripe_airbyte_quickbooks.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from data.airflow.dags.airbyte_sync import trigger_airbyte_sync
from data.airflow.dags.stripe_product_to_quickbooks_item import sync_to_quickbooks

with DAG(
    dag_id="stripe_airbyte_to_quickbooks",
    description="Sync Stripe ‚Üí PostgreSQL (Airbyte) ‚Üí QuickBooks",
    schedule_interval=timedelta(hours=6),
    default_args={
        "owner": "data-engineering",
        "retries": 2,
    },
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=["stripe", "airbyte", "quickbooks", "finance"],
) as dag:
    
    # 1. Sync desde Stripe a PostgreSQL usando Airbyte
    airbyte_sync = PythonOperator(
        task_id="sync_stripe_to_postgres",
        python_callable=trigger_airbyte_sync,
        op_kwargs={
            "connection_id": Variable.get("AIRBYTE_STRIPE_POSTGRES_CONNECTION_ID"),
            "timeout_minutes": 180,
            "validate_connection": True,
        },
    )
    
    # 2. Procesar y sincronizar a QuickBooks
    # (Usar tu l√≥gica existente de stripe_product_to_quickbooks_item.py)
    quickbooks_sync = PythonOperator(
        task_id="sync_to_quickbooks",
        python_callable=sync_to_quickbooks,
        op_kwargs={
            # Tus par√°metros existentes
        },
    )
    
    # Dependencias
    airbyte_sync >> quickbooks_sync
```

**Paso 6: Configurar Variables en Airflow**

```python
# En Airflow UI ‚Üí Admin ‚Üí Variables
AIRBYTE_STRIPE_POSTGRES_CONNECTION_ID = "abc-123-def-456"
```

**Paso 7: Verificar y Monitorear**

```bash
# Ver logs de sincronizaci√≥n en Airbyte UI
# Ver logs en Airflow
# Verificar datos en PostgreSQL
psql -h postgres.data.svc.cluster.local -d analytics -c \
  "SELECT COUNT(*) FROM stripe_raw.customers;"
```

## üìö Recursos Adicionales

- **Lista completa de conectores**: https://docs.airbyte.com/integrations/
- **Documentaci√≥n de cada conector**: https://docs.airbyte.com/integrations/sources/
- **Gu√≠as de configuraci√≥n**: https://docs.airbyte.com/operator-guides/
- **Troubleshooting**: https://docs.airbyte.com/troubleshooting/

## üöÄ Pr√≥ximos Pasos Recomendados

### Para tu Plataforma

1. **Configurar Stripe ‚Üí PostgreSQL**:
   - ‚úÖ Ya tienes Stripe API key configurado
   - ‚úÖ Ya tienes integraci√≥n QuickBooks
   - üîÑ Crear conexi√≥n Airbyte para sincronizar datos hist√≥ricos
   - üîÑ Integrar con tu DAG existente

2. **Configurar HubSpot ‚Üí PostgreSQL**:
   - ‚úÖ Ya tienes HubSpot token configurado
   - ‚úÖ Ya tienes workflows en Kestra
   - üîÑ Crear conexi√≥n Airbyte para analytics
   - üîÑ Consolidar datos de HubSpot para an√°lisis

3. **Configurar PostgreSQL CDC**:
   - üîÑ Para sincronizaci√≥n en tiempo real
   - üîÑ Para backup incremental
   - üîÑ Para multi-regi√≥n replication

4. **Monitorear**:
   - üîÑ Configurar dashboards en Grafana
   - üîÑ Alertas en Prometheus
   - üîÑ Logs estructurados en Loki

## üìä M√©tricas y Monitoreo

### M√©tricas Clave por Conector

**Stripe**:
- N√∫mero de registros sincronizados (customers, payments)
- Tiempo de sincronizaci√≥n
- Errores de rate limit
- Lag de datos (diferencia entre creaci√≥n y sync)

**HubSpot**:
- N√∫mero de contactos/deals sincronizados
- Tiempo de sincronizaci√≥n
- Errores de rate limit
- Actualizaci√≥n de propiedades personalizadas

**PostgreSQL**:
- Velocidad de sincronizaci√≥n (records/segundo)
- Lag de CDC (para replicaci√≥n)
- Tama√±o de WAL logs
- Uso de replication slots

### Dashboard de Grafana

Crear dashboard con:
- Tasa de √©xito de sincronizaciones
- Tiempo promedio de sincronizaci√≥n
- Volumen de datos sincronizados
- Errores y retries
- Uso de recursos (CPU/memoria)

## ‚ö†Ô∏è Troubleshooting Avanzado

### Problemas Comunes y Soluciones

**1. Rate Limits (Stripe/HubSpot)**:
```python
# Soluci√≥n: Reducir frecuencia o usar lookback window
# En Airbyte: Configurar lookback_window_days
# En Airflow: Aumentar intervalo entre syncs
```

**2. Timeout en Sincronizaciones Grandes**:
```python
# Soluci√≥n: Aumentar timeout en Airflow
trigger_airbyte_sync(
    connection_id="...",
    timeout_minutes=360,  # 6 horas para syncs grandes
)
```

**3. Datos Faltantes**:
- Verificar `start_date` en source
- Verificar filtros en streams
- Verificar permisos de API key
- Verificar logs de Airbyte para errores espec√≠ficos

**4. CDC No Funciona (PostgreSQL)**:
```sql
-- Verificar replication slot
SELECT * FROM pg_replication_slots;

-- Verificar WAL retention
SHOW wal_keep_size;

-- Verificar permisos
\du airbyte_user
```

## üí∞ Costos y Recursos

### Estimaci√≥n de Recursos por Conector

| Conector | Workers | CPU | Memoria | Storage |
|----------|---------|-----|---------|---------|
| Stripe | 1-2 | 1-2 cores | 2-4GB | N/A |
| HubSpot | 1-2 | 1-2 cores | 2-4GB | N/A |
| PostgreSQL (Source) | 1 | 2-4 cores | 4-8GB | N/A |
| PostgreSQL (Dest) | 1 | 2-4 cores | 4-8GB | Variable |
| Snowflake | 1 | 1-2 cores | 2-4GB | N/A |
| S3 | 1 | 1 core | 2GB | Variable |

**Nota**: Los recursos dependen del volumen de datos. Ajustar seg√∫n necesidades.

### Costos de Destinos

- **PostgreSQL**: Costo de instancia (si es managed)
- **Snowflake**: Basado en compute credits y storage
- **S3**: Muy bajo (~$0.023/GB/mes)
- **BigQuery**: Basado en queries y storage

## üîê Seguridad y Mejores Pr√°cticas

### Mejores Pr√°cticas

1. **Credenciales**:
   - ‚úÖ Usar External Secrets siempre
   - ‚úÖ Rotar credenciales regularmente
   - ‚úÖ Usar permisos m√≠nimos necesarios
   - ‚úÖ No hardcodear en c√≥digo

2. **NetworkPolicies**:
   - ‚úÖ Ya configuradas en `security/networkpolicies/airbyte.yaml`
   - ‚úÖ Restringir acceso solo a servicios necesarios

3. **Monitoreo**:
   - ‚úÖ Alertas en fallos de sincronizaci√≥n
   - ‚úÖ Alertas en rate limits
   - ‚úÖ Alertas en timeouts
   - ‚úÖ Dashboard de m√©tricas

4. **Backup**:
   - ‚úÖ Backup de configuraci√≥n de Airbyte
   - ‚úÖ Backup de metadata de conexiones
   - ‚úÖ Backup de datos en destinos

## üìö Referencias Adicionales

- **Documentaci√≥n oficial**: https://docs.airbyte.com/
- **Gu√≠as de conectores**: https://docs.airbyte.com/integrations/sources/
- **API Reference**: https://airbyte-public-api-docs.s3.us-east-2.amazonaws.com/rapidoc-api-docs.html
- **Troubleshooting**: https://docs.airbyte.com/troubleshooting/
- **Comunidad**: https://airbyte.com/community

---

**Nota**: Estos conectores son los m√°s populares seg√∫n la comunidad de Airbyte y casos de uso comunes en empresas. La elecci√≥n final depende de tus necesidades espec√≠ficas.

**√öltima actualizaci√≥n**: 2025-01-15  
**Versi√≥n del documento**: 2.0

## üìñ Documentaci√≥n Relacionada

- **Arquitecturas y Ejemplos Avanzados**: Ver `AIRBYTE_ARCHITECTURE_EXAMPLES.md`
  - Diagramas de arquitectura completos
  - Patterns de integraci√≥n (fan-out, fan-in, pipeline en cadena)
  - Casos de uso avanzados con c√≥digo
  - Performance tuning
  - Ejemplos de ETL con validaci√≥n
  - Sincronizaci√≥n condicional y event-driven

- **Gu√≠a Completa de Airbyte**: `README_AIRBYTE.md`
- **Mejoras Implementadas**: `IMPROVEMENTS_AIRBYTE.md`
- **Quick Start**: `QUICK_START_AIRBYTE.md`


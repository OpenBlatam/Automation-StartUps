---
title: "Escalabilidad Performance Recomendaciones"
category: "escalabilidad_performance_recomendaciones.md"
tags: []
created: "2025-10-29"
path: "escalabilidad_performance_recomendaciones.md"
---

# ‚ö° **ESCALABILIDAD Y PERFORMANCE - SISTEMA DE RECOMENDACIONES**

## **√çNDICE**
1. [Arquitectura Escalable](#arquitectura)
2. [Optimizaci√≥n de Base de Datos](#database)
3. [Caching Inteligente](#caching)
4. [Load Balancing](#loadbalancing)
5. [Microservicios](#microservicios)
6. [CDN y Distribuci√≥n](#cdn)
7. [Monitoreo de Performance](#monitoreo)
8. [Optimizaci√≥n de Algoritmos](#algoritmos)
9. [Auto-scaling](#autoscaling)
10. [Casos de Uso de Escala](#casos)
11. [Mejores Pr√°cticas](#mejores)
12. [Troubleshooting](#troubleshooting)

---

## **1. ARQUITECTURA ESCALABLE** {#arquitectura}

### **Arquitectura de Microservicios**
```python
# Arquitectura escalable para recomendaciones
class ScalableRecommendationArchitecture:
    def __init__(self):
        self.services = {
            'user_service': UserService(),
            'product_service': ProductService(),
            'interaction_service': InteractionService(),
            'recommendation_service': RecommendationService(),
            'analytics_service': AnalyticsService()
        }
        
        self.load_balancers = {
            'api_gateway': LoadBalancer(),
            'recommendation_engine': LoadBalancer(),
            'database_cluster': LoadBalancer()
        }
    
    def handle_recommendation_request(self, user_id, context):
        """Manejar solicitud de recomendaciones de forma escalable"""
        # 1. Validar usuario (servicio independiente)
        user = self.services['user_service'].get_user(user_id)
        
        # 2. Obtener contexto del producto (servicio independiente)
        product_context = self.services['product_service'].get_context(context)
        
        # 3. Generar recomendaciones (servicio independiente)
        recommendations = self.services['recommendation_service'].generate(
            user, product_context
        )
        
        # 4. Registrar interacci√≥n (servicio independiente)
        self.services['interaction_service'].log_interaction(
            user_id, 'recommendation_viewed', recommendations
        )
        
        return recommendations
```

### **Patr√≥n de Event Sourcing**
```python
# Event Sourcing para escalabilidad
class EventSourcingRecommendations:
    def __init__(self):
        self.event_store = EventStore()
        self.projections = {
            'user_profile': UserProfileProjection(),
            'product_affinity': ProductAffinityProjection(),
            'recommendation_cache': RecommendationCacheProjection()
        }
    
    def process_user_interaction(self, user_id, event_type, data):
        """Procesar interacci√≥n del usuario como evento"""
        event = {
            'id': self.generate_event_id(),
            'user_id': user_id,
            'type': event_type,
            'data': data,
            'timestamp': datetime.utcnow(),
            'version': self.get_next_version(user_id)
        }
        
        # Almacenar evento
        self.event_store.append_event(event)
        
        # Actualizar proyecciones
        for projection in self.projections.values():
            projection.handle_event(event)
    
    def get_user_recommendations(self, user_id):
        """Obtener recomendaciones basadas en eventos"""
        # Reconstruir estado del usuario desde eventos
        user_events = self.event_store.get_events(user_id)
        user_state = self.reconstruct_user_state(user_events)
        
        # Generar recomendaciones
        return self.generate_recommendations(user_state)
```

---

## **2. OPTIMIZACI√ìN DE BASE DE DATOS** {#database}

### **Sharding Horizontal**
```python
# Sharding horizontal para escalabilidad
class DatabaseSharding:
    def __init__(self):
        self.shards = {
            'shard_1': DatabaseConnection('shard1.example.com'),
            'shard_2': DatabaseConnection('shard2.example.com'),
            'shard_3': DatabaseConnection('shard3.example.com'),
            'shard_4': DatabaseConnection('shard4.example.com')
        }
        self.shard_count = len(self.shards)
    
    def get_shard(self, user_id):
        """Determinar shard basado en user_id"""
        shard_index = hash(user_id) % self.shard_count
        return self.shards[f'shard_{shard_index + 1}']
    
    def get_user_data(self, user_id):
        """Obtener datos del usuario desde el shard correcto"""
        shard = self.get_shard(user_id)
        return shard.query(f"SELECT * FROM users WHERE user_id = '{user_id}'")
    
    def get_recommendations(self, user_id):
        """Obtener recomendaciones desde el shard correcto"""
        shard = self.get_shard(user_id)
        return shard.query(f"SELECT * FROM recommendations WHERE user_id = '{user_id}'")
```

### **√çndices Optimizados**
```sql
-- √çndices optimizados para recomendaciones
-- √çndice compuesto para consultas de recomendaciones
CREATE INDEX idx_recommendations_user_score 
ON recommendations (user_id, score DESC, created_at DESC);

-- √çndice para consultas de interacciones
CREATE INDEX idx_interactions_user_timestamp 
ON user_interactions (user_id, timestamp DESC);

-- √çndice para consultas de productos
CREATE INDEX idx_products_category_price 
ON products (category, price, availability);

-- √çndice parcial para datos activos
CREATE INDEX idx_active_users 
ON users (user_id) 
WHERE status = 'active';

-- √çndice de texto completo para b√∫squeda
CREATE INDEX idx_products_search 
ON products USING gin(to_tsvector('spanish', title || ' ' || description));
```

### **Particionamiento de Tablas**
```sql
-- Particionamiento por fecha para logs
CREATE TABLE user_interactions (
    id SERIAL,
    user_id VARCHAR(50),
    action VARCHAR(100),
    timestamp TIMESTAMP,
    data JSONB
) PARTITION BY RANGE (timestamp);

-- Crear particiones mensuales
CREATE TABLE user_interactions_2024_01 
PARTITION OF user_interactions 
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE user_interactions_2024_02 
PARTITION OF user_interactions 
FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

-- Particionamiento por hash para usuarios
CREATE TABLE user_profiles (
    user_id VARCHAR(50),
    profile_data JSONB,
    created_at TIMESTAMP
) PARTITION BY HASH (user_id);

-- Crear particiones por hash
CREATE TABLE user_profiles_0 PARTITION OF user_profiles FOR VALUES WITH (modulus 4, remainder 0);
CREATE TABLE user_profiles_1 PARTITION OF user_profiles FOR VALUES WITH (modulus 4, remainder 1);
CREATE TABLE user_profiles_2 PARTITION OF user_profiles FOR VALUES WITH (modulus 4, remainder 2);
CREATE TABLE user_profiles_3 PARTITION OF user_profiles FOR VALUES WITH (modulus 4, remainder 3);
```

---

## **3. CACHING INTELIGENTE** {#caching}

### **Sistema de Caching Multi-Nivel**
```python
# Sistema de caching inteligente
import redis
from functools import wraps
import json

class IntelligentCaching:
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)
        self.memory_cache = {}
        self.cache_ttl = {
            'user_profile': 3600,  # 1 hora
            'product_data': 1800,  # 30 minutos
            'recommendations': 300,  # 5 minutos
            'analytics': 7200  # 2 horas
        }
    
    def cache_recommendations(self, user_id, recommendations):
        """Cachear recomendaciones con TTL inteligente"""
        cache_key = f"recommendations:{user_id}"
        cache_data = {
            'recommendations': recommendations,
            'timestamp': datetime.utcnow().isoformat(),
            'ttl': self.cache_ttl['recommendations']
        }
        
        self.redis_client.setex(
            cache_key, 
            self.cache_ttl['recommendations'], 
            json.dumps(cache_data)
        )
    
    def get_cached_recommendations(self, user_id):
        """Obtener recomendaciones del cache"""
        cache_key = f"recommendations:{user_id}"
        cached_data = self.redis_client.get(cache_key)
        
        if cached_data:
            return json.loads(cached_data)['recommendations']
        return None
    
    def cache_with_invalidation(self, key, data, ttl=3600):
        """Cache con invalidaci√≥n inteligente"""
        # Cachear datos
        self.redis_client.setex(key, ttl, json.dumps(data))
        
        # Configurar invalidaci√≥n por eventos
        self.setup_cache_invalidation(key, data)
    
    def setup_cache_invalidation(self, key, data):
        """Configurar invalidaci√≥n de cache basada en eventos"""
        if 'user_id' in data:
            # Invalidar cache cuando el usuario actualiza su perfil
            self.redis_client.sadd(f"invalidation:user:{data['user_id']}", key)
        
        if 'product_id' in data:
            # Invalidar cache cuando el producto se actualiza
            self.redis_client.sadd(f"invalidation:product:{data['product_id']}", key)
```

### **Cache Warming**
```python
# Sistema de cache warming
class CacheWarming:
    def __init__(self, caching_system):
        self.caching = caching_system
        self.warming_strategies = {
            'popular_users': self.warm_popular_users,
            'trending_products': self.warm_trending_products,
            'new_users': self.warm_new_users,
            'seasonal_content': self.warm_seasonal_content
        }
    
    def warm_cache(self, strategy='popular_users'):
        """Calentar cache usando estrategia espec√≠fica"""
        if strategy in self.warming_strategies:
            self.warming_strategies[strategy]()
    
    def warm_popular_users(self):
        """Calentar cache para usuarios populares"""
        popular_users = self.get_popular_users(limit=1000)
        
        for user in popular_users:
            recommendations = self.generate_recommendations(user['user_id'])
            self.caching.cache_recommendations(user['user_id'], recommendations)
    
    def warm_trending_products(self):
        """Calentar cache para productos trending"""
        trending_products = self.get_trending_products(limit=500)
        
        for product in trending_products:
            product_data = self.get_product_data(product['product_id'])
            self.caching.cache_product_data(product['product_id'], product_data)
    
    def warm_new_users(self):
        """Calentar cache para usuarios nuevos"""
        new_users = self.get_new_users(limit=100)
        
        for user in new_users:
            # Generar recomendaciones basadas en demograf√≠a
            recommendations = self.generate_demographic_recommendations(user)
            self.caching.cache_recommendations(user['user_id'], recommendations)
```

---

## **4. LOAD BALANCING** {#loadbalancing}

### **Load Balancer Inteligente**
```python
# Load balancer inteligente para recomendaciones
class IntelligentLoadBalancer:
    def __init__(self):
        self.servers = {
            'server_1': {'host': 'rec1.example.com', 'weight': 1, 'health': True},
            'server_2': {'host': 'rec2.example.com', 'weight': 1, 'health': True},
            'server_3': {'host': 'rec3.example.com', 'weight': 2, 'health': True},
            'server_4': {'host': 'rec4.example.com', 'weight': 1, 'health': True}
        }
        self.health_checker = HealthChecker()
        self.metrics_collector = MetricsCollector()
    
    def select_server(self, user_id, request_type):
        """Seleccionar servidor basado en algoritmo inteligente"""
        # Filtrar servidores saludables
        healthy_servers = [s for s in self.servers.values() if s['health']]
        
        if not healthy_servers:
            raise Exception("No healthy servers available")
        
        # Seleccionar servidor basado en estrategia
        if request_type == 'recommendations':
            return self.select_by_user_affinity(user_id, healthy_servers)
        elif request_type == 'analytics':
            return self.select_by_load(healthy_servers)
        else:
            return self.select_by_round_robin(healthy_servers)
    
    def select_by_user_affinity(self, user_id, servers):
        """Seleccionar servidor basado en afinidad del usuario"""
        # Usar hash del user_id para consistencia
        server_index = hash(user_id) % len(servers)
        return servers[server_index]
    
    def select_by_load(self, servers):
        """Seleccionar servidor con menor carga"""
        server_loads = self.metrics_collector.get_server_loads()
        min_load_server = min(servers, key=lambda s: server_loads.get(s['host'], 0))
        return min_load_server
    
    def health_check(self):
        """Verificar salud de servidores"""
        for server_name, server in self.servers.items():
            is_healthy = self.health_checker.check_server(server['host'])
            server['health'] = is_healthy
            
            if not is_healthy:
                self.handle_unhealthy_server(server_name)
    
    def handle_unhealthy_server(self, server_name):
        """Manejar servidor no saludable"""
        # Redirigir tr√°fico a otros servidores
        self.servers[server_name]['weight'] = 0
        
        # Notificar al equipo de operaciones
        self.notify_operations_team(f"Server {server_name} is unhealthy")
```

### **Circuit Breaker Pattern**
```python
# Circuit breaker para prevenir cascadas de fallos
class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN
    
    def call(self, func, *args, **kwargs):
        """Ejecutar funci√≥n con circuit breaker"""
        if self.state == 'OPEN':
            if self.should_attempt_reset():
                self.state = 'HALF_OPEN'
            else:
                raise Exception("Circuit breaker is OPEN")
        
        try:
            result = func(*args, **kwargs)
            self.on_success()
            return result
        except Exception as e:
            self.on_failure()
            raise e
    
    def on_success(self):
        """Manejar √©xito"""
        self.failure_count = 0
        self.state = 'CLOSED'
    
    def on_failure(self):
        """Manejar fallo"""
        self.failure_count += 1
        self.last_failure_time = datetime.utcnow()
        
        if self.failure_count >= self.failure_threshold:
            self.state = 'OPEN'
    
    def should_attempt_reset(self):
        """Verificar si se debe intentar reset"""
        if self.last_failure_time is None:
            return True
        
        time_since_failure = (datetime.utcnow() - self.last_failure_time).total_seconds()
        return time_since_failure >= self.timeout
```

---

## **5. MICROSERVICIOS** {#microservicios}

### **Servicio de Recomendaciones**
```python
# Microservicio de recomendaciones
from flask import Flask, request, jsonify
import asyncio

app = Flask(__name__)

class RecommendationMicroservice:
    def __init__(self):
        self.user_service = UserServiceClient()
        self.product_service = ProductServiceClient()
        self.interaction_service = InteractionServiceClient()
        self.recommendation_engine = RecommendationEngine()
    
    @app.route('/recommendations/<user_id>', methods=['GET'])
    async def get_recommendations(self, user_id):
        """Obtener recomendaciones para usuario"""
        try:
            # Obtener datos del usuario de forma as√≠ncrona
            user_data = await self.user_service.get_user_async(user_id)
            
            # Obtener contexto del producto
            product_context = await self.product_service.get_context_async(
                request.args.get('context', '')
            )
            
            # Generar recomendaciones
            recommendations = await self.recommendation_engine.generate_async(
                user_data, product_context
            )
            
            # Registrar interacci√≥n
            await self.interaction_service.log_interaction_async(
                user_id, 'recommendation_viewed', recommendations
            )
            
            return jsonify({
                'user_id': user_id,
                'recommendations': recommendations,
                'timestamp': datetime.utcnow().isoformat()
            })
            
        except Exception as e:
            return jsonify({'error': str(e)}), 500
    
    @app.route('/recommendations/batch', methods=['POST'])
    async def get_batch_recommendations(self):
        """Obtener recomendaciones para m√∫ltiples usuarios"""
        user_ids = request.json.get('user_ids', [])
        
        # Procesar en paralelo
        tasks = [self.get_recommendations(user_id) for user_id in user_ids]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return jsonify({
            'results': results,
            'processed_count': len(user_ids)
        })

if __name__ == '__main__':
    service = RecommendationMicroservice()
    app.run(host='0.0.0.0', port=5000)
```

### **Servicio de Analytics**
```python
# Microservicio de analytics
class AnalyticsMicroservice:
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.aggregator = MetricsAggregator()
        self.reporter = MetricsReporter()
    
    @app.route('/analytics/recommendations', methods=['GET'])
    def get_recommendation_metrics(self):
        """Obtener m√©tricas de recomendaciones"""
        time_range = request.args.get('time_range', '24h')
        
        metrics = {
            'total_recommendations': self.metrics_collector.get_total_recommendations(time_range),
            'click_through_rate': self.metrics_collector.get_ctr(time_range),
            'conversion_rate': self.metrics_collector.get_conversion_rate(time_range),
            'average_score': self.metrics_collector.get_average_score(time_range),
            'top_products': self.metrics_collector.get_top_products(time_range)
        }
        
        return jsonify(metrics)
    
    @app.route('/analytics/real-time', methods=['GET'])
    def get_real_time_metrics(self):
        """Obtener m√©tricas en tiempo real"""
        metrics = {
            'requests_per_second': self.metrics_collector.get_rps(),
            'active_users': self.metrics_collector.get_active_users(),
            'server_health': self.metrics_collector.get_server_health(),
            'cache_hit_rate': self.metrics_collector.get_cache_hit_rate()
        }
        
        return jsonify(metrics)
```

---

## **6. CDN Y DISTRIBUCI√ìN** {#cdn}

### **Distribuci√≥n de Contenido**
```python
# Sistema de CDN para recomendaciones
class CDNRecommendations:
    def __init__(self):
        self.cdn_providers = {
            'cloudflare': CloudflareCDN(),
            'aws_cloudfront': AWSCloudFront(),
            'azure_cdn': AzureCDN()
        }
        self.primary_cdn = 'cloudflare'
        self.fallback_cdn = 'aws_cloudfront'
    
    def distribute_recommendations(self, user_id, recommendations):
        """Distribuir recomendaciones a trav√©s de CDN"""
        # Generar contenido est√°tico
        static_content = self.generate_static_content(recommendations)
        
        # Subir a CDN primario
        cdn_url = self.cdn_providers[self.primary_cdn].upload_content(
            f"recommendations/{user_id}.json",
            static_content
        )
        
        # Configurar cache headers
        self.set_cache_headers(cdn_url, ttl=300)  # 5 minutos
        
        return cdn_url
    
    def get_recommendations_from_cdn(self, user_id):
        """Obtener recomendaciones desde CDN"""
        try:
            # Intentar CDN primario
            cdn_url = f"https://cdn.example.com/recommendations/{user_id}.json"
            response = requests.get(cdn_url, timeout=2)
            
            if response.status_code == 200:
                return response.json()
            
        except requests.RequestException:
            pass
        
        # Fallback a CDN secundario
        try:
            fallback_url = f"https://fallback-cdn.example.com/recommendations/{user_id}.json"
            response = requests.get(fallback_url, timeout=2)
            
            if response.status_code == 200:
                return response.json()
                
        except requests.RequestException:
            pass
        
        # Fallback a API directa
        return self.get_recommendations_from_api(user_id)
```

### **Edge Computing**
```python
# Edge computing para recomendaciones
class EdgeRecommendations:
    def __init__(self):
        self.edge_locations = {
            'us-east': EdgeLocation('us-east-1'),
            'us-west': EdgeLocation('us-west-1'),
            'europe': EdgeLocation('eu-west-1'),
            'asia': EdgeLocation('ap-southeast-1')
        }
    
    def get_edge_location(self, user_ip):
        """Determinar ubicaci√≥n edge basada en IP del usuario"""
        location = self.geoip_lookup(user_ip)
        
        if location['country'] in ['US', 'CA']:
            return 'us-east' if location['region'] in ['east', 'central'] else 'us-west'
        elif location['country'] in ['GB', 'DE', 'FR', 'ES']:
            return 'europe'
        elif location['country'] in ['JP', 'KR', 'SG', 'AU']:
            return 'asia'
        else:
            return 'us-east'  # Default
    
    def process_at_edge(self, user_id, request_data):
        """Procesar recomendaciones en edge location"""
        edge_location = self.get_edge_location(request_data['user_ip'])
        edge_server = self.edge_locations[edge_location]
        
        # Verificar si hay cache local
        cached_recommendations = edge_server.get_cache(user_id)
        if cached_recommendations:
            return cached_recommendations
        
        # Procesar en edge
        recommendations = edge_server.process_recommendations(user_id, request_data)
        
        # Cachear localmente
        edge_server.set_cache(user_id, recommendations, ttl=300)
        
        return recommendations
```

---

## **7. MONITOREO DE PERFORMANCE** {#monitoreo}

### **M√©tricas de Performance**
```python
# Sistema de monitoreo de performance
class PerformanceMonitor:
    def __init__(self):
        self.metrics = {
            'response_time': ResponseTimeMetric(),
            'throughput': ThroughputMetric(),
            'error_rate': ErrorRateMetric(),
            'cpu_usage': CPUUsageMetric(),
            'memory_usage': MemoryUsageMetric(),
            'database_connections': DatabaseConnectionsMetric()
        }
        self.alerting = AlertingSystem()
    
    def record_metric(self, metric_name, value, tags=None):
        """Registrar m√©trica"""
        if metric_name in self.metrics:
            self.metrics[metric_name].record(value, tags)
            
            # Verificar alertas
            self.check_alerts(metric_name, value)
    
    def check_alerts(self, metric_name, value):
        """Verificar alertas de performance"""
        alerts = {
            'response_time': lambda v: v > 2.0,  # > 2 segundos
            'error_rate': lambda v: v > 0.05,   # > 5%
            'cpu_usage': lambda v: v > 0.80,    # > 80%
            'memory_usage': lambda v: v > 0.90  # > 90%
        }
        
        if metric_name in alerts and alerts[metric_name](value):
            self.alerting.send_alert(metric_name, value)
    
    def get_performance_summary(self, time_range='1h'):
        """Obtener resumen de performance"""
        summary = {}
        
        for metric_name, metric in self.metrics.items():
            summary[metric_name] = {
                'current': metric.get_current_value(),
                'average': metric.get_average(time_range),
                'max': metric.get_max(time_range),
                'min': metric.get_min(time_range),
                'trend': metric.get_trend(time_range)
            }
        
        return summary
```

### **APM (Application Performance Monitoring)**
```python
# APM para recomendaciones
class RecommendationAPM:
    def __init__(self):
        self.tracer = Tracer()
        self.profiler = Profiler()
        self.memory_monitor = MemoryMonitor()
    
    def trace_recommendation_request(self, user_id):
        """Trazar solicitud de recomendaciones"""
        with self.tracer.start_span('recommendation_request') as span:
            span.set_tag('user_id', user_id)
            span.set_tag('service', 'recommendation_service')
            
            # Trazar sub-operaciones
            with self.tracer.start_span('get_user_data') as user_span:
                user_data = self.get_user_data(user_id)
                user_span.set_tag('user_data_size', len(str(user_data)))
            
            with self.tracer.start_span('generate_recommendations') as rec_span:
                recommendations = self.generate_recommendations(user_data)
                rec_span.set_tag('recommendations_count', len(recommendations))
            
            return recommendations
    
    def profile_memory_usage(self):
        """Perfilar uso de memoria"""
        memory_snapshot = self.memory_monitor.take_snapshot()
        
        # Identificar objetos que consumen m√°s memoria
        top_objects = memory_snapshot.get_top_objects(limit=10)
        
        # Generar reporte
        report = {
            'total_memory': memory_snapshot.total_memory,
            'peak_memory': memory_snapshot.peak_memory,
            'top_objects': top_objects,
            'memory_leaks': memory_snapshot.detect_leaks()
        }
        
        return report
```

---

## **8. OPTIMIZACI√ìN DE ALGORITMOS** {#algoritmos}

### **Algoritmos Optimizados**
```python
# Algoritmos de recomendaci√≥n optimizados para escala
class OptimizedRecommendationAlgorithms:
    def __init__(self):
        self.collaborative_filtering = OptimizedCollaborativeFiltering()
        self.content_based = OptimizedContentBased()
        self.hybrid = OptimizedHybrid()
    
    def optimized_collaborative_filtering(self, user_id, n_recommendations=10):
        """Filtrado colaborativo optimizado"""
        # Usar matriz dispersa para eficiencia de memoria
        user_item_matrix = self.get_sparse_user_item_matrix()
        
        # Calcular similitudes usando operaciones vectorizadas
        similarities = self.calculate_similarities_vectorized(user_item_matrix)
        
        # Usar √≠ndices para b√∫squeda r√°pida
        user_similarities = similarities[user_id]
        top_similar_users = self.get_top_similar_users(user_similarities, k=50)
        
        # Generar recomendaciones usando operaciones batch
        recommendations = self.generate_recommendations_batch(
            user_id, top_similar_users, n_recommendations
        )
        
        return recommendations
    
    def optimized_content_based(self, user_id, n_recommendations=10):
        """Filtrado basado en contenido optimizado"""
        # Obtener perfil del usuario
        user_profile = self.get_user_profile(user_id)
        
        # Usar TF-IDF optimizado
        tfidf_matrix = self.get_optimized_tfidf_matrix()
        
        # Calcular similitud usando producto punto vectorizado
        similarities = self.calculate_cosine_similarity_vectorized(
            user_profile, tfidf_matrix
        )
        
        # Obtener top productos
        top_products = self.get_top_products(similarities, n_recommendations)
        
        return top_products
    
    def calculate_similarities_vectorized(self, matrix):
        """Calcular similitudes usando operaciones vectorizadas"""
        # Normalizar matriz
        normalized_matrix = self.normalize_matrix(matrix)
        
        # Calcular similitud coseno usando producto punto
        similarities = np.dot(normalized_matrix, normalized_matrix.T)
        
        return similarities
```

### **Caching de Algoritmos**
```python
# Caching inteligente para algoritmos
class AlgorithmCaching:
    def __init__(self):
        self.cache = {}
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0
        }
    
    def cached_collaborative_filtering(self, user_id, n_recommendations=10):
        """Filtrado colaborativo con cache"""
        cache_key = f"cf:{user_id}:{n_recommendations}"
        
        # Verificar cache
        if cache_key in self.cache:
            self.cache_stats['hits'] += 1
            return self.cache[cache_key]
        
        # Generar recomendaciones
        recommendations = self.optimized_collaborative_filtering(user_id, n_recommendations)
        
        # Cachear resultado
        self.cache[cache_key] = recommendations
        self.cache_stats['misses'] += 1
        
        # Limpiar cache si es necesario
        if len(self.cache) > 10000:
            self.evict_oldest_entries()
        
        return recommendations
    
    def evict_oldest_entries(self):
        """Eliminar entradas m√°s antiguas del cache"""
        # Implementar LRU eviction
        oldest_key = min(self.cache.keys(), key=lambda k: self.cache[k]['timestamp'])
        del self.cache[oldest_key]
        self.cache_stats['evictions'] += 1
```

---

## **9. AUTO-SCALING** {#autoscaling}

### **Auto-scaling Horizontal**
```python
# Auto-scaling horizontal para recomendaciones
class HorizontalAutoScaling:
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.scaling_policies = {
            'cpu_based': CPUBasedScalingPolicy(),
            'memory_based': MemoryBasedScalingPolicy(),
            'request_based': RequestBasedScalingPolicy()
        }
        self.scaling_actions = {
            'scale_up': self.scale_up,
            'scale_down': self.scale_down,
            'no_action': lambda: None
        }
    
    def evaluate_scaling_decision(self):
        """Evaluar decisi√≥n de scaling"""
        current_metrics = self.metrics_collector.get_current_metrics()
        
        # Evaluar cada pol√≠tica
        scaling_decisions = []
        for policy_name, policy in self.scaling_policies.items():
            decision = policy.evaluate(current_metrics)
            scaling_decisions.append(decision)
        
        # Tomar decisi√≥n final
        final_decision = self.aggregate_decisions(scaling_decisions)
        
        # Ejecutar acci√≥n
        self.scaling_actions[final_decision]()
    
    def scale_up(self):
        """Escalar hacia arriba"""
        current_instances = self.get_current_instance_count()
        max_instances = self.get_max_instance_count()
        
        if current_instances < max_instances:
            new_instances = min(current_instances + 2, max_instances)
            self.launch_instances(new_instances - current_instances)
            
            self.log_scaling_event('scale_up', current_instances, new_instances)
    
    def scale_down(self):
        """Escalar hacia abajo"""
        current_instances = self.get_current_instance_count()
        min_instances = self.get_min_instance_count()
        
        if current_instances > min_instances:
            new_instances = max(current_instances - 1, min_instances)
            self.terminate_instances(current_instances - new_instances)
            
            self.log_scaling_event('scale_down', current_instances, new_instances)
```

### **Auto-scaling Vertical**
```python
# Auto-scaling vertical para recursos
class VerticalAutoScaling:
    def __init__(self):
        self.resource_monitor = ResourceMonitor()
        self.scaling_thresholds = {
            'cpu_scale_up': 0.80,
            'cpu_scale_down': 0.30,
            'memory_scale_up': 0.85,
            'memory_scale_down': 0.40
        }
    
    def evaluate_vertical_scaling(self):
        """Evaluar scaling vertical"""
        current_resources = self.resource_monitor.get_current_resources()
        
        # Evaluar CPU
        if current_resources['cpu'] > self.scaling_thresholds['cpu_scale_up']:
            self.scale_cpu_up()
        elif current_resources['cpu'] < self.scaling_thresholds['cpu_scale_down']:
            self.scale_cpu_down()
        
        # Evaluar memoria
        if current_resources['memory'] > self.scaling_thresholds['memory_scale_up']:
            self.scale_memory_up()
        elif current_resources['memory'] < self.scaling_thresholds['memory_scale_down']:
            self.scale_memory_down()
    
    def scale_cpu_up(self):
        """Aumentar CPU"""
        current_cpu = self.get_current_cpu()
        new_cpu = min(current_cpu * 1.5, self.get_max_cpu())
        
        self.update_instance_cpu(new_cpu)
        self.log_scaling_event('cpu_up', current_cpu, new_cpu)
    
    def scale_memory_up(self):
        """Aumentar memoria"""
        current_memory = self.get_current_memory()
        new_memory = min(current_memory * 1.5, self.get_max_memory())
        
        self.update_instance_memory(new_memory)
        self.log_scaling_event('memory_up', current_memory, new_memory)
```

---

## **10. CASOS DE USO DE ESCALA** {#casos}

### **Caso 1: E-commerce con 1M+ Usuarios**
```python
# Arquitectura para e-commerce masivo
class MassiveEcommerceRecommendations:
    def __init__(self):
        self.architecture = {
            'load_balancer': AWSApplicationLoadBalancer(),
            'api_gateway': AWSAPIGateway(),
            'microservices': {
                'user_service': UserMicroservice(),
                'product_service': ProductMicroservice(),
                'recommendation_service': RecommendationMicroservice(),
                'analytics_service': AnalyticsMicroservice()
            },
            'databases': {
                'primary': AuroraCluster(),
                'read_replicas': [AuroraReadReplica() for _ in range(3)],
                'cache': ElastiCacheCluster()
            },
            'cdn': CloudFrontDistribution(),
            'monitoring': CloudWatchMonitoring()
        }
    
    def handle_peak_traffic(self, traffic_multiplier=10):
        """Manejar tr√°fico pico"""
        # Escalar horizontalmente
        self.architecture['microservices']['recommendation_service'].scale_to(
            instances=20 * traffic_multiplier
        )
        
        # Activar read replicas adicionales
        self.architecture['databases']['read_replicas'].scale_to(
            replicas=5 * traffic_multiplier
        )
        
        # Configurar CDN para cache agresivo
        self.architecture['cdn'].set_cache_ttl(3600)  # 1 hora
        
        # Activar auto-scaling
        self.architecture['load_balancer'].enable_auto_scaling()
```

### **Caso 2: SaaS con 100K+ Empresas**
```python
# Arquitectura para SaaS multi-tenant
class MultiTenantSaaSRecommendations:
    def __init__(self):
        self.tenant_isolation = TenantIsolation()
        self.resource_quota = ResourceQuotaManager()
        self.performance_monitoring = PerformanceMonitoring()
    
    def handle_multi_tenant_requests(self, tenant_id, user_id):
        """Manejar solicitudes multi-tenant"""
        # Verificar cuota del tenant
        if not self.resource_quota.check_quota(tenant_id):
            raise QuotaExceededException(f"Tenant {tenant_id} quota exceeded")
        
        # Aislar datos del tenant
        isolated_data = self.tenant_isolation.get_tenant_data(tenant_id, user_id)
        
        # Generar recomendaciones
        recommendations = self.generate_tenant_recommendations(
            tenant_id, isolated_data
        )
        
        # Monitorear performance por tenant
        self.performance_monitoring.record_tenant_metric(
            tenant_id, 'recommendation_generated', 1
        )
        
        return recommendations
```

---

## **11. MEJORES PR√ÅCTICAS** {#mejores}

### **Principios de Escalabilidad**
1. **Dise√±o Stateless**: Servicios sin estado
2. **Caching Agresivo**: Cache en m√∫ltiples niveles
3. **Asincron√≠a**: Operaciones as√≠ncronas
4. **Particionamiento**: Dividir datos grandes
5. **Monitoreo Continuo**: Vigilar m√©tricas clave

### **Patrones de Escalabilidad**
```python
# Patrones de escalabilidad
class ScalabilityPatterns:
    def __init__(self):
        self.patterns = {
            'database_sharding': self.implement_database_sharding,
            'read_replicas': self.implement_read_replicas,
            'caching_layers': self.implement_caching_layers,
            'async_processing': self.implement_async_processing,
            'microservices': self.implement_microservices
        }
    
    def implement_database_sharding(self):
        """Implementar sharding de base de datos"""
        # Dividir datos por user_id
        shard_key = lambda user_id: hash(user_id) % 4
        
        # Configurar conexiones por shard
        shard_connections = {
            i: DatabaseConnection(f'shard_{i}.example.com')
            for i in range(4)
        }
        
        return shard_connections
    
    def implement_read_replicas(self):
        """Implementar r√©plicas de lectura"""
        # Configurar r√©plicas de lectura
        read_replicas = [
            DatabaseConnection('read_replica_1.example.com'),
            DatabaseConnection('read_replica_2.example.com'),
            DatabaseConnection('read_replica_3.example.com')
        ]
        
        # Balanceador de carga para r√©plicas
        replica_balancer = LoadBalancer(read_replicas)
        
        return replica_balancer
```

---

## **12. TROUBLESHOOTING** {#troubleshooting}

### **Problemas Comunes de Escalabilidad**

#### **Alto Uso de CPU**
```python
# Soluci√≥n para alto uso de CPU
def optimize_cpu_usage():
    """Optimizar uso de CPU"""
    # 1. Usar operaciones vectorizadas
    similarities = np.dot(user_matrix, item_matrix.T)
    
    # 2. Paralelizar operaciones
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(process_batch, batch) for batch in batches]
        results = [future.result() for future in futures]
    
    # 3. Usar cache para evitar rec√°lculos
    cached_result = cache.get(computation_key)
    if cached_result:
        return cached_result
    
    # 4. Optimizar algoritmos
    optimized_algorithm = OptimizedRecommendationAlgorithm()
    return optimized_algorithm.compute()
```

#### **Alto Uso de Memoria**
```python
# Soluci√≥n para alto uso de memoria
def optimize_memory_usage():
    """Optimizar uso de memoria"""
    # 1. Usar matrices dispersas
    sparse_matrix = scipy.sparse.csr_matrix(dense_matrix)
    
    # 2. Procesar datos en chunks
    for chunk in read_data_in_chunks(chunk_size=1000):
        process_chunk(chunk)
    
    # 3. Liberar memoria expl√≠citamente
    del large_object
    gc.collect()
    
    # 4. Usar generadores
    for item in data_generator():
        process_item(item)
```

#### **Lentitud de Base de Datos**
```python
# Soluci√≥n para lentitud de base de datos
def optimize_database_performance():
    """Optimizar performance de base de datos"""
    # 1. Crear √≠ndices apropiados
    create_index('recommendations', 'user_id, score DESC')
    
    # 2. Usar consultas preparadas
    prepared_query = connection.prepare("SELECT * FROM recommendations WHERE user_id = ?")
    
    # 3. Implementar paginaci√≥n
    offset = page * page_size
    query = f"SELECT * FROM recommendations LIMIT {page_size} OFFSET {offset}"
    
    # 4. Usar read replicas
    read_connection = get_read_replica_connection()
    results = read_connection.execute(query)
    
    return results
```

---

## **üéØ PR√ìXIMOS PASOS**

1. **Auditor√≠a de Performance**: Evaluar estado actual
2. **Implementar Monitoreo**: Configurar m√©tricas clave
3. **Optimizar Algoritmos**: Mejorar eficiencia
4. **Configurar Auto-scaling**: Automatizar escalado
5. **Probar Carga**: Realizar pruebas de estr√©s

---

## **üìû SOPORTE**

- **Consultor√≠a de Escalabilidad**: [Especialistas en arquitectura]
- **Monitoreo de Performance**: [Herramientas de APM]
- **Optimizaci√≥n de Base de Datos**: [DBA especializados]
- **Soporte T√©cnico**: [Asistencia para escalabilidad]

---

**¬°Con estas optimizaciones de escalabilidad y performance, tu sistema de recomendaciones podr√° manejar cualquier volumen de tr√°fico!** ‚ö°




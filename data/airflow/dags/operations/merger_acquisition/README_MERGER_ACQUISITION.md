# Orquestaci√≥n de Integraciones para Fusiones y Adquisiciones

Este DAG (`merger_acquisition_integration`) orquesta la integraci√≥n de m√∫ltiples sistemas durante procesos de fusiones o adquisiciones.

## üöÄ Mejoras Implementadas

### Caracter√≠sticas Avanzadas

1. **Retry Logic Inteligente**
   - Reintentos autom√°ticos con exponential backoff
   - Soporte para tenacity (si est√° disponible)
   - Manejo de errores transitorios (conexi√≥n, timeout)

2. **Procesamiento por Chunks**
   - Procesamiento eficiente de grandes vol√∫menes de datos
   - Configurable via par√°metro `chunk_size`
   - Logging de progreso para datasets grandes

3. **Sistema de Backups**
   - Creaci√≥n autom√°tica de backups antes de cargar datos
   - Tablas de backup con timestamp
   - Restauraci√≥n f√°cil en caso de problemas

4. **Soporte S3**
   - Lectura directa de archivos desde S3
   - Soporte para CSV, Excel, JSON
   - Parsing autom√°tico de paths S3

5. **Validaci√≥n Mejorada**
   - Validaci√≥n temprana de configuraciones
   - Estad√≠sticas detalladas de extracci√≥n
   - Manejo robusto de errores por sistema

6. **Logging Estructurado**
   - Logs con emojis para f√°cil identificaci√≥n (‚úì √©xito, ‚úó error)
   - Progreso detallado por sistema
   - Estad√≠sticas de rendimiento

7. **Manejo de Memoria**
   - Procesamiento por chunks para evitar problemas de memoria
   - Limpieza autom√°tica de recursos
   - Optimizaci√≥n para grandes datasets

8. **Sistema de Notificaciones**
   - Soporte para Slack, Webhooks y Email
   - Notificaciones autom√°ticas basadas en estado
   - Configuraci√≥n flexible de canales

9. **Exportaci√≥n de Reportes**
   - Exportaci√≥n a JSON, CSV y HTML
   - Reportes detallados con m√©tricas
   - Archivos timestamped para trazabilidad

10. **Comparaci√≥n de Datos**
    - Comparaci√≥n antes/despu√©s de la carga
    - Identificaci√≥n de registros nuevos, actualizados y eliminados
    - An√°lisis de cambios por categor√≠a

11. **Rollback Autom√°tico**
    - Rollback autom√°tico en caso de errores cr√≠ticos
    - Restauraci√≥n desde backups
    - Configuraci√≥n de umbral de error

12. **Sistema de Cache**
    - Cache de extracciones para evitar re-extracciones
    - Cache en PostgreSQL con TTL configurable
    - Reducci√≥n de carga en sistemas fuente

13. **Validaci√≥n de Integridad Referencial**
    - Validaci√≥n de relaciones entre tablas
    - Detecci√≥n de registros hu√©rfanos
    - Configuraci√≥n flexible de reglas

14. **Enriquecimiento de Datos**
    - Enriquecimiento con APIs externas
    - Procesamiento en batches
    - Manejo robusto de errores

15. **M√©tricas Prometheus**
    - M√©tricas de extracci√≥n, carga y errores
    - Histogramas de duraci√≥n
    - Contadores de cache hits
    - Integraci√≥n con sistemas de monitoreo

16. **Sistema de Auditor√≠a**
    - Registro completo de todas las operaciones
    - Trazabilidad de cambios
    - Logs con detalles de ejecuci√≥n
    - Integraci√≥n con sistemas de compliance

17. **Validaci√≥n de Calidad Personalizada**
    - Reglas de completitud, unicidad y validez
    - Validaci√≥n por campo y categor√≠a
    - Umbrales configurables
    - Reportes de violaciones

18. **Detecci√≥n de Drift de Datos**
    - Comparaci√≥n con baseline
    - Identificaci√≥n de cambios significativos
    - Alertas de drift
    - An√°lisis de tendencias

19. **Linaje de Datos**
    - Trazabilidad completa de datos
    - Mapeo de sistemas fuente a destino
    - Registro de transformaciones
    - ID √∫nico de linaje

20. **Verificaci√≥n de Integridad con Hash**
    - C√°lculo de hash SHA256 de datos
    - Verificaci√≥n de integridad
    - Detecci√≥n de corrupci√≥n
    - Validaci√≥n de consistencia

21. **Procesamiento Paralelo**
    - Extracci√≥n paralela de m√∫ltiples sistemas
    - ThreadPoolExecutor para mejor rendimiento
    - Procesamiento concurrente hasta 5 workers
    - Reducci√≥n significativa de tiempo de ejecuci√≥n

22. **Validaci√≥n de Esquemas**
    - Validaci√≥n de estructura de datos
    - Verificaci√≥n de tipos de campos
    - Validaci√≥n de restricciones (longitud, rangos)
    - Detecci√≥n temprana de problemas de formato

23. **Health Checks de Sistemas**
    - Verificaci√≥n de salud antes de extraer
    - Medici√≥n de tiempo de respuesta
    - Detecci√≥n de sistemas no disponibles
    - Prevenci√≥n de extracciones fallidas

24. **Circuit Breaker Pattern**
    - Protecci√≥n contra fallos en cascada
    - Estados: closed, open, half-open
    - Recuperaci√≥n autom√°tica
    - Configuraci√≥n de umbrales

25. **Encriptaci√≥n de Datos Sensibles**
    - Encriptaci√≥n de campos sensibles
    - Hash SHA256 para protecci√≥n
    - Marcado de campos encriptados
    - Preparado para encriptaci√≥n real

26. **Compresi√≥n de Datos**
    - Soporte para gzip y zlib
    - Reducci√≥n de tama√±o de almacenamiento
    - Compresi√≥n/descompresi√≥n autom√°tica
    - Optimizaci√≥n de transferencia

27. **Detecci√≥n de Anomal√≠as**
    - An√°lisis estad√≠stico de datos num√©ricos
    - Detecci√≥n usando desviaci√≥n est√°ndar
    - Identificaci√≥n de valores at√≠picos
    - Estad√≠sticas por campo

28. **Carga Incremental**
    - Procesamiento solo de cambios
    - Identificaci√≥n de registros nuevos/actualizados
    - Reducci√≥n de tiempo de procesamiento
    - Optimizaci√≥n de recursos

29. **Resoluci√≥n de Conflictos**
    - M√∫ltiples estrategias de resoluci√≥n
    - Merge inteligente de datos
    - Priorizaci√≥n por timestamp
    - Configuraci√≥n flexible

30. **Dead Letter Queue (DLQ)**
    - Almacenamiento de registros con errores
    - Reintento posterior
    - An√°lisis de errores
    - Trazabilidad de fallos

31. **Reglas de Negocio**
    - Aplicaci√≥n de reglas personalizadas
    - Transformaciones condicionales
    - C√°lculos din√°micos
    - Validaci√≥n de l√≥gica de negocio

## Flujo del Proceso

```
Health Checks (opcional)
    ‚Üì
Extracci√≥n Empresa A ‚îÄ‚îÄ‚îê
                      ‚îú‚îÄ‚Üí Transformaci√≥n ‚Üí Validaci√≥n Esquema (opcional)
Extracci√≥n Empresa B ‚îÄ‚îÄ‚îò                    ‚Üì
                                    Validaci√≥n Datos
                                    ‚Üì
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ                                       ‚îÇ
        Validaci√≥n Calidad (opcional)          Detecci√≥n Anomal√≠as (opcional)
                    ‚îÇ                                       ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚Üì
                                 Carga Sistema Unificado
                                    ‚Üì
                                 Reporte de Estado
                                    ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                                                         ‚îÇ
Comparaci√≥n (opcional)                          Exportaci√≥n (opcional)
        ‚îÇ                                                         ‚îÇ
Integridad Referencial (opcional)          Notificaciones (opcional)
        ‚îÇ                                                         ‚îÇ
Rollback (si error)                                    Linaje de Datos
```

## Componentes

### 1. Extracci√≥n de Datos
- **Empresa A**: Extrae datos de todos los sistemas configurados
- **Empresa B**: Extrae datos de todos los sistemas configurados
- **Sistemas soportados**:
  - PostgreSQL
  - MySQL
  - APIs REST
  - Archivos CSV/Excel (S3)

### 2. Transformaci√≥n
- Normaliza datos de ambas empresas a formato com√∫n
- Aplica reglas de mapeo de campos
- Deduplicaci√≥n
- Enriquecimiento de datos

### 3. Validaci√≥n
- Valida campos requeridos
- Validaci√≥n de formatos (email, n√∫meros, fechas)
- Validaci√≥n de rangos y longitudes
- Modo estricto o permisivo

### 4. Carga
- Carga datos validados al sistema unificado
- Soporta PostgreSQL, MySQL, APIs REST
- Manejo de errores y reintentos

### 5. Reporte
- Genera reporte completo del estado de la integraci√≥n
- M√©tricas de calidad y tasa de √©xito
- Detalles por categor√≠a

## Uso

### Ejecuci√≥n Manual desde UI

1. Ir a Airflow UI ‚Üí DAGs ‚Üí `merger_acquisition_integration`
2. Click en "Trigger DAG w/ config"
3. Proporcionar par√°metros en formato JSON

### Ejecuci√≥n v√≠a CLI

```bash
airflow dags trigger merger_acquisition_integration \
  --conf '{
    "company_a_config": {...},
    "company_b_config": {...},
    "unified_system_config": {...},
    "transformation_rules": {...}
  }'
```

### Ejecuci√≥n v√≠a API

```python
import requests
from airflow.api.client.local_client import Client

client = Client(None, None)
client.trigger_dag(
    dag_id="merger_acquisition_integration",
    conf={
        "company_a_config": {...},
        "company_b_config": {...},
        "unified_system_config": {...},
        "transformation_rules": {...}
    }
)
```

## Configuraci√≥n de Par√°metros

### company_a_config

Configuraci√≥n para extraer datos de la empresa A:

```json
{
  "systems": [
    {
      "type": "postgres",
      "name": "crm_company_a",
      "conn_id": "postgres_company_a",
      "tables": ["customers", "orders", "products"],
      "where_clause": "created_at >= '2020-01-01'"
    },
    {
      "type": "mysql",
      "name": "erp_company_a",
      "conn_id": "mysql_company_a",
      "tables": ["clients", "transactions"],
      "where_clause": "status = 'active'"
    },
    {
      "type": "api",
      "name": "salesforce_company_a",
      "conn_id": "http_salesforce_a",
      "endpoint": "/services/data/v52.0/query/?q=SELECT+Id,Name+FROM+Account",
      "headers": {
        "Authorization": "Bearer {{token}}"
      },
      "data_key": "records"
    }
  ]
}
```

### company_b_config

Similar a `company_a_config` pero para empresa B:

```json
{
  "systems": [
    {
      "type": "postgres",
      "name": "crm_company_b",
      "conn_id": "postgres_company_b",
      "tables": ["clients", "orders", "items"]
    }
  ]
}
```

### unified_system_config

Configuraci√≥n del sistema unificado destino:

```json
{
  "type": "postgres",
  "conn_id": "postgres_unified",
  "schema": "unified"
}
```

O para API:

```json
{
  "type": "api",
  "conn_id": "http_unified_api",
  "endpoint": "/api/v1/data/load",
  "headers": {
    "Content-Type": "application/json",
    "Authorization": "Bearer {{token}}"
  }
}
```

### transformation_rules

Reglas de transformaci√≥n y mapeo:

```json
{
  "mappings": {
    "customers": {
      "company_a": ["customers", "customer_data"],
      "company_b": ["clients", "customers"],
      "field_mapping": {
        "email": ["email", "e_mail", "email_address"],
        "name": ["name", "full_name", "customer_name"],
        "phone": ["phone", "phone_number"],
        "address": ["address", "street_address"]
      }
    },
    "orders": {
      "company_a": ["orders"],
      "company_b": ["transactions", "orders"],
      "field_mapping": {
        "order_id": ["order_id", "id", "transaction_id"],
        "customer_id": ["customer_id", "client_id"],
        "amount": ["amount", "total", "order_total"],
        "date": ["date", "order_date", "created_at"]
      }
    }
  },
  "transformations": [
    {
      "type": "deduplicate",
      "key_fields": ["email", "order_id"]
    },
    {
      "type": "enrich",
      "rules": {
        "integration_date": "{{current_date}}",
        "source": "merger_acquisition"
      }
    }
  ]
}
```

## Ejemplo Completo

### Escenario: Fusi√≥n de dos empresas de e-commerce

**Empresa A** usa:
- PostgreSQL para CRM
- MySQL para ERP
- Salesforce para ventas

**Empresa B** usa:
- PostgreSQL para CRM
- API REST para inventario

**Sistema Unificado**: PostgreSQL con schema `unified`

```json
{
  "company_a_config": {
    "systems": [
      {
        "type": "postgres",
        "name": "crm_a",
        "conn_id": "postgres_a",
        "tables": ["customers", "orders"]
      },
      {
        "type": "mysql",
        "name": "erp_a",
        "conn_id": "mysql_a",
        "tables": ["products", "inventory"]
      },
      {
        "type": "api",
        "name": "salesforce_a",
        "conn_id": "http_salesforce",
        "endpoint": "/services/data/v52.0/query/?q=SELECT+Id,Name,Email+FROM+Contact",
        "data_key": "records"
      }
    ]
  },
  "company_b_config": {
    "systems": [
      {
        "type": "postgres",
        "name": "crm_b",
        "conn_id": "postgres_b",
        "tables": ["clients", "transactions"]
      },
      {
        "type": "api",
        "name": "inventory_b",
        "conn_id": "http_inventory_b",
        "endpoint": "/api/inventory/items",
        "data_key": "items"
      }
    ]
  },
  "unified_system_config": {
    "type": "postgres",
    "conn_id": "postgres_unified",
    "schema": "unified"
  },
  "transformation_rules": {
    "mappings": {
      "customers": {
        "company_a": ["customers"],
        "company_b": ["clients"],
        "field_mapping": {
          "email": ["email", "e_mail"],
          "name": ["name", "full_name", "customer_name"],
          "phone": ["phone", "phone_number"]
        }
      },
      "orders": {
        "company_a": ["orders"],
        "company_b": ["transactions"],
        "field_mapping": {
          "order_id": ["order_id", "id", "transaction_id"],
          "customer_id": ["customer_id", "client_id"],
          "amount": ["amount", "total"],
          "date": ["date", "order_date"]
        }
      },
      "products": {
        "company_a": ["products"],
        "company_b": ["items"],
        "field_mapping": {
          "product_id": ["product_id", "id", "item_id"],
          "name": ["name", "product_name", "item_name"],
          "price": ["price", "unit_price"]
        }
      }
    },
    "transformations": [
      {
        "type": "deduplicate",
        "key_fields": ["email"]
      }
    ]
  },
  "dry_run": false,
  "validation_strict": true,
  "generate_detailed_report": true
}
```

## Modo Dry-Run

Para probar el flujo sin cargar datos:

```json
{
  "dry_run": true,
  ...
}
```

En modo dry-run:
- Se extraen los datos
- Se transforman
- Se validan
- **NO se cargan** al sistema unificado
- Se genera reporte de lo que se cargar√≠a

## Validaci√≥n

### Modo Estricto (default)

- Falla si hay registros inv√°lidos
- Requiere todos los campos obligatorios
- Valida formatos estrictamente

### Modo Permisivo

```json
{
  "validation_strict": false,
  ...
}
```

- Contin√∫a aunque haya registros inv√°lidos
- Genera warnings en lugar de errores
- √ötil para an√°lisis de calidad de datos

## Reporte de Estado

El reporte incluye:

- **Resumen ejecutivo**:
  - Total de registros extra√≠dos (A + B)
  - Total transformados y cargados
  - Tasa de validaci√≥n
  - Estado general

- **Detalles por etapa**:
  - Extracci√≥n: sistemas procesados, registros por sistema
  - Transformaci√≥n: registros por categor√≠a
  - Validaci√≥n: registros v√°lidos/inv√°lidos, errores
  - Carga: registros cargados, tablas, errores

- **M√©tricas**:
  - Tasa de extracci√≥n ‚Üí carga
  - Score de calidad de datos
  - Tasa de √©xito general

## Configuraci√≥n de Notificaciones

### notification_config

Configuraci√≥n para enviar notificaciones:

```json
{
  "slack": {
    "enabled": true,
    "webhook_url": "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"
  },
  "webhook": {
    "enabled": true,
    "url": "https://your-webhook-endpoint.com/notify"
  },
  "email": {
    "enabled": true,
    "to": ["team@company.com", "manager@company.com"]
  }
}
```

Las notificaciones se env√≠an autom√°ticamente al completar la integraci√≥n con el estado y m√©tricas.

## Exportaci√≥n de Reportes

Para exportar reportes a archivo:

```json
{
  "export_report": true,
  "export_format": "html"
}
```

Formatos disponibles:
- `json`: Formato JSON completo (m√°quina)
- `csv`: Resumen en CSV (Excel)
- `html`: Reporte visual HTML (humano)

## Comparaci√≥n de Datos

Para comparar datos antes y despu√©s de la carga:

```json
{
  "enable_comparison": true
}
```

La comparaci√≥n muestra:
- Registros antes y despu√©s
- Registros nuevos agregados
- Registros actualizados
- Registros eliminados

## Rollback Autom√°tico

Para habilitar rollback autom√°tico en caso de errores:

```json
{
  "enable_rollback_on_error": true
}
```

El rollback se ejecuta autom√°ticamente si:
- La tasa de error es > 50%
- Hay backups disponibles
- El sistema destino es PostgreSQL

## Sistema de Cache

Para habilitar cache de extracciones:

```json
{
  "enable_cache": true
}
```

El cache:
- Reduce carga en sistemas fuente
- Acelera re-ejecuciones
- TTL de 1 hora por defecto
- Almacenado en PostgreSQL

## Validaci√≥n de Integridad Referencial

Para validar relaciones entre tablas:

```json
{
  "enable_referential_integrity_check": true,
  "referential_integrity_rules": [
    {
      "parent_category": "customers",
      "child_category": "orders",
      "parent_key": "customer_id",
      "child_key": "customer_id"
    }
  ]
}
```

La validaci√≥n detecta:
- Registros hu√©rfanos (orders sin customer)
- Relaciones rotas
- Problemas de integridad

## Enriquecimiento de Datos

Para enriquecer datos con APIs externas:

```json
{
  "enable_data_enrichment": true,
  "enrichment_config": {
    "enabled": true,
    "api_url": "https://api.example.com/enrich",
    "api_key": "your-api-key",
    "batch_size": 100,
    "field_mapping": {
      "email": "email_address",
      "name": "full_name"
    }
  }
}
```

## M√©tricas Prometheus

Las m√©tricas est√°n habilitadas por defecto y exponen:

- `merger_acquisition_records_extracted_total`: Total de registros extra√≠dos
- `merger_acquisition_records_loaded_total`: Total de registros cargados
- `merger_acquisition_duration_seconds`: Duraci√≥n por etapa
- `merger_acquisition_errors_total`: Total de errores
- `merger_acquisition_cache_hits_total`: Cache hits

Para deshabilitar:
```json
{
  "enable_metrics": false
}
```

## Sistema de Auditor√≠a

El sistema de auditor√≠a est√° habilitado por defecto y registra:

- Todas las operaciones realizadas
- Detalles de ejecuci√≥n
- Usuario y timestamp
- Resultados y errores

Para deshabilitar:
```json
{
  "enable_audit_log": false
}
```

Los registros se almacenan en `integration_audit_log` con √≠ndices para b√∫squeda r√°pida.

## Validaci√≥n de Calidad Personalizada

Para definir reglas de calidad personalizadas:

```json
{
  "enable_data_quality_rules": true,
  "data_quality_rules": [
    {
      "name": "completeness_email",
      "type": "completeness",
      "field": "email",
      "threshold": 95.0
    },
    {
      "name": "uniqueness_customer_id",
      "type": "uniqueness",
      "field": "customer_id",
      "threshold": 100.0
    },
    {
      "name": "validity_email_format",
      "type": "validity",
      "field": "email",
      "condition": "regex",
      "pattern": "^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$",
      "threshold": 98.0
    }
  ]
}
```

Tipos de reglas:
- `completeness`: Porcentaje de campos no nulos
- `uniqueness`: Porcentaje de valores √∫nicos
- `validity`: Porcentaje de valores v√°lidos (regex o range)

## Detecci√≥n de Drift

Para detectar drift de datos:

```json
{
  "enable_drift_detection": true
}
```

La detecci√≥n compara:
- Registros agregados
- Registros eliminados
- Registros modificados
- Porcentaje de drift total

## Linaje de Datos

El linaje est√° habilitado por defecto y genera:

- ID √∫nico de linaje
- Sistemas fuente
- Transformaciones aplicadas
- Sistema destino
- Timestamp

Para deshabilitar:
```json
{
  "enable_data_lineage": false
}
```

## Procesamiento Paralelo

Para habilitar extracci√≥n paralela de sistemas:

```json
{
  "enable_parallel_extraction": true
}
```

Beneficios:
- Reducci√≥n de tiempo de ejecuci√≥n
- Hasta 5 workers paralelos
- Mejor utilizaci√≥n de recursos
- Fallos aislados por sistema

## Validaci√≥n de Esquemas

Para validar estructura de datos:

```json
{
  "enable_schema_validation": true,
  "expected_schema": {
    "required_fields": ["email", "name", "customer_id"],
    "field_types": {
      "email": "string",
      "customer_id": "integer",
      "amount": "float",
      "active": "boolean"
    },
    "field_constraints": {
      "email": {
        "min_length": 5,
        "max_length": 255
      },
      "amount": {
        "min_value": 0,
        "max_value": 1000000
      }
    }
  }
}
```

## Health Checks

Para verificar salud de sistemas antes de extraer:

```json
{
  "enable_health_checks": true
}
```

Los health checks verifican:
- Disponibilidad de sistemas
- Tiempo de respuesta
- Estado de conexiones
- APIs y endpoints

## Circuit Breaker

Para proteger contra fallos en cascada:

```json
{
  "enable_circuit_breaker": true
}
```

El circuit breaker:
- Abre el circuito despu√©s de N fallos
- Bloquea llamadas cuando est√° abierto
- Intenta recuperaci√≥n despu√©s de timeout
- Transici√≥n a half-open para testing

## Encriptaci√≥n de Datos Sensibles

Para encriptar campos sensibles:

```json
{
  "enable_encryption": true,
  "sensitive_fields": ["ssn", "credit_card", "password", "api_key"]
}
```

Los campos sensibles se marcan y protegen antes de almacenar.

## Detecci√≥n de Anomal√≠as

Para detectar anomal√≠as en datos num√©ricos:

```json
{
  "enable_anomaly_detection": true,
  "anomaly_threshold": 3.0
}
```

La detecci√≥n:
- Analiza campos num√©ricos (amount, price, quantity, etc.)
- Usa desviaci√≥n est√°ndar (default: 3œÉ)
- Identifica valores at√≠picos
- Proporciona estad√≠sticas (mean, std, min, max)

## Carga Incremental

Para procesar solo cambios:

```json
{
  "enable_incremental_load": true,
  "incremental_key": "id"
}
```

La carga incremental:
- Identifica registros nuevos o actualizados
- Compara con √∫ltimo timestamp
- Reduce tiempo de procesamiento
- Optimiza uso de recursos

## Resoluci√≥n de Conflictos

Para manejar conflictos entre empresas:

```json
{
  "conflict_resolution_strategy": "latest"
}
```

Estrategias disponibles:
- `latest`: Usar registro m√°s reciente (por timestamp)
- `company_a`: Priorizar empresa A
- `company_b`: Priorizar empresa B
- `merge`: Combinar campos de ambas empresas

## Dead Letter Queue

Para capturar registros con errores:

```json
{
  "enable_dead_letter_queue": true
}
```

La DLQ:
- Almacena registros que fallaron
- Permite reintento posterior
- Facilita an√°lisis de errores
- Tabla: `integration_dlq`

## Reglas de Negocio

Para aplicar reglas personalizadas:

```json
{
  "enable_business_rules": true,
  "business_rules": [
    {
      "name": "calculate_total",
      "type": "field_comparison",
      "field1": "quantity",
      "field2": "price",
      "operator": ">",
      "action": "calculate",
      "target_field": "total",
      "formula": "record['quantity'] * record['price']"
    },
    {
      "name": "set_status",
      "type": "value_check",
      "field": "amount",
      "expected_value": 0,
      "action": "set_value",
      "target_field": "status",
      "value": "inactive"
    }
  ]
}
```

## Monitoreo

### Logs

Revisar logs de cada task:
- `extract_company_a_data`: Extracci√≥n empresa A
- `extract_company_b_data`: Extracci√≥n empresa B
- `transform_to_common_format`: Transformaci√≥n
- `validate_transformed_data`: Validaci√≥n
- `load_to_unified_system`: Carga
- `generate_status_report`: Reporte
- `compare_data_changes`: Comparaci√≥n (opcional)
- `export_report_file`: Exportaci√≥n (opcional)
- `send_notifications`: Notificaciones (opcional)
- `rollback_on_error`: Rollback (opcional, solo en errores)

### M√©tricas Airflow

El DAG expone m√©tricas que se pueden monitorear en:
- Airflow UI ‚Üí DAG Runs
- Grafana (si est√° configurado)
- Prometheus (si est√° configurado)

## Troubleshooting

### Error: "No se pueden conectar a la base de datos"

- Verificar que las conexiones est√©n configuradas en Airflow
- Verificar credenciales en Admin ‚Üí Connections

### Error: "Validaci√≥n fall√≥"

- Revisar logs de `validate_transformed_data`
- Verificar reglas de validaci√≥n
- Considerar usar `validation_strict: false` para an√°lisis

### Error: "No se encontraron datos"

- Verificar configuraci√≥n de sistemas
- Verificar permisos de acceso
- Verificar cl√°usulas WHERE

### Datos duplicados

- Ajustar reglas de deduplicaci√≥n en `transformation_rules`
- Verificar campos clave en `field_mapping`

## Extensi√≥n

### Agregar nuevos tipos de sistemas

Editar `extract_company_a_data` y `extract_company_b_data`:

```python
elif system_type == "sap":
    # Implementar extracci√≥n SAP
    pass
elif system_type == "oracle":
    # Implementar extracci√≥n Oracle
    pass
```

### Agregar nuevas transformaciones

Editar `transform_to_common_format`:

```python
elif transform_type == "normalize_currency":
    # Normalizar monedas
    pass
elif transform_type == "geocode_addresses":
    # Geocodificar direcciones
    pass
```

### Agregar nuevas validaciones

Editar `validate_transformed_data`:

```python
# Agregar reglas de validaci√≥n personalizadas
if "custom_validation" in rules:
    # Aplicar validaci√≥n personalizada
    pass
```

## Mejores Pr√°cticas

1. **Siempre probar en dry-run primero**
   - Usar `dry_run: true` para validar el flujo completo sin cargar datos
   - Revisar los registros que se cargar√≠an

2. **Configurar chunking para grandes vol√∫menes**
   - Para datasets > 10,000 registros, ajustar `chunk_size`
   - Monitorear uso de memoria durante la ejecuci√≥n

3. **Habilitar backups en producci√≥n**
   - Siempre usar `enable_backup: true` en producci√≥n
   - Las tablas de backup se crean autom√°ticamente con timestamp

4. **Validar configuraciones antes de ejecutar**
   - El DAG valida autom√°ticamente las configuraciones
   - Errores de configuraci√≥n se detectan temprano

5. **Monitorear logs durante la ejecuci√≥n**
   - Buscar s√≠mbolos ‚úì (√©xito) y ‚úó (error)
   - Revisar estad√≠sticas de extracci√≥n por sistema

6. **Revisar reportes despu√©s de cada ejecuci√≥n**
   - El reporte incluye m√©tricas detalladas
   - Identificar sistemas con problemas

7. **Documentar reglas de transformaci√≥n espec√≠ficas**
   - Mantener mapeos de campos actualizados
   - Documentar transformaciones personalizadas

8. **Usar versionado de configuraciones**
   - Guardar configuraciones en control de versiones
   - Etiquetar configuraciones por ejecuci√≥n

9. **Ejecutar en horarios de bajo tr√°fico**
   - Para minimizar impacto en sistemas fuente
   - Coordinar con equipos de operaciones

10. **Configurar retries apropiados**
    - Ajustar `max_retries_extraction` seg√∫n la confiabilidad de los sistemas
    - Monitorear tasa de √©xito de reintentos

11. **Configurar notificaciones**
    - Habilitar notificaciones para monitoreo en tiempo real
    - Configurar canales apropiados (Slack, Email, Webhooks)
    - Ajustar severidad seg√∫n necesidades

12. **Exportar reportes regularmente**
    - Habilitar exportaci√≥n de reportes para auditor√≠a
    - Elegir formato apropiado (JSON para m√°quinas, HTML para humanos)
    - Almacenar reportes hist√≥ricos

13. **Usar comparaci√≥n de datos**
    - Habilitar comparaci√≥n para validar cambios
    - Revisar m√©tricas de nuevos/actualizados/eliminados
    - Usar para auditor√≠a y compliance

14. **Configurar rollback autom√°tico**
    - Habilitar solo en producci√≥n con backups confiables
    - Ajustar umbral de error seg√∫n tolerancia al riesgo
    - Monitorear ejecuciones de rollback

15. **Usar cache para optimizaci√≥n**
    - Habilitar cache para extracciones repetitivas
    - Configurar TTL apropiado seg√∫n frecuencia de cambios
    - Monitorear tasa de cache hits

16. **Validar integridad referencial**
    - Definir reglas de relaciones entre tablas
    - Validar despu√©s de cada carga
    - Corregir registros hu√©rfanos antes de producci√≥n

17. **Enriquecer datos cuando sea necesario**
    - Usar APIs externas para datos adicionales
    - Configurar batch size apropiado
    - Manejar rate limits de APIs

18. **Monitorear con Prometheus**
    - Habilitar m√©tricas para observabilidad
    - Configurar dashboards en Grafana
    - Establecer alertas basadas en m√©tricas

19. **Mantener auditor√≠a completa**
    - Habilitar logs de auditor√≠a para compliance
    - Revisar registros regularmente
    - Usar para troubleshooting y an√°lisis

20. **Definir reglas de calidad personalizadas**
    - Establecer umbrales de calidad
    - Validar completitud, unicidad y validez
    - Corregir violaciones antes de producci√≥n

21. **Monitorear drift de datos**
    - Establecer baseline de referencia
    - Detectar cambios significativos
    - Investigar causas de drift

22. **Usar linaje de datos**
    - Habilitar para trazabilidad completa
    - Documentar transformaciones
    - Usar para an√°lisis de impacto

23. **Usar procesamiento paralelo**
    - Habilitar para m√∫ltiples sistemas
    - Mejorar tiempo de ejecuci√≥n
    - Monitorear uso de recursos

24. **Validar esquemas de datos**
    - Definir esquemas esperados
    - Validar antes de transformar
    - Detectar problemas temprano

25. **Verificar salud de sistemas**
    - Habilitar health checks antes de extraer
    - Evitar extracciones fallidas
    - Monitorear tiempo de respuesta

26. **Usar circuit breaker para sistemas cr√≠ticos**
    - Proteger contra fallos en cascada
    - Configurar umbrales apropiados
    - Monitorear estados del circuito

27. **Encriptar datos sensibles**
    - Identificar campos sensibles
    - Habilitar encriptaci√≥n
    - Verificar que campos est√©n protegidos

28. **Detectar anomal√≠as en datos**
    - Habilitar detecci√≥n para campos num√©ricos
    - Revisar valores at√≠picos
    - Investigar causas de anomal√≠as
    - Ajustar umbral seg√∫n necesidades

29. **Usar carga incremental**
    - Habilitar para optimizar tiempo
    - Configurar campo de timestamp
    - Monitorear registros procesados
    - Reducir carga en sistemas

30. **Configurar resoluci√≥n de conflictos**
    - Elegir estrategia apropiada
    - Probar diferentes estrategias
    - Documentar decisiones
    - Monitorear conflictos resueltos

31. **Usar Dead Letter Queue**
    - Habilitar para capturar errores
    - Revisar registros en DLQ regularmente
    - Reintentar procesamiento
    - Analizar patrones de errores

32. **Aplicar reglas de negocio**
    - Definir reglas espec√≠ficas del dominio
    - Validar l√≥gica de negocio
    - Aplicar transformaciones condicionales
    - Documentar reglas aplicadas

## Seguridad

- Las conexiones de base de datos se manejan v√≠a Airflow Connections
- No hardcodear credenciales en configuraciones
- Usar secretos de Kubernetes/Vault para producci√≥n
- Validar permisos de acceso antes de ejecutar

## Soporte

Para problemas o preguntas:
1. Revisar logs del DAG
2. Verificar configuraci√≥n de par√°metros
3. Consultar documentaci√≥n de Airflow
4. Contactar al equipo de Data Engineering



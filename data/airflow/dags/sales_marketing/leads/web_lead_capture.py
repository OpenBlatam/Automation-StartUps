from __future__ import annotations

from datetime import timedelta, datetime
from typing import Any, Dict, List, Optional, Tuple, Callable
from contextlib import contextmanager
import json
import hashlib
import re
import time
from functools import lru_cache
from collections import defaultdict

import pendulum
import structlog
import os
from pydantic import BaseModel, EmailStr, Field, validator, ValidationError
from tenacity import retry, stop_after_attempt, wait_exponential
from pybreaker import CircuitBreaker
import httpx
import socket
import dns.resolver
import ipaddress
from cachetools import TTLCache
from airflow.decorators import dag, task
from airflow.models.param import Param
from airflow.operators.python import get_current_context
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.exceptions import AirflowFailException
from airflow.stats import Stats
from airflow.providers.postgres.hooks.postgres import PostgresHook

# Try to import concurrent processing utilities
try:
    from concurrent.futures import ThreadPoolExecutor, as_completed
    CONCURRENT_AVAILABLE = True
except ImportError:
    CONCURRENT_AVAILABLE = False

# Callbacks y notificaciones
try:
    from data.airflow.plugins.etl_callbacks import on_task_failure, sla_miss_callback
    from data.airflow.plugins.etl_notifications import notify_slack
except ImportError:
    def on_task_failure(context): pass
    def sla_miss_callback(dag, task_list, blocking_task_list, slas, blocking_tis): pass
    def notify_slack(message): pass

# Sistema de notificaciones mejorado
class NotificationManager:
    """Sistema de notificaciones mejorado con m√∫ltiples canales y templates"""
    
    def __init__(self):
        self.slack_webhook = os.getenv("SLACK_WEBHOOK_URL")
        self.email_enabled = os.getenv("EMAIL_NOTIFICATIONS_ENABLED", "false").lower() == "true"
        self.webhook_url = os.getenv("NOTIFICATION_WEBHOOK_URL")
        self.notification_cache = TTLCache(maxsize=1000, ttl=NOTIFICATION_CACHE_TTL)
        
        # Prioridades de notificaciones (mayor n√∫mero = mayor prioridad)
        self.notification_priorities = {
            "escalation_critical": 100,
            "high_value_lead": 90,
            "risk_churn_risk": 80,
            "risk_conversion_risk": 75,
            "crm_error": 70,
            "performance_alert": 60,
            "data_quality_": 50,
            "milestone_": 40,
            "lead_captured": 30,
            "onboarding_triggered": 20,
            "daily_digest": 10,
            "system_health": 5
        }
        
        # Cola de notificaciones pendientes (para rate limiting inteligente)
        self.pending_notifications = []
        self.max_notifications_per_minute = int(os.getenv("MAX_NOTIFICATIONS_PER_MINUTE", "30"))
        self.notification_timestamps = []
    
    def _get_notification_priority(self, event_type: str) -> int:
        """Obtiene la prioridad de una notificaci√≥n"""
        # Buscar coincidencia exacta primero
        if event_type in self.notification_priorities:
            return self.notification_priorities[event_type]
        
        # Buscar por prefijo
        for prefix, priority in self.notification_priorities.items():
            if event_type.startswith(prefix):
                return priority
        
        return 0  # Prioridad por defecto
    
    def _should_notify(self, event_type: str, key: str, force: bool = False) -> bool:
        """
        Verifica si debe enviar notificaci√≥n con rate limiting inteligente.
        Las notificaciones de alta prioridad siempre se env√≠an.
        """
        if force:
            return True
        
        # Notificaciones cr√≠ticas siempre se env√≠an
        priority = self._get_notification_priority(event_type)
        if priority >= 80:
            return True
        
        # Rate limiting para notificaciones normales
        cache_key = f"{event_type}:{key}"
        if cache_key in self.notification_cache:
            return False
        
        # Verificar l√≠mite por minuto
        now = time.time()
        self.notification_timestamps = [ts for ts in self.notification_timestamps if now - ts < 60]
        
        if len(self.notification_timestamps) >= self.max_notifications_per_minute:
            # Si hay muchas notificaciones, solo permitir las de alta prioridad
            if priority < 50:
                return False
        
        self.notification_cache[cache_key] = True
        self.notification_timestamps.append(now)
        return True
    
    def notify_lead_captured(self, lead_data: Dict[str, Any], context: Dict[str, Any] = None):
        """Notifica captura exitosa de lead"""
        email = lead_data.get("email", "unknown")
        # Solo notificar leads calificados o de alto valor para evitar spam
        score = lead_data.get("score", 0)
        if score < QUALIFIED_SCORE_THRESHOLD and not self._should_notify("lead_captured", email):
            return
        priority = lead_data.get("priority", "low")
        source = lead_data.get("source", "unknown")
        
        message = f"""
üéØ *Nuevo Lead Capturado*

*Email:* {email}
*Score:* {score} ({priority})
*Fuente:* {source}
*Nombre:* {lead_data.get('first_name', '')} {lead_data.get('last_name', '')}
*Empresa:* {lead_data.get('company', 'N/A')}

*Estado:* {"‚úÖ Calificado" if score >= QUALIFIED_SCORE_THRESHOLD else "‚è≥ En revisi√≥n"}
*Pipeline ID:* {lead_data.get('pipeline_id', 'N/A')}
"""
        self._send_notification(message, "lead_captured", context)
    
    def notify_spam_detected(self, lead_data: Dict[str, Any], spam_score: int, indicators: List[str]):
        """Notifica detecci√≥n de spam"""
        email = lead_data.get("email", "unknown")
        if not self._should_notify("spam_detected", email):
            return
        
        message = f"""
‚ö†Ô∏è *Spam Detectado*

*Email:* {email}
*Score de Spam:* {spam_score}
*Indicadores:* {', '.join(indicators[:5])}
*IP:* {lead_data.get('ip_address', 'N/A')}
"""
        self._send_notification(message, "spam_detected")
    
    def notify_high_value_lead(self, lead_data: Dict[str, Any], context: Dict[str, Any] = None):
        """
        Notifica lead de alto valor con m√©tricas avanzadas y insights accionables.
        Incluye: LTV, CLV, ROI, Purchase Intent, Sales Cycle, Product Fit, y m√°s.
        """
        email = lead_data.get("email", "unknown")
        score = lead_data.get("score", 0)
        
        # Extraer m√©tricas avanzadas
        ltv_data = lead_data.get("ltv_prediction", {})
        clv_data = lead_data.get("enhanced_clv_prediction", {})
        roi_data = lead_data.get("advanced_roi_analysis", {})
        purchase_intent = lead_data.get("purchase_intent_advanced", {})
        sales_cycle = lead_data.get("sales_cycle_prediction", {})
        product_fit = lead_data.get("advanced_product_fit_analysis", {})
        behavioral = lead_data.get("dynamic_behavioral_scoring", {})
        prioritization = lead_data.get("intelligent_prioritization", {})
        journey = lead_data.get("customer_journey_analysis", {})
        
        # Calcular urgencia y prioridad
        urgency_score = score
        if purchase_intent.get("intent_score", 0) >= 70:
            urgency_score += 10
        if ltv_data.get("predicted_ltv", 0) >= 50000:
            urgency_score += 10
        if behavioral.get("behavioral_score", 0) >= 80:
            urgency_score += 5
        
        urgency_level = "CR√çTICO" if urgency_score >= 90 else "ALTO" if urgency_score >= 75 else "MEDIO"
        
        # Construir mensaje rico con todas las m√©tricas
        message = f"""
üî• *LEAD DE ALTO VALOR - {urgency_level}*

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìä *INFORMACI√ìN B√ÅSICA*
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
*Email:* {email}
*Nombre:* {lead_data.get('first_name', '')} {lead_data.get('last_name', '')}
*Empresa:* {lead_data.get('company', 'N/A')}
*Score:* {score}/100 ({lead_data.get('priority', 'medium').upper()})
*Asignado a:* {lead_data.get('assigned_to', 'Pendiente')}
*Pipeline ID:* {lead_data.get('pipeline_id', 'N/A')}
*Lead ID:* {lead_data.get('lead_ext_id', 'N/A')}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üí∞ *VALOR PREDICHO*
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
*LTV Predicho:* ${ltv_data.get('predicted_ltv', 0):,.0f}
*CLV Mejorado:* ${clv_data.get('predicted_clv', 0):,.0f}
*ROI Esperado:* {roi_data.get('expected_roi', 0):.1f}%
*Payback Period:* {roi_data.get('payback_period_months', 0):.1f} meses
*Valor Estimado:* ${lead_data.get('estimated_value', 0):,.0f}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üéØ *INTENCI√ìN Y CONVERSI√ìN*
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
*Purchase Intent Score:* {purchase_intent.get('intent_score', 0)}/100
*Probabilidad Conversi√≥n:* {purchase_intent.get('conversion_probability', 0):.1%}
*Velocidad de Compra:* {purchase_intent.get('purchase_velocity', 'normal')}
*Product Fit Score:* {product_fit.get('overall_fit_score', 0)}/100
*Fit Category:* {product_fit.get('fit_category', 'N/A')}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚è±Ô∏è *CICLO DE VENTAS*
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
*Ciclo Predicho:* {sales_cycle.get('predicted_cycle_days', 0)} d√≠as
*Probabilidad Cierre R√°pido:* {sales_cycle.get('quick_close_probability', 0):.1%}
*Etapa Actual:* {journey.get('current_stage', 'N/A')}
*Tiempo √ìptimo Respuesta:* {lead_data.get('optimal_response_time_hours', 'N/A')} horas

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üß† *AN√ÅLISIS COMPORTAMENTAL*
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
*Behavioral Score:* {behavioral.get('behavioral_score', 0)}/100
*Prioridad Inteligente:* {prioritization.get('final_priority_score', 0)}/100
*Engagement Level:* {behavioral.get('engagement_level', 'N/A')}
*Sentiment Score:* {lead_data.get('sentiment_advanced', {}).get('overall_sentiment_score', 0)}/100

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìà *INSIGHTS Y RECOMENDACIONES*
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
"""
        
        # Agregar insights espec√≠ficos
        insights = []
        
        if purchase_intent.get("intent_score", 0) >= 70:
            insights.append("üî• Alta intenci√≥n de compra - Contacto inmediato recomendado")
        
        if ltv_data.get("predicted_ltv", 0) >= 50000:
            insights.append("üí∞ Lead de muy alto valor - Priorizar seguimiento")
        
        if sales_cycle.get("predicted_cycle_days", 999) <= 30:
            insights.append("‚ö° Ciclo de ventas corto - Oportunidad r√°pida")
        
        if product_fit.get("overall_fit_score", 0) >= 80:
            insights.append("‚úÖ Excelente fit de producto - Alta probabilidad de √©xito")
        
        if behavioral.get("engagement_level") == "high":
            insights.append("üì± Alto engagement - Lead muy activo")
        
        if journey.get("current_stage") in ["decision", "purchase"]:
            insights.append("üéØ En etapa de decisi√≥n - Cerrar oportunidad")
        
        # Agregar recomendaciones de canal
        recommended_channel = lead_data.get("optimal_contact_channel", "email")
        if purchase_intent.get("intent_score", 0) >= 80:
            recommended_channel = "phone"
        elif behavioral.get("engagement_level") == "high":
            recommended_channel = "slack" if lead_data.get("social_media_analysis", {}).get("slack_presence") else "email"
        
        message += "\n".join(f"‚Ä¢ {insight}" for insight in insights) if insights else "‚Ä¢ Seguimiento est√°ndar recomendado"
        
        message += f"""

*Canal Recomendado:* {recommended_channel.upper()}
*Urgencia:* {urgency_level}
*Acci√≥n Requerida:* {"SEGUIMIENTO INMEDIATO" if urgency_score >= 85 else "Seguimiento prioritario"}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
"""
        
        # Enviar notificaci√≥n con contexto enriquecido
        enriched_context = {
            **(context or {}),
            "urgency_score": urgency_score,
            "urgency_level": urgency_level,
            "recommended_channel": recommended_channel,
            "insights": insights,
            "metrics": {
                "ltv": ltv_data.get("predicted_ltv", 0),
                "clv": clv_data.get("predicted_clv", 0),
                "roi": roi_data.get("expected_roi", 0),
                "purchase_intent": purchase_intent.get("intent_score", 0),
                "sales_cycle_days": sales_cycle.get("predicted_cycle_days", 0),
                "product_fit": product_fit.get("overall_fit_score", 0),
                "behavioral_score": behavioral.get("behavioral_score", 0)
            }
        }
        
        self._send_notification(message, "high_value_lead", enriched_context)
        
        # Si es cr√≠tico, enviar tambi√©n notificaci√≥n de escalaci√≥n
        if urgency_score >= 90:
            self._send_escalation_notification(lead_data, enriched_context)
    
    def notify_crm_sync_error(self, lead_data: Dict[str, Any], error: str, crm_type: str):
        """Notifica error en sincronizaci√≥n CRM"""
        email = lead_data.get("email", "unknown")
        
        message = f"""
‚ùå *Error en Sincronizaci√≥n CRM*

*Email:* {email}
*CRM:* {crm_type}
*Error:* {error}
*Lead ID:* {lead_data.get('lead_ext_id', 'N/A')}

*Acci√≥n:* Revisar conectividad con {crm_type}
"""
        self._send_notification(message, "crm_error")
    
    def notify_onboarding_triggered(self, lead_data: Dict[str, Any], onboarding_id: str):
        """Notifica que se dispar√≥ onboarding"""
        email = lead_data.get("email", "unknown")
        
        message = f"""
üöÄ *Onboarding Disparado Autom√°ticamente*

*Email:* {email}
*Onboarding ID:* {onboarding_id}
*Score:* {lead_data.get('score', 0)}
*Pipeline ID:* {lead_data.get('pipeline_id', 'N/A')}
"""
        self._send_notification(message, "onboarding_triggered")
    
    def _send_escalation_notification(self, lead_data: Dict[str, Any], context: Dict[str, Any] = None):
        """Env√≠a notificaci√≥n de escalaci√≥n para leads cr√≠ticos"""
        email = lead_data.get("email", "unknown")
        urgency_score = context.get("urgency_score", 0) if context else 0
        
        ltv_value = context.get('metrics', {}).get('ltv', 0) if context else 0
        intent_value = context.get('metrics', {}).get('purchase_intent', 0) if context else 0
        
        escalation_message = f"""
üö® *ESCALACI√ìN CR√çTICA - LEAD DE M√ÅXIMA PRIORIDAD* üö®

*Lead:* {email}
*Urgency Score:* {urgency_score}/100
*LTV Predicho:* ${ltv_value:,.0f}
*Purchase Intent:* {intent_value}/100

*ACCI√ìN INMEDIATA REQUERIDA*
Este lead requiere atenci√≥n inmediata del equipo de ventas senior.

*Recomendaciones:*
{chr(10).join(f"‚Ä¢ {insight}" for insight in context.get('insights', [])) if context and context.get('insights') else "‚Ä¢ Contacto inmediato recomendado"}

*Canal Recomendado:* {context.get('recommended_channel', 'phone').upper() if context else 'PHONE'}
*Lead ID:* {lead_data.get('lead_ext_id', 'N/A')}
*Pipeline ID:* {lead_data.get('pipeline_id', 'N/A')}
"""
        self._send_notification(escalation_message, "escalation_critical", context)
    
    def _build_slack_blocks(self, message: str, event_type: str, context: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """Construye bloques de Slack para notificaciones ricas"""
        blocks = []
        
        # Header block
        header_emoji = {
            "high_value_lead": "üî•",
            "escalation_critical": "üö®",
            "lead_captured": "üéØ",
            "spam_detected": "‚ö†Ô∏è",
            "crm_error": "‚ùå",
            "onboarding_triggered": "üöÄ"
        }.get(event_type, "üì¢")
        
        blocks.append({
            "type": "header",
            "text": {
                "type": "plain_text",
                "text": f"{header_emoji} Lead Capture Notification",
                "emoji": True
            }
        })
        
        # Divider
        blocks.append({"type": "divider"})
        
        # Message block
        blocks.append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": message
            }
        })
        
        # Context fields if available
        if context and context.get("metrics"):
            metrics = context["metrics"]
            fields = []
            
            if metrics.get("ltv", 0) > 0:
                fields.append({
                    "type": "mrkdwn",
                    "text": f"*LTV:* ${metrics['ltv']:,.0f}"
                })
            
            if metrics.get("purchase_intent", 0) > 0:
                fields.append({
                    "type": "mrkdwn",
                    "text": f"*Intent:* {metrics['purchase_intent']}/100"
                })
            
            if metrics.get("roi", 0) > 0:
                fields.append({
                    "type": "mrkdwn",
                    "text": f"*ROI:* {metrics['roi']:.1f}%"
                })
            
            if fields:
                blocks.append({
                    "type": "section",
                    "fields": fields[:10]  # Slack limit
                })
        
        # Action buttons for high value leads
        if event_type == "high_value_lead" and context:
            blocks.append({"type": "divider"})
            blocks.append({
                "type": "actions",
                "elements": [
                    {
                        "type": "button",
                        "text": {
                            "type": "plain_text",
                            "text": "Ver en CRM"
                        },
                        "url": f"https://crm.example.com/leads/{context.get('lead_id', '')}",
                        "style": "primary"
                    },
                    {
                        "type": "button",
                        "text": {
                            "type": "plain_text",
                            "text": "Asignar Vendedor"
                        },
                        "value": f"assign_{context.get('lead_id', '')}",
                        "style": "primary"
                    }
                ]
            })
        
        return blocks
    
    def _send_notification(self, message: str, event_type: str, context: Dict[str, Any] = None):
        """
        Env√≠a notificaci√≥n a todos los canales configurados con soporte para formatos ricos.
        Incluye retry logic y circuit breaker para resiliencia.
        """
        # Slack con formato rico (blocks)
        if self.slack_webhook:
            try:
                with httpx.Client(timeout=10.0) as client:
                    # Intentar usar blocks para eventos importantes
                    use_blocks = event_type in ["high_value_lead", "escalation_critical"]
                    
                    if use_blocks:
                        blocks = self._build_slack_blocks(message, event_type, context)
                        payload = {
                            "blocks": blocks,
                            "username": "Lead Capture Bot",
                            "icon_emoji": ":robot_face:"
                        }
                    else:
                        payload = {
                            "text": message,
                            "username": "Lead Capture Bot",
                            "icon_emoji": ":robot_face:"
                        }
                    
                    # Retry logic para Slack
                    max_retries = 3
                    for attempt in range(max_retries):
                        try:
                            response = client.post(self.slack_webhook, json=payload, timeout=10.0)
                            response.raise_for_status()
                            logger.info("notification_sent", channel="slack", event_type=event_type, format="blocks" if use_blocks else "text")
                            break
                        except httpx.HTTPStatusError as e:
                            if attempt == max_retries - 1:
                                raise
                            time.sleep(2 ** attempt)
                    
            except Exception as e:
                logger.error("slack_notification_failed", error=str(e), event_type=event_type)
        
        # Webhook personalizado con payload enriquecido
        if self.webhook_url:
            try:
                with httpx.Client(timeout=10.0) as client:
                    payload = {
                        "event_type": event_type,
                        "message": message,
                        "timestamp": datetime.utcnow().isoformat(),
                        "context": context or {},
                        "metadata": {
                            "source": "lead_capture_dag",
                            "dag_id": context.get("dag", {}).get("dag_id", "unknown") if context else "unknown",
                            "run_id": context.get("dag_run", {}).get("run_id", "unknown") if context else "unknown"
                        }
                    }
                    
                    # Retry logic para webhook
                    max_retries = 3
                    for attempt in range(max_retries):
                        try:
                            response = client.post(self.webhook_url, json=payload, timeout=10.0)
                            response.raise_for_status()
                            logger.info("notification_sent", channel="webhook", event_type=event_type)
                            break
                        except httpx.HTTPStatusError as e:
                            if attempt == max_retries - 1:
                                raise
                            time.sleep(2 ** attempt)
                    
            except Exception as e:
                logger.error("webhook_notification_failed", error=str(e), event_type=event_type)
        
        # Email (si est√° habilitado) - con formato HTML
        if self.email_enabled and event_type in ["high_value_lead", "escalation_critical", "crm_error"]:
            try:
                self._send_email_notification(message, event_type, context)
            except Exception as e:
                logger.error("email_notification_failed", error=str(e), event_type=event_type)
        
        # SMS para escalaciones cr√≠ticas (si est√° configurado)
        sms_enabled = os.getenv("SMS_NOTIFICATIONS_ENABLED", "false").lower() == "true"
        if sms_enabled and event_type == "escalation_critical":
            try:
                self._send_sms_notification(message, context)
            except Exception as e:
                logger.error("sms_notification_failed", error=str(e))
    
    def _send_email_notification(self, message: str, event_type: str, context: Dict[str, Any] = None):
        """Env√≠a notificaci√≥n por email con formato HTML"""
        # Convertir markdown a HTML b√°sico
        html_message = message.replace("*", "<strong>").replace("\n", "<br>")
        html_message = html_message.replace("<strong>", "</strong>", html_message.count("<strong>") - 1)
        
        # Aqu√≠ se integrar√≠a con un servicio de email (SendGrid, SES, etc.)
        # Por ahora solo logueamos
        logger.info("email_notification_prepared", event_type=event_type, recipient=context.get("email") if context else "unknown")
    
    def _send_sms_notification(self, message: str, context: Dict[str, Any] = None):
        """Env√≠a notificaci√≥n por SMS para alertas cr√≠ticas"""
        # Extraer informaci√≥n clave para SMS (m√°s corto)
        sms_text = f"üö® Lead cr√≠tico: {context.get('email', 'unknown') if context else 'unknown'}. Score: {context.get('urgency_score', 0) if context else 0}. Acci√≥n inmediata requerida."
        
        # Aqu√≠ se integrar√≠a con un servicio de SMS (Twilio, etc.)
        # Por ahora solo logueamos
        logger.info("sms_notification_prepared", message_preview=sms_text[:50])
    
    def notify_conversion_milestone(self, lead_data: Dict[str, Any], milestone: str, context: Dict[str, Any] = None):
        """Notifica hitos importantes en el proceso de conversi√≥n"""
        email = lead_data.get("email", "unknown")
        milestone_emojis = {
            "first_contact": "üëã",
            "meeting_scheduled": "üìÖ",
            "demo_completed": "üé¨",
            "proposal_sent": "üìÑ",
            "negotiation_started": "ü§ù",
            "closed_won": "üéâ",
            "closed_lost": "‚ùå"
        }
        
        emoji = milestone_emojis.get(milestone, "üìå")
        message = f"""
{emoji} *Hito de Conversi√≥n: {milestone.replace('_', ' ').title()}*

*Lead:* {email}
*Score:* {lead_data.get('score', 0)}
*Pipeline ID:* {lead_data.get('pipeline_id', 'N/A')}
*Valor Estimado:* ${lead_data.get('estimated_value', 0):,.0f}

*Siguiente Acci√≥n:* {self._get_next_action_for_milestone(milestone)}
"""
        self._send_notification(message, f"milestone_{milestone}", context)
    
    def _get_next_action_for_milestone(self, milestone: str) -> str:
        """Retorna la siguiente acci√≥n recomendada para un hito"""
        actions = {
            "first_contact": "Programar reuni√≥n de descubrimiento",
            "meeting_scheduled": "Preparar materiales de presentaci√≥n",
            "demo_completed": "Enviar propuesta comercial",
            "proposal_sent": "Seguimiento en 48 horas",
            "negotiation_started": "Cerrar t√©rminos y condiciones",
            "closed_won": "Iniciar proceso de onboarding",
            "closed_lost": "Analizar raz√≥n de p√©rdida"
        }
        return actions.get(milestone, "Seguimiento est√°ndar")
    
    def notify_risk_alert(self, lead_data: Dict[str, Any], risk_type: str, risk_score: float, context: Dict[str, Any] = None):
        """Notifica alertas de riesgo (churn, p√©rdida, etc.)"""
        email = lead_data.get("email", "unknown")
        
        risk_messages = {
            "churn_risk": f"‚ö†Ô∏è *Alerta de Riesgo de Churn*\n\nLead: {email}\nRiesgo: {risk_score:.1%}\nAcci√≥n: Contacto inmediato para retenci√≥n",
            "conversion_risk": f"‚ö†Ô∏è *Riesgo de P√©rdida de Conversi√≥n*\n\nLead: {email}\nRiesgo: {risk_score:.1%}\nAcci√≥n: Re-engagement urgente",
            "engagement_drop": f"‚ö†Ô∏è *Ca√≠da de Engagement*\n\nLead: {email}\nScore: {risk_score:.1f}\nAcci√≥n: Re-activaci√≥n de nurturing"
        }
        
        message = risk_messages.get(risk_type, f"‚ö†Ô∏è *Alerta de Riesgo*\n\nLead: {email}\nTipo: {risk_type}\nScore: {risk_score}")
        self._send_notification(message, f"risk_{risk_type}", context)

# Instancia global del notification manager
notification_manager = NotificationManager()

# OpenTelemetry Tracing (opcional)
try:
    from opentelemetry import trace
    tracer = trace.get_tracer(__name__)
except Exception:
    tracer = None

# Configurar logging estructurado
logger = structlog.get_logger(__name__)

# ============================================================================
# CONSTANTES Y CONFIGURACI√ìN
# ============================================================================

# Timeouts y l√≠mites
DEFAULT_TIMEOUT = float(os.getenv("LEAD_CAPTURE_TIMEOUT", "30.0"))
DNS_TIMEOUT = float(os.getenv("DNS_TIMEOUT", "3.0"))
ENRICHMENT_TIMEOUT = float(os.getenv("ENRICHMENT_TIMEOUT", "2.0"))
HTTP_TIMEOUT = float(os.getenv("HTTP_TIMEOUT", "5.0"))

# L√≠mites de rate limiting
RATE_LIMIT_EMAIL_MAX = int(os.getenv("RATE_LIMIT_EMAIL_MAX", "5"))
RATE_LIMIT_EMAIL_PERIOD = int(os.getenv("RATE_LIMIT_EMAIL_PERIOD", "3600"))
RATE_LIMIT_IP_MAX = int(os.getenv("RATE_LIMIT_IP_MAX", "10"))
RATE_LIMIT_IP_PERIOD = int(os.getenv("RATE_LIMIT_IP_PERIOD", "3600"))

# Thresholds de scoring
SPAM_SCORE_THRESHOLD = int(os.getenv("SPAM_SCORE_THRESHOLD", "50"))
HIGH_VALUE_SCORE_THRESHOLD = int(os.getenv("HIGH_VALUE_SCORE_THRESHOLD", "70"))
QUALIFIED_SCORE_THRESHOLD = int(os.getenv("QUALIFIED_SCORE_THRESHOLD", "40"))

# Cache TTLs
ENRICHMENT_CACHE_TTL = int(os.getenv("ENRICHMENT_CACHE_TTL", "3600"))
DNS_CACHE_TTL = int(os.getenv("DNS_CACHE_TTL", "3600"))
NOTIFICATION_CACHE_TTL = int(os.getenv("NOTIFICATION_CACHE_TTL", "300"))

# Retry configuration
MAX_RETRIES = int(os.getenv("MAX_RETRIES", "3"))
RETRY_MIN_WAIT = int(os.getenv("RETRY_MIN_WAIT", "4"))
RETRY_MAX_WAIT = int(os.getenv("RETRY_MAX_WAIT", "10"))

# Batch processing configuration
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "10"))
MAX_WORKERS = int(os.getenv("MAX_WORKERS", "4"))
ENABLE_PARALLEL_PROCESSING = os.getenv("ENABLE_PARALLEL_PROCESSING", "false").lower() == "true"

# Performance monitoring
ENABLE_PERFORMANCE_PROFILING = os.getenv("ENABLE_PERFORMANCE_PROFILING", "false").lower() == "true"

# Circuit breakers para diferentes servicios
crm_circuit_breaker = CircuitBreaker(
    fail_max=5,
    timeout_duration=60,
    expected_exception=Exception
)

onboarding_circuit_breaker = CircuitBreaker(
    fail_max=3,
    timeout_duration=120,
    expected_exception=Exception
)

# Circuit breaker para APIs de enriquecimiento
enrichment_circuit_breaker = CircuitBreaker(
    fail_max=5,
    timeout_duration=30,
    expected_exception=Exception
)

# Cache para enriquecimiento de datos (evitar llamadas duplicadas)
enrichment_cache = TTLCache(maxsize=int(os.getenv("CACHE_MAXSIZE", "1000")), ttl=ENRICHMENT_CACHE_TTL)

# Rate limiter (si est√° disponible)
rate_limiter = None
redis_client = None
try:
    from limits import storage
    from limits.strategies import MovingWindowRateLimiter
    
    # Intentar usar Redis si est√° disponible
    redis_url = os.getenv("REDIS_URL", "redis://localhost:6379/0")
    try:
        storage_backend = storage.RedisStorage(redis_url)
        rate_limiter = MovingWindowRateLimiter(storage_backend)
        import redis
        redis_client = redis.from_url(redis_url)
    except Exception:
        # Fallback a in-memory storage
        storage_backend = storage.MemoryStorage()
        rate_limiter = MovingWindowRateLimiter(storage_backend)
        logger.info("rate_limiter_initialized", backend="memory")
except ImportError:
    logger.warning("limits library not available, rate limiting disabled")

# Dead Letter Queue para errores
DLQ_PATH = os.getenv("LEAD_DLQ_PATH", "/tmp/lead_capture_dlq.jsonl")

# Enterprise features: Distributed locking e idempotency
try:
    from airflow.models import Variable
    VARIABLES_AVAILABLE = True
except ImportError:
    VARIABLES_AVAILABLE = False
    logger.warning("Airflow Variables not available, distributed locking disabled")

def _acquire_distributed_lock(lock_key: str, timeout_seconds: int = 600, conn_id: str = None) -> bool:
    """
    Adquiere un lock distribuido usando PostgreSQL o Airflow Variables.
    
    Returns:
        True si el lock fue adquirido, False si ya existe
    """
    if not VARIABLES_AVAILABLE:
        return True  # Fallback: permitir ejecuci√≥n si Variables no est√° disponible
    
    try:
        existing = Variable.get(lock_key, default_var=None)
        if existing:
            # Verificar si el lock expir√≥
            try:
                lock_data = json.loads(existing)
                expires_at = lock_data.get("expires_at", 0)
                now = int(time.time())
                if expires_at > now:
                    # Lock a√∫n activo
                    return False
            except Exception:
                # Formato inv√°lido, asumir que expir√≥
                pass
        
        # Crear nuevo lock
        now = int(time.time())
        lock_data = {
            "acquired_at": now,
            "expires_at": now + timeout_seconds,
            "run_id": os.environ.get("AIRFLOW_RUN_ID", "unknown"),
        }
        Variable.set(lock_key, json.dumps(lock_data))
        return True
    except Exception as e:
        logger.warning("distributed_lock_failed", lock_key=lock_key, error=str(e))
        return True  # Fallback: permitir ejecuci√≥n si falla

def _release_distributed_lock(lock_key: str) -> None:
    """Libera un lock distribuido."""
    if not VARIABLES_AVAILABLE:
        return
    try:
        Variable.delete(lock_key)
    except Exception as e:
        logger.debug("lock_release_failed", lock_key=lock_key, error=str(e))

def _generate_idempotency_key(lead_data: Dict[str, Any]) -> str:
    """Genera una clave de idempotencia √∫nica para un lead."""
    # Usar email + timestamp del d√≠a para idempotencia diaria
    email = lead_data.get("email", "")
    timestamp = datetime.utcnow().strftime("%Y-%m-%d")
    key_string = f"{email}:{timestamp}"
    return hashlib.sha256(key_string.encode()).hexdigest()[:32]

def _check_idempotency(idempotency_key: str, conn_id: str) -> tuple[bool, Optional[Dict[str, Any]]]:
    """
    Verifica si un lead ya fue procesado (idempotencia).
    
    Returns:
        (is_duplicate, previous_result)
    """
    if not VARIABLES_AVAILABLE:
        return False, None
    
    try:
        existing = Variable.get(f"idempotency:{idempotency_key}", default_var=None)
        if existing:
            try:
                result_data = json.loads(existing)
                return True, result_data
            except Exception:
                return True, None
        return False, None
    except Exception:
        return False, None

def _set_idempotency_result(idempotency_key: str, result_data: Dict[str, Any], ttl_hours: int = 24) -> None:
    """Guarda el resultado de una operaci√≥n para idempotencia."""
    if not VARIABLES_AVAILABLE:
        return
    try:
        Variable.set(f"idempotency:{idempotency_key}", json.dumps(result_data))
    except Exception as e:
        logger.debug("idempotency_save_failed", key=idempotency_key[:16], error=str(e))

def _calculate_churn_risk(lead_data: Dict[str, Any], conn_id: str) -> Dict[str, Any]:
    """Calcula el riesgo de churn del lead."""
    churn_score = 0
    factors = {}
    
    # Factor 1: Engagement bajo
    behavior = lead_data.get("behavior_tracking", {})
    if behavior.get("engagement_level") == "low":
        churn_score += 30
        factors["low_engagement"] = 30
    
    # Factor 2: Score bajo
    lead_score = lead_data.get("score", 0)
    if lead_score < 30:
        churn_score += 25
        factors["low_score"] = 25
    
    # Factor 3: Sin respuesta a comunicaciones
    communication = lead_data.get("communication_tracking", {})
    if communication.get("total_touchpoints", 0) > 5 and communication.get("channels", {}).get("email", {}).get("replied", 0) == 0:
        churn_score += 20
        factors["no_response"] = 20
    
    # Factor 4: Tiempo sin actividad
    # (esto requerir√≠a datos hist√≥ricos, simplificado aqu√≠)
    
    churn_score = min(100, max(0, churn_score))
    
    if churn_score >= 60:
        risk_level = "high"
    elif churn_score >= 40:
        risk_level = "medium"
    else:
        risk_level = "low"
    
    return {
        "churn_risk_score": churn_score,
        "churn_risk_level": risk_level,
        "factors": factors
    }

def _estimate_lifetime_value(lead_data: Dict[str, Any], conn_id: str) -> Dict[str, Any]:
    """Estima el lifetime value mejorado del lead."""
    # Usar LTV prediction si est√° disponible
    ltv_pred = lead_data.get("ltv_prediction", {})
    if ltv_pred:
        return {
            "ltv_realistic": ltv_pred.get("predicted_ltv_usd", 0),
            "ltv_low": ltv_pred.get("ltv_low_usd", 0),
            "ltv_high": ltv_pred.get("ltv_high_usd", 0),
            "source": "prediction"
        }
    
    # Fallback a c√°lculo simple
    value_analysis = lead_data.get("value_analysis", {})
    estimated_value = value_analysis.get("estimated_value_usd", 0)
    
    return {
        "ltv_realistic": estimated_value,
        "ltv_low": estimated_value * 0.7,
        "ltv_high": estimated_value * 1.3,
        "source": "heuristic"
    }

def save_to_dlq(item: Dict[str, Any], error: str, context: Dict[str, Any] = None) -> None:
    """Guarda un lead fallido en dead letter queue."""
    try:
        os.makedirs(os.path.dirname(DLQ_PATH), exist_ok=True)
        dlq_record = {
            "timestamp": datetime.utcnow().isoformat(),
            "lead_data": item,
            "error": error,
            "context": context or {},
            "retried": False,
            "dag_run_id": context.get("dag_run_id") if context else None,
        }
        with open(DLQ_PATH, "a") as f:
            f.write(json.dumps(dlq_record) + "\n")
        logger.warning("lead_saved_to_dlq", error=error, email=item.get("email"))
        if Stats:
            try:
                Stats.incr("lead_capture.dlq.saved", 1)
            except Exception:
                pass
    except Exception as e:
        logger.error("dlq_save_error", error=str(e), exc_info=True)

# ============================================================================
# FUNCIONES HELPER AVANZADAS ADICIONALES - ENTERPRISE FEATURES
# ============================================================================

# Cost tracking para APIs externas
def _track_api_cost(api_name: str, operation: str, cost_estimate: float, conn_id: str = None) -> None:
    """Rastrea costos estimados de llamadas a APIs externas."""
    try:
        if VARIABLES_AVAILABLE:
            cost_key = f"lead_capture_api_costs_{api_name}"
            current_costs = Variable.get(cost_key, default_var="{}")
            costs = json.loads(current_costs) if current_costs else {}
            
            today = datetime.utcnow().strftime("%Y-%m-%d")
            if today not in costs:
                costs[today] = {}
            if operation not in costs[today]:
                costs[today][operation] = 0.0
            
            costs[today][operation] += cost_estimate
            Variable.set(cost_key, json.dumps(costs))
            
            logger.debug(
                "api_cost_tracked",
                api_name=api_name,
                operation=operation,
                cost=cost_estimate,
                daily_total=sum(costs[today].values())
            )
    except Exception as e:
        logger.warning("cost_tracking_failed", error=str(e))

# Jitter para prevenir thundering herd
def _add_jitter(base_delay: float, jitter_percent: float = 0.2) -> float:
    """A√±ade jitter aleatorio a un delay para prevenir thundering herd."""
    import random
    jitter = base_delay * jitter_percent * (2 * random.random() - 1)
    return max(0, base_delay + jitter)

# Progress tracking
def _log_progress(operation: str, current: int, total: int, context: Dict[str, Any] = None) -> None:
    """Registra progreso de una operaci√≥n."""
    if total > 0:
        progress_pct = (current / total) * 100
        logger.info(
            "operation_progress",
            operation=operation,
            current=current,
            total=total,
            progress_pct=round(progress_pct, 1),
            context=context or {}
        )

# Predictive alerts
def _detect_predictive_alerts(lead_data: Dict[str, Any], historical_data: Dict[str, Any] = None, conn_id: str = None) -> List[Dict[str, Any]]:
    """Detecta alertas predictivas basadas en tendencias y patrones."""
    alerts = []
    
    # Alerta: Churn risk alto
    churn_risk = lead_data.get("churn_risk", {})
    if churn_risk.get("churn_risk_level") in ["high", "critical"]:
        alerts.append({
            "type": "high_churn_risk",
            "severity": churn_risk.get("churn_risk_level"),
            "message": f"Alto riesgo de churn ({churn_risk.get('churn_risk_score')}%)",
            "recommendation": "; ".join(churn_risk.get("mitigation_strategies", [])) if isinstance(churn_risk.get("mitigation_strategies"), list) else ""
        })
    
    # Alerta: Calidad de datos baja
    quality_grade = lead_data.get("quality_analysis", {}).get("grade", "F")
    if quality_grade in ["D", "F"]:
        alerts.append({
            "type": "low_data_quality",
            "severity": "high",
            "message": f"Calidad de datos baja (Grado {quality_grade})",
            "recommendation": "Enriquecer datos del lead antes de procesar"
        })
    
    return alerts

# ML Scoring integration
def _get_ml_prediction(lead_data: Dict[str, Any], model_endpoint: str = None) -> Dict[str, Any]:
    """Obtiene predicci√≥n de modelo ML externo."""
    if not model_endpoint:
        model_endpoint = os.getenv("ML_SCORING_ENDPOINT")
    
    if not model_endpoint:
        return {}
    
    try:
        with httpx.Client(timeout=5.0) as client:
            features = {
                "email_domain_score": lead_data.get("email_validation", {}).get("domain_score", 0),
                "has_company": 1 if lead_data.get("company") else 0,
                "has_phone": 1 if lead_data.get("phone") else 0,
                "score": lead_data.get("score", 0),
                "quality_score": lead_data.get("quality_analysis", {}).get("quality_score", 0),
            }
            
            response = client.post(model_endpoint, json={"features": features})
            response.raise_for_status()
            ml_result = response.json()
            
            _track_api_cost("ml_scoring", "prediction", 0.001, None)
            
            return {
                "ml_score": ml_result.get("score", 0),
                "ml_confidence": ml_result.get("confidence", 0),
                "ml_prediction": ml_result.get("prediction", "unknown")
            }
    except Exception as e:
        logger.warning("ml_prediction_failed", error=str(e))
        return {}

# Data quality scoring
def _calculate_data_quality_score(lead_data: Dict[str, Any]) -> Dict[str, Any]:
    """Calcula score de calidad de datos del lead."""
    quality_score = 0
    factors = {}
    
    # Completitud (40 puntos)
    if lead_data.get("email"):
        quality_score += 10
        factors["has_email"] = True
    if lead_data.get("first_name") and lead_data.get("last_name"):
        quality_score += 10
        factors["has_full_name"] = True
    if lead_data.get("phone"):
        quality_score += 10
        factors["has_phone"] = True
    if lead_data.get("company"):
        quality_score += 10
        factors["has_company"] = True
    
    # Validaci√≥n (30 puntos)
    if lead_data.get("email_validation", {}).get("valid_mx"):
        quality_score += 15
        factors["email_validated"] = True
    if lead_data.get("company_validation", {}).get("validated"):
        quality_score += 10
        factors["company_validated"] = True
    
    quality_score = min(quality_score, 100)
    
    return {
        "data_quality_score": quality_score,
        "data_quality_grade": (
            "A" if quality_score >= 80 else
            "B" if quality_score >= 60 else
            "C" if quality_score >= 40 else
            "D" if quality_score >= 20 else "F"
        ),
        "factors": factors
    }

# Advanced error recovery
def _recover_from_error(error: Exception, context: Dict[str, Any], lead_data: Dict[str, Any] = None) -> Dict[str, Any]:
    """Recuperaci√≥n avanzada de errores con estrategias m√∫ltiples."""
    recovery_strategy = {
        "attempted": False,
        "strategy": None,
        "success": False,
        "fallback_data": None
    }
    
    error_type = type(error).__name__
    error_message = str(error)
    
    # Estrategia 1: Retry con exponential backoff
    if "timeout" in error_message.lower() or "connection" in error_message.lower():
        recovery_strategy["strategy"] = "exponential_backoff_retry"
        recovery_strategy["attempted"] = True
        # Implementar retry logic aqu√≠ si es necesario
        logger.info("error_recovery_attempted", strategy="exponential_backoff", error_type=error_type)
    
    # Estrategia 2: Fallback a datos en cache
    elif lead_data and lead_data.get("email"):
        cached_data = enrichment_cache.get(lead_data.get("email"))
        if cached_data:
            recovery_strategy["strategy"] = "cache_fallback"
            recovery_strategy["fallback_data"] = cached_data
            recovery_strategy["success"] = True
            logger.info("error_recovered_from_cache", email=lead_data.get("email"))
    
    # Estrategia 3: Degradaci√≥n graceful
    else:
        recovery_strategy["strategy"] = "graceful_degradation"
        recovery_strategy["success"] = True
        recovery_strategy["fallback_data"] = lead_data or {}
        logger.warning("error_graceful_degradation", error_type=error_type)
    
    return recovery_strategy

# Advanced caching with TTL and invalidation
def _get_cached_with_validation(cache_key: str, validation_func: Callable = None) -> Optional[Dict[str, Any]]:
    """Obtiene datos del cache con validaci√≥n opcional."""
    try:
        cached_data = enrichment_cache.get(cache_key)
        if cached_data:
            # Validar si se proporciona funci√≥n de validaci√≥n
            if validation_func and not validation_func(cached_data):
                enrichment_cache.pop(cache_key, None)
                return None
            return cached_data
    except Exception as e:
        logger.debug("cache_get_failed", cache_key=cache_key[:20], error=str(e))
    return None

def _invalidate_cache_pattern(pattern: str) -> int:
    """Invalida entradas del cache que coincidan con un patr√≥n."""
    invalidated_count = 0
    try:
        # Para TTLCache, necesitar√≠amos iterar sobre las claves
        # Esto es una implementaci√≥n simplificada
        if hasattr(enrichment_cache, 'keys'):
            keys_to_remove = [k for k in enrichment_cache.keys() if pattern in str(k)]
            for key in keys_to_remove:
                enrichment_cache.pop(key, None)
                invalidated_count += 1
        logger.info("cache_invalidated", pattern=pattern, count=invalidated_count)
    except Exception as e:
        logger.warning("cache_invalidation_failed", error=str(e))
    return invalidated_count

# Compliance tracking
def _track_compliance_event(event_type: str, lead_data: Dict[str, Any], conn_id: str = None) -> None:
    """Rastrea eventos de cumplimiento (GDPR, CCPA, etc.)."""
    try:
        if VARIABLES_AVAILABLE:
            compliance_key = f"lead_capture_compliance_{event_type}"
            events = Variable.get(compliance_key, default_var="[]")
            events_list = json.loads(events) if events else []
            
            event_record = {
                "timestamp": datetime.utcnow().isoformat(),
                "event_type": event_type,
                "lead_email": lead_data.get("email", ""),
                "lead_id": lead_data.get("lead_ext_id"),
                "data_processed": {
                    "email": bool(lead_data.get("email")),
                    "phone": bool(lead_data.get("phone")),
                    "company": bool(lead_data.get("company"))
                },
                "consent_given": lead_data.get("consent_given", False),
                "source": lead_data.get("source", "unknown")
            }
            
            events_list.append(event_record)
            # Mantener solo √∫ltimos 1000 eventos
            if len(events_list) > 1000:
                events_list = events_list[-1000:]
            
            Variable.set(compliance_key, json.dumps(events_list))
            logger.debug("compliance_event_tracked", event_type=event_type)
    except Exception as e:
        logger.warning("compliance_tracking_failed", error=str(e))

# Security enhancements
def _sanitize_input(data: Any, input_type: str = "string") -> Any:
    """Sanitiza inputs para prevenir inyecciones."""
    if input_type == "string" and isinstance(data, str):
        # Remover caracteres peligrosos
        dangerous_chars = ['<', '>', '&', '"', "'", ';', '(', ')', '{', '}']
        sanitized = data
        for char in dangerous_chars:
            sanitized = sanitized.replace(char, '')
        return sanitized.strip()
    elif input_type == "email" and isinstance(data, str):
        # Validar formato de email b√°sico
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        if re.match(email_pattern, data):
            return data.lower().strip()
        return None
    return data

def _validate_security_headers(headers: Dict[str, str]) -> bool:
    """Valida headers de seguridad en requests."""
    required_headers = ["User-Agent", "Content-Type"]
    for header in required_headers:
        if header not in headers:
            logger.warning("missing_security_header", header=header)
            return False
    return True

# Performance optimization
def _optimize_query(query: str, params: Dict[str, Any] = None) -> str:
    """Optimiza queries SQL b√°sicas."""
    # Remover espacios extra
    optimized = re.sub(r'\s+', ' ', query.strip())
    
    # Sugerir √≠ndices si es necesario (simplificado)
    if "WHERE" in optimized.upper() and "email" in optimized.lower():
        logger.debug("query_optimization_suggestion", suggestion="Consider index on email column")
    
    return optimized

# Advanced retry with circuit breaker integration
def _retry_with_circuit_breaker(
    func: Callable,
    circuit_breaker: CircuitBreaker,
    max_retries: int = 3,
    *args,
    **kwargs
) -> Any:
    """Ejecuta funci√≥n con retry y circuit breaker."""
    last_exception = None
    
    for attempt in range(max_retries):
        try:
            return circuit_breaker.call(func, *args, **kwargs)
        except Exception as e:
            last_exception = e
            if attempt < max_retries - 1:
                wait_time = _add_jitter(2 ** attempt, 0.2)  # Exponential backoff con jitter
                logger.warning(
                    "retry_attempt",
                    attempt=attempt + 1,
                    max_retries=max_retries,
                    wait_seconds=wait_time,
                    error=str(e)
                )
                time.sleep(wait_time)
            else:
                logger.error("retry_exhausted", max_retries=max_retries, error=str(e))
    
    raise last_exception

# Real-time monitoring integration
def _send_metric_to_monitoring(metric_name: str, value: float, tags: Dict[str, str] = None) -> None:
    """Env√≠a m√©trica a sistema de monitoreo en tiempo real."""
    try:
        # Integraci√≥n con StatsD/Prometheus si est√° disponible
        if Stats:
            try:
                Stats.incr(f"lead_capture.{metric_name}", int(value))
            except Exception:
                pass
        
        # Logging estructurado para sistemas de monitoreo
        logger.info(
            "metric_emitted",
            metric_name=metric_name,
            value=value,
            tags=tags or {}
        )
    except Exception as e:
        logger.debug("monitoring_metric_failed", error=str(e))

# Advanced analytics
def _calculate_conversion_funnel(lead_data: Dict[str, Any], conn_id: str) -> Dict[str, Any]:
    """Calcula funnel de conversi√≥n para el lead."""
    funnel = {
        "stages": {
            "awareness": 100,  # Todos los leads empiezan aqu√≠
            "interest": 0,
            "consideration": 0,
            "intent": 0,
            "purchase": 0
        },
        "current_stage": "awareness",
        "probability_by_stage": {}
    }
    
    score = lead_data.get("score", 0)
    quality_grade = lead_data.get("quality_analysis", {}).get("grade", "F")
    
    # Calcular probabilidad por etapa
    if score >= 75:
        funnel["stages"]["interest"] = 90
        funnel["stages"]["consideration"] = 70
        funnel["stages"]["intent"] = 50
        funnel["stages"]["purchase"] = 30
        funnel["current_stage"] = "interest"
    elif score >= 50:
        funnel["stages"]["interest"] = 60
        funnel["stages"]["consideration"] = 40
        funnel["stages"]["intent"] = 20
        funnel["current_stage"] = "awareness"
    elif score >= 30:
        funnel["stages"]["interest"] = 30
        funnel["current_stage"] = "awareness"
    
    # Ajustar por calidad
    if quality_grade in ["A", "B"]:
        for stage in funnel["stages"]:
            if stage != "awareness":
                funnel["stages"][stage] = min(100, funnel["stages"][stage] * 1.2)
    
    return funnel

# Load balancing para APIs externas
def _get_api_endpoint_with_load_balancing(api_name: str) -> str:
    """Obtiene endpoint de API con balanceo de carga."""
    endpoints = os.getenv(f"{api_name.upper()}_ENDPOINTS", "").split(",")
    if not endpoints or not endpoints[0]:
        # Fallback a endpoint √∫nico
        return os.getenv(f"{api_name.upper()}_ENDPOINT", "")
    
    # Round-robin simple usando timestamp
    import random
    selected = random.choice([e.strip() for e in endpoints if e.strip()])
    logger.debug("load_balanced_endpoint_selected", api_name=api_name, endpoint=selected[:50])
    return selected

# Rate limiting avanzado con sliding window
def _check_rate_limit_advanced(identifier: str, limit: int, window_seconds: int, conn_id: str = None) -> Tuple[bool, Optional[int]]:
    """Verifica rate limit usando sliding window avanzado."""
    try:
        if redis_client:
            key = f"rate_limit:{identifier}"
            current = redis_client.get(key)
            if current and int(current) >= limit:
                # Calcular tiempo hasta reset
                ttl = redis_client.ttl(key)
                return False, ttl
            # Incrementar contador
            pipe = redis_client.pipeline()
            pipe.incr(key)
            pipe.expire(key, window_seconds)
            pipe.execute()
            return True, None
        else:
            # Fallback a rate limiter b√°sico
            if rate_limiter:
                return rate_limiter.hit(f"{identifier}:{window_seconds}", limit, window_seconds), None
    except Exception as e:
        logger.warning("rate_limit_check_failed", error=str(e))
    return True, None

# Data validation avanzada con schemas
def _validate_against_schema(data: Dict[str, Any], schema: Dict[str, Any]) -> Tuple[bool, List[str]]:
    """Valida datos contra un schema definido."""
    errors = []
    
    for field, rules in schema.items():
        value = data.get(field)
        
        # Required check
        if rules.get("required", False) and not value:
            errors.append(f"Campo requerido faltante: {field}")
            continue
        
        if value is None:
            continue
        
        # Type check
        expected_type = rules.get("type")
        if expected_type and not isinstance(value, expected_type):
            errors.append(f"Tipo incorrecto para {field}: esperado {expected_type}, obtenido {type(value).__name__}")
        
        # Min/Max length
        if isinstance(value, str):
            min_len = rules.get("min_length")
            max_len = rules.get("max_length")
            if min_len and len(value) < min_len:
                errors.append(f"{field} muy corto: m√≠nimo {min_len} caracteres")
            if max_len and len(value) > max_len:
                errors.append(f"{field} muy largo: m√°ximo {max_len} caracteres")
        
        # Pattern validation
        pattern = rules.get("pattern")
        if pattern and isinstance(value, str):
            if not re.match(pattern, value):
                errors.append(f"{field} no coincide con patr√≥n requerido")
    
    return len(errors) == 0, errors

# Batch processing optimizado
def _process_batch_optimized(items: List[Dict[str, Any]], processor: Callable, batch_size: int = None, conn_id: str = None) -> List[Dict[str, Any]]:
    """Procesa items en batches optimizados con paralelizaci√≥n."""
    if batch_size is None:
        batch_size = _get_optimal_batch_size("batch_processing", conn_id) if conn_id else BATCH_SIZE
    
    results = []
    total_items = len(items)
    
    if CONCURRENT_AVAILABLE and ENABLE_PARALLEL_PROCESSING:
        # Procesamiento paralelo
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = []
            for i in range(0, total_items, batch_size):
                batch = items[i:i + batch_size]
                future = executor.submit(_process_batch_sync, batch, processor)
                futures.append(future)
            
            for i, future in enumerate(as_completed(futures)):
                try:
                    batch_results = future.result()
                    results.extend(batch_results)
                    _log_progress("batch_processing", len(results), total_items)
                except Exception as e:
                    logger.error("batch_processing_error", batch_index=i, error=str(e))
    else:
        # Procesamiento secuencial
        for i in range(0, total_items, batch_size):
            batch = items[i:i + batch_size]
            batch_results = _process_batch_sync(batch, processor)
            results.extend(batch_results)
            _log_progress("batch_processing", len(results), total_items)
    
    return results

def _process_batch_sync(batch: List[Dict[str, Any]], processor: Callable) -> List[Dict[str, Any]]:
    """Procesa un batch de forma s√≠ncrona."""
    results = []
    for item in batch:
        try:
            result = processor(item)
            results.append(result)
        except Exception as e:
            logger.error("item_processing_failed", error=str(e), item_id=item.get("id"))
            results.append(item)  # Mantener item original en caso de error
    return results

# Data enrichment con m√∫ltiples fuentes
def _enrich_from_multiple_sources(lead_data: Dict[str, Any], sources: List[str], conn_id: str = None) -> Dict[str, Any]:
    """Enriquece datos desde m√∫ltiples fuentes en paralelo."""
    enriched_data = lead_data.get("enriched_data", {})
    
    if CONCURRENT_AVAILABLE:
        with ThreadPoolExecutor(max_workers=min(len(sources), MAX_WORKERS)) as executor:
            futures = {}
            for source in sources:
                if source == "clearbit":
                    future = executor.submit(_enrich_from_clearbit, lead_data)
                    futures[future] = "clearbit"
                elif source == "hunter":
                    future = executor.submit(_enrich_from_hunter, lead_data)
                    futures[future] = "hunter"
                elif source == "linkedin":
                    future = executor.submit(_enrich_from_linkedin, lead_data)
                    futures[future] = "linkedin"
            
            for future in as_completed(futures):
                source_name = futures[future]
                try:
                    source_data = future.result()
                    if source_data:
                        enriched_data[source_name] = source_data
                except Exception as e:
                    logger.warning("enrichment_source_failed", source=source_name, error=str(e))
    
    lead_data["enriched_data"] = enriched_data
    lead_data["is_enriched"] = len(enriched_data) > 0
    return lead_data

def _enrich_from_clearbit(lead_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """Enriquece desde Clearbit (simplificado)."""
    # Implementaci√≥n simplificada - en producci√≥n usar√≠a la API real
    return None

def _enrich_from_hunter(lead_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """Enriquece desde Hunter.io (simplificado)."""
    return None

def _enrich_from_linkedin(lead_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """Enriquece desde LinkedIn (simplificado)."""
    return None

# An√°lisis de tendencias temporales
def _analyze_temporal_trends(conn_id: str, days: int = 30) -> Dict[str, Any]:
    """Analiza tendencias temporales de leads."""
    trends = {
        "period_days": days,
        "total_leads": 0,
        "avg_score": 0,
        "qualified_rate": 0,
        "trend_direction": "stable",
        "growth_rate": 0,
        "peak_day": None,
        "peak_hour": None
    }
    
    try:
        hook = PostgresHook(postgres_conn_id=conn_id)
        with hook.get_conn() as conn:
            with conn.cursor() as cur:
                # Estad√≠sticas generales
                cur.execute("""
                    SELECT 
                        COUNT(*) as total,
                        AVG(score) as avg_score,
                        COUNT(CASE WHEN score >= 60 THEN 1 END) as qualified
                    FROM leads
                    WHERE created_at >= NOW() - INTERVAL '%s days'
                """, (days,))
                stats = cur.fetchone()
                
                if stats and stats[0]:
                    trends["total_leads"] = stats[0]
                    trends["avg_score"] = float(stats[1] or 0)
                    trends["qualified_rate"] = (stats[2] / stats[0] * 100) if stats[0] > 0 else 0
                
                # D√≠a pico
                cur.execute("""
                    SELECT DATE(created_at) as day, COUNT(*) as count
                    FROM leads
                    WHERE created_at >= NOW() - INTERVAL '%s days'
                    GROUP BY day
                    ORDER BY count DESC
                    LIMIT 1
                """, (days,))
                peak_day = cur.fetchone()
                if peak_day:
                    trends["peak_day"] = peak_day[0].isoformat() if hasattr(peak_day[0], 'isoformat') else str(peak_day[0])
                
                # Hora pico
                cur.execute("""
                    SELECT EXTRACT(HOUR FROM created_at) as hour, COUNT(*) as count
                    FROM leads
                    WHERE created_at >= NOW() - INTERVAL '%s days'
                    GROUP BY hour
                    ORDER BY count DESC
                    LIMIT 1
                """, (days,))
                peak_hour = cur.fetchone()
                if peak_hour:
                    trends["peak_hour"] = int(peak_hour[0])
                
                # Tasa de crecimiento (comparar primera y segunda mitad del per√≠odo)
                cur.execute("""
                    SELECT 
                        COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '%s days' AND created_at < NOW() - INTERVAL '%s days') as first_half,
                        COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '%s days') as second_half
                    FROM leads
                    WHERE created_at >= NOW() - INTERVAL '%s days'
                """, (days, days/2, days/2, days))
                growth = cur.fetchone()
                if growth and growth[0] and growth[1]:
                    first_half = growth[0]
                    second_half = growth[1]
                    if first_half > 0:
                        growth_rate = ((second_half - first_half) / first_half) * 100
                        trends["growth_rate"] = round(growth_rate, 2)
                        trends["trend_direction"] = "increasing" if growth_rate > 5 else "decreasing" if growth_rate < -5 else "stable"
    
    except Exception as e:
        logger.warning("temporal_trends_analysis_failed", error=str(e))
    
    return trends

# Scoring din√°mico basado en reglas
def _calculate_dynamic_score(lead_data: Dict[str, Any], rules: List[Dict[str, Any]] = None) -> Dict[str, Any]:
    """Calcula score din√°mico basado en reglas configurables."""
    if rules is None:
        # Reglas por defecto
        rules = [
            {"condition": {"score": {"gte": 75}}, "action": "boost", "value": 10},
            {"condition": {"quality_grade": {"in": ["A", "B"]}}, "action": "boost", "value": 5},
            {"condition": {"churn_risk_level": {"eq": "high"}}, "action": "penalize", "value": -15},
        ]
    
    base_score = lead_data.get("score", 0)
    adjustments = []
    final_score = base_score
    
    for rule in rules:
        condition = rule.get("condition", {})
        action = rule.get("action", "boost")
        value = rule.get("value", 0)
        
        # Evaluar condici√≥n (simplificado)
        condition_met = True
        for field, criteria in condition.items():
            field_value = lead_data.get(field) or lead_data.get("quality_analysis", {}).get(field)
            
            if "gte" in criteria:
                if not (field_value and field_value >= criteria["gte"]):
                    condition_met = False
            elif "in" in criteria:
                if field_value not in criteria["in"]:
                    condition_met = False
            elif "eq" in criteria:
                if field_value != criteria["eq"]:
                    condition_met = False
        
        if condition_met:
            if action == "boost":
                final_score += value
                adjustments.append({"type": "boost", "value": value, "reason": rule.get("reason", "rule_match")})
            elif action == "penalize":
                final_score += value  # value ya es negativo
                adjustments.append({"type": "penalize", "value": value, "reason": rule.get("reason", "rule_match")})
    
    final_score = max(0, min(100, final_score))
    
    return {
        "base_score": base_score,
        "final_score": final_score,
        "adjustments": adjustments,
        "adjustment_total": final_score - base_score
    }

# ============================================================================
# FUNCIONES HELPER FINALES - ENTERPRISE FEATURES AVANZADAS
# ============================================================================

# ML Model Versioning
def _get_ml_model_version(model_name: str, conn_id: str = None) -> Dict[str, Any]:
    """Obtiene versi√≥n actual del modelo ML y metadatos."""
    model_info = {
        "model_name": model_name,
        "version": "1.0.0",
        "deployed_at": None,
        "performance_metrics": {},
        "is_active": True
    }
    
    try:
        if VARIABLES_AVAILABLE:
            model_key = f"lead_capture_ml_model_{model_name}"
            model_data = Variable.get(model_key, default_var=None)
            if model_data:
                model_info.update(json.loads(model_data))
    except Exception as e:
        logger.warning("ml_model_version_retrieval_failed", error=str(e))
    
    return model_info

# Advanced A/B Testing Framework
def _assign_ab_test_variant(lead_data: Dict[str, Any], test_name: str, variants: List[str] = None) -> str:
    """Asigna variante de A/B test de forma determin√≠stica."""
    if variants is None:
        variants = ["control", "variant_a", "variant_b"]
    
    # Usar hash del email para asignaci√≥n determin√≠stica
    email = lead_data.get("email", "")
    if email:
        hash_value = int(hashlib.sha256(email.encode()).hexdigest(), 16)
        variant_index = hash_value % len(variants)
        return variants[variant_index]
    
    # Fallback aleatorio
    import random
    return random.choice(variants)

def _track_ab_test_result(test_name: str, variant: str, outcome: str, lead_data: Dict[str, Any], conn_id: str = None) -> None:
    """Rastrea resultado de A/B test."""
    try:
        if VARIABLES_AVAILABLE:
            test_key = f"lead_capture_ab_test_{test_name}"
            test_data = Variable.get(test_key, default_var="{}")
            test_results = json.loads(test_data) if test_data else {}
            
            if variant not in test_results:
                test_results[variant] = {"total": 0, "conversions": 0, "revenue": 0.0}
            
            test_results[variant]["total"] += 1
            if outcome == "conversion":
                test_results[variant]["conversions"] += 1
                test_results[variant]["revenue"] += lead_data.get("ltv_estimate", {}).get("ltv_realistic", 0)
            
            Variable.set(test_key, json.dumps(test_results))
            logger.debug("ab_test_tracked", test_name=test_name, variant=variant, outcome=outcome)
    except Exception as e:
        logger.warning("ab_test_tracking_failed", error=str(e))

# Real-time Anomaly Detection
def _detect_realtime_anomalies(lead_data: Dict[str, Any], historical_stats: Dict[str, Any] = None) -> Dict[str, Any]:
    """Detecta anomal√≠as en tiempo real usando estad√≠sticas hist√≥ricas."""
    anomalies = {
        "detected": False,
        "anomalies": [],
        "severity": "low"
    }
    
    if not historical_stats:
        return anomalies
    
    # Anomal√≠a 1: Score muy alto comparado con hist√≥rico
    current_score = lead_data.get("score", 0)
    avg_score = historical_stats.get("avg_score", 50)
    std_score = historical_stats.get("std_score", 10)
    
    if current_score > avg_score + (3 * std_score):
        anomalies["detected"] = True
        anomalies["anomalies"].append({
            "type": "unusually_high_score",
            "value": current_score,
            "expected_range": f"{avg_score - std_score:.1f} - {avg_score + std_score:.1f}",
            "severity": "medium"
        })
    
    # Anomal√≠a 2: LTV muy alto
    current_ltv = lead_data.get("ltv_estimate", {}).get("ltv_realistic", 0)
    avg_ltv = historical_stats.get("avg_ltv", 10000)
    
    if current_ltv > avg_ltv * 5:
        anomalies["detected"] = True
        anomalies["anomalies"].append({
            "type": "unusually_high_ltv",
            "value": current_ltv,
            "expected_max": avg_ltv * 2,
            "severity": "high"
        })
    
    # Determinar severidad general
    if any(a.get("severity") == "high" for a in anomalies["anomalies"]):
        anomalies["severity"] = "high"
    elif any(a.get("severity") == "medium" for a in anomalies["anomalies"]):
        anomalies["severity"] = "medium"
    
    return anomalies

# Advanced Data Transformation Pipeline
def _transform_lead_data(lead_data: Dict[str, Any], transformations: List[Dict[str, Any]] = None) -> Dict[str, Any]:
    """Aplica transformaciones de datos configurables."""
    if transformations is None:
        # Transformaciones por defecto
        transformations = [
            {"type": "normalize_email", "field": "email"},
            {"type": "normalize_phone", "field": "phone"},
            {"type": "title_case", "field": "first_name"},
            {"type": "title_case", "field": "last_name"},
        ]
    
    transformed_data = lead_data.copy()
    
    for transformation in transformations:
        trans_type = transformation.get("type")
        field = transformation.get("field")
        
        if field not in transformed_data:
            continue
        
        value = transformed_data[field]
        if not isinstance(value, str):
            continue
        
        if trans_type == "normalize_email":
            transformed_data[field] = value.lower().strip()
        elif trans_type == "normalize_phone":
            # Remover caracteres no num√©ricos excepto +
            transformed_data[field] = re.sub(r'[^\d+]', '', value)
        elif trans_type == "title_case":
            transformed_data[field] = value.title()
        elif trans_type == "uppercase":
            transformed_data[field] = value.upper()
        elif trans_type == "lowercase":
            transformed_data[field] = value.lower()
        elif trans_type == "trim":
            transformed_data[field] = value.strip()
    
    return transformed_data

# Multi-tenant Support
def _get_tenant_config(tenant_id: str, conn_id: str = None) -> Dict[str, Any]:
    """Obtiene configuraci√≥n espec√≠fica de tenant."""
    default_config = {
        "tenant_id": tenant_id,
        "rate_limits": {"email": 5, "ip": 10},
        "scoring_rules": {},
        "enrichment_sources": ["clearbit", "hunter"],
        "crm_type": "salesforce",
        "features": {}
    }
    
    try:
        if VARIABLES_AVAILABLE:
            tenant_key = f"lead_capture_tenant_{tenant_id}"
            tenant_config = Variable.get(tenant_key, default_var=None)
            if tenant_config:
                config = json.loads(tenant_config)
                default_config.update(config)
    except Exception as e:
        logger.warning("tenant_config_retrieval_failed", tenant_id=tenant_id, error=str(e))
    
    return default_config

# Advanced Encryption/Tokenization
def _encrypt_sensitive_data(data: str, encryption_key: str = None) -> str:
    """Encripta datos sensibles (simplificado - en producci√≥n usar librer√≠as criptogr√°ficas)."""
    if not encryption_key:
        encryption_key = os.getenv("ENCRYPTION_KEY", "default_key")
    
    # En producci√≥n, usar cryptography.fernet o similar
    # Aqu√≠ solo retornamos un hash como placeholder
    encrypted = hashlib.sha256(f"{data}:{encryption_key}".encode()).hexdigest()
    return f"ENC:{encrypted[:32]}"

def _decrypt_sensitive_data(encrypted_data: str, encryption_key: str = None) -> Optional[str]:
    """Desencripta datos sensibles."""
    if not encrypted_data.startswith("ENC:"):
        return encrypted_data
    
    # En producci√≥n, implementar desencriptaci√≥n real
    logger.warning("decryption_not_implemented", message="Decryption requires proper cryptographic library")
    return None

# GraphQL-like Query Builder
def _query_leads(filters: Dict[str, Any], conn_id: str) -> List[Dict[str, Any]]:
    """Construye y ejecuta queries flexibles tipo GraphQL."""
    try:
        hook = PostgresHook(postgres_conn_id=conn_id)
        with hook.get_conn() as conn:
            with conn.cursor() as cur:
                # Construir query din√°micamente
                where_clauses = []
                params = []
                
                if filters.get("email"):
                    where_clauses.append("email = %s")
                    params.append(filters["email"])
                
                if filters.get("score_min"):
                    where_clauses.append("score >= %s")
                    params.append(filters["score_min"])
                
                if filters.get("created_after"):
                    where_clauses.append("created_at >= %s")
                    params.append(filters["created_after"])
                
                where_sql = " AND ".join(where_clauses) if where_clauses else "1=1"
                limit = filters.get("limit", 100)
                
                query = f"SELECT * FROM leads WHERE {where_sql} LIMIT %s"
                params.append(limit)
                
                cur.execute(query, params)
                columns = [desc[0] for desc in cur.description]
                results = [dict(zip(columns, row)) for row in cur.fetchall()]
                
                return results
    except Exception as e:
        logger.error("query_leads_failed", error=str(e))
        return []

# Advanced Workflow Orchestration
def _orchestrate_workflow(workflow_def: Dict[str, Any], lead_data: Dict[str, Any], conn_id: str = None) -> Dict[str, Any]:
    """Orquesta workflow complejo basado en definici√≥n."""
    workflow_result = {
        "workflow_id": workflow_def.get("id"),
        "status": "running",
        "steps_completed": [],
        "steps_failed": [],
        "final_data": lead_data
    }
    
    steps = workflow_def.get("steps", [])
    
    for step in steps:
        step_name = step.get("name")
        step_condition = step.get("condition")
        step_action = step.get("action")
        
        # Evaluar condici√≥n
        should_execute = True
        if step_condition:
            # Evaluaci√≥n simplificada de condici√≥n
            condition_field = step_condition.get("field")
            condition_operator = step_condition.get("operator", "eq")
            condition_value = step_condition.get("value")
            
            field_value = lead_data.get(condition_field)
            
            if condition_operator == "eq" and field_value != condition_value:
                should_execute = False
            elif condition_operator == "gte" and (not field_value or field_value < condition_value):
                should_execute = False
        
        if should_execute:
            try:
                # Ejecutar acci√≥n (simplificado)
                if step_action == "enrich":
                    lead_data = _enrich_from_multiple_sources(lead_data, step.get("sources", []), conn_id)
                elif step_action == "notify":
                    _route_notification(step.get("notification_type", "info"), step.get("priority", "medium"), lead_data)
                
                workflow_result["steps_completed"].append(step_name)
            except Exception as e:
                workflow_result["steps_failed"].append({"step": step_name, "error": str(e)})
                logger.error("workflow_step_failed", step=step_name, error=str(e))
    
    workflow_result["status"] = "completed" if not workflow_result["steps_failed"] else "partial"
    workflow_result["final_data"] = lead_data
    
    return workflow_result

# Data Streaming Support
def _stream_lead_event(event_type: str, lead_data: Dict[str, Any], stream_config: Dict[str, Any] = None) -> None:
    """Env√≠a evento a stream de datos (Kafka, Kinesis, etc.)."""
    if not stream_config:
        stream_config = {
            "enabled": os.getenv("STREAMING_ENABLED", "false").lower() == "true",
            "endpoint": os.getenv("STREAMING_ENDPOINT"),
            "topic": os.getenv("STREAMING_TOPIC", "lead_events")
        }
    
    if not stream_config.get("enabled") or not stream_config.get("endpoint"):
        return
    
    try:
        event_payload = {
            "event_type": event_type,
            "timestamp": datetime.utcnow().isoformat(),
            "lead_id": lead_data.get("lead_ext_id"),
            "data": lead_data
        }
        
        # En producci√≥n, usar librer√≠a de streaming apropiada
        # Aqu√≠ solo logueamos
        logger.info("lead_event_streamed", event_type=event_type, lead_id=lead_data.get("lead_ext_id"))
    except Exception as e:
        logger.warning("streaming_failed", error=str(e))

# Advanced Reporting and Visualization
def _generate_visualization_data(metrics: Dict[str, Any], chart_type: str = "line") -> Dict[str, Any]:
    """Genera datos formateados para visualizaci√≥n."""
    viz_data = {
        "chart_type": chart_type,
        "data": [],
        "labels": [],
        "metadata": {
            "generated_at": datetime.utcnow().isoformat(),
            "time_range": metrics.get("time_range", "24h")
        }
    }
    
    if chart_type == "line":
        # Formato para gr√°fico de l√≠nea
        for key, value in metrics.items():
            if isinstance(value, (int, float)):
                viz_data["data"].append({"x": key, "y": value})
                viz_data["labels"].append(key)
    elif chart_type == "pie":
        # Formato para gr√°fico de pie
        total = sum(v for v in metrics.values() if isinstance(v, (int, float)))
        for key, value in metrics.items():
            if isinstance(value, (int, float)):
                viz_data["data"].append({
                    "label": key,
                    "value": value,
                    "percentage": round((value / total * 100) if total > 0 else 0, 2)
                })
    
    return viz_data

# ============================================================================
# FUNCIONES HELPER ULTRA AVANZADAS - ENTERPRISE FEATURES FINALES
# ============================================================================

# Advanced Distributed Tracing
def _create_trace_context(operation_name: str, metadata: Dict[str, Any] = None) -> Dict[str, Any]:
    """Crea contexto de tracing distribuido."""
    trace_context = {
        "trace_id": hashlib.sha256(f"{operation_name}:{datetime.utcnow().isoformat()}".encode()).hexdigest()[:32],
        "span_id": hashlib.sha256(f"{operation_name}:{time.time()}".encode()).hexdigest()[:16],
        "operation": operation_name,
        "start_time": datetime.utcnow().isoformat(),
        "metadata": metadata or {}
    }
    
    if tracer:
        span = tracer.start_span(operation_name)
        for key, value in (metadata or {}).items():
            span.set_attribute(key, str(value))
        trace_context["span"] = span
    
    return trace_context

# Advanced Circuit Breaker Management
def _get_circuit_breaker_status() -> Dict[str, Any]:
    """Obtiene estado de todos los circuit breakers."""
    status = {
        "circuit_breakers": {},
        "overall_status": "healthy"
    }
    
    try:
        status["circuit_breakers"]["crm"] = {
            "state": crm_circuit_breaker.current_state,
            "fail_counter": crm_circuit_breaker.fail_counter,
            "success_counter": crm_circuit_breaker.success_counter
        }
        
        status["circuit_breakers"]["enrichment"] = {
            "state": enrichment_circuit_breaker.current_state,
            "fail_counter": enrichment_circuit_breaker.fail_counter,
            "success_counter": enrichment_circuit_breaker.success_counter
        }
        
        status["circuit_breakers"]["onboarding"] = {
            "state": onboarding_circuit_breaker.current_state,
            "fail_counter": onboarding_circuit_breaker.fail_counter,
            "success_counter": onboarding_circuit_breaker.success_counter
        }
        
        # Determinar estado general
        open_breakers = sum(1 for cb in status["circuit_breakers"].values() if cb["state"] == "open")
        if open_breakers > 0:
            status["overall_status"] = "degraded" if open_breakers < len(status["circuit_breakers"]) else "unhealthy"
    except Exception as e:
        logger.warning("circuit_breaker_status_failed", error=str(e))
    
    return status

# Advanced Rate Limiting con Token Bucket
def _check_token_bucket_rate_limit(identifier: str, tokens: int = 1, capacity: int = 10, refill_rate: float = 1.0, conn_id: str = None) -> Tuple[bool, Optional[float]]:
    """Implementa rate limiting con algoritmo Token Bucket."""
    try:
        if redis_client:
            key = f"token_bucket:{identifier}"
            now = time.time()
            
            # Obtener estado actual del bucket
            bucket_data = redis_client.hgetall(key)
            
            if not bucket_data:
                # Inicializar bucket
                bucket_data = {
                    "tokens": str(capacity),
                    "last_refill": str(now)
                }
            else:
                # Calcular tokens a agregar
                last_refill = float(bucket_data.get("last_refill", now))
                elapsed = now - last_refill
                tokens_to_add = elapsed * refill_rate
                current_tokens = float(bucket_data.get("tokens", 0))
                new_tokens = min(capacity, current_tokens + tokens_to_add)
                
                bucket_data["tokens"] = str(new_tokens)
                bucket_data["last_refill"] = str(now)
            
            current_tokens = float(bucket_data["tokens"])
            
            if current_tokens >= tokens:
                # Consumir tokens
                bucket_data["tokens"] = str(current_tokens - tokens)
                redis_client.hset(key, mapping=bucket_data)
                redis_client.expire(key, 3600)  # Expirar despu√©s de 1 hora
                return True, None
            else:
                # Calcular tiempo hasta pr√≥ximo token disponible
                time_until_next = (tokens - current_tokens) / refill_rate
                return False, time_until_next
    except Exception as e:
        logger.warning("token_bucket_rate_limit_failed", error=str(e))
    
    return True, None

# Advanced Caching con Redis
def _cache_with_redis(key: str, value: Any, ttl: int = 3600, conn_id: str = None) -> bool:
    """Cachea datos en Redis con TTL."""
    try:
        if redis_client:
            serialized_value = json.dumps(value, default=str)
            redis_client.setex(key, ttl, serialized_value)
            return True
    except Exception as e:
        logger.warning("redis_cache_failed", key=key[:50], error=str(e))
    return False

def _get_from_redis_cache(key: str, conn_id: str = None) -> Optional[Any]:
    """Obtiene datos del cache de Redis."""
    try:
        if redis_client:
            cached = redis_client.get(key)
            if cached:
                return json.loads(cached)
    except Exception as e:
        logger.warning("redis_cache_get_failed", key=key[:50], error=str(e))
    return None

# Advanced Data Validation Rules Engine
def _validate_with_rules_engine(data: Dict[str, Any], rules: List[Dict[str, Any]]) -> Tuple[bool, List[str]]:
    """Valida datos usando motor de reglas configurable."""
    errors = []
    
    for rule in rules:
        rule_type = rule.get("type")
        field = rule.get("field")
        condition = rule.get("condition")
        message = rule.get("message", f"Validation failed for {field}")
        
        value = data.get(field)
        
        if rule_type == "required" and not value:
            errors.append(message)
        elif rule_type == "min_length" and isinstance(value, str) and len(value) < condition:
            errors.append(f"{message} (minimum length: {condition})")
        elif rule_type == "max_length" and isinstance(value, str) and len(value) > condition:
            errors.append(f"{message} (maximum length: {condition})")
        elif rule_type == "pattern" and isinstance(value, str) and not re.match(condition, value):
            errors.append(message)
        elif rule_type == "min_value" and isinstance(value, (int, float)) and value < condition:
            errors.append(f"{message} (minimum value: {condition})")
        elif rule_type == "max_value" and isinstance(value, (int, float)) and value > condition:
            errors.append(f"{message} (maximum value: {condition})")
        elif rule_type == "custom" and condition:
            # Ejecutar funci√≥n de validaci√≥n personalizada
            try:
                if not condition(value, data):
                    errors.append(message)
            except Exception as e:
                logger.warning("custom_validation_failed", rule=rule_type, error=str(e))
    
    return len(errors) == 0, errors

# Advanced Metrics Collection
def _collect_advanced_metrics(operation: str, duration: float, success: bool, metadata: Dict[str, Any] = None) -> None:
    """Recolecta m√©tricas avanzadas de operaciones."""
    try:
        metrics_data = {
            "operation": operation,
            "duration": duration,
            "success": success,
            "timestamp": datetime.utcnow().isoformat(),
            "metadata": metadata or {}
        }
        
        # Enviar a Prometheus si est√° disponible
        if PROMETHEUS_AVAILABLE:
            if success:
                lead_metrics.get("captured", DummyMetric()).inc(1)
            else:
                lead_metrics.get("rate_limited", DummyMetric()).inc(1)
        
        # Guardar en Variables para an√°lisis hist√≥rico
        if VARIABLES_AVAILABLE:
            metrics_key = f"lead_capture_metrics_{operation}"
            metrics_history = Variable.get(metrics_key, default_var="[]")
            history = json.loads(metrics_history) if metrics_history else []
            
            history.append(metrics_data)
            # Mantener √∫ltimos 1000 registros
            if len(history) > 1000:
                history = history[-1000:]
            
            Variable.set(metrics_key, json.dumps(history))
        
        logger.info("advanced_metrics_collected", **metrics_data)
    except Exception as e:
        logger.warning("metrics_collection_failed", error=str(e))

# Advanced Alerting System
def _send_advanced_alert(alert_type: str, severity: str, message: str, context: Dict[str, Any] = None) -> None:
    """Env√≠a alertas avanzadas con routing inteligente."""
    alert = {
        "alert_type": alert_type,
        "severity": severity,
        "message": message,
        "timestamp": datetime.utcnow().isoformat(),
        "context": context or {}
    }
    
    # Routing basado en severidad
    channels = _route_notification(alert_type, severity, alert)
    
    # Enviar a cada canal
    for channel in channels:
        try:
            if channel == "slack" and notification_manager.slack_webhook:
                notification_manager._send_notification(message, alert_type, context)
            elif channel == "email":
                # Implementar env√≠o de email
                logger.info("email_alert_sent", alert_type=alert_type)
            elif channel == "pagerduty":
                # Implementar integraci√≥n con PagerDuty
                logger.info("pagerduty_alert_sent", alert_type=alert_type)
        except Exception as e:
            logger.warning("alert_send_failed", channel=channel, error=str(e))
    
    # Guardar en historial de alertas
    if VARIABLES_AVAILABLE:
        alerts_key = "lead_capture_alerts_history"
        alerts_history = Variable.get(alerts_key, default_var="[]")
        alerts_list = json.loads(alerts_history) if alerts_history else []
        
        alerts_list.append(alert)
        if len(alerts_list) > 500:
            alerts_list = alerts_list[-500:]
        
        Variable.set(alerts_key, json.dumps(alerts_list))

# Advanced Data Quality Rules
def _apply_data_quality_rules(lead_data: Dict[str, Any], rules: List[Dict[str, Any]] = None) -> Dict[str, Any]:
    """Aplica reglas de calidad de datos."""
    if rules is None:
        # Reglas por defecto
        rules = [
            {"field": "email", "rule": "valid_email", "action": "reject"},
            {"field": "phone", "rule": "valid_phone", "action": "warn"},
            {"field": "company", "rule": "min_length", "value": 2, "action": "warn"}
        ]
    
    quality_result = {
        "passed": True,
        "warnings": [],
        "errors": [],
        "score": 100
    }
    
    for rule in rules:
        field = rule.get("field")
        rule_type = rule.get("rule")
        action = rule.get("action", "warn")
        value = lead_data.get(field)
        
        if not value:
            continue
        
        if rule_type == "valid_email" and "@" not in str(value):
            if action == "reject":
                quality_result["errors"].append(f"Invalid email format: {field}")
                quality_result["passed"] = False
                quality_result["score"] -= 20
            else:
                quality_result["warnings"].append(f"Email format may be invalid: {field}")
                quality_result["score"] -= 5
        
        elif rule_type == "min_length" and len(str(value)) < rule.get("value", 0):
            if action == "reject":
                quality_result["errors"].append(f"{field} too short")
                quality_result["passed"] = False
                quality_result["score"] -= 15
            else:
                quality_result["warnings"].append(f"{field} may be too short")
                quality_result["score"] -= 3
    
    quality_result["score"] = max(0, quality_result["score"])
    return quality_result

# Advanced Retry con Exponential Backoff Mejorado
def _retry_with_advanced_backoff(
    func: Callable,
    max_retries: int = 3,
    initial_delay: float = 1.0,
    max_delay: float = 60.0,
    exponential_base: float = 2.0,
    jitter: bool = True,
    *args,
    **kwargs
) -> Any:
    """Retry avanzado con exponential backoff mejorado."""
    last_exception = None
    
    for attempt in range(max_retries):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            last_exception = e
            
            if attempt < max_retries - 1:
                # Calcular delay con exponential backoff
                delay = min(max_delay, initial_delay * (exponential_base ** attempt))
                
                # A√±adir jitter si est√° habilitado
                if jitter:
                    delay = _add_jitter(delay, 0.2)
                
                logger.warning(
                    "retry_with_backoff",
                    attempt=attempt + 1,
                    max_retries=max_retries,
                    delay_seconds=delay,
                    error=str(e)
                )
                
                time.sleep(delay)
            else:
                logger.error("retry_exhausted", max_retries=max_retries, error=str(e))
    
    raise last_exception

# Advanced Batch Processing con Chunking
def _process_in_chunks(items: List[Any], chunk_size: int, processor: Callable, conn_id: str = None) -> List[Any]:
    """Procesa items en chunks con manejo avanzado de errores."""
    results = []
    failed_items = []
    
    for i in range(0, len(items), chunk_size):
        chunk = items[i:i + chunk_size]
        chunk_results = []
        
        for item in chunk:
            try:
                result = processor(item)
                chunk_results.append(result)
                results.append(result)
            except Exception as e:
                logger.error("chunk_item_failed", item_id=str(item.get("id", "unknown")), error=str(e))
                failed_items.append({"item": item, "error": str(e)})
        
        # Logging de progreso
        _log_progress("chunk_processing", len(results), len(items))
        
        # Peque√±o delay entre chunks para evitar sobrecarga
        if i + chunk_size < len(items):
            time.sleep(0.1)
    
    if failed_items:
        logger.warning("chunk_processing_completed_with_errors", total=len(items), failed=len(failed_items))
    
    return results

# Advanced Error Recovery con Multiple Strategies
def _recover_with_strategies(error: Exception, context: Dict[str, Any], strategies: List[str] = None) -> Dict[str, Any]:
    """Recuperaci√≥n avanzada con m√∫ltiples estrategias en orden."""
    if strategies is None:
        strategies = ["retry", "cache_fallback", "degraded_mode", "dlq"]
    
    recovery_result = {
        "attempted_strategies": [],
        "successful_strategy": None,
        "recovered": False,
        "data": None
    }
    
    for strategy in strategies:
        recovery_result["attempted_strategies"].append(strategy)
        
        try:
            if strategy == "retry":
                # Ya implementado en otras funciones
                continue
            elif strategy == "cache_fallback":
                # Intentar obtener de cache
                cache_key = context.get("cache_key")
                if cache_key:
                    cached = _get_from_redis_cache(cache_key) or enrichment_cache.get(cache_key)
                    if cached:
                        recovery_result["successful_strategy"] = strategy
                        recovery_result["recovered"] = True
                        recovery_result["data"] = cached
                        break
            elif strategy == "degraded_mode":
                # Modo degradado - continuar con datos m√≠nimos
                recovery_result["successful_strategy"] = strategy
                recovery_result["recovered"] = True
                recovery_result["data"] = context.get("fallback_data", {})
                break
            elif strategy == "dlq":
                # Guardar en DLQ para procesamiento posterior
                save_to_dlq(context.get("item", {}), str(error), context)
                recovery_result["successful_strategy"] = strategy
                recovery_result["recovered"] = True
                break
        except Exception as e:
            logger.warning("recovery_strategy_failed", strategy=strategy, error=str(e))
            continue
    
    return recovery_result

# Advanced Performance Monitoring
def _monitor_performance(operation: str, func: Callable, *args, **kwargs) -> Tuple[Any, Dict[str, Any]]:
    """Monitorea performance de una operaci√≥n con m√©tricas detalladas."""
    import time
    start_time = time.time()
    start_memory = None
    
    try:
        # Medir memoria inicial si es posible
        import sys
        if hasattr(sys, 'getsizeof'):
            start_memory = sys.getsizeof(args) + sys.getsizeof(kwargs)
        
        # Ejecutar operaci√≥n
        result = func(*args, **kwargs)
        
        # Calcular m√©tricas
        duration = time.time() - start_time
        end_memory = None
        if start_memory and hasattr(sys, 'getsizeof'):
            end_memory = sys.getsizeof(result) if result else 0
        
        metrics = {
            "operation": operation,
            "duration_seconds": round(duration, 4),
            "success": True,
            "memory_delta_mb": round((end_memory - start_memory) / (1024 * 1024), 4) if end_memory and start_memory else None
        }
        
        # Enviar m√©tricas
        _collect_advanced_metrics(operation, duration, True, metrics)
        
        return result, metrics
        
    except Exception as e:
        duration = time.time() - start_time
        metrics = {
            "operation": operation,
            "duration_seconds": round(duration, 4),
            "success": False,
            "error": str(e)
        }
        
        _collect_advanced_metrics(operation, duration, False, metrics)
        raise

# ============================================================================
# FUNCIONES HELPER FINALES ULTRA AVANZADAS - ENTERPRISE FEATURES
# ============================================================================

# Advanced API Versioning
def _get_api_version(api_name: str, conn_id: str = None) -> Dict[str, Any]:
    """Obtiene versi√≥n de API y compatibilidad."""
    version_info = {
        "api_name": api_name,
        "current_version": "v1",
        "supported_versions": ["v1"],
        "deprecated_versions": [],
        "end_of_life": None
    }
    
    try:
        if VARIABLES_AVAILABLE:
            version_key = f"lead_capture_api_version_{api_name}"
            version_data = Variable.get(version_key, default_var=None)
            if version_data:
                version_info.update(json.loads(version_data))
    except Exception as e:
        logger.warning("api_version_retrieval_failed", error=str(e))
    
    return version_info

# Advanced Security Scanning
def _scan_for_security_issues(lead_data: Dict[str, Any]) -> Dict[str, Any]:
    """Escanea datos en busca de problemas de seguridad."""
    security_issues = {
        "issues_found": False,
        "issues": [],
        "severity": "low"
    }
    
    # Escanear por SQL injection patterns
    sql_patterns = ["'", ";", "--", "/*", "*/", "xp_", "sp_"]
    for field, value in lead_data.items():
        if isinstance(value, str):
            for pattern in sql_patterns:
                if pattern in value.lower():
                    security_issues["issues_found"] = True
                    security_issues["issues"].append({
                        "type": "potential_sql_injection",
                        "field": field,
                        "pattern": pattern,
                        "severity": "high"
                    })
    
    # Escanear por XSS patterns
    xss_patterns = ["<script", "javascript:", "onerror=", "onload="]
    for field, value in lead_data.items():
        if isinstance(value, str):
            for pattern in xss_patterns:
                if pattern.lower() in value.lower():
                    security_issues["issues_found"] = True
                    security_issues["issues"].append({
                        "type": "potential_xss",
                        "field": field,
                        "pattern": pattern,
                        "severity": "high"
                    })
    
    # Determinar severidad general
    if any(issue.get("severity") == "high" for issue in security_issues["issues"]):
        security_issues["severity"] = "high"
    elif security_issues["issues"]:
        security_issues["severity"] = "medium"
    
    return security_issues

# Advanced Data Masking/Anonymization
def _mask_sensitive_data(data: Dict[str, Any], fields_to_mask: List[str] = None) -> Dict[str, Any]:
    """Enmascara datos sensibles para logging/an√°lisis."""
    if fields_to_mask is None:
        fields_to_mask = ["email", "phone", "ssn", "credit_card"]
    
    masked_data = data.copy()
    
    for field in fields_to_mask:
        if field in masked_data and masked_data[field]:
            value = str(masked_data[field])
            if len(value) > 4:
                # Mostrar solo primeros y √∫ltimos caracteres
                masked_data[field] = f"{value[:2]}***{value[-2:]}"
            else:
                masked_data[field] = "****"
    
    return masked_data

# Advanced Concurrency Control
def _acquire_semaphore(semaphore_name: str, max_concurrent: int = 5, timeout: int = 60, conn_id: str = None) -> bool:
    """Adquiere sem√°foro para control de concurrencia."""
    try:
        if redis_client:
            key = f"semaphore:{semaphore_name}"
            now = time.time()
            
            # Limpiar entradas expiradas
            redis_client.zremrangebyscore(key, 0, now - timeout)
            
            # Contar entradas activas
            active_count = redis_client.zcard(key)
            
            if active_count < max_concurrent:
                # Adquirir sem√°foro
                member = f"{os.getpid()}:{now}"
                redis_client.zadd(key, {member: now})
                redis_client.expire(key, timeout)
                return True
            else:
                return False
    except Exception as e:
        logger.warning("semaphore_acquisition_failed", error=str(e))
        return True  # Fallback: permitir ejecuci√≥n
    
    return True

def _release_semaphore(semaphore_name: str, conn_id: str = None) -> None:
    """Libera sem√°foro."""
    try:
        if redis_client:
            key = f"semaphore:{semaphore_name}"
            member = f"{os.getpid()}:*"
            redis_client.zrem(key, member)
    except Exception as e:
        logger.warning("semaphore_release_failed", error=str(e))

# Advanced State Management
def _save_state(state_key: str, state_data: Dict[str, Any], ttl: int = 3600, conn_id: str = None) -> bool:
    """Guarda estado con TTL."""
    try:
        if redis_client:
            state = {
                "data": state_data,
                "timestamp": datetime.utcnow().isoformat(),
                "ttl": ttl
            }
            redis_client.setex(f"state:{state_key}", ttl, json.dumps(state))
            return True
        elif VARIABLES_AVAILABLE:
            Variable.set(f"state:{state_key}", json.dumps(state_data))
            return True
    except Exception as e:
        logger.warning("state_save_failed", error=str(e))
    return False

def _load_state(state_key: str, conn_id: str = None) -> Optional[Dict[str, Any]]:
    """Carga estado guardado."""
    try:
        if redis_client:
            state_json = redis_client.get(f"state:{state_key}")
            if state_json:
                state = json.loads(state_json)
                return state.get("data")
        elif VARIABLES_AVAILABLE:
            state_data = Variable.get(f"state:{state_key}", default_var=None)
            if state_data:
                return json.loads(state_data)
    except Exception as e:
        logger.warning("state_load_failed", error=str(e))
    return None

# Advanced Configuration Management
def _get_config_with_fallback(config_key: str, default_value: Any = None, fallback_keys: List[str] = None) -> Any:
    """Obtiene configuraci√≥n con fallback a claves alternativas."""
    # Intentar clave principal
    try:
        if VARIABLES_AVAILABLE:
            value = Variable.get(config_key, default_var=None)
            if value:
                return json.loads(value) if isinstance(value, str) and value.startswith("{") else value
    except Exception:
        pass
    
    # Intentar claves de fallback
    if fallback_keys:
        for fallback_key in fallback_keys:
            try:
                if VARIABLES_AVAILABLE:
                    value = Variable.get(fallback_key, default_var=None)
                    if value:
                        return json.loads(value) if isinstance(value, str) and value.startswith("{") else value
            except Exception:
                continue
    
    # Retornar valor por defecto
    return default_value

# Advanced Dependency Injection
class _ServiceContainer:
    """Contenedor simple de servicios para dependency injection."""
    def __init__(self):
        self._services = {}
    
    def register(self, service_name: str, service_instance: Any) -> None:
        """Registra un servicio."""
        self._services[service_name] = service_instance
    
    def get(self, service_name: str) -> Optional[Any]:
        """Obtiene un servicio."""
        return self._services.get(service_name)
    
    def has(self, service_name: str) -> bool:
        """Verifica si un servicio est√° registrado."""
        return service_name in self._services

_service_container = _ServiceContainer()

def _register_service(service_name: str, service_instance: Any) -> None:
    """Registra un servicio en el contenedor."""
    _service_container.register(service_name, service_instance)

def _get_service(service_name: str) -> Optional[Any]:
    """Obtiene un servicio del contenedor."""
    return _service_container.get(service_name)

# Advanced Plugin System
def _load_plugin(plugin_name: str, plugin_config: Dict[str, Any] = None) -> Optional[Callable]:
    """Carga plugin din√°micamente."""
    try:
        plugin_path = os.getenv(f"PLUGIN_{plugin_name.upper()}_PATH")
        if plugin_path and os.path.exists(plugin_path):
            # En producci√≥n, usar importlib para cargar m√≥dulos din√°micamente
            logger.info("plugin_loaded", plugin_name=plugin_name)
            return lambda *args, **kwargs: None  # Placeholder
    except Exception as e:
        logger.warning("plugin_load_failed", plugin_name=plugin_name, error=str(e))
    return None

# Advanced Testing Utilities
def _create_mock_lead(overrides: Dict[str, Any] = None) -> Dict[str, Any]:
    """Crea lead mock para testing."""
    mock_lead = {
        "email": "test@example.com",
        "first_name": "Test",
        "last_name": "User",
        "company": "Test Company",
        "phone": "+1234567890",
        "source": "test",
        "score": 50,
        "created_at": datetime.utcnow().isoformat()
    }
    
    if overrides:
        mock_lead.update(overrides)
    
    return mock_lead

def _assert_lead_valid(lead_data: Dict[str, Any]) -> Tuple[bool, List[str]]:
    """Valida estructura de lead para testing."""
    errors = []
    required_fields = ["email", "first_name", "last_name"]
    
    for field in required_fields:
        if field not in lead_data:
            errors.append(f"Missing required field: {field}")
    
    if lead_data.get("email") and "@" not in lead_data["email"]:
        errors.append("Invalid email format")
    
    return len(errors) == 0, errors

# Advanced Documentation Generation
def _generate_api_documentation(endpoints: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Genera documentaci√≥n de API autom√°ticamente."""
    docs = {
        "version": "1.0.0",
        "generated_at": datetime.utcnow().isoformat(),
        "endpoints": []
    }
    
    for endpoint in endpoints:
        endpoint_doc = {
            "path": endpoint.get("path"),
            "method": endpoint.get("method", "POST"),
            "description": endpoint.get("description"),
            "parameters": endpoint.get("parameters", []),
            "responses": endpoint.get("responses", {})
        }
        docs["endpoints"].append(endpoint_doc)
    
    return docs

# Advanced Health Check Aggregator
def _aggregate_health_checks(checks: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Agrega resultados de m√∫ltiples health checks."""
    aggregated = {
        "overall_status": "healthy",
        "checks_count": len(checks),
        "healthy_count": 0,
        "degraded_count": 0,
        "unhealthy_count": 0,
        "details": checks
    }
    
    for check in checks:
        status = check.get("status", "unknown")
        if status == "healthy":
            aggregated["healthy_count"] += 1
        elif status == "degraded":
            aggregated["degraded_count"] += 1
        elif status == "unhealthy":
            aggregated["unhealthy_count"] += 1
    
    # Determinar estado general
    if aggregated["unhealthy_count"] > 0:
        aggregated["overall_status"] = "unhealthy"
    elif aggregated["degraded_count"] > 0:
        aggregated["overall_status"] = "degraded"
    
    return aggregated

# Advanced Cache Warming
def _warm_cache(cache_keys: List[str], loader_func: Callable, conn_id: str = None) -> Dict[str, Any]:
    """Pre-carga cache con datos frecuentemente accedidos."""
    warming_result = {
        "keys_warmed": 0,
        "keys_failed": 0,
        "errors": []
    }
    
    for key in cache_keys:
        try:
            data = loader_func(key)
            if data:
                _cache_with_redis(key, data, ttl=3600, conn_id=conn_id)
                warming_result["keys_warmed"] += 1
        except Exception as e:
            warming_result["keys_failed"] += 1
            warming_result["errors"].append({"key": key, "error": str(e)})
    
    logger.info("cache_warmed", **warming_result)
    return warming_result

# Advanced Request Batching
def _batch_requests(requests: List[Dict[str, Any]], batch_size: int = 10, delay: float = 0.1) -> List[Dict[str, Any]]:
    """Agrupa requests en batches para optimizaci√≥n."""
    results = []
    
    for i in range(0, len(requests), batch_size):
        batch = requests[i:i + batch_size]
        
        # Procesar batch (simplificado)
        batch_results = []
        for req in batch:
            try:
                # Aqu√≠ se procesar√≠a el request real
                batch_results.append({"request": req, "status": "success"})
            except Exception as e:
                batch_results.append({"request": req, "status": "error", "error": str(e)})
        
        results.extend(batch_results)
        
        # Delay entre batches
        if i + batch_size < len(requests):
            time.sleep(delay)
    
    return results

# Advanced Feature Toggle System
def _check_feature_toggle(feature_name: str, user_id: str = None, default: bool = False) -> bool:
    """Sistema avanzado de feature toggles con soporte para usuarios espec√≠ficos."""
    try:
        if VARIABLES_AVAILABLE:
            # Feature toggle global
            toggle_key = f"feature_toggle_{feature_name}"
            global_toggle = Variable.get(toggle_key, default_var=None)
            
            if global_toggle:
                toggle_data = json.loads(global_toggle) if isinstance(global_toggle, str) else global_toggle
                
                # Verificar si est√° habilitado globalmente
                if not toggle_data.get("enabled", False):
                    return False
                
                # Verificar usuarios espec√≠ficos
                if user_id:
                    allowed_users = toggle_data.get("allowed_users", [])
                    if allowed_users and user_id not in allowed_users:
                        return False
                
                # Verificar porcentaje de rollout
                rollout_percentage = toggle_data.get("rollout_percentage", 100)
                if rollout_percentage < 100:
                    # Usar hash del user_id para determinismo
                    if user_id:
                        hash_value = int(hashlib.sha256(user_id.encode()).hexdigest(), 16)
                        user_percentage = (hash_value % 100) + 1
                        if user_percentage > rollout_percentage:
                            return False
                
                return True
    except Exception as e:
        logger.warning("feature_toggle_check_failed", error=str(e))
    
    return default

# Advanced Logging Context Manager
@contextmanager
def _logging_context(**context_vars):
    """Context manager para logging estructurado."""
    try:
        # Agregar variables de contexto al logger
        logger.info("context_started", **context_vars)
        yield
        logger.info("context_completed", **context_vars)
    except Exception as e:
        logger.error("context_error", error=str(e), **context_vars)
        raise
    finally:
        logger.debug("context_ended", **context_vars)

# ============================================================================
# FUNCIONES HELPER FINALES EXTRA AVANZADAS - ENTERPRISE FEATURES
# ============================================================================

# Advanced Async Processing Support
def _process_async(items: List[Dict[str, Any]], processor: Callable, max_workers: int = None, conn_id: str = None) -> List[Dict[str, Any]]:
    """Procesa items de forma as√≠ncrona con ThreadPoolExecutor."""
    if max_workers is None:
        max_workers = MAX_WORKERS if CONCURRENT_AVAILABLE else 1
    
    if not CONCURRENT_AVAILABLE or max_workers == 1:
        # Procesamiento secuencial
        return [processor(item) for item in items]
    
    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(processor, item): item for item in items}
        
        for future in as_completed(futures):
            item = futures[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                logger.error("async_processing_failed", item_id=item.get("id"), error=str(e))
                results.append(item)  # Mantener item original
    
    return results

# Advanced Queue Management
def _enqueue_lead(lead_data: Dict[str, Any], queue_name: str = "default", priority: int = 0, conn_id: str = None) -> bool:
    """Encola lead en queue con prioridad."""
    try:
        if redis_client:
            queue_key = f"queue:{queue_name}"
            queue_item = {
                "data": lead_data,
                "priority": priority,
                "enqueued_at": datetime.utcnow().isoformat(),
                "retries": 0
            }
            # Usar sorted set para prioridad
            score = priority * 1000000 + time.time()  # Prioridad + timestamp
            redis_client.zadd(queue_key, {json.dumps(queue_item): score})
            return True
    except Exception as e:
        logger.warning("queue_enqueue_failed", error=str(e))
    return False

def _dequeue_lead(queue_name: str = "default", conn_id: str = None) -> Optional[Dict[str, Any]]:
    """Desencola lead de mayor prioridad."""
    try:
        if redis_client:
            queue_key = f"queue:{queue_name}"
            # Obtener item de mayor prioridad (mayor score)
            items = redis_client.zrevrange(queue_key, 0, 0)
            if items:
                queue_item = json.loads(items[0])
                redis_client.zrem(queue_key, items[0])
                return queue_item.get("data")
    except Exception as e:
        logger.warning("queue_dequeue_failed", error=str(e))
    return None

# Advanced Data Synchronization
def _sync_data_between_systems(source_system: str, target_system: str, data: Dict[str, Any], sync_config: Dict[str, Any] = None) -> Dict[str, Any]:
    """Sincroniza datos entre sistemas con configuraci√≥n avanzada."""
    sync_result = {
        "source": source_system,
        "target": target_system,
        "synced": False,
        "timestamp": datetime.utcnow().isoformat(),
        "errors": []
    }
    
    try:
        # Mapeo de campos si est√° configurado
        field_mapping = sync_config.get("field_mapping", {}) if sync_config else {}
        synced_data = {}
        
        for source_field, target_field in field_mapping.items():
            if source_field in data:
                synced_data[target_field] = data[source_field]
        
        # Si no hay mapeo, usar datos originales
        if not synced_data:
            synced_data = data
        
        # Transformaciones adicionales
        transformations = sync_config.get("transformations", []) if sync_config else []
        for transformation in transformations:
            trans_type = transformation.get("type")
            if trans_type == "filter":
                # Filtrar campos
                allowed_fields = transformation.get("fields", [])
                synced_data = {k: v for k, v in synced_data.items() if k in allowed_fields}
        
        sync_result["synced"] = True
        sync_result["data"] = synced_data
        
        logger.info("data_synced", source=source_system, target=target_system)
    except Exception as e:
        sync_result["errors"].append(str(e))
        logger.error("data_sync_failed", error=str(e))
    
    return sync_result

# Advanced Backup and Restore
def _backup_lead_data(lead_id: str, conn_id: str) -> Optional[str]:
    """Crea backup de datos de lead."""
    try:
        hook = PostgresHook(postgres_conn_id=conn_id)
        with hook.get_conn() as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT * FROM leads WHERE id = %s", (lead_id,))
                columns = [desc[0] for desc in cur.description]
                row = cur.fetchone()
                
                if row:
                    backup_data = dict(zip(columns, row))
                    backup_key = f"backup:lead:{lead_id}:{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
                    
                    if VARIABLES_AVAILABLE:
                        Variable.set(backup_key, json.dumps(backup_data))
                        return backup_key
    except Exception as e:
        logger.error("backup_failed", lead_id=lead_id, error=str(e))
    return None

def _restore_lead_data(backup_key: str, conn_id: str) -> bool:
    """Restaura datos de lead desde backup."""
    try:
        if VARIABLES_AVAILABLE:
            backup_data = Variable.get(backup_key, default_var=None)
            if backup_data:
                data = json.loads(backup_data)
                hook = PostgresHook(postgres_conn_id=conn_id)
                with hook.get_conn() as conn:
                    with conn.cursor() as cur:
                        # Restaurar datos (simplificado)
                        cur.execute("""
                            UPDATE leads SET 
                                email = %s, first_name = %s, last_name = %s
                            WHERE id = %s
                        """, (data.get("email"), data.get("first_name"), data.get("last_name"), data.get("id")))
                        conn.commit()
                        return True
    except Exception as e:
        logger.error("restore_failed", backup_key=backup_key, error=str(e))
    return False

# Advanced Migration Utilities
def _migrate_lead_data(source_schema: Dict[str, Any], target_schema: Dict[str, Any], data: Dict[str, Any]) -> Dict[str, Any]:
    """Migra datos entre esquemas."""
    migrated_data = {}
    
    # Mapeo de campos
    field_mapping = target_schema.get("field_mapping", {})
    
    for source_field, target_field in field_mapping.items():
        if source_field in data:
            migrated_data[target_field] = data[source_field]
        elif target_field in target_schema.get("defaults", {}):
            migrated_data[target_field] = target_schema["defaults"][target_field]
    
    # Transformaciones
    transformations = target_schema.get("transformations", [])
    for transformation in transformations:
        trans_type = transformation.get("type")
        if trans_type == "rename" and transformation.get("from") in migrated_data:
            migrated_data[transformation.get("to")] = migrated_data.pop(transformation.get("from"))
    
    return migrated_data

# Advanced Monitoring Dashboard Data
def _generate_dashboard_data(time_range: str = "24h", conn_id: str = None) -> Dict[str, Any]:
    """Genera datos para dashboard de monitoreo."""
    dashboard = {
        "time_range": time_range,
        "generated_at": datetime.utcnow().isoformat(),
        "metrics": {},
        "alerts": [],
        "trends": {}
    }
    
    try:
        if conn_id:
            hook = PostgresHook(postgres_conn_id=conn_id)
            with hook.get_conn() as conn:
                with conn.cursor() as cur:
                    # M√©tricas b√°sicas
                    cur.execute("""
                        SELECT 
                            COUNT(*) as total,
                            AVG(score) as avg_score,
                            COUNT(CASE WHEN score >= 60 THEN 1 END) as qualified
                        FROM leads
                        WHERE created_at >= NOW() - INTERVAL '1 day'
                    """)
                    stats = cur.fetchone()
                    
                    if stats:
                        dashboard["metrics"] = {
                            "total_leads": stats[0],
                            "avg_score": float(stats[1] or 0),
                            "qualified_leads": stats[2],
                            "qualification_rate": (stats[2] / stats[0] * 100) if stats[0] > 0 else 0
                        }
    except Exception as e:
        logger.warning("dashboard_data_generation_failed", error=str(e))
    
    return dashboard

# Advanced API Gateway Integration
def _route_via_api_gateway(endpoint: str, method: str = "POST", payload: Dict[str, Any] = None, conn_id: str = None) -> Dict[str, Any]:
    """Rutea request a trav√©s de API Gateway."""
    gateway_url = os.getenv("API_GATEWAY_URL")
    if not gateway_url:
        return {"error": "API Gateway not configured"}
    
    try:
        with httpx.Client(timeout=10.0) as client:
            url = f"{gateway_url}/{endpoint}"
            headers = {
                "Content-Type": "application/json",
                "X-API-Key": os.getenv("API_GATEWAY_KEY", "")
            }
            
            if method.upper() == "POST":
                response = client.post(url, json=payload or {}, headers=headers)
            elif method.upper() == "GET":
                response = client.get(url, params=payload or {}, headers=headers)
            else:
                return {"error": f"Unsupported method: {method}"}
            
            response.raise_for_status()
            return response.json()
    except Exception as e:
        logger.error("api_gateway_request_failed", endpoint=endpoint, error=str(e))
        return {"error": str(e)}

# Advanced Webhook Management
def _register_webhook(webhook_url: str, events: List[str], secret: str = None, conn_id: str = None) -> str:
    """Registra webhook para eventos espec√≠ficos."""
    webhook_id = hashlib.sha256(f"{webhook_url}:{datetime.utcnow().isoformat()}".encode()).hexdigest()[:16]
    
    try:
        if VARIABLES_AVAILABLE:
            webhook_key = f"webhook:{webhook_id}"
            webhook_data = {
                "url": webhook_url,
                "events": events,
                "secret": secret,
                "created_at": datetime.utcnow().isoformat(),
                "active": True
            }
            Variable.set(webhook_key, json.dumps(webhook_data))
            logger.info("webhook_registered", webhook_id=webhook_id, events=events)
    except Exception as e:
        logger.warning("webhook_registration_failed", error=str(e))
    
    return webhook_id

def _trigger_webhooks(event_type: str, data: Dict[str, Any], conn_id: str = None) -> Dict[str, Any]:
    """Dispara webhooks registrados para un evento."""
    triggered = {
        "event_type": event_type,
        "webhooks_triggered": 0,
        "webhooks_failed": 0,
        "errors": []
    }
    
    try:
        if VARIABLES_AVAILABLE:
            # Buscar webhooks registrados (simplificado)
            # En producci√≥n, esto se har√≠a con una b√∫squeda m√°s eficiente
            webhook_pattern = "webhook:*"
            # Implementaci√≥n simplificada
            logger.info("webhooks_triggered", event_type=event_type)
    except Exception as e:
        logger.warning("webhook_trigger_failed", error=str(e))
    
    return triggered

# Advanced Data Export Formats
def _export_to_format(data: List[Dict[str, Any]], format: str = "json", options: Dict[str, Any] = None) -> str:
    """Exporta datos a m√∫ltiples formatos."""
    if format == "json":
        return json.dumps(data, indent=2, default=str)
    elif format == "csv":
        import csv
        import io
        output = io.StringIO()
        if data:
            writer = csv.DictWriter(output, fieldnames=data[0].keys())
            writer.writeheader()
            writer.writerows(data)
        return output.getvalue()
    elif format == "xml":
        # Conversi√≥n simplificada a XML
        xml_lines = ["<?xml version='1.0' encoding='UTF-8'?>", "<leads>"]
        for item in data:
            xml_lines.append("  <lead>")
            for key, value in item.items():
                xml_lines.append(f"    <{key}>{value}</{key}>")
            xml_lines.append("  </lead>")
        xml_lines.append("</leads>")
        return "\n".join(xml_lines)
    else:
        return str(data)

# Advanced Integration Patterns
def _apply_integration_pattern(pattern: str, data: Dict[str, Any], config: Dict[str, Any] = None) -> Dict[str, Any]:
    """Aplica patrones de integraci√≥n comunes."""
    result = data.copy()
    
    if pattern == "event_sourcing":
        # Agregar metadata de evento
        result["event_id"] = hashlib.sha256(f"{pattern}:{datetime.utcnow().isoformat()}".encode()).hexdigest()[:16]
        result["event_timestamp"] = datetime.utcnow().isoformat()
        result["event_type"] = config.get("event_type", "lead_captured") if config else "lead_captured"
    
    elif pattern == "cqrs":
        # Separar comandos y queries
        command_data = {k: v for k, v in data.items() if k not in ["id", "created_at", "updated_at"]}
        query_data = {k: v for k, v in data.items() if k in ["id", "created_at", "updated_at", "score"]}
        result["command"] = command_data
        result["query"] = query_data
    
    elif pattern == "saga":
        # Agregar informaci√≥n de saga
        result["saga_id"] = config.get("saga_id") if config else hashlib.sha256(f"{datetime.utcnow().isoformat()}".encode()).hexdigest()[:16]
        result["saga_step"] = config.get("step", 1) if config else 1
        result["compensate_on_failure"] = config.get("compensate", True) if config else True
    
    return result

# Advanced Rate Limiting per User/Organization
def _check_user_rate_limit(user_id: str, operation: str, limit: int = 100, window: int = 3600, conn_id: str = None) -> Tuple[bool, Optional[int]]:
    """Verifica rate limit por usuario/organizaci√≥n."""
    try:
        if redis_client:
            key = f"rate_limit:user:{user_id}:{operation}"
            current = redis_client.get(key)
            
            if current and int(current) >= limit:
                ttl = redis_client.ttl(key)
                return False, ttl
            
            pipe = redis_client.pipeline()
            pipe.incr(key)
            pipe.expire(key, window)
            pipe.execute()
            return True, None
    except Exception as e:
        logger.warning("user_rate_limit_check_failed", error=str(e))
    return True, None

# Advanced Data Validation Pipeline
def _validate_pipeline(data: Dict[str, Any], validators: List[Callable]) -> Tuple[bool, List[str]]:
    """Ejecuta pipeline de validadores."""
    errors = []
    
    for validator in validators:
        try:
            is_valid, validator_errors = validator(data)
            if not is_valid:
                errors.extend(validator_errors)
        except Exception as e:
            errors.append(f"Validator error: {str(e)}")
    
    return len(errors) == 0, errors

# Advanced Error Aggregation
def _aggregate_errors(errors: List[Exception]) -> Dict[str, Any]:
    """Agrega errores por tipo y frecuencia."""
    error_summary = {
        "total_errors": len(errors),
        "error_types": {},
        "most_common": None,
        "error_messages": []
    }
    
    for error in errors:
        error_type = type(error).__name__
        error_message = str(error)
        
        if error_type not in error_summary["error_types"]:
            error_summary["error_types"][error_type] = 0
        error_summary["error_types"][error_type] += 1
        
        error_summary["error_messages"].append({
            "type": error_type,
            "message": error_message[:200]  # Limitar longitud
        })
    
    # Encontrar error m√°s com√∫n
    if error_summary["error_types"]:
        error_summary["most_common"] = max(error_summary["error_types"].items(), key=lambda x: x[1])
    
    return error_summary

# Advanced Performance Optimization
def _optimize_performance(operation: str, func: Callable, *args, **kwargs) -> Tuple[Any, Dict[str, Any]]:
    """Optimiza performance de operaci√≥n con caching y memoization."""
    # Generar cache key basado en argumentos
    cache_key = f"perf_cache:{operation}:{hashlib.sha256(str(args).encode() + str(kwargs).encode()).hexdigest()[:16]}"
    
    # Intentar obtener de cache
    cached_result = _get_from_redis_cache(cache_key)
    if cached_result:
        logger.debug("performance_cache_hit", operation=operation)
        return cached_result, {"cached": True}
    
    # Ejecutar funci√≥n
    result, metrics = _monitor_performance(operation, func, *args, **kwargs)
    
    # Guardar en cache si fue exitoso
    if metrics.get("success"):
        _cache_with_redis(cache_key, result, ttl=300)  # Cache por 5 minutos
    
    return result, {**metrics, "cached": False}

# ============================================================================
# FUNCIONES HELPER FINALES ULTRA AVANZADAS - ENTERPRISE FEATURES
# ============================================================================

# Advanced Data Compression
def _compress_data(data: str, algorithm: str = "gzip") -> bytes:
    """Comprime datos para almacenamiento eficiente."""
    try:
        if algorithm == "gzip":
            import gzip
            return gzip.compress(data.encode('utf-8'))
        elif algorithm == "zlib":
            import zlib
            return zlib.compress(data.encode('utf-8'))
        else:
            return data.encode('utf-8')
    except Exception as e:
        logger.warning("data_compression_failed", error=str(e))
        return data.encode('utf-8')

def _decompress_data(compressed_data: bytes, algorithm: str = "gzip") -> str:
    """Descomprime datos."""
    try:
        if algorithm == "gzip":
            import gzip
            return gzip.decompress(compressed_data).decode('utf-8')
        elif algorithm == "zlib":
            import zlib
            return zlib.decompress(compressed_data).decode('utf-8')
        else:
            return compressed_data.decode('utf-8')
    except Exception as e:
        logger.warning("data_decompression_failed", error=str(e))
        return compressed_data.decode('utf-8', errors='ignore')

# Advanced Caching Strategies
def _cache_with_strategy(key: str, value: Any, strategy: str = "lru", ttl: int = 3600, conn_id: str = None) -> bool:
    """Cachea datos usando estrategia especificada."""
    try:
        if strategy == "redis" and redis_client:
            return _cache_with_redis(key, value, ttl, conn_id)
        elif strategy == "memory":
            if enrichment_cache:
                enrichment_cache[key] = value
                return True
        elif strategy == "hybrid":
            # Cache en ambos Redis y memoria
            redis_success = _cache_with_redis(key, value, ttl, conn_id) if redis_client else False
            memory_success = False
            if enrichment_cache:
                enrichment_cache[key] = value
                memory_success = True
            return redis_success or memory_success
    except Exception as e:
        logger.warning("cache_with_strategy_failed", strategy=strategy, error=str(e))
    return False

# Advanced Monitoring and Alerting
def _create_monitoring_alert(alert_name: str, condition: Callable, threshold: Any, current_value: Any, conn_id: str = None) -> Dict[str, Any]:
    """Crea alerta de monitoreo con condici√≥n personalizada."""
    alert_result = {
        "alert_name": alert_name,
        "triggered": False,
        "severity": "medium",
        "current_value": current_value,
        "threshold": threshold,
        "timestamp": datetime.utcnow().isoformat()
    }
    
    try:
        if condition(current_value, threshold):
            alert_result["triggered"] = True
            _send_advanced_alert(alert_name, "medium", f"Alert {alert_name} triggered: {current_value} vs {threshold}")
    except Exception as e:
        logger.warning("monitoring_alert_failed", error=str(e))
    
    return alert_result

# Advanced Security Features
def _validate_security_policy(data: Dict[str, Any], policy: Dict[str, Any] = None) -> Tuple[bool, List[str]]:
    """Valida datos contra pol√≠tica de seguridad."""
    violations = []
    
    if policy is None:
        # Pol√≠tica por defecto
        policy = {
            "max_field_length": 1000,
            "forbidden_patterns": ["<script", "javascript:", "eval("],
            "required_encryption": ["ssn", "credit_card"]
        }
    
    # Validar longitud de campos
    max_length = policy.get("max_field_length", 1000)
    for field, value in data.items():
        if isinstance(value, str) and len(value) > max_length:
            violations.append(f"Field {field} exceeds maximum length")
    
    # Validar patrones prohibidos
    forbidden_patterns = policy.get("forbidden_patterns", [])
    for field, value in data.items():
        if isinstance(value, str):
            for pattern in forbidden_patterns:
                if pattern.lower() in value.lower():
                    violations.append(f"Forbidden pattern found in {field}: {pattern}")
    
    return len(violations) == 0, violations

# Advanced Performance Optimizations
def _optimize_database_query(query: str, params: Dict[str, Any] = None) -> str:
    """Optimiza query de base de datos."""
    optimized = query.strip()
    
    # Remover comentarios
    optimized = re.sub(r'--.*', '', optimized)
    optimized = re.sub(r'/\*.*?\*/', '', optimized, flags=re.DOTALL)
    
    # Normalizar espacios
    optimized = re.sub(r'\s+', ' ', optimized)
    
    # Sugerencias de optimizaci√≥n
    suggestions = []
    if "SELECT *" in optimized.upper():
        suggestions.append("Consider selecting specific columns instead of *")
    if "WHERE" in optimized.upper() and "LIKE" in optimized.upper():
        suggestions.append("Consider using full-text search for LIKE queries")
    
    if suggestions:
        logger.debug("query_optimization_suggestions", suggestions=suggestions)
    
    return optimized

# Advanced Integration Capabilities
def _integrate_with_external_service(service_name: str, endpoint: str, method: str, payload: Dict[str, Any] = None, auth_config: Dict[str, Any] = None) -> Dict[str, Any]:
    """Integra con servicio externo con autenticaci√≥n avanzada."""
    try:
        with httpx.Client(timeout=10.0) as client:
            headers = {"Content-Type": "application/json"}
            
            # Agregar autenticaci√≥n
            if auth_config:
                auth_type = auth_config.get("type", "bearer")
                if auth_type == "bearer":
                    headers["Authorization"] = f"Bearer {auth_config.get('token')}"
                elif auth_type == "api_key":
                    headers[auth_config.get("header_name", "X-API-Key")] = auth_config.get("api_key")
            
            if method.upper() == "POST":
                response = client.post(endpoint, json=payload or {}, headers=headers)
            elif method.upper() == "GET":
                response = client.get(endpoint, params=payload or {}, headers=headers)
            else:
                return {"error": f"Unsupported method: {method}"}
            
            response.raise_for_status()
            return response.json()
    except Exception as e:
        logger.error("external_service_integration_failed", service=service_name, error=str(e))
        return {"error": str(e)}

# Advanced Testing and Debugging Tools
def _create_test_scenario(scenario_name: str, test_data: Dict[str, Any], expected_result: Dict[str, Any] = None) -> Dict[str, Any]:
    """Crea escenario de prueba para testing."""
    scenario = {
        "name": scenario_name,
        "test_data": test_data,
        "expected_result": expected_result,
        "created_at": datetime.utcnow().isoformat()
    }
    
    if VARIABLES_AVAILABLE:
        scenario_key = f"test_scenario_{scenario_name}"
        Variable.set(scenario_key, json.dumps(scenario))
    
    return scenario

def _debug_lead_processing(lead_data: Dict[str, Any], debug_level: str = "info") -> Dict[str, Any]:
    """Herramienta de debugging para procesamiento de leads."""
    debug_info = {
        "lead_id": lead_data.get("lead_ext_id"),
        "email": lead_data.get("email"),
        "debug_level": debug_level,
        "timestamp": datetime.utcnow().isoformat(),
        "data_snapshot": _mask_sensitive_data(lead_data.copy()),
        "processing_steps": []
    }
    
    if debug_level == "verbose":
        debug_info["full_data"] = lead_data
        debug_info["data_size"] = len(json.dumps(lead_data))
    
    logger.debug("lead_debug_info", **debug_info)
    return debug_info

# Advanced Configuration Management
def _load_config_from_file(config_path: str) -> Dict[str, Any]:
    """Carga configuraci√≥n desde archivo."""
    try:
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                return json.load(f)
    except Exception as e:
        logger.warning("config_load_failed", config_path=config_path, error=str(e))
    return {}

def _merge_configs(base_config: Dict[str, Any], override_config: Dict[str, Any]) -> Dict[str, Any]:
    """Fusiona configuraciones con override."""
    merged = base_config.copy()
    
    for key, value in override_config.items():
        if isinstance(value, dict) and key in merged and isinstance(merged[key], dict):
            merged[key] = _merge_configs(merged[key], value)
        else:
            merged[key] = value
    
    return merged

# Advanced Error Handling
def _handle_error_with_context(error: Exception, context: Dict[str, Any], error_handler: Callable = None) -> Dict[str, Any]:
    """Maneja errores con contexto completo."""
    error_info = {
        "error_type": type(error).__name__,
        "error_message": str(error),
        "context": context,
        "timestamp": datetime.utcnow().isoformat(),
        "handled": False
    }
    
    # Clasificar error
    error_classification = _classify_error(error, context)
    error_info["classification"] = error_classification
    
    # Ejecutar handler personalizado si existe
    if error_handler:
        try:
            result = error_handler(error, context)
            error_info["handled"] = True
            error_info["handler_result"] = result
        except Exception as e:
            logger.error("error_handler_failed", error=str(e))
    
    # Guardar en historial de errores
    if VARIABLES_AVAILABLE:
        errors_key = "lead_capture_errors_history"
        errors_history = Variable.get(errors_key, default_var="[]")
        errors_list = json.loads(errors_history) if errors_history else []
        
        errors_list.append(error_info)
        if len(errors_list) > 1000:
            errors_list = errors_list[-1000:]
        
        Variable.set(errors_key, json.dumps(errors_list))
    
    return error_info

# Advanced Data Processing
def _process_data_pipeline(data: Dict[str, Any], pipeline: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Procesa datos a trav√©s de pipeline configurable."""
    result = data.copy()
    
    for step in pipeline:
        step_type = step.get("type")
        step_config = step.get("config", {})
        
        try:
            if step_type == "transform":
                result = _transform_lead_data(result, step_config.get("transformations"))
            elif step_type == "validate":
                is_valid, errors = _validate_with_rules_engine(result, step_config.get("rules", []))
                if not is_valid:
                    result["validation_errors"] = errors
            elif step_type == "enrich":
                sources = step_config.get("sources", [])
                result = _enrich_from_multiple_sources(result, sources)
            elif step_type == "filter":
                filter_fields = step_config.get("fields", [])
                result = {k: v for k, v in result.items() if k in filter_fields}
        except Exception as e:
            logger.error("pipeline_step_failed", step_type=step_type, error=str(e))
            result["pipeline_errors"] = result.get("pipeline_errors", []) + [str(e)]
    
    return result

# Advanced Metrics Dashboard
def _generate_metrics_dashboard(metrics: Dict[str, Any], dashboard_config: Dict[str, Any] = None) -> Dict[str, Any]:
    """Genera dashboard de m√©tricas con configuraci√≥n avanzada."""
    dashboard = {
        "title": dashboard_config.get("title", "Lead Capture Metrics") if dashboard_config else "Lead Capture Metrics",
        "time_range": dashboard_config.get("time_range", "24h") if dashboard_config else "24h",
        "widgets": [],
        "generated_at": datetime.utcnow().isoformat()
    }
    
    # Agregar widgets basados en m√©tricas
    for metric_name, metric_value in metrics.items():
        widget = {
            "type": "metric",
            "name": metric_name,
            "value": metric_value,
            "format": dashboard_config.get("formats", {}).get(metric_name, "number") if dashboard_config else "number"
        }
        dashboard["widgets"].append(widget)
    
    return dashboard

# Advanced Retry with Custom Strategy
def _retry_with_custom_strategy(func: Callable, strategy: Dict[str, Any], *args, **kwargs) -> Any:
    """Retry con estrategia completamente personalizable."""
    max_retries = strategy.get("max_retries", 3)
    delay_func = strategy.get("delay_func", lambda attempt: 2 ** attempt)
    should_retry = strategy.get("should_retry", lambda e: True)
    on_retry = strategy.get("on_retry", lambda attempt, error: None)
    
    last_exception = None
    
    for attempt in range(max_retries):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            last_exception = e
            
            if attempt < max_retries - 1 and should_retry(e):
                delay = delay_func(attempt)
                on_retry(attempt, e)
                time.sleep(delay)
            else:
                break
    
    raise last_exception

# Advanced Cache Invalidation Strategies
def _invalidate_cache_with_strategy(strategy: str, pattern: str = None, keys: List[str] = None, conn_id: str = None) -> int:
    """Invalida cache usando estrategia especificada."""
    invalidated = 0
    
    try:
        if strategy == "pattern" and pattern:
            invalidated = _invalidate_cache_pattern(pattern)
        elif strategy == "keys" and keys:
            for key in keys:
                if redis_client:
                    redis_client.delete(key)
                    invalidated += 1
                elif enrichment_cache:
                    enrichment_cache.pop(key, None)
                    invalidated += 1
        elif strategy == "all":
            if redis_client:
                # Invalidar todos los keys del patr√≥n (simplificado)
                invalidated = _invalidate_cache_pattern("lead_capture:*")
            elif enrichment_cache:
                enrichment_cache.clear()
                invalidated = len(enrichment_cache)
    except Exception as e:
        logger.warning("cache_invalidation_failed", strategy=strategy, error=str(e))
    
    return invalidated

# Advanced Data Quality Assurance
def _assure_data_quality(lead_data: Dict[str, Any], quality_checks: List[str] = None) -> Dict[str, Any]:
    """Asegura calidad de datos con checks m√∫ltiples."""
    if quality_checks is None:
        quality_checks = ["completeness", "validity", "consistency", "accuracy"]
    
    quality_report = {
        "overall_score": 100,
        "checks": {},
        "passed": True
    }
    
    for check in quality_checks:
        if check == "completeness":
            score = _calculate_data_quality_score(lead_data).get("data_quality_score", 0)
            quality_report["checks"]["completeness"] = score
            quality_report["overall_score"] = min(quality_report["overall_score"], score)
        elif check == "validity":
            is_valid, errors = _validate_against_schema(lead_data, {
                "email": {"required": True, "pattern": r'^[^@]+@[^@]+\.[^@]+$'},
                "first_name": {"required": True, "min_length": 1}
            })
            quality_report["checks"]["validity"] = 100 if is_valid else 50
            if not is_valid:
                quality_report["passed"] = False
        elif check == "consistency":
            # Verificar consistencia entre campos
            email_domain = lead_data.get("email", "").split("@")[-1] if "@" in lead_data.get("email", "") else ""
            company_domain = lead_data.get("company_validation", {}).get("domain", "")
            consistency_score = 100 if email_domain and company_domain and email_domain == company_domain else 70
            quality_report["checks"]["consistency"] = consistency_score
    
    return quality_report

# ============================================================================
# FUNCIONES HELPER ULTRA AVANZADAS - NEXT LEVEL ENTERPRISE FEATURES
# ============================================================================

# Advanced Machine Learning Integration
def _train_ml_model(model_name: str, training_data: List[Dict[str, Any]], model_type: str = "classification", conn_id: str = None) -> Dict[str, Any]:
    """Entrena un modelo de ML con datos hist√≥ricos."""
    try:
        # Placeholder para entrenamiento de modelo
        # En producci√≥n, esto se integrar√≠a con scikit-learn, TensorFlow, etc.
        model_info = {
            "model_name": model_name,
            "model_type": model_type,
            "training_samples": len(training_data),
            "trained_at": datetime.utcnow().isoformat(),
            "version": "1.0.0",
            "accuracy": 0.85,  # Placeholder
            "features": list(training_data[0].keys()) if training_data else []
        }
        
        if VARIABLES_AVAILABLE:
            model_key = f"ml_model_{model_name}"
            Variable.set(model_key, json.dumps(model_info))
        
        logger.info("ml_model_trained", model_name=model_name, samples=len(training_data))
        return model_info
    except Exception as e:
        logger.error("ml_model_training_failed", model_name=model_name, error=str(e))
        return {"error": str(e)}

def _predict_with_ensemble(lead_data: Dict[str, Any], models: List[str] = None, conn_id: str = None) -> Dict[str, Any]:
    """Predicci√≥n usando ensemble de m√∫ltiples modelos."""
    if models is None:
        models = ["conversion_model", "ltv_model", "churn_model"]
    
    predictions = {}
    weights = {"conversion_model": 0.4, "ltv_model": 0.3, "churn_model": 0.3}
    
    for model_name in models:
        try:
            # Obtener predicci√≥n de cada modelo
            model_pred = _get_ml_prediction(lead_data, model_endpoint=f"api/{model_name}")
            predictions[model_name] = model_pred.get("prediction", 0)
        except Exception as e:
            logger.warning("ensemble_model_failed", model=model_name, error=str(e))
            predictions[model_name] = 0.5  # Default neutral
    
    # Weighted average
    ensemble_score = sum(predictions.get(m, 0) * weights.get(m, 0.33) for m in models)
    
    return {
        "ensemble_prediction": ensemble_score,
        "individual_predictions": predictions,
        "weights": weights,
        "confidence": "high" if all(p > 0.7 or p < 0.3 for p in predictions.values()) else "medium"
    }

# Advanced Real-time Analytics
def _calculate_realtime_metrics(time_window_minutes: int = 5, conn_id: str = None) -> Dict[str, Any]:
    """Calcula m√©tricas en tiempo real para el √∫ltimo per√≠odo."""
    try:
        hook = PostgresHook(postgres_conn_id=conn_id) if conn_id else None
        if not hook:
            return {"error": "Database connection required"}
        
        with hook.get_conn() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT 
                        COUNT(*) as total_leads,
                        AVG(score) as avg_score,
                        COUNT(CASE WHEN is_qualified = true THEN 1 END) as qualified_count,
                        COUNT(CASE WHEN is_spam = true THEN 1 END) as spam_count,
                        COUNT(CASE WHEN score >= 70 THEN 1 END) as high_value_count
                    FROM leads
                    WHERE created_at >= NOW() - INTERVAL '%s minutes'
                """, (time_window_minutes,))
                
                row = cur.fetchone()
                if row:
                    return {
                        "time_window_minutes": time_window_minutes,
                        "total_leads": row[0] or 0,
                        "avg_score": float(row[1] or 0),
                        "qualified_count": row[2] or 0,
                        "spam_count": row[3] or 0,
                        "high_value_count": row[4] or 0,
                        "qualification_rate": (row[2] / row[0] * 100) if row[0] > 0 else 0,
                        "spam_rate": (row[3] / row[0] * 100) if row[0] > 0 else 0,
                        "calculated_at": datetime.utcnow().isoformat()
                    }
    except Exception as e:
        logger.error("realtime_metrics_failed", error=str(e))
    
    return {"error": "Failed to calculate metrics"}

def _detect_realtime_anomalies_window(metrics: Dict[str, Any], baseline: Dict[str, Any] = None, conn_id: str = None) -> Dict[str, Any]:
    """Detecta anomal√≠as comparando m√©tricas actuales con baseline."""
    anomalies = []
    
    if not baseline:
        # Usar baseline hist√≥rico si est√° disponible
        if VARIABLES_AVAILABLE:
            baseline_str = Variable.get("lead_capture_baseline_metrics", default_var="{}")
            baseline = json.loads(baseline_str) if baseline_str else {}
    
    if baseline:
        # Comparar m√©tricas actuales con baseline
        for key in ["avg_score", "qualification_rate", "spam_rate"]:
            if key in metrics and key in baseline:
                current = metrics[key]
                baseline_val = baseline[key]
                deviation = abs(current - baseline_val) / baseline_val if baseline_val > 0 else 0
                
                if deviation > 0.3:  # 30% de desviaci√≥n
                    anomalies.append({
                        "metric": key,
                        "current": current,
                        "baseline": baseline_val,
                        "deviation_percent": deviation * 100,
                        "severity": "high" if deviation > 0.5 else "medium"
                    })
    
    return {
        "anomalies_detected": len(anomalies),
        "anomalies": anomalies,
        "timestamp": datetime.utcnow().isoformat()
    }

# Advanced Workflow Orchestration
def _orchestrate_multi_step_workflow(workflow_config: Dict[str, Any], lead_data: Dict[str, Any], conn_id: str = None) -> Dict[str, Any]:
    """Orquesta un workflow multi-paso con dependencias y condiciones."""
    workflow_result = {
        "workflow_id": workflow_config.get("id", "default"),
        "steps_executed": [],
        "steps_skipped": [],
        "steps_failed": [],
        "final_state": "pending",
        "started_at": datetime.utcnow().isoformat()
    }
    
    steps = workflow_config.get("steps", [])
    context = lead_data.copy()
    
    for step in steps:
        step_id = step.get("id")
        step_type = step.get("type")
        condition = step.get("condition")
        action = step.get("action")
        
        try:
            # Evaluar condici√≥n si existe
            if condition:
                condition_met = _evaluate_condition(condition, context)
                if not condition_met:
                    workflow_result["steps_skipped"].append(step_id)
                    continue
            
            # Ejecutar acci√≥n
            if step_type == "transform":
                context = _transform_lead_data(context, action.get("transformations"))
            elif step_type == "enrich":
                context = _enrich_from_multiple_sources(context, action.get("sources"))
            elif step_type == "validate":
                is_valid, errors = _validate_with_rules_engine(context, action.get("rules"))
                if not is_valid:
                    workflow_result["steps_failed"].append({
                        "step_id": step_id,
                        "errors": errors
                    })
                    if step.get("stop_on_error", False):
                        break
            elif step_type == "notify":
                _send_advanced_alert("workflow_step", "info", f"Step {step_id} completed", context)
            elif step_type == "custom":
                # Ejecutar funci√≥n personalizada
                custom_func = action.get("function")
                if custom_func and callable(custom_func):
                    context = custom_func(context)
            
            workflow_result["steps_executed"].append(step_id)
        except Exception as e:
            logger.error("workflow_step_failed", step_id=step_id, error=str(e))
            workflow_result["steps_failed"].append({
                "step_id": step_id,
                "error": str(e)
            })
            if step.get("stop_on_error", False):
                break
    
    workflow_result["final_state"] = "completed" if len(workflow_result["steps_failed"]) == 0 else "partial"
    workflow_result["completed_at"] = datetime.utcnow().isoformat()
    workflow_result["final_context"] = context
    
    return workflow_result

def _evaluate_condition(condition: Dict[str, Any], context: Dict[str, Any]) -> bool:
    """Eval√∫a una condici√≥n contra el contexto."""
    condition_type = condition.get("type")
    field = condition.get("field")
    operator = condition.get("operator")
    value = condition.get("value")
    
    field_value = context.get(field) if field else None
    
    if condition_type == "comparison":
        if operator == "equals":
            return field_value == value
        elif operator == "greater_than":
            return float(field_value or 0) > float(value)
        elif operator == "less_than":
            return float(field_value or 0) < float(value)
        elif operator == "contains":
            return str(value) in str(field_value or "")
        elif operator == "in":
            return field_value in (value if isinstance(value, list) else [value])
    elif condition_type == "logical":
        sub_conditions = condition.get("conditions", [])
        logic_op = condition.get("operator", "and")
        
        results = [_evaluate_condition(c, context) for c in sub_conditions]
        
        if logic_op == "and":
            return all(results)
        elif logic_op == "or":
            return any(results)
    
    return False

# Advanced Security Features
def _perform_security_audit(lead_data: Dict[str, Any], audit_level: str = "standard") -> Dict[str, Any]:
    """Realiza auditor√≠a de seguridad completa del lead."""
    audit_results = {
        "audit_level": audit_level,
        "timestamp": datetime.utcnow().isoformat(),
        "checks": {},
        "overall_risk": "low",
        "recommendations": []
    }
    
    # Check 1: SQL Injection patterns
    sql_injection_check = _scan_for_security_issues(lead_data)
    audit_results["checks"]["sql_injection"] = sql_injection_check.get("sql_injection_risk", "low")
    
    # Check 2: XSS patterns
    xss_check = sql_injection_check.get("xss_risk", "low")
    audit_results["checks"]["xss"] = xss_check
    
    # Check 3: Data validation
    is_valid, violations = _validate_security_policy(lead_data)
    audit_results["checks"]["data_validation"] = "passed" if is_valid else "failed"
    if violations:
        audit_results["recommendations"].append(f"Fix {len(violations)} validation violations")
    
    # Check 4: Sensitive data exposure
    sensitive_fields = ["ssn", "credit_card", "password", "api_key"]
    exposed_fields = [field for field in sensitive_fields if field in lead_data]
    audit_results["checks"]["sensitive_data"] = "exposed" if exposed_fields else "safe"
    if exposed_fields:
        audit_results["recommendations"].append(f"Mask or encrypt {len(exposed_fields)} sensitive fields")
    
    # Check 5: Rate limiting compliance
    email = lead_data.get("email")
    if email:
        rate_ok, _ = _check_rate_limit_advanced(email, limit=10, window_seconds=3600)
        audit_results["checks"]["rate_limiting"] = "compliant" if rate_ok else "violation"
    
    # Calculate overall risk
    risk_factors = [
        audit_results["checks"].get("sql_injection") == "high",
        audit_results["checks"].get("xss") == "high",
        audit_results["checks"].get("data_validation") == "failed",
        audit_results["checks"].get("sensitive_data") == "exposed"
    ]
    
    risk_count = sum(risk_factors)
    if risk_count >= 3:
        audit_results["overall_risk"] = "critical"
    elif risk_count >= 2:
        audit_results["overall_risk"] = "high"
    elif risk_count >= 1:
        audit_results["overall_risk"] = "medium"
    
    return audit_results

def _encrypt_pii_data(data: Dict[str, Any], fields_to_encrypt: List[str] = None) -> Dict[str, Any]:
    """Encripta datos PII (Personally Identifiable Information)."""
    if fields_to_encrypt is None:
        fields_to_encrypt = ["email", "phone", "ssn", "credit_card", "address"]
    
    encrypted_data = data.copy()
    encryption_key = os.getenv("ENCRYPTION_KEY", "default_key_change_in_production")
    
    for field in fields_to_encrypt:
        if field in encrypted_data and encrypted_data[field]:
            try:
                encrypted_value = _encrypt_sensitive_data(str(encrypted_data[field]), encryption_key)
                encrypted_data[f"{field}_encrypted"] = encrypted_value
                # Opcionalmente, remover el campo original
                if os.getenv("REMOVE_PII_AFTER_ENCRYPTION", "false").lower() == "true":
                    encrypted_data.pop(field, None)
            except Exception as e:
                logger.warning("pii_encryption_failed", field=field, error=str(e))
    
    return encrypted_data

# Advanced Performance Optimization
def _optimize_memory_usage(data: Dict[str, Any]) -> Dict[str, Any]:
    """Optimiza uso de memoria removiendo datos innecesarios."""
    optimized = {}
    
    # Remover campos None
    optimized = {k: v for k, v in data.items() if v is not None}
    
    # Truncar strings largos
    max_string_length = int(os.getenv("MAX_STRING_LENGTH", "1000"))
    for key, value in optimized.items():
        if isinstance(value, str) and len(value) > max_string_length:
            optimized[key] = value[:max_string_length] + "...[truncated]"
    
    # Comprimir listas grandes
    max_list_length = int(os.getenv("MAX_LIST_LENGTH", "100"))
    for key, value in optimized.items():
        if isinstance(value, list) and len(value) > max_list_length:
            optimized[key] = value[:max_list_length] + [f"...{len(value) - max_list_length} more items"]
    
    return optimized

def _parallel_process_with_priority(items: List[Dict[str, Any]], processor: Callable, priority_func: Callable = None, max_workers: int = None, conn_id: str = None) -> List[Dict[str, Any]]:
    """Procesa items en paralelo con sistema de prioridades."""
    if not CONCURRENT_AVAILABLE:
        return [_process_batch_sync([item], processor)[0] for item in items]
    
    if max_workers is None:
        max_workers = MAX_WORKERS
    
    # Ordenar por prioridad si se proporciona funci√≥n de prioridad
    if priority_func:
        items_with_priority = [(priority_func(item), item) for item in items]
        items_with_priority.sort(reverse=True)  # Mayor prioridad primero
        items = [item for _, item in items_with_priority]
    
    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(processor, item): item for item in items}
        
        for future in as_completed(futures):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                item = futures[future]
                logger.error("parallel_processing_error", error=str(e), item_id=item.get("id"))
                results.append({"error": str(e), "item": item})
    
    return results

# Advanced Data Processing
def _apply_data_transformation_pipeline(data: Dict[str, Any], transformations: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Aplica pipeline de transformaciones de datos."""
    result = data.copy()
    
    for transform in transformations:
        transform_type = transform.get("type")
        transform_config = transform.get("config", {})
        
        try:
            if transform_type == "normalize":
                # Normalizar campos
                fields = transform_config.get("fields", [])
                for field in fields:
                    if field in result and isinstance(result[field], str):
                        result[field] = result[field].strip().lower()
            
            elif transform_type == "format":
                # Formatear campos
                format_rules = transform_config.get("rules", {})
                for field, format_type in format_rules.items():
                    if field in result:
                        if format_type == "phone":
                            # Normalizar tel√©fono
                            phone = re.sub(r'[^\d+]', '', str(result[field]))
                            result[field] = phone
                        elif format_type == "email":
                            result[field] = str(result[field]).lower().strip()
            
            elif transform_type == "calculate":
                # Calcular campos derivados
                calculations = transform_config.get("calculations", {})
                for field, expression in calculations.items():
                    try:
                        # Evaluar expresi√≥n simple (en producci√≥n usar algo m√°s seguro)
                        result[field] = eval(expression, {"data": result, "math": __import__("math")})
                    except Exception as e:
                        logger.warning("calculation_failed", field=field, error=str(e))
            
            elif transform_type == "filter":
                # Filtrar campos
                keep_fields = transform_config.get("keep", [])
                if keep_fields:
                    result = {k: v for k, v in result.items() if k in keep_fields}
            
            elif transform_type == "enrich":
                # Enriquecer con datos externos
                enrichment_sources = transform_config.get("sources", [])
                result = _enrich_from_multiple_sources(result, enrichment_sources)
        except Exception as e:
            logger.error("transformation_failed", transform_type=transform_type, error=str(e))
    
    return result

# Advanced Integration Patterns
def _implement_saga_pattern(operations: List[Dict[str, Any]], lead_data: Dict[str, Any], conn_id: str = None) -> Dict[str, Any]:
    """Implementa patr√≥n Saga para transacciones distribuidas."""
    saga_result = {
        "saga_id": f"saga_{datetime.utcnow().timestamp()}",
        "operations": [],
        "compensations": [],
        "status": "in_progress",
        "started_at": datetime.utcnow().isoformat()
    }
    
    executed_operations = []
    
    try:
        for i, operation in enumerate(operations):
            op_id = operation.get("id", f"op_{i}")
            op_func = operation.get("function")
            compensation_func = operation.get("compensation")
            
            try:
                # Ejecutar operaci√≥n
                result = op_func(lead_data) if callable(op_func) else {}
                executed_operations.append({
                    "id": op_id,
                    "result": result,
                    "status": "success"
                })
                saga_result["operations"].append(op_id)
                
                # Guardar compensaci√≥n
                if compensation_func:
                    saga_result["compensations"].append({
                        "operation_id": op_id,
                        "compensation": compensation_func
                    })
            except Exception as e:
                logger.error("saga_operation_failed", operation_id=op_id, error=str(e))
                
                # Ejecutar compensaciones en orden inverso
                for comp in reversed(saga_result["compensations"]):
                    try:
                        comp["compensation"](lead_data)
                    except Exception as comp_error:
                        logger.error("saga_compensation_failed", operation_id=comp["operation_id"], error=str(comp_error))
                
                saga_result["status"] = "failed"
                saga_result["error"] = str(e)
                break
        
        if saga_result["status"] != "failed":
            saga_result["status"] = "completed"
    except Exception as e:
        logger.error("saga_execution_failed", error=str(e))
        saga_result["status"] = "failed"
        saga_result["error"] = str(e)
    
    saga_result["completed_at"] = datetime.utcnow().isoformat()
    return saga_result

# Advanced Monitoring
def _create_custom_metric(metric_name: str, value: float, tags: Dict[str, str] = None, metric_type: str = "gauge") -> None:
    """Crea m√©trica personalizada para monitoreo."""
    try:
        metric_data = {
            "name": metric_name,
            "value": value,
            "tags": tags or {},
            "type": metric_type,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # Enviar a sistema de monitoreo
        _send_metric_to_monitoring(metric_name, value, tags)
        
        # Guardar en Airflow Variables si est√° disponible
        if VARIABLES_AVAILABLE:
            metrics_key = f"custom_metrics_{metric_name}"
            Variable.set(metrics_key, json.dumps(metric_data))
        
        logger.debug("custom_metric_created", metric_name=metric_name, value=value)
    except Exception as e:
        logger.warning("custom_metric_failed", metric_name=metric_name, error=str(e))

def _track_performance_baseline(operation_name: str, duration: float, conn_id: str = None) -> Dict[str, Any]:
    """Rastrea baseline de performance para operaciones."""
    try:
        if VARIABLES_AVAILABLE:
            baseline_key = f"performance_baseline_{operation_name}"
            baseline_str = Variable.get(baseline_key, default_var="{}")
            baseline = json.loads(baseline_str) if baseline_str else {}
            
            # Actualizar estad√≠sticas
            durations = baseline.get("durations", [])
            durations.append(duration)
            if len(durations) > 100:
                durations = durations[-100:]  # Mantener √∫ltimas 100
            
            baseline["durations"] = durations
            baseline["avg_duration"] = sum(durations) / len(durations)
            baseline["min_duration"] = min(durations)
            baseline["max_duration"] = max(durations)
            baseline["p95_duration"] = sorted(durations)[int(len(durations) * 0.95)] if durations else 0
            baseline["last_updated"] = datetime.utcnow().isoformat()
            
            Variable.set(baseline_key, json.dumps(baseline))
            
            return baseline
    except Exception as e:
        logger.warning("performance_baseline_failed", operation=operation_name, error=str(e))
    
    return {}

# ============================================================================
# FUNCIONES HELPER ULTRA AVANZADAS - ENTERPRISE MAXIMUM FEATURES
# ============================================================================

# Advanced Intelligent Caching
def _cache_with_intelligent_invalidation(key: str, value: Any, ttl: int = 3600, invalidation_strategy: str = "time_based", dependencies: List[str] = None, conn_id: str = None) -> bool:
    """Cache inteligente con invalidaci√≥n basada en dependencias y estrategias."""
    try:
        # Guardar en cache
        cache_success = _cache_with_redis(key, value, ttl, conn_id) if redis_client else False
        
        if not cache_success and enrichment_cache:
            enrichment_cache[key] = value
            cache_success = True
        
        # Registrar dependencias para invalidaci√≥n inteligente
        if dependencies and VARIABLES_AVAILABLE:
            deps_key = f"cache_deps_{key}"
            Variable.set(deps_key, json.dumps({
                "dependencies": dependencies,
                "invalidation_strategy": invalidation_strategy,
                "cached_at": datetime.utcnow().isoformat()
            }))
        
        return cache_success
    except Exception as e:
        logger.warning("intelligent_cache_failed", key=key, error=str(e))
        return False

def _invalidate_cache_by_dependency(dependency_key: str, conn_id: str = None) -> int:
    """Invalida cache basado en dependencias."""
    invalidated = 0
    
    try:
        if VARIABLES_AVAILABLE:
            # Buscar todas las claves de dependencias
            all_vars = Variable.get("cache_dependencies_list", default_var="[]")
            deps_list = json.loads(all_vars) if all_vars else []
            
            for cache_key in deps_list:
                deps_key = f"cache_deps_{cache_key}"
                deps_data = Variable.get(deps_key, default_var="{}")
                if deps_data:
                    deps_info = json.loads(deps_data)
                    if dependency_key in deps_info.get("dependencies", []):
                        # Invalidar este cache
                        if redis_client:
                            redis_client.delete(cache_key)
                        elif enrichment_cache:
                            enrichment_cache.pop(cache_key, None)
                        invalidated += 1
    except Exception as e:
        logger.warning("dependency_invalidation_failed", dependency=dependency_key, error=str(e))
    
    return invalidated

# Advanced Dynamic Schema Validation
def _validate_with_dynamic_schema(data: Dict[str, Any], schema_definition: Dict[str, Any]) -> Tuple[bool, List[str], Dict[str, Any]]:
    """Valida datos contra un schema din√°mico con reglas complejas."""
    errors = []
    warnings = []
    
    required_fields = schema_definition.get("required", [])
    field_rules = schema_definition.get("fields", {})
    custom_validators = schema_definition.get("validators", [])
    
    # Validar campos requeridos
    for field in required_fields:
        if field not in data or data[field] is None:
            errors.append(f"Required field '{field}' is missing")
    
    # Validar reglas por campo
    for field, rules in field_rules.items():
        if field in data:
            value = data[field]
            
            # Tipo
            expected_type = rules.get("type")
            if expected_type:
                type_map = {"string": str, "integer": int, "float": float, "boolean": bool, "dict": dict, "list": list}
                expected_py_type = type_map.get(expected_type)
                if expected_py_type and not isinstance(value, expected_py_type):
                    errors.append(f"Field '{field}' must be of type {expected_type}")
            
            # Longitud m√≠nima/m√°xima
            if isinstance(value, str):
                min_length = rules.get("min_length")
                max_length = rules.get("max_length")
                if min_length and len(value) < min_length:
                    errors.append(f"Field '{field}' must be at least {min_length} characters")
                if max_length and len(value) > max_length:
                    errors.append(f"Field '{field}' must be at most {max_length} characters")
            
            # Patr√≥n regex
            pattern = rules.get("pattern")
            if pattern and isinstance(value, str):
                if not re.match(pattern, value):
                    errors.append(f"Field '{field}' does not match required pattern")
            
            # Valores permitidos
            allowed_values = rules.get("allowed_values")
            if allowed_values and value not in allowed_values:
                errors.append(f"Field '{field}' must be one of {allowed_values}")
            
            # Validaci√≥n personalizada
            custom_validator = rules.get("validator")
            if custom_validator and callable(custom_validator):
                try:
                    if not custom_validator(value):
                        errors.append(f"Field '{field}' failed custom validation")
                except Exception as e:
                    warnings.append(f"Custom validator for '{field}' raised exception: {str(e)}")
    
    # Validadores globales
    for validator in custom_validators:
        if callable(validator):
            try:
                result = validator(data)
                if not result:
                    errors.append("Global validation failed")
            except Exception as e:
                warnings.append(f"Global validator raised exception: {str(e)}")
    
    return len(errors) == 0, errors, {"warnings": warnings}

# Advanced Error Recovery with Multiple Strategies
def _recover_with_multi_strategy_advanced(error: Exception, context: Dict[str, Any], strategies: List[str] = None) -> Dict[str, Any]:
    """Recuperaci√≥n avanzada de errores con m√∫ltiples estrategias en orden."""
    if strategies is None:
        strategies = ["retry", "fallback", "degrade", "circuit_breaker", "dlq"]
    
    recovery_result = {
        "error_type": type(error).__name__,
        "error_message": str(error),
        "strategies_attempted": [],
        "strategy_succeeded": None,
        "recovered": False,
        "final_result": None
    }
    
    for strategy in strategies:
        recovery_result["strategies_attempted"].append(strategy)
        
        try:
            if strategy == "retry":
                # Intentar retry con exponential backoff
                func = context.get("function")
                if func and callable(func):
                    result = _retry_with_advanced_backoff(func, max_retries=2, *context.get("args", []), **context.get("kwargs", {}))
                    recovery_result["recovered"] = True
                    recovery_result["strategy_succeeded"] = "retry"
                    recovery_result["final_result"] = result
                    break
            
            elif strategy == "fallback":
                # Usar valor fallback
                fallback_value = context.get("fallback_value")
                if fallback_value is not None:
                    recovery_result["recovered"] = True
                    recovery_result["strategy_succeeded"] = "fallback"
                    recovery_result["final_result"] = fallback_value
                    break
            
            elif strategy == "degrade":
                # Degradaci√≥n graceful
                degraded_func = context.get("degraded_function")
                if degraded_func and callable(degraded_func):
                    result = degraded_func(*context.get("args", []), **context.get("kwargs", {}))
                    recovery_result["recovered"] = True
                    recovery_result["strategy_succeeded"] = "degrade"
                    recovery_result["final_result"] = result
                    break
            
            elif strategy == "circuit_breaker":
                # Verificar si circuit breaker est√° abierto
                cb_name = context.get("circuit_breaker_name")
                if cb_name:
                    recovery_result["recovered"] = False
                    recovery_result["strategy_succeeded"] = None
                    break
            
            elif strategy == "dlq":
                # Guardar en Dead Letter Queue
                item = context.get("item")
                if item:
                    save_to_dlq(item, str(error), context)
                    recovery_result["recovered"] = False  # No se recuper√≥, pero se guard√≥
                    recovery_result["strategy_succeeded"] = "dlq"
                    break
        except Exception as recovery_error:
            logger.warning("recovery_strategy_failed", strategy=strategy, error=str(recovery_error))
            continue
    
    return recovery_result

# Advanced Performance Profiling
def _profile_operation_detailed(operation_name: str, func: Callable, *args, **kwargs) -> Tuple[Any, Dict[str, Any]]:
    """Profiling detallado de operaciones con m√©tricas avanzadas."""
    try:
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        
        # M√©tricas iniciales
        cpu_before = process.cpu_percent()
        memory_before = process.memory_info().rss / 1024 / 1024  # MB
        start_time = time.time()
        
        try:
            result = func(*args, **kwargs)
            
            # M√©tricas finales
            end_time = time.time()
            duration = end_time - start_time
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss / 1024 / 1024  # MB
            
            metrics = {
                "operation": operation_name,
                "duration_seconds": duration,
                "cpu_percent": cpu_after - cpu_before,
                "memory_mb_before": memory_before,
                "memory_mb_after": memory_after,
                "memory_mb_delta": memory_after - memory_before,
                "success": True,
                "timestamp": datetime.utcnow().isoformat()
            }
            
            # Guardar baseline
            _track_performance_baseline(operation_name, duration)
            
            return result, metrics
        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time
            
            metrics = {
                "operation": operation_name,
                "duration_seconds": duration,
                "success": False,
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }
            raise
        finally:
            # Logging de m√©tricas
            logger.info("operation_profiled", **metrics)
    except ImportError:
        # psutil no disponible, usar profiling b√°sico
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            duration = time.time() - start_time
            return result, {"operation": operation_name, "duration_seconds": duration, "success": True}
        except Exception as e:
            duration = time.time() - start_time
            return None, {"operation": operation_name, "duration_seconds": duration, "success": False, "error": str(e)}

# Advanced Custom Metrics with Aggregation
def _create_custom_metric_advanced(metric_name: str, value: float, metric_type: str = "gauge", tags: Dict[str, str] = None, aggregation: str = "last", conn_id: str = None) -> None:
    """Crea m√©trica personalizada avanzada con agregaci√≥n."""
    try:
        metric_data = {
            "name": metric_name,
            "value": value,
            "type": metric_type,
            "tags": tags or {},
            "timestamp": datetime.utcnow().isoformat(),
            "aggregation": aggregation
        }
        
        if VARIABLES_AVAILABLE:
            metrics_key = f"custom_metric_{metric_name}"
            
            if aggregation == "sum":
                # Sumar valores
                existing = Variable.get(metrics_key, default_var="0")
                total = float(existing) + value
                Variable.set(metrics_key, str(total))
            elif aggregation == "avg":
                # Promedio
                existing_data = Variable.get(metrics_key, default_var='{"count": 0, "sum": 0}')
                data = json.loads(existing_data) if existing_data else {"count": 0, "sum": 0}
                data["count"] += 1
                data["sum"] += value
                data["avg"] = data["sum"] / data["count"]
                Variable.set(metrics_key, json.dumps(data))
            elif aggregation == "max":
                # M√°ximo
                existing = Variable.get(metrics_key, default_var="0")
                max_val = max(float(existing), value)
                Variable.set(metrics_key, str(max_val))
            elif aggregation == "min":
                # M√≠nimo
                existing = Variable.get(metrics_key, default_var="999999")
                min_val = min(float(existing), value)
                Variable.set(metrics_key, str(min_val))
            else:
                # Last (default)
                Variable.set(metrics_key, json.dumps(metric_data))
        
        _send_metric_to_monitoring(metric_name, value, tags)
        logger.debug("custom_metric_created_advanced", metric_name=metric_name, value=value, aggregation=aggregation)
    except Exception as e:
        logger.warning("custom_metric_advanced_failed", metric_name=metric_name, error=str(e))

# ============================================================================
# FUNCIONES HELPER ULTRA AVANZADAS - ENTERPRISE ULTIMATE FEATURES
# ============================================================================

# Advanced Data Synchronization with Conflict Resolution
def _sync_data_with_conflict_resolution(source_data: Dict[str, Any], target_data: Dict[str, Any], resolution_strategy: str = "last_write_wins", conn_id: str = None) -> Dict[str, Any]:
    """Sincroniza datos con resoluci√≥n de conflictos avanzada."""
    conflicts = []
    resolved_data = target_data.copy()
    
    for key, source_value in source_data.items():
        if key in target_data:
            target_value = target_data[key]
            
            # Detectar conflicto
            if source_value != target_value:
                conflicts.append({
                    "field": key,
                    "source_value": source_value,
                    "target_value": target_value,
                    "resolved_value": None
                })
                
                # Aplicar estrategia de resoluci√≥n
                if resolution_strategy == "last_write_wins":
                    resolved_data[key] = source_value
                    conflicts[-1]["resolved_value"] = source_value
                    conflicts[-1]["strategy"] = "last_write_wins"
                
                elif resolution_strategy == "merge":
                    if isinstance(source_value, dict) and isinstance(target_value, dict):
                        resolved_data[key] = {**target_value, **source_value}
                        conflicts[-1]["resolved_value"] = resolved_data[key]
                        conflicts[-1]["strategy"] = "merge"
                    else:
                        resolved_data[key] = source_value
                        conflicts[-1]["resolved_value"] = source_value
                
                elif resolution_strategy == "higher_priority":
                    # Asumir que source tiene mayor prioridad
                    resolved_data[key] = source_value
                    conflicts[-1]["resolved_value"] = source_value
                    conflicts[-1]["strategy"] = "higher_priority"
                
                elif resolution_strategy == "custom":
                    # Estrategia personalizada
                    custom_resolver = resolution_strategy.get("resolver")
                    if custom_resolver and callable(custom_resolver):
                        resolved_value = custom_resolver(key, source_value, target_value)
                        resolved_data[key] = resolved_value
                        conflicts[-1]["resolved_value"] = resolved_value
                        conflicts[-1]["strategy"] = "custom"
        else:
            # No hay conflicto, agregar nuevo campo
            resolved_data[key] = source_value
    
    return {
        "resolved_data": resolved_data,
        "conflicts": conflicts,
        "conflicts_count": len(conflicts),
        "resolution_strategy": resolution_strategy
    }

# Advanced Adaptive Batch Processing
def _process_adaptive_batch(items: List[Dict[str, Any]], processor: Callable, initial_batch_size: int = 10, conn_id: str = None) -> List[Dict[str, Any]]:
    """Procesamiento de batch adaptativo que ajusta tama√±o seg√∫n performance."""
    results = []
    current_batch_size = initial_batch_size
    min_batch_size = 1
    max_batch_size = 100
    
    i = 0
    while i < len(items):
        batch = items[i:i + current_batch_size]
        batch_start_time = time.time()
        
        try:
            # Procesar batch
            batch_results = _process_batch_sync(batch, processor)
            results.extend(batch_results)
            
            # Calcular tiempo de procesamiento
            batch_duration = time.time() - batch_start_time
            avg_time_per_item = batch_duration / len(batch) if batch else 0
            
            # Ajustar tama√±o de batch basado en performance
            if avg_time_per_item < 0.1:  # Muy r√°pido, aumentar batch
                current_batch_size = min(max_batch_size, int(current_batch_size * 1.5))
            elif avg_time_per_item > 1.0:  # Muy lento, reducir batch
                current_batch_size = max(min_batch_size, int(current_batch_size * 0.7))
            
            logger.debug("adaptive_batch_processed", 
                        batch_size=len(batch), 
                        duration=batch_duration,
                        avg_time_per_item=avg_time_per_item,
                        new_batch_size=current_batch_size)
            
        except Exception as e:
            logger.error("adaptive_batch_failed", batch_size=len(batch), error=str(e))
            # Reducir batch size en caso de error
            current_batch_size = max(min_batch_size, int(current_batch_size * 0.5))
        
        i += len(batch)
    
    return results

# Advanced Intelligent Alerting
def _create_intelligent_alert(alert_config: Dict[str, Any], current_metrics: Dict[str, Any], historical_metrics: Dict[str, Any] = None, conn_id: str = None) -> Dict[str, Any]:
    """Crea alerta inteligente basada en m√©tricas actuales e hist√≥ricas."""
    alert = {
        "alert_id": f"alert_{datetime.utcnow().timestamp()}",
        "alert_name": alert_config.get("name", "unnamed"),
        "severity": alert_config.get("severity", "medium"),
        "triggered": False,
        "conditions_met": [],
        "timestamp": datetime.utcnow().isoformat()
    }
    
    conditions = alert_config.get("conditions", [])
    
    for condition in conditions:
        metric_name = condition.get("metric")
        operator = condition.get("operator")
        threshold = condition.get("threshold")
        comparison_type = condition.get("comparison_type", "absolute")  # absolute o relative
        
        current_value = current_metrics.get(metric_name)
        
        if current_value is None:
            continue
        
        condition_met = False
        
        if comparison_type == "absolute":
            # Comparaci√≥n absoluta
            if operator == "greater_than" and current_value > threshold:
                condition_met = True
            elif operator == "less_than" and current_value < threshold:
                condition_met = True
            elif operator == "equals" and current_value == threshold:
                condition_met = True
        
        elif comparison_type == "relative" and historical_metrics:
            # Comparaci√≥n relativa con hist√≥rico
            historical_value = historical_metrics.get(metric_name)
            if historical_value:
                deviation = abs(current_value - historical_value) / historical_value if historical_value > 0 else 0
                threshold_percent = threshold / 100.0
                
                if operator == "greater_than" and deviation > threshold_percent:
                    condition_met = True
                elif operator == "less_than" and deviation < threshold_percent:
                    condition_met = True
        
        if condition_met:
            alert["conditions_met"].append({
                "metric": metric_name,
                "current_value": current_value,
                "threshold": threshold,
                "operator": operator
            })
    
    # Trigger alert si se cumplen todas las condiciones
    if len(alert["conditions_met"]) == len(conditions) and len(conditions) > 0:
        alert["triggered"] = True
        _send_advanced_alert(
            alert["alert_name"],
            alert["severity"],
            f"Alert {alert['alert_name']} triggered with {len(alert['conditions_met'])} conditions met",
            {"alert": alert, "metrics": current_metrics}
        )
    
    return alert

# Advanced Data Quality Scoring
def _calculate_data_quality_score_advanced(lead_data: Dict[str, Any], quality_rules: List[Dict[str, Any]] = None) -> Dict[str, Any]:
    """Calcula score de calidad de datos avanzado con reglas configurables."""
    if quality_rules is None:
        quality_rules = [
            {"field": "email", "weight": 0.3, "checks": ["required", "format", "domain"]},
            {"field": "first_name", "weight": 0.2, "checks": ["required", "length"]},
            {"field": "company", "weight": 0.2, "checks": ["required", "length"]},
            {"field": "phone", "weight": 0.15, "checks": ["format"]},
            {"field": "message", "weight": 0.15, "checks": ["length", "content"]}
        ]
    
    total_score = 0.0
    max_score = 0.0
    field_scores = {}
    
    for rule in quality_rules:
        field = rule.get("field")
        weight = rule.get("weight", 0.1)
        checks = rule.get("checks", [])
        
        field_value = lead_data.get(field)
        field_score = 0.0
        max_field_score = len(checks) * 10  # 10 puntos por check
        
        for check in checks:
            check_score = 0.0
            
            if check == "required":
                if field_value and str(field_value).strip():
                    check_score = 10.0
            
            elif check == "format":
                if field == "email":
                    if "@" in str(field_value or "") and "." in str(field_value or ""):
                        check_score = 10.0
                elif field == "phone":
                    if re.match(r'[\d\s\-\+\(\)]+', str(field_value or "")):
                        check_score = 10.0
            
            elif check == "domain":
                if field == "email":
                    email = str(field_value or "")
                    if "@" in email:
                        domain = email.split("@")[1]
                        # Verificar que el dominio no sea temporal
                        if domain and not any(temp in domain.lower() for temp in ["temp", "fake", "test"]):
                            check_score = 10.0
            
            elif check == "length":
                if field_value:
                    value_str = str(field_value)
                    if 2 <= len(value_str) <= 100:
                        check_score = 10.0
            
            elif check == "content":
                if field_value:
                    value_str = str(field_value)
                    if len(value_str) >= 10:  # M√≠nimo contenido
                        check_score = 10.0
            
            field_score += check_score
        
        # Normalizar score del campo
        normalized_field_score = (field_score / max_field_score) * 100 if max_field_score > 0 else 0
        weighted_score = normalized_field_score * weight
        
        field_scores[field] = {
            "raw_score": field_score,
            "max_score": max_field_score,
            "normalized_score": normalized_field_score,
            "weighted_score": weighted_score
        }
        
        total_score += weighted_score
        max_score += weight * 100
    
    overall_score = (total_score / max_score * 100) if max_score > 0 else 0
    
    quality_level = (
        "excellent" if overall_score >= 90 else
        "good" if overall_score >= 70 else
        "fair" if overall_score >= 50 else
        "poor" if overall_score >= 30 else
        "very_poor"
    )
    
    return {
        "overall_score": round(overall_score, 2),
        "quality_level": quality_level,
        "field_scores": field_scores,
        "max_possible_score": 100,
        "recommendations": _generate_quality_recommendations(field_scores)
    }

def _generate_quality_recommendations(field_scores: Dict[str, Any]) -> List[str]:
    """Genera recomendaciones para mejorar calidad de datos."""
    recommendations = []
    
    for field, scores in field_scores.items():
        if scores.get("normalized_score", 0) < 50:
            recommendations.append(f"Improve {field} quality (current score: {scores.get('normalized_score', 0):.1f}%)")
    
    return recommendations

# ============================================================================
# GENERACI√ìN DE DESCRIPCIONES DE PRODUCTOS PARA E-COMMERCE
# ============================================================================

def generate_product_description(
    product_name: str,
    product_features: List[str],
    product_category: str = "general",
    buyer_type: str = "general",
    price_range: str = "medium",
    use_llm: bool = True,
    language: str = "es",
    max_length: int = 500,
    include_seo: bool = True,
    tone: str = "professional"
) -> Dict[str, Any]:
    """
    Genera descripciones de productos atractivas y optimizadas para e-commerce.
    
    Args:
        product_name: Nombre del producto
        product_features: Lista de caracter√≠sticas/beneficios del producto
        product_category: Categor√≠a del producto (electronics, fashion, home, etc.)
        buyer_type: Tipo de comprador objetivo:
            - "eco-friendly": Enfoque en sostenibilidad y medio ambiente
            - "luxury": Enfoque en exclusividad y calidad premium
            - "budget": Enfoque en valor y precio
            - "tech-savvy": Enfoque en tecnolog√≠a y especificaciones
            - "health-conscious": Enfoque en salud y bienestar
            - "minimalist": Enfoque en simplicidad y dise√±o
            - "general": Enfoque balanceado
        price_range: Rango de precio (low, medium, high, premium)
        use_llm: Si usar LLM para generaci√≥n (requiere OPENAI_API_KEY)
        language: Idioma de la descripci√≥n (es, en, etc.)
        max_length: Longitud m√°xima de la descripci√≥n
        include_seo: Incluir optimizaci√≥n SEO con palabras clave
        tone: Tono de la descripci√≥n (professional, friendly, enthusiastic, etc.)
    
    Returns:
        Dict con:
            - description: Descripci√≥n principal
            - short_description: Versi√≥n corta (150-200 chars)
            - seo_keywords: Lista de palabras clave SEO
            - benefits: Lista de beneficios destacados
            - call_to_action: Llamado a la acci√≥n
            - variants: Variantes para A/B testing
            - metadata: Informaci√≥n adicional
    """
    try:
        # Templates base por tipo de comprador
        buyer_templates = {
            "eco-friendly": {
                "intro": "Descubre {product_name}, dise√±ado con el planeta en mente.",
                "benefits_focus": ["sostenible", "ecol√≥gico", "responsable", "renovable", "reciclable"],
                "cta": "Haz una elecci√≥n consciente hoy"
            },
            "luxury": {
                "intro": "Experimenta {product_name}, donde la excelencia se encuentra con la elegancia.",
                "benefits_focus": ["exclusivo", "premium", "artesanal", "sofisticado", "refinado"],
                "cta": "Eleva tu experiencia"
            },
            "budget": {
                "intro": "{product_name} - Calidad excepcional sin comprometer tu presupuesto.",
                "benefits_focus": ["valor", "econ√≥mico", "accesible", "rentable", "eficiente"],
                "cta": "Obt√©n el mejor valor"
            },
            "tech-savvy": {
                "intro": "{product_name} - Tecnolog√≠a de vanguardia al alcance de tus manos.",
                "benefits_focus": ["innovador", "avanzado", "inteligente", "eficiente", "conectado"],
                "cta": "Descubre la tecnolog√≠a del futuro"
            },
            "health-conscious": {
                "intro": "{product_name} - Tu bienestar es nuestra prioridad.",
                "benefits_focus": ["saludable", "natural", "seguro", "beneficioso", "cuidado"],
                "cta": "Invierte en tu salud"
            },
            "minimalist": {
                "intro": "{product_name} - Dise√±o simple, funcionalidad excepcional.",
                "benefits_focus": ["simple", "elegante", "funcional", "esencial", "limpio"],
                "cta": "Simplifica tu vida"
            },
            "general": {
                "intro": "Conoce {product_name}, dise√±ado para superar tus expectativas.",
                "benefits_focus": ["calidad", "confiable", "vers√°til", "pr√°ctico", "excelente"],
                "cta": "Descubre m√°s"
            }
        }
        
        template = buyer_templates.get(buyer_type, buyer_templates["general"])
        
        # Si se solicita LLM y est√° disponible
        if use_llm:
            llm_description = _generate_product_description_with_llm(
                product_name=product_name,
                product_features=product_features,
                product_category=product_category,
                buyer_type=buyer_type,
                price_range=price_range,
                language=language,
                max_length=max_length,
                template=template,
                tone=tone
            )
            if llm_description:
                return llm_description
        
        # Fallback a generaci√≥n con templates
        return _generate_product_description_template(
            product_name=product_name,
            product_features=product_features,
            product_category=product_category,
            buyer_type=buyer_type,
            price_range=price_range,
            template=template,
            language=language,
            max_length=max_length,
            include_seo=include_seo,
            tone=tone
        )
        
    except Exception as e:
        logger.error("product_description_generation_error", 
                    error=str(e), 
                    product_name=product_name,
                    exc_info=True)
        # Retornar descripci√≥n b√°sica en caso de error
        return {
            "description": f"{product_name}. {', '.join(product_features[:3])}.",
            "short_description": f"{product_name} - {product_features[0] if product_features else 'Calidad garantizada'}.",
            "seo_keywords": [product_name.lower(), product_category],
            "benefits": product_features[:3],
            "call_to_action": "Descubre m√°s",
            "variants": [],
            "metadata": {"generation_method": "fallback", "error": str(e)}
        }


def _generate_product_description_with_llm(
    product_name: str,
    product_features: List[str],
    product_category: str,
    buyer_type: str,
    price_range: str,
    language: str,
    max_length: int,
    template: Dict[str, Any],
    tone: str
) -> Optional[Dict[str, Any]]:
    """Genera descripci√≥n usando LLM (OpenAI)."""
    try:
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            return None
        
        # Construir prompt
        buyer_type_descriptions = {
            "eco-friendly": "compradores conscientes del medio ambiente que valoran la sostenibilidad",
            "luxury": "compradores que buscan productos exclusivos y de alta calidad",
            "budget": "compradores que buscan el mejor valor por su dinero",
            "tech-savvy": "compradores que valoran la tecnolog√≠a y las especificaciones t√©cnicas",
            "health-conscious": "compradores preocupados por su salud y bienestar",
            "minimalist": "compradores que aprecian el dise√±o simple y funcional",
            "general": "compradores en general"
        }
        
        buyer_description = buyer_type_descriptions.get(buyer_type, "compradores en general")
        
        prompt = f"""Eres un experto en copywriting para e-commerce. Genera una descripci√≥n de producto atractiva y optimizada para conversi√≥n.

Producto: {product_name}
Categor√≠a: {product_category}
Caracter√≠sticas: {', '.join(product_features)}
Tipo de comprador: {buyer_description}
Rango de precio: {price_range}
Tono: {tone}
Idioma: {language}
Longitud m√°xima: {max_length} caracteres

Requisitos:
1. Destaca los beneficios principales, no solo las caracter√≠sticas
2. Usa lenguaje persuasivo y orientado a la acci√≥n
3. Incluye palabras clave relevantes para SEO
4. Adapta el mensaje al tipo de comprador objetivo
5. Crea urgencia y deseo de compra
6. Incluye un llamado a la acci√≥n claro

Genera:
- Descripci√≥n principal (m√°ximo {max_length} caracteres)
- Descripci√≥n corta (150-200 caracteres)
- 5-7 palabras clave SEO
- 3-5 beneficios principales
- Un llamado a la acci√≥n

Formato JSON:
{{
    "description": "...",
    "short_description": "...",
    "seo_keywords": ["...", "..."],
    "benefits": ["...", "..."],
    "call_to_action": "..."
}}"""
        
        # Llamar a OpenAI
        headers = {
            "Authorization": f"Bearer {openai_api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            "messages": [
                {
                    "role": "system",
                    "content": "Eres un experto en copywriting para e-commerce. Genera descripciones de productos persuasivas y optimizadas para conversi√≥n. Responde siempre en formato JSON v√°lido."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.7,
            "max_tokens": 1000
        }
        
        response = httpx.post(
            "https://api.openai.com/v1/chat/completions",
            json=payload,
            headers=headers,
            timeout=30.0
        )
        
        if response.status_code == 200:
            data = response.json()
            content = data["choices"][0]["message"]["content"]
            
            # Extraer JSON de la respuesta
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                llm_result = json.loads(json_match.group())
                
                # Generar variantes para A/B testing
                variants = _generate_description_variants(
                    base_description=llm_result.get("description", ""),
                    product_name=product_name,
                    buyer_type=buyer_type
                )
                
                return {
                    "description": llm_result.get("description", ""),
                    "short_description": llm_result.get("short_description", ""),
                    "seo_keywords": llm_result.get("seo_keywords", []),
                    "benefits": llm_result.get("benefits", []),
                    "call_to_action": llm_result.get("call_to_action", "Descubre m√°s"),
                    "variants": variants,
                    "metadata": {
                        "generation_method": "llm",
                        "model": data.get("model", "unknown"),
                        "tokens_used": data.get("usage", {}).get("total_tokens", 0),
                        "buyer_type": buyer_type,
                        "price_range": price_range
                    }
                }
        
        return None
        
    except Exception as e:
        logger.warning("llm_description_generation_failed", 
                      error=str(e), 
                      product_name=product_name)
        return None


def _generate_product_description_template(
    product_name: str,
    product_features: List[str],
    product_category: str,
    buyer_type: str,
    price_range: str,
    template: Dict[str, Any],
    language: str,
    max_length: int,
    include_seo: bool,
    tone: str
) -> Dict[str, Any]:
    """Genera descripci√≥n usando templates inteligentes."""
    
    # Construir introducci√≥n
    intro = template["intro"].format(product_name=product_name)
    
    # Construir secci√≥n de beneficios
    benefits_text = "Caracter√≠sticas destacadas:\n"
    for i, feature in enumerate(product_features[:5], 1):
        benefits_text += f"‚Ä¢ {feature}\n"
    
    # Construir descripci√≥n principal
    description_parts = [
        intro,
        "",
        benefits_text,
        "",
        f"Perfecto para {buyer_type.replace('-', ' ')} que buscan calidad y {template['benefits_focus'][0]}."
    ]
    
    description = "\n".join(description_parts)
    
    # Ajustar longitud
    if len(description) > max_length:
        description = description[:max_length-3] + "..."
    
    # Generar descripci√≥n corta
    short_description = f"{product_name} - {product_features[0] if product_features else 'Calidad garantizada'}. {template['cta']}."
    if len(short_description) > 200:
        short_description = short_description[:197] + "..."
    
    # Generar keywords SEO
    seo_keywords = [product_name.lower(), product_category]
    seo_keywords.extend([kw.lower() for kw in template["benefits_focus"][:3]])
    if include_seo:
        seo_keywords.extend([f"{product_name} {product_category}", f"mejor {product_category}"])
    
    # Generar variantes
    variants = _generate_description_variants(
        base_description=description,
        product_name=product_name,
        buyer_type=buyer_type
    )
    
    return {
        "description": description,
        "short_description": short_description,
        "seo_keywords": list(set(seo_keywords)),
        "benefits": product_features[:5],
        "call_to_action": template["cta"],
        "variants": variants,
        "metadata": {
            "generation_method": "template",
            "buyer_type": buyer_type,
            "price_range": price_range,
            "tone": tone
        }
    }


def _generate_description_variants(
    base_description: str,
    product_name: str,
    buyer_type: str,
    num_variants: int = 3
) -> List[Dict[str, Any]]:
    """Genera variantes de descripci√≥n para A/B testing."""
    variants = []
    
    # Variante 1: Enfoque en beneficios
    variant1 = {
        "type": "benefits_focused",
        "description": base_description.replace("Caracter√≠sticas destacadas:", "Beneficios que transforman tu experiencia:"),
        "name": "Enfoque en beneficios"
    }
    variants.append(variant1)
    
    # Variante 2: Enfoque en problema-soluci√≥n
    variant2 = {
        "type": "problem_solution",
        "description": f"¬øBuscas {product_name}? Encuentra la soluci√≥n perfecta. {base_description[:200]}...",
        "name": "Enfoque problema-soluci√≥n"
    }
    variants.append(variant2)
    
    # Variante 3: Enfoque en urgencia
    variant3 = {
        "type": "urgency",
        "description": base_description + "\n\nNo pierdas la oportunidad de experimentar la diferencia.",
        "name": "Enfoque en urgencia"
    }
    variants.append(variant3)
    
    return variants[:num_variants]


def generate_batch_product_descriptions(
    products: List[Dict[str, Any]],
    buyer_type: str = "general",
    use_llm: bool = True,
    language: str = "es"
) -> List[Dict[str, Any]]:
    """
    Genera descripciones para m√∫ltiples productos en batch.
    
    Args:
        products: Lista de dicts con keys: name, features, category, price_range
        buyer_type: Tipo de comprador objetivo
        use_llm: Usar LLM para generaci√≥n
        language: Idioma
    
    Returns:
        Lista de descripciones generadas
    """
    results = []
    
    for product in products:
        try:
            description = generate_product_description(
                product_name=product.get("name", "Producto"),
                product_features=product.get("features", []),
                product_category=product.get("category", "general"),
                buyer_type=buyer_type,
                price_range=product.get("price_range", "medium"),
                use_llm=use_llm,
                language=language
            )
            description["product_id"] = product.get("id")
            description["product_name"] = product.get("name")
            results.append(description)
        except Exception as e:
            logger.error("batch_product_description_error",
                        error=str(e),
                        product_id=product.get("id"))
            results.append({
                "product_id": product.get("id"),
                "error": str(e),
                "description": f"{product.get('name', 'Producto')} - Ver detalles."
            })
    
    return results


# ============================================================================
# FUNCIONALIDADES AVANZADAS DE GENERACI√ìN DE DESCRIPCIONES
# ============================================================================

def score_product_description(
    description: str,
    product_category: str = "general",
    buyer_type: str = "general"
) -> Dict[str, Any]:
    """
    Eval√∫a y punt√∫a una descripci√≥n de producto en m√∫ltiples dimensiones.
    
    Args:
        description: Descripci√≥n a evaluar
        product_category: Categor√≠a del producto
        buyer_type: Tipo de comprador objetivo
    
    Returns:
        Dict con scores y recomendaciones
    """
    scores = {
        "readability": 0,
        "persuasiveness": 0,
        "seo_optimization": 0,
        "benefit_focus": 0,
        "length_optimization": 0,
        "cta_effectiveness": 0,
        "overall_score": 0
    }
    
    # An√°lisis de legibilidad
    word_count = len(description.split())
    sentence_count = description.count('.') + description.count('!') + description.count('?')
    avg_sentence_length = word_count / max(sentence_count, 1)
    
    # Score de legibilidad (ideal: 15-20 palabras por oraci√≥n)
    if 10 <= avg_sentence_length <= 25:
        scores["readability"] = 100
    elif 5 <= avg_sentence_length < 10 or 25 < avg_sentence_length <= 35:
        scores["readability"] = 70
    else:
        scores["readability"] = 40
    
    # An√°lisis de persuasi√≥n
    persuasive_words = [
        "exclusivo", "limitado", "garantizado", "premium", "innovador",
        "revolucionario", "transforma", "mejora", "aumenta", "optimiza",
        "perfecto", "ideal", "esencial", "indispensable", "recomendado"
    ]
    persuasive_count = sum(1 for word in persuasive_words if word.lower() in description.lower())
    scores["persuasiveness"] = min(100, 50 + (persuasive_count * 10))
    
    # An√°lisis SEO
    description_lower = description.lower()
    seo_indicators = [
        len([w for w in description_lower.split() if len(w) > 4]),  # Palabras largas
        description_lower.count(product_category.lower()) if product_category else 0,
        len(set(description_lower.split())) / max(len(description_lower.split()), 1)  # Diversidad l√©xica
    ]
    scores["seo_optimization"] = min(100, int(sum(seo_indicators) / len(seo_indicators) * 33))
    
    # Enfoque en beneficios
    benefit_words = ["beneficio", "ventaja", "ahorra", "mejora", "aumenta", "reduce", "optimiza"]
    benefit_count = sum(1 for word in benefit_words if word.lower() in description_lower)
    scores["benefit_focus"] = min(100, 30 + (benefit_count * 15))
    
    # Optimizaci√≥n de longitud
    if 200 <= len(description) <= 500:
        scores["length_optimization"] = 100
    elif 100 <= len(description) < 200 or 500 < len(description) <= 800:
        scores["length_optimization"] = 70
    else:
        scores["length_optimization"] = 40
    
    # Efectividad del CTA
    cta_phrases = ["compra", "ordena", "obt√©n", "descubre", "explora", "prueba", "agrega"]
    has_cta = any(phrase in description_lower for phrase in cta_phrases)
    scores["cta_effectiveness"] = 100 if has_cta else 50
    
    # Score general (promedio ponderado)
    weights = {
        "readability": 0.15,
        "persuasiveness": 0.25,
        "seo_optimization": 0.20,
        "benefit_focus": 0.20,
        "length_optimization": 0.10,
        "cta_effectiveness": 0.10
    }
    
    scores["overall_score"] = sum(scores[key] * weights[key] for key in weights.keys())
    
    # Recomendaciones
    recommendations = []
    if scores["readability"] < 70:
        recommendations.append("Simplifica las oraciones para mejorar la legibilidad")
    if scores["persuasiveness"] < 70:
        recommendations.append("Agrega m√°s palabras persuasivas y emocionales")
    if scores["seo_optimization"] < 70:
        recommendations.append("Incluye m√°s palabras clave relevantes para SEO")
    if scores["benefit_focus"] < 70:
        recommendations.append("Enf√≥cate m√°s en beneficios que en caracter√≠sticas")
    if scores["length_optimization"] < 70:
        recommendations.append("Ajusta la longitud (ideal: 200-500 caracteres)")
    if scores["cta_effectiveness"] < 70:
        recommendations.append("Incluye un llamado a la acci√≥n claro")
    
    return {
        "scores": scores,
        "recommendations": recommendations,
        "grade": (
            "A" if scores["overall_score"] >= 85 else
            "B" if scores["overall_score"] >= 70 else
            "C" if scores["overall_score"] >= 55 else
            "D" if scores["overall_score"] >= 40 else "F"
        ),
        "analysis": {
            "word_count": word_count,
            "sentence_count": sentence_count,
            "avg_sentence_length": round(avg_sentence_length, 1),
            "persuasive_words_found": persuasive_count,
            "benefit_words_found": benefit_count
        }
    }


def generate_platform_specific_description(
    product_name: str,
    product_features: List[str],
    platform: str = "general",
    buyer_type: str = "general",
    language: str = "es"
) -> Dict[str, Any]:
    """
    Genera descripciones optimizadas para plataformas espec√≠ficas de e-commerce.
    
    Args:
        product_name: Nombre del producto
        product_features: Lista de caracter√≠sticas
        platform: Plataforma objetivo (amazon, shopify, mercadolibre, etsy, general)
        buyer_type: Tipo de comprador
        language: Idioma
    
    Returns:
        Dict con descripci√≥n optimizada para la plataforma
    """
    # Obtener descripci√≥n base
    base_description = generate_product_description(
        product_name=product_name,
        product_features=product_features,
        buyer_type=buyer_type,
        language=language
    )
    
    platform_configs = {
        "amazon": {
            "max_title_length": 200,
            "max_bullets": 5,
            "max_bullet_length": 1000,
            "required_elements": ["title", "bullets", "description"],
            "style": "factual, keyword-rich"
        },
        "shopify": {
            "max_title_length": 60,
            "max_bullets": 8,
            "max_bullet_length": 500,
            "required_elements": ["title", "description", "benefits"],
            "style": "conversational, benefit-focused"
        },
        "mercadolibre": {
            "max_title_length": 60,
            "max_bullets": 10,
            "max_bullet_length": 500,
            "required_elements": ["title", "bullets", "description"],
            "style": "direct, value-focused"
        },
        "etsy": {
            "max_title_length": 140,
            "max_bullets": 5,
            "max_bullet_length": 300,
            "required_elements": ["title", "story", "description"],
            "style": "artisanal, story-driven"
        },
        "general": {
            "max_title_length": 60,
            "max_bullets": 5,
            "max_bullet_length": 500,
            "required_elements": ["title", "description"],
            "style": "balanced"
        }
    }
    
    config = platform_configs.get(platform, platform_configs["general"])
    
    # Generar t√≠tulo optimizado
    title = product_name
    if len(title) > config["max_title_length"]:
        title = title[:config["max_title_length"]-3] + "..."
    
    # Generar bullets optimizados
    bullets = []
    for i, feature in enumerate(product_features[:config["max_bullets"]], 1):
        bullet = f"‚Ä¢ {feature}"
        if len(bullet) > config["max_bullet_length"]:
            bullet = bullet[:config["max_bullet_length"]-3] + "..."
        bullets.append(bullet)
    
    # Ajustar descripci√≥n seg√∫n estilo de plataforma
    description = base_description.get("description", "")
    if platform == "amazon":
        # M√°s factual, menos marketing
        description = description.replace("Descubre", "Incluye")
        description = description.replace("Experimenta", "Caracter√≠sticas")
    elif platform == "shopify":
        # M√°s conversacional
        description = f"¬øBuscas {product_name}? {description}"
    elif platform == "etsy":
        # Agregar elemento de historia
        description = f"Con amor y atenci√≥n al detalle, {description.lower()}"
    
    return {
        "platform": platform,
        "title": title,
        "bullets": bullets,
        "description": description,
        "short_description": base_description.get("short_description", ""),
        "seo_keywords": base_description.get("seo_keywords", []),
        "benefits": base_description.get("benefits", []),
        "call_to_action": base_description.get("call_to_action", ""),
        "platform_specific": {
            "style": config["style"],
            "max_lengths": {
                "title": config["max_title_length"],
                "bullets": config["max_bullet_length"]
            }
        },
        "metadata": {
            **base_description.get("metadata", {}),
            "platform": platform,
            "optimized_for": platform
        }
    }


def generate_social_media_content(
    product_name: str,
    product_features: List[str],
    platform: str = "instagram",
    buyer_type: str = "general",
    language: str = "es"
) -> Dict[str, Any]:
    """
    Genera contenido optimizado para redes sociales.
    
    Args:
        product_name: Nombre del producto
        product_features: Lista de caracter√≠sticas
        platform: Plataforma (instagram, facebook, twitter, linkedin, tiktok)
        buyer_type: Tipo de comprador
        language: Idioma
    
    Returns:
        Dict con contenido para redes sociales
    """
    base_description = generate_product_description(
        product_name=product_name,
        product_features=product_features,
        buyer_type=buyer_type,
        language=language,
        max_length=280  # Twitter limit
    )
    
    platform_configs = {
        "instagram": {
            "max_caption_length": 2200,
            "hashtags_count": 10,
            "style": "visual, engaging, emoji-friendly",
            "include_hashtags": True
        },
        "facebook": {
            "max_caption_length": 5000,
            "hashtags_count": 5,
            "style": "conversational, community-focused",
            "include_hashtags": True
        },
        "twitter": {
            "max_caption_length": 280,
            "hashtags_count": 3,
            "style": "concise, punchy, trending",
            "include_hashtags": True
        },
        "linkedin": {
            "max_caption_length": 3000,
            "hashtags_count": 5,
            "style": "professional, value-driven",
            "include_hashtags": True
        },
        "tiktok": {
            "max_caption_length": 300,
            "hashtags_count": 5,
            "style": "trendy, fun, viral",
            "include_hashtags": True
        }
    }
    
    config = platform_configs.get(platform, platform_configs["instagram"])
    
    # Generar caption base
    caption = base_description.get("short_description", "")
    
    # Ajustar seg√∫n plataforma
    if platform == "twitter":
        caption = caption[:277] + "..."
    elif platform == "tiktok":
        caption = f"‚ú® {caption} ‚ú®"
    elif platform == "linkedin":
        caption = f"üíº {product_name}\n\n{caption}\n\n¬øQu√© opinas?"
    elif platform == "instagram":
        caption = f"üåü {caption}\n\n"
    
    # Generar hashtags
    hashtags = []
    if config["include_hashtags"]:
        category = "producto"  # En producci√≥n vendr√≠a del producto
        hashtags.extend([
            f"#{product_name.replace(' ', '')}",
            f"#{category}",
            "#comprar",
            "#oferta" if buyer_type == "budget" else "#premium"
        ])
        hashtags.extend([f"#{kw.replace(' ', '')}" for kw in base_description.get("seo_keywords", [])[:config["hashtags_count"]-3]])
    
    hashtags = hashtags[:config["hashtags_count"]]
    hashtag_string = " ".join(hashtags)
    
    # Generar diferentes variantes
    variants = {
        "short": caption[:100] + "..." if len(caption) > 100 else caption,
        "medium": caption,
        "with_hashtags": f"{caption}\n\n{hashtag_string}",
        "question_format": f"¬øConoces {product_name}? {caption}",
        "benefit_focused": f"Beneficios de {product_name}:\n" + "\n".join([f"‚úì {b}" for b in base_description.get("benefits", [])[:3]])
    }
    
    return {
        "platform": platform,
        "caption": caption,
        "hashtags": hashtags,
        "hashtag_string": hashtag_string,
        "full_post": f"{caption}\n\n{hashtag_string}",
        "variants": variants,
        "character_count": len(caption),
        "max_length": config["max_caption_length"],
        "metadata": {
            "platform": platform,
            "style": config["style"],
            "buyer_type": buyer_type
        }
    }


def optimize_description_based_on_performance(
    description: str,
    performance_data: Dict[str, Any],
    product_category: str = "general"
) -> Dict[str, Any]:
    """
    Optimiza una descripci√≥n bas√°ndose en datos de rendimiento hist√≥ricos.
    
    Args:
        description: Descripci√≥n actual
        performance_data: Dict con m√©tricas de rendimiento:
            - conversion_rate: Tasa de conversi√≥n
            - click_through_rate: CTR
            - engagement_rate: Tasa de engagement
            - avg_time_on_page: Tiempo promedio en p√°gina
            - bounce_rate: Tasa de rebote
        product_category: Categor√≠a del producto
    
    Returns:
        Dict con descripci√≥n optimizada y recomendaciones
    """
    current_score = score_product_description(description, product_category)
    
    # Analizar m√©tricas de rendimiento
    conversion_rate = performance_data.get("conversion_rate", 0)
    ctr = performance_data.get("click_through_rate", 0)
    engagement = performance_data.get("engagement_rate", 0)
    bounce_rate = performance_data.get("bounce_rate", 100)
    
    # Identificar problemas
    issues = []
    optimizations = []
    
    if conversion_rate < 2.0:
        issues.append("Baja tasa de conversi√≥n")
        optimizations.append("Agregar m√°s elementos de urgencia y escasez")
        optimizations.append("Mejorar el llamado a la acci√≥n")
    
    if ctr < 1.0:
        issues.append("Bajo CTR")
        optimizations.append("Mejorar el t√≠tulo con palabras m√°s atractivas")
        optimizations.append("Incluir n√∫meros y estad√≠sticas")
    
    if bounce_rate > 60:
        issues.append("Alta tasa de rebote")
        optimizations.append("Mejorar la introducci√≥n para captar atenci√≥n inmediata")
        optimizations.append("Agregar bullets con beneficios claros")
    
    if engagement < 30:
        issues.append("Bajo engagement")
        optimizations.append("Hacer el contenido m√°s interactivo")
        optimizations.append("Agregar preguntas para generar curiosidad")
    
    # Generar versi√≥n optimizada
    optimized_description = description
    
    # Aplicar optimizaciones b√°sicas
    if "Baja tasa de conversi√≥n" in issues:
        # Agregar urgencia
        if "limitado" not in optimized_description.lower():
            optimized_description = f"Oferta limitada: {optimized_description}"
    
    if "Bajo CTR" in issues:
        # Mejorar con n√∫meros
        if not any(char.isdigit() for char in optimized_description):
            optimized_description = optimized_description.replace(
                "m√∫ltiples", "5+"
            ).replace("varios", "3+")
    
    # Re-scorar la versi√≥n optimizada
    optimized_score = score_product_description(optimized_description, product_category)
    
    return {
        "original_description": description,
        "optimized_description": optimized_description,
        "original_score": current_score["overall_score"],
        "optimized_score": optimized_score["overall_score"],
        "improvement": optimized_score["overall_score"] - current_score["overall_score"],
        "issues_identified": issues,
        "optimizations_applied": optimizations,
        "recommendations": optimized_score["recommendations"],
        "performance_analysis": {
            "conversion_rate": conversion_rate,
            "ctr": ctr,
            "engagement_rate": engagement,
            "bounce_rate": bounce_rate
        }
    }


def generate_competitive_analysis(
    product_name: str,
    product_features: List[str],
    competitor_descriptions: List[str],
    buyer_type: str = "general"
) -> Dict[str, Any]:
    """
    Analiza descripciones de competidores y genera insights.
    
    Args:
        product_name: Nombre del producto
        product_features: Caracter√≠sticas del producto
        competitor_descriptions: Lista de descripciones de competidores
        buyer_type: Tipo de comprador
    
    Returns:
        Dict con an√°lisis competitivo y recomendaciones
    """
    # Analizar descripciones de competidores
    competitor_analysis = []
    
    for i, comp_desc in enumerate(competitor_descriptions, 1):
        comp_score = score_product_description(comp_desc, buyer_type=buyer_type)
        competitor_analysis.append({
            "competitor_id": f"competitor_{i}",
            "description": comp_desc[:200] + "..." if len(comp_desc) > 200 else comp_desc,
            "score": comp_score["overall_score"],
            "grade": comp_score["grade"],
            "strengths": [k for k, v in comp_score["scores"].items() if v >= 80],
            "weaknesses": [k for k, v in comp_score["scores"].items() if v < 60]
        })
    
    # Generar descripci√≥n propia
    own_description = generate_product_description(
        product_name=product_name,
        product_features=product_features,
        buyer_type=buyer_type
    )
    
    own_score = score_product_description(
        own_description.get("description", ""),
        buyer_type=buyer_type
    )
    
    # Comparar con competencia
    avg_competitor_score = sum(c["score"] for c in competitor_analysis) / len(competitor_analysis) if competitor_analysis else 0
    
    # Identificar oportunidades
    opportunities = []
    if own_score["overall_score"] < avg_competitor_score:
        opportunities.append("Mejorar el score general para superar a la competencia")
    
    # Identificar palabras clave de competidores
    all_competitor_text = " ".join(competitor_descriptions).lower()
    competitor_keywords = {}
    for word in all_competitor_text.split():
        if len(word) > 4:  # Palabras significativas
            competitor_keywords[word] = competitor_keywords.get(word, 0) + 1
    
    top_competitor_keywords = sorted(
        competitor_keywords.items(),
        key=lambda x: x[1],
        reverse=True
    )[:10]
    
    # Recomendaciones
    recommendations = []
    if own_score["scores"]["persuasiveness"] < 70:
        recommendations.append("Aumentar el uso de palabras persuasivas como los competidores")
    if own_score["scores"]["seo_optimization"] < 70:
        recommendations.append(f"Incluir palabras clave populares: {', '.join([kw[0] for kw in top_competitor_keywords[:5]])}")
    
    return {
        "own_description": own_description,
        "own_score": own_score["overall_score"],
        "own_grade": own_score["grade"],
        "competitor_analysis": competitor_analysis,
        "average_competitor_score": round(avg_competitor_score, 2),
        "competitive_position": (
            "leader" if own_score["overall_score"] > avg_competitor_score + 10 else
            "strong" if own_score["overall_score"] > avg_competitor_score else
            "competitive" if abs(own_score["overall_score"] - avg_competitor_score) <= 5 else
            "needs_improvement"
        ),
        "top_competitor_keywords": [kw[0] for kw in top_competitor_keywords],
        "opportunities": opportunities,
        "recommendations": recommendations,
        "comparison": {
            "score_difference": round(own_score["overall_score"] - avg_competitor_score, 2),
            "is_above_average": own_score["overall_score"] > avg_competitor_score
        }
    }


def _get_product_features(product_name: str) -> List[str]:
    """Obtiene caracter√≠sticas de un producto (mock - en producci√≥n vendr√≠a de BD)."""
    features_map = {
        "Enterprise Plan": [
            "Acceso ilimitado a todas las funcionalidades",
            "Soporte prioritario 24/7",
            "Integraciones avanzadas",
            "Personalizaci√≥n completa",
            "Gesti√≥n de m√∫ltiples usuarios"
        ],
        "Professional Plan": [
            "Acceso a funcionalidades principales",
            "Soporte por email",
            "Integraciones est√°ndar",
            "Hasta 10 usuarios",
            "Reportes avanzados"
        ],
        "Starter Plan": [
            "Funcionalidades b√°sicas",
            "Soporte comunitario",
            "Integraciones b√°sicas",
            "Hasta 3 usuarios",
            "Reportes est√°ndar"
        ],
        "API Access": [
            "API REST completa",
            "Documentaci√≥n detallada",
            "Webhooks en tiempo real",
            "Rate limiting configurable",
            "Autenticaci√≥n segura"
        ],
        "Premium Support": [
            "Respuesta garantizada en 1 hora",
            "Soporte t√©cnico dedicado",
            "Consultor√≠a incluida",
            "Acceso a beta features",
            "SLA 99.9%"
        ]
    }
    
    # Buscar coincidencia parcial
    for key, features in features_map.items():
        if key.lower() in product_name.lower() or product_name.lower() in key.lower():
            return features
    
    # Retornar caracter√≠sticas gen√©ricas
    return [
        "Funcionalidades avanzadas",
        "Soporte t√©cnico",
        "F√°cil de usar",
        "Escalable",
        "Seguro y confiable"
    ]


# ============================================================================
# FUNCIONALIDADES AVANZADAS ADICIONALES
# ============================================================================

def generate_product_faqs(
    product_name: str,
    product_features: List[str],
    product_category: str = "general",
    buyer_type: str = "general",
    num_faqs: int = 5,
    use_llm: bool = True
) -> Dict[str, Any]:
    """
    Genera preguntas frecuentes (FAQs) autom√°ticas para un producto.
    
    Args:
        product_name: Nombre del producto
        product_features: Lista de caracter√≠sticas
        product_category: Categor√≠a del producto
        buyer_type: Tipo de comprador
        num_faqs: N√∫mero de FAQs a generar
        use_llm: Usar LLM para generaci√≥n
    
    Returns:
        Dict con FAQs generadas
    """
    try:
        if use_llm:
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if openai_api_key:
                faqs = _generate_faqs_with_llm(
                    product_name, product_features, product_category, 
                    buyer_type, num_faqs, openai_api_key
                )
                if faqs:
                    return faqs
        
        # Fallback a generaci√≥n con templates
        return _generate_faqs_template(
            product_name, product_features, product_category, 
            buyer_type, num_faqs
        )
        
    except Exception as e:
        logger.error("faq_generation_error", error=str(e), product_name=product_name)
        return {"faqs": [], "error": str(e)}


def _generate_faqs_with_llm(
    product_name: str,
    product_features: List[str],
    product_category: str,
    buyer_type: str,
    num_faqs: int,
    api_key: str
) -> Optional[Dict[str, Any]]:
    """Genera FAQs usando LLM."""
    try:
        prompt = f"""Genera {num_faqs} preguntas frecuentes (FAQs) para el siguiente producto:

Producto: {product_name}
Categor√≠a: {product_category}
Caracter√≠sticas: {', '.join(product_features[:5])}
Tipo de comprador: {buyer_type}

Las FAQs deben:
1. Ser relevantes y comunes para este tipo de producto
2. Cubrir aspectos como especificaciones, uso, garant√≠a, env√≠o, etc.
3. Tener respuestas claras y √∫tiles (m√°ximo 150 palabras por respuesta)
4. Estar en espa√±ol

Formato JSON:
{{
    "faqs": [
        {{"question": "...", "answer": "..."}},
        {{"question": "...", "answer": "..."}}
    ]
}}"""
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            "messages": [
                {
                    "role": "system",
                    "content": "Eres un experto en e-commerce. Genera FAQs √∫tiles y relevantes. Responde siempre en formato JSON v√°lido."
                },
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.7,
            "max_tokens": 1500
        }
        
        response = httpx.post(
            "https://api.openai.com/v1/chat/completions",
            json=payload,
            headers=headers,
            timeout=30.0
        )
        
        if response.status_code == 200:
            data = response.json()
            content = data["choices"][0]["message"]["content"]
            
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                faqs_data = json.loads(json_match.group())
                return {
                    "faqs": faqs_data.get("faqs", []),
                    "total": len(faqs_data.get("faqs", [])),
                    "generation_method": "llm",
                    "metadata": {
                        "product_name": product_name,
                        "category": product_category,
                        "buyer_type": buyer_type
                    }
                }
        
        return None
        
    except Exception as e:
        logger.warning("llm_faq_generation_failed", error=str(e))
        return None


def _generate_faqs_template(
    product_name: str,
    product_features: List[str],
    product_category: str,
    buyer_type: str,
    num_faqs: int
) -> Dict[str, Any]:
    """Genera FAQs usando templates."""
    template_faqs = [
        {
            "question": f"¬øQu√© incluye {product_name}?",
            "answer": f"{product_name} incluye: {', '.join(product_features[:3])}. Ideal para usuarios que buscan {buyer_type.replace('-', ' ')}."
        },
        {
            "question": f"¬øC√≥mo funciona {product_name}?",
            "answer": f"{product_name} est√° dise√±ado para ser f√°cil de usar. Simplemente sigue las instrucciones incluidas y estar√°s listo en minutos."
        },
        {
            "question": "¬øCu√°l es el tiempo de entrega?",
            "answer": "El tiempo de entrega var√≠a seg√∫n tu ubicaci√≥n, generalmente entre 3-7 d√≠as h√°biles. Te enviaremos un email con el n√∫mero de seguimiento."
        },
        {
            "question": "¬øOfrecen garant√≠a?",
            "answer": f"S√≠, {product_name} incluye garant√≠a del fabricante. Consulta los t√©rminos y condiciones para m√°s detalles."
        },
        {
            "question": "¬øPuedo devolver el producto si no estoy satisfecho?",
            "answer": "S√≠, ofrecemos pol√≠tica de devoluci√≥n dentro de los primeros 30 d√≠as. El producto debe estar en su estado original."
        },
        {
            "question": f"¬øEs {product_name} compatible con otros productos?",
            "answer": f"{product_name} es compatible con la mayor√≠a de productos est√°ndar en la categor√≠a {product_category}."
        },
        {
            "question": "¬øNecesito conocimientos t√©cnicos para usarlo?",
            "answer": f"No, {product_name} est√° dise√±ado para ser intuitivo y f√°cil de usar, sin necesidad de conocimientos t√©cnicos avanzados."
        }
    ]
    
    return {
        "faqs": template_faqs[:num_faqs],
        "total": len(template_faqs[:num_faqs]),
        "generation_method": "template",
        "metadata": {
            "product_name": product_name,
            "category": product_category,
            "buyer_type": buyer_type
        }
    }


def generate_multiple_variants(
    product_name: str,
    product_features: List[str],
    num_variants: int = 5,
    buyer_type: str = "general",
    strategy: str = "diverse"
) -> Dict[str, Any]:
    """
    Genera m√∫ltiples variantes de descripci√≥n con diferentes estrategias.
    
    Args:
        product_name: Nombre del producto
        product_features: Lista de caracter√≠sticas
        num_variants: N√∫mero de variantes a generar
        buyer_type: Tipo de comprador
        strategy: Estrategia de variaci√≥n:
            - "diverse": Variedad de estilos
            - "length": Diferentes longitudes
            - "focus": Diferentes enfoques (beneficios, caracter√≠sticas, emocional)
            - "tone": Diferentes tonos (formal, casual, entusiasta)
    
    Returns:
        Dict con variantes generadas y an√°lisis
    """
    variants = []
    
    strategies_map = {
        "diverse": ["benefit_focused", "feature_focused", "emotional", "technical", "storytelling"],
        "length": ["short", "medium", "long", "very_long"],
        "focus": ["benefits", "features", "use_cases", "comparison", "testimonials"],
        "tone": ["professional", "casual", "enthusiastic", "friendly", "authoritative"]
    }
    
    variant_types = strategies_map.get(strategy, strategies_map["diverse"])
    
    for i, variant_type in enumerate(variant_types[:num_variants], 1):
        try:
            # Ajustar par√°metros seg√∫n tipo de variante
            if variant_type in ["short", "medium", "long", "very_long"]:
                max_lengths = {"short": 150, "medium": 300, "long": 500, "very_long": 800}
                max_length = max_lengths.get(variant_type, 300)
                tone = "professional"
            elif variant_type in ["professional", "casual", "enthusiastic", "friendly", "authoritative"]:
                max_length = 400
                tone = variant_type
            else:
                max_length = 400
                tone = "professional"
            
            description = generate_product_description(
                product_name=product_name,
                product_features=product_features,
                buyer_type=buyer_type,
                max_length=max_length,
                tone=tone
            )
            
            # Aplicar modificaciones seg√∫n estrategia
            if variant_type == "benefit_focused":
                description["description"] = description["description"].replace(
                    "Caracter√≠sticas destacadas:", "Beneficios principales:"
                )
            elif variant_type == "emotional":
                description["description"] = f"üíù {description['description']}"
            elif variant_type == "storytelling":
                description["description"] = f"Descubre la historia detr√°s de {product_name}. {description['description']}"
            
            # Scorar cada variante
            score = score_product_description(
                description.get("description", ""),
                buyer_type=buyer_type
            )
            
            variants.append({
                "variant_id": f"variant_{i}",
                "variant_type": variant_type,
                "description": description,
                "score": score["overall_score"],
                "grade": score["grade"],
                "analysis": score["analysis"]
            })
            
        except Exception as e:
            logger.warning("variant_generation_failed", 
                         variant_type=variant_type, 
                         error=str(e))
    
    # Ordenar por score
    variants.sort(key=lambda x: x["score"], reverse=True)
    
    return {
        "variants": variants,
        "total_variants": len(variants),
        "strategy": strategy,
        "best_variant": variants[0] if variants else None,
        "average_score": sum(v["score"] for v in variants) / len(variants) if variants else 0,
        "metadata": {
            "product_name": product_name,
            "buyer_type": buyer_type,
            "generation_timestamp": datetime.utcnow().isoformat()
        }
    }


def analyze_description_sentiment(
    description: str,
    product_category: str = "general"
) -> Dict[str, Any]:
    """
    Analiza el sentimiento y tono de una descripci√≥n de producto.
    
    Args:
        description: Descripci√≥n a analizar
        product_category: Categor√≠a del producto
    
    Returns:
        Dict con an√°lisis de sentimiento
    """
    description_lower = description.lower()
    
    # Palabras positivas
    positive_words = [
        "excelente", "perfecto", "ideal", "mejor", "superior", "premium",
        "innovador", "revolucionario", "transforma", "mejora", "aumenta",
        "garantizado", "confiable", "seguro", "eficiente", "r√°pido", "f√°cil"
    ]
    
    # Palabras negativas (generalmente no deber√≠an aparecer)
    negative_words = [
        "problema", "error", "falla", "defecto", "lento", "dif√≠cil",
        "complicado", "limitado", "restricci√≥n"
    ]
    
    # Palabras de urgencia
    urgency_words = [
        "limitado", "exclusivo", "ahora", "hoy", "urgente", "r√°pido",
        "oferta", "descuento", "solo", "√∫ltimas unidades"
    ]
    
    # Palabras emocionales
    emotional_words = [
        "amor", "pasi√≥n", "sue√±o", "deseo", "felicidad", "satisfacci√≥n",
        "orgullo", "confianza", "seguridad", "tranquilidad"
    ]
    
    positive_count = sum(1 for word in positive_words if word in description_lower)
    negative_count = sum(1 for word in negative_words if word in description_lower)
    urgency_count = sum(1 for word in urgency_words if word in description_lower)
    emotional_count = sum(1 for word in emotional_words if word in description_lower)
    
    # Calcular sentimiento
    total_words = len(description.split())
    sentiment_score = ((positive_count - negative_count) / max(total_words, 1)) * 100
    sentiment_score = max(-100, min(100, sentiment_score))
    
    # Determinar sentimiento
    if sentiment_score > 20:
        sentiment = "muy_positivo"
    elif sentiment_score > 5:
        sentiment = "positivo"
    elif sentiment_score > -5:
        sentiment = "neutral"
    elif sentiment_score > -20:
        sentiment = "negativo"
    else:
        sentiment = "muy_negativo"
    
    # Determinar tono
    if urgency_count > 2:
        tone = "urgente"
    elif emotional_count > 3:
        tone = "emocional"
    elif positive_count > 5:
        tone = "entusiasta"
    else:
        tone = "informativo"
    
    return {
        "sentiment": sentiment,
        "sentiment_score": round(sentiment_score, 2),
        "tone": tone,
        "word_analysis": {
            "positive_words": positive_count,
            "negative_words": negative_count,
            "urgency_words": urgency_count,
            "emotional_words": emotional_count,
            "total_words": total_words
        },
        "recommendations": _get_sentiment_recommendations(
            sentiment_score, urgency_count, emotional_count
        )
    }


def _get_sentiment_recommendations(
    sentiment_score: float,
    urgency_count: int,
    emotional_count: int
) -> List[str]:
    """Genera recomendaciones basadas en an√°lisis de sentimiento."""
    recommendations = []
    
    if sentiment_score < 10:
        recommendations.append("Aumentar el uso de palabras positivas")
    
    if urgency_count < 1:
        recommendations.append("Considerar agregar elementos de urgencia para aumentar conversi√≥n")
    
    if emotional_count < 2:
        recommendations.append("Incluir m√°s palabras emocionales para conectar con el comprador")
    
    if sentiment_score > 50:
        recommendations.append("El sentimiento es muy positivo, mantener este nivel")
    
    return recommendations


def export_description_formats(
    description_data: Dict[str, Any],
    formats: List[str] = ["json", "html", "markdown", "csv"]
) -> Dict[str, Any]:
    """
    Exporta descripciones a m√∫ltiples formatos.
    
    Args:
        description_data: Datos de descripci√≥n a exportar
        formats: Lista de formatos (json, html, markdown, csv, xml)
    
    Returns:
        Dict con descripciones en cada formato
    """
    exports = {}
    
    for format_type in formats:
        try:
            if format_type == "json":
                exports["json"] = json.dumps(description_data, indent=2, ensure_ascii=False)
            
            elif format_type == "html":
                desc = description_data.get("description", "")
                short_desc = description_data.get("short_description", "")
                bullets = description_data.get("benefits", [])
                
                html = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>{description_data.get('product_name', 'Producto')}</title>
</head>
<body>
    <h1>{description_data.get('product_name', 'Producto')}</h1>
    <p><strong>{short_desc}</strong></p>
    <div>{desc}</div>
    <ul>
"""
                for bullet in bullets:
                    html += f"        <li>{bullet}</li>\n"
                html += """    </ul>
</body>
</html>"""
                exports["html"] = html
            
            elif format_type == "markdown":
                desc = description_data.get("description", "")
                short_desc = description_data.get("short_description", "")
                bullets = description_data.get("benefits", [])
                keywords = description_data.get("seo_keywords", [])
                
                md = f"""# {description_data.get('product_name', 'Producto')}

## Descripci√≥n Corta
{short_desc}

## Descripci√≥n Completa
{desc}

## Beneficios
"""
                for bullet in bullets:
                    md += f"- {bullet}\n"
                
                if keywords:
                    md += f"\n## Palabras Clave SEO\n"
                    md += ", ".join(keywords)
                
                exports["markdown"] = md
            
            elif format_type == "csv":
                import csv
                from io import StringIO
                
                output = StringIO()
                writer = csv.writer(output)
                writer.writerow(["Campo", "Valor"])
                writer.writerow(["product_name", description_data.get("product_name", "")])
                writer.writerow(["short_description", description_data.get("short_description", "")])
                writer.writerow(["description", description_data.get("description", "")])
                writer.writerow(["seo_keywords", ", ".join(description_data.get("seo_keywords", []))])
                writer.writerow(["call_to_action", description_data.get("call_to_action", "")])
                
                exports["csv"] = output.getvalue()
            
            elif format_type == "xml":
                desc = description_data.get("description", "")
                short_desc = description_data.get("short_description", "")
                
                xml = f"""<?xml version="1.0" encoding="UTF-8"?>
<product>
    <name>{description_data.get('product_name', 'Producto')}</name>
    <short_description>{short_desc}</short_description>
    <description><![CDATA[{desc}]]></description>
    <seo_keywords>
"""
                for keyword in description_data.get("seo_keywords", []):
                    xml += f"        <keyword>{keyword}</keyword>\n"
                xml += """    </seo_keywords>
    <call_to_action>""" + description_data.get("call_to_action", "") + """</call_to_action>
</product>"""
                exports["xml"] = xml
                
        except Exception as e:
            logger.warning("export_format_failed", format=format_type, error=str(e))
            exports[format_type] = None
    
    return {
        "formats": exports,
        "available_formats": list(exports.keys()),
        "export_timestamp": datetime.utcnow().isoformat()
    }


def cache_product_description(
    product_id: str,
    description_data: Dict[str, Any],
    conn_id: str = None,
    ttl_hours: int = 24
) -> bool:
    """
    Guarda una descripci√≥n en cache para reutilizaci√≥n.
    
    Args:
        product_id: ID del producto
        description_data: Datos de descripci√≥n
        conn_id: ID de conexi√≥n PostgreSQL
        ttl_hours: Tiempo de vida del cache en horas
    
    Returns:
        True si se guard√≥ exitosamente
    """
    try:
        if not conn_id:
            return False
        
        hook = PostgresHook(postgres_conn_id=conn_id)
        with hook.get_conn() as conn:
            with conn.cursor() as cur:
                # Crear tabla si no existe
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS product_descriptions_cache (
                        product_id VARCHAR(255) PRIMARY KEY,
                        description_data JSONB NOT NULL,
                        created_at TIMESTAMPTZ DEFAULT NOW(),
                        expires_at TIMESTAMPTZ DEFAULT NOW() + INTERVAL '24 hours',
                        hits INT DEFAULT 0,
                        last_accessed_at TIMESTAMPTZ DEFAULT NOW()
                    );
                    CREATE INDEX IF NOT EXISTS idx_desc_cache_expires 
                    ON product_descriptions_cache(expires_at);
                """)
                
                # Insertar o actualizar
                cur.execute("""
                    INSERT INTO product_descriptions_cache 
                    (product_id, description_data, expires_at)
                    VALUES (%s, %s, NOW() + INTERVAL '%s hours')
                    ON CONFLICT (product_id) DO UPDATE SET
                        description_data = EXCLUDED.description_data,
                        expires_at = NOW() + INTERVAL '%s hours',
                        created_at = NOW(),
                        hits = product_descriptions_cache.hits + 1,
                        last_accessed_at = NOW()
                """, (product_id, json.dumps(description_data), ttl_hours, ttl_hours))
                
                conn.commit()
                return True
                
    except Exception as e:
        logger.warning("description_cache_save_failed", 
                      product_id=product_id, 
                      error=str(e))
        return False


def get_cached_description(
    product_id: str,
    conn_id: str = None
) -> Optional[Dict[str, Any]]:
    """
    Obtiene una descripci√≥n desde cache.
    
    Args:
        product_id: ID del producto
        conn_id: ID de conexi√≥n PostgreSQL
    
    Returns:
        Datos de descripci√≥n si existe y no ha expirado, None en caso contrario
    """
    try:
        if not conn_id:
            return None
        
        hook = PostgresHook(postgres_conn_id=conn_id)
        with hook.get_conn() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT description_data, expires_at
                    FROM product_descriptions_cache
                    WHERE product_id = %s 
                    AND expires_at > NOW()
                """, (product_id,))
                
                result = cur.fetchone()
                if result:
                    # Actualizar hits y last_accessed
                    cur.execute("""
                        UPDATE product_descriptions_cache
                        SET hits = hits + 1,
                            last_accessed_at = NOW()
                        WHERE product_id = %s
                    """, (product_id,))
                    conn.commit()
                    
                    return json.loads(result[0])
        
        return None
        
    except Exception as e:
        logger.warning("description_cache_get_failed", 
                      product_id=product_id, 
                      error=str(e))
        return None


# ============================================================================
# INTEGRACIONES Y FUNCIONALIDADES AVANZADAS ADICIONALES
# ============================================================================

def sync_description_to_ecommerce_platform(
    product_id: str,
    description_data: Dict[str, Any],
    platform: str = "stripe",
    api_key: str = None
) -> Dict[str, Any]:
    """
    Sincroniza descripciones generadas con plataformas de e-commerce.
    
    Args:
        product_id: ID del producto
        description_data: Datos de descripci√≥n generada
        platform: Plataforma (stripe, shopify, woocommerce)
        api_key: API key de la plataforma
    
    Returns:
        Dict con resultado de la sincronizaci√≥n
    """
    try:
        if platform == "stripe":
            return _sync_to_stripe(product_id, description_data, api_key)
        elif platform == "shopify":
            return _sync_to_shopify(product_id, description_data, api_key)
        elif platform == "woocommerce":
            return _sync_to_woocommerce(product_id, description_data, api_key)
        else:
            return {"success": False, "error": f"Plataforma no soportada: {platform}"}
    except Exception as e:
        logger.error("ecommerce_sync_error", platform=platform, error=str(e))
        return {"success": False, "error": str(e)}


def _sync_to_stripe(product_id: str, description_data: Dict[str, Any], api_key: str = None) -> Dict[str, Any]:
    """Sincroniza descripci√≥n con Stripe."""
    try:
        if not api_key:
            api_key = os.getenv("STRIPE_API_KEY")
        
        if not api_key:
            return {"success": False, "error": "Stripe API key no configurada"}
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/x-www-form-urlencoded"
        }
        
        # Preparar datos
        data = {
            "description": description_data.get("description", ""),
            "metadata[short_description]": description_data.get("short_description", ""),
            "metadata[seo_keywords]": ",".join(description_data.get("seo_keywords", [])),
            "metadata[generated_at]": datetime.utcnow().isoformat()
        }
        
        response = httpx.post(
            f"https://api.stripe.com/v1/products/{product_id}",
            data=data,
            headers=headers,
            timeout=30.0
        )
        
        if response.status_code in [200, 201]:
            return {
                "success": True,
                "platform": "stripe",
                "product_id": product_id,
                "updated_at": datetime.utcnow().isoformat()
            }
        else:
            return {
                "success": False,
                "error": f"Stripe API error: {response.status_code}",
                "response": response.text
            }
            
    except Exception as e:
        return {"success": False, "error": str(e)}


def _sync_to_shopify(product_id: str, description_data: Dict[str, Any], api_key: str = None) -> Dict[str, Any]:
    """Sincroniza descripci√≥n con Shopify."""
    try:
        shop_url = os.getenv("SHOPIFY_STORE_URL")
        if not api_key:
            api_key = os.getenv("SHOPIFY_API_KEY")
        
        if not shop_url or not api_key:
            return {"success": False, "error": "Shopify credentials no configuradas"}
        
        headers = {
            "X-Shopify-Access-Token": api_key,
            "Content-Type": "application/json"
        }
        
        payload = {
            "product": {
                "id": product_id,
                "body_html": description_data.get("description", ""),
                "metafields_global_description_tag": description_data.get("short_description", "")
            }
        }
        
        response = httpx.put(
            f"https://{shop_url}/admin/api/2024-01/products/{product_id}.json",
            json=payload,
            headers=headers,
            timeout=30.0
        )
        
        if response.status_code in [200, 201]:
            return {
                "success": True,
                "platform": "shopify",
                "product_id": product_id,
                "updated_at": datetime.utcnow().isoformat()
            }
        else:
            return {
                "success": False,
                "error": f"Shopify API error: {response.status_code}",
                "response": response.text
            }
            
    except Exception as e:
        return {"success": False, "error": str(e)}


def _sync_to_woocommerce(product_id: str, description_data: Dict[str, Any], api_key: str = None) -> Dict[str, Any]:
    """Sincroniza descripci√≥n con WooCommerce."""
    try:
        store_url = os.getenv("WOCOMMERCE_STORE_URL")
        consumer_key = os.getenv("WOCOMMERCE_CONSUMER_KEY")
        consumer_secret = os.getenv("WOCOMMERCE_CONSUMER_SECRET")
        
        if not store_url or not consumer_key or not consumer_secret:
            return {"success": False, "error": "WooCommerce credentials no configuradas"}
        
        payload = {
            "description": description_data.get("description", ""),
            "short_description": description_data.get("short_description", "")
        }
        
        response = httpx.put(
            f"{store_url}/wp-json/wc/v3/products/{product_id}",
            json=payload,
            auth=(consumer_key, consumer_secret),
            timeout=30.0
        )
        
        if response.status_code in [200, 201]:
            return {
                "success": True,
                "platform": "woocommerce",
                "product_id": product_id,
                "updated_at": datetime.utcnow().isoformat()
            }
        else:
            return {
                "success": False,
                "error": f"WooCommerce API error: {response.status_code}",
                "response": response.text
            }
            
    except Exception as e:
        return {"success": False, "error": str(e)}


def generate_multimedia_content(
    product_name: str,
    product_features: List[str],
    product_category: str = "general"
) -> Dict[str, Any]:
    """
    Genera contenido multimedia para productos (alt text, t√≠tulos de im√°genes, etc.).
    
    Args:
        product_name: Nombre del producto
        product_features: Lista de caracter√≠sticas
        product_category: Categor√≠a del producto
    
    Returns:
        Dict con contenido multimedia generado
    """
    # Generar alt text para im√°genes
    alt_text = f"{product_name} - {product_category}"
    if product_features:
        alt_text += f" con {product_features[0].lower()}"
    
    # T√≠tulos para im√°genes
    image_titles = [
        f"{product_name} - Vista frontal",
        f"{product_name} - Detalles",
        f"{product_name} - En uso",
        f"{product_name} - Caracter√≠sticas principales"
    ]
    
    # Descripciones para videos
    video_descriptions = [
        f"Descubre {product_name} y todas sus caracter√≠sticas en este video.",
        f"Conoce c√≥mo {product_name} puede mejorar tu experiencia.",
        f"Video demostrativo de {product_name} - {', '.join(product_features[:3])}"
    ]
    
    # Captions para redes sociales
    social_captions = {
        "instagram": f"‚ú® {product_name} ‚ú®\n\n{', '.join(product_features[:3])}\n\n#producto #{product_category}",
        "facebook": f"Descubre {product_name}. {product_features[0] if product_features else 'Calidad garantizada'}.",
        "twitter": f"{product_name}: {product_features[0] if product_features else 'Nuevo producto disponible'}"
    }
    
    return {
        "alt_text": alt_text,
        "image_titles": image_titles,
        "video_descriptions": video_descriptions,
        "social_captions": social_captions,
        "metadata": {
            "product_name": product_name,
            "category": product_category,
            "generated_at": datetime.utcnow().isoformat()
        }
    }


def generate_seo_meta_tags(
    product_name: str,
    description_data: Dict[str, Any],
    product_category: str = "general",
    product_price: float = None,
    product_image_url: str = None
) -> Dict[str, Any]:
    """
    Genera meta tags optimizados para SEO.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        product_category: Categor√≠a del producto
        product_price: Precio del producto
        product_image_url: URL de imagen del producto
    
    Returns:
        Dict con meta tags generados
    """
    short_desc = description_data.get("short_description", "")
    keywords = description_data.get("seo_keywords", [])
    
    # Meta tags b√°sicos
    meta_tags = {
        "title": f"{product_name} | {product_category.title()}",
        "description": short_desc[:160] if len(short_desc) > 160 else short_desc,
        "keywords": ", ".join(keywords[:10]),
        "og:title": product_name,
        "og:description": short_desc[:200],
        "og:type": "product",
        "og:image": product_image_url or "",
        "twitter:card": "summary_large_image",
        "twitter:title": product_name,
        "twitter:description": short_desc[:200]
    }
    
    # Schema.org structured data
    schema_data = {
        "@context": "https://schema.org/",
        "@type": "Product",
        "name": product_name,
        "description": description_data.get("description", ""),
        "category": product_category,
        "offers": {
            "@type": "Offer",
            "price": str(product_price) if product_price else "",
            "priceCurrency": "USD"
        } if product_price else None
    }
    
    # HTML meta tags string
    html_meta_tags = "\n".join([
        f'<meta name="{k}" content="{v}">' 
        for k, v in meta_tags.items() 
        if not k.startswith("og:") and not k.startswith("twitter:")
    ])
    
    html_meta_tags += "\n" + "\n".join([
        f'<meta property="{k}" content="{v}">' 
        for k, v in meta_tags.items() 
        if k.startswith("og:")
    ])
    
    html_meta_tags += "\n" + "\n".join([
        f'<meta name="{k}" content="{v}">' 
        for k, v in meta_tags.items() 
        if k.startswith("twitter:")
    ])
    
    html_meta_tags += f'\n<script type="application/ld+json">\n{json.dumps(schema_data, indent=2, ensure_ascii=False)}\n</script>'
    
    return {
        "meta_tags": meta_tags,
        "schema_data": schema_data,
        "html_meta_tags": html_meta_tags,
        "metadata": {
            "generated_at": datetime.utcnow().isoformat(),
            "product_name": product_name
        }
    }


def generate_email_marketing_content(
    product_name: str,
    description_data: Dict[str, Any],
    email_type: str = "promotional",
    buyer_type: str = "general"
) -> Dict[str, Any]:
    """
    Genera contenido optimizado para email marketing.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        email_type: Tipo de email (promotional, newsletter, abandoned_cart, welcome)
        buyer_type: Tipo de comprador
    
    Returns:
        Dict con contenido de email generado
    """
    short_desc = description_data.get("short_description", "")
    benefits = description_data.get("benefits", [])
    cta = description_data.get("call_to_action", "Descubre m√°s")
    
    email_templates = {
        "promotional": {
            "subject": f"üéâ Descubre {product_name} - Oferta Especial",
            "preheader": short_desc[:100],
            "header": f"¬°Hola! üëã",
            "body": f"""
            <p>Tenemos algo especial para ti: <strong>{product_name}</strong></p>
            <p>{short_desc}</p>
            <ul>
            """ + "\n".join([f"<li>{b}</li>" for b in benefits[:3]]) + """
            </ul>
            <p>No te pierdas esta oportunidad √∫nica.</p>
            """,
            "cta_text": cta,
            "cta_url": "#",
            "footer": "Gracias por confiar en nosotros."
        },
        "newsletter": {
            "subject": f"üì∞ Nuevo: {product_name}",
            "preheader": f"Conoce nuestro nuevo producto",
            "header": "Novedades",
            "body": f"""
            <h2>{product_name}</h2>
            <p>{description_data.get('description', '')[:300]}...</p>
            <p><a href="#">Leer m√°s</a></p>
            """,
            "cta_text": "Ver producto",
            "cta_url": "#"
        },
        "abandoned_cart": {
            "subject": f"‚è∞ Te esperamos - {product_name}",
            "preheader": "Tu carrito te est√° esperando",
            "header": "¬°No te lo pierdas!",
            "body": f"""
            <p>Vimos que te interesaste en <strong>{product_name}</strong>.</p>
            <p>{short_desc}</p>
            <p>Completa tu compra ahora y obt√©n env√≠o gratis.</p>
            """,
            "cta_text": "Completar compra",
            "cta_url": "#"
        },
        "welcome": {
            "subject": f"Bienvenido - Conoce {product_name}",
            "preheader": "Gracias por unirte",
            "header": "¬°Bienvenido!",
            "body": f"""
            <p>Gracias por unirte a nosotros.</p>
            <p>Como nuevo miembro, queremos presentarte <strong>{product_name}</strong>:</p>
            <p>{short_desc}</p>
            """,
            "cta_text": "Explorar",
            "cta_url": "#"
        }
    }
    
    template = email_templates.get(email_type, email_templates["promotional"])
    
    return {
        "email_type": email_type,
        "subject": template["subject"],
        "preheader": template["preheader"],
        "content": {
            "header": template["header"],
            "body": template["body"],
            "cta": {
                "text": template["cta_text"],
                "url": template["cta_url"]
            },
            "footer": template.get("footer", "")
        },
        "metadata": {
            "product_name": product_name,
            "buyer_type": buyer_type,
            "generated_at": datetime.utcnow().isoformat()
        }
    }


def version_product_description(
    product_id: str,
    description_data: Dict[str, Any],
    version_notes: str = "",
    conn_id: str = None
) -> Dict[str, Any]:
    """
    Crea una versi√≥n de una descripci√≥n para tracking y rollback.
    
    Args:
        product_id: ID del producto
        description_data: Datos de descripci√≥n
        version_notes: Notas sobre esta versi√≥n
        conn_id: ID de conexi√≥n PostgreSQL
    
    Returns:
        Dict con informaci√≥n de versi√≥n creada
    """
    try:
        if not conn_id:
            return {"success": False, "error": "conn_id requerido"}
        
        hook = PostgresHook(postgres_conn_id=conn_id)
        with hook.get_conn() as conn:
            with conn.cursor() as cur:
                # Crear tabla de versiones si no existe
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS product_description_versions (
                        id SERIAL PRIMARY KEY,
                        product_id VARCHAR(255) NOT NULL,
                        version_number INT NOT NULL,
                        description_data JSONB NOT NULL,
                        version_notes TEXT,
                        created_at TIMESTAMPTZ DEFAULT NOW(),
                        created_by VARCHAR(255),
                        is_active BOOLEAN DEFAULT FALSE,
                        UNIQUE(product_id, version_number)
                    );
                    CREATE INDEX IF NOT EXISTS idx_desc_versions_product 
                    ON product_description_versions(product_id);
                    CREATE INDEX IF NOT EXISTS idx_desc_versions_active 
                    ON product_description_versions(product_id, is_active) 
                    WHERE is_active = TRUE;
                """)
                
                # Obtener siguiente n√∫mero de versi√≥n
                cur.execute("""
                    SELECT COALESCE(MAX(version_number), 0) + 1
                    FROM product_description_versions
                    WHERE product_id = %s
                """, (product_id,))
                next_version = cur.fetchone()[0]
                
                # Desactivar versiones anteriores
                cur.execute("""
                    UPDATE product_description_versions
                    SET is_active = FALSE
                    WHERE product_id = %s
                """, (product_id,))
                
                # Insertar nueva versi√≥n
                cur.execute("""
                    INSERT INTO product_description_versions
                    (product_id, version_number, description_data, version_notes, is_active)
                    VALUES (%s, %s, %s, %s, TRUE)
                """, (product_id, next_version, json.dumps(description_data), version_notes))
                
                conn.commit()
                
                return {
                    "success": True,
                    "product_id": product_id,
                    "version_number": next_version,
                    "version_notes": version_notes,
                    "created_at": datetime.utcnow().isoformat()
                }
                
    except Exception as e:
        logger.error("description_versioning_error", 
                    product_id=product_id, 
                    error=str(e))
        return {"success": False, "error": str(e)}


def get_description_version(
    product_id: str,
    version_number: int = None,
    conn_id: str = None
) -> Optional[Dict[str, Any]]:
    """
    Obtiene una versi√≥n espec√≠fica de descripci√≥n.
    
    Args:
        product_id: ID del producto
        version_number: N√∫mero de versi√≥n (None para versi√≥n activa)
        conn_id: ID de conexi√≥n PostgreSQL
    
    Returns:
        Datos de descripci√≥n de la versi√≥n solicitada
    """
    try:
        if not conn_id:
            return None
        
        hook = PostgresHook(postgres_conn_id=conn_id)
        with hook.get_conn() as conn:
            with conn.cursor() as cur:
                if version_number:
                    cur.execute("""
                        SELECT description_data, version_number, created_at, version_notes
                        FROM product_description_versions
                        WHERE product_id = %s AND version_number = %s
                    """, (product_id, version_number))
                else:
                    cur.execute("""
                        SELECT description_data, version_number, created_at, version_notes
                        FROM product_description_versions
                        WHERE product_id = %s AND is_active = TRUE
                        ORDER BY version_number DESC
                        LIMIT 1
                    """, (product_id,))
                
                result = cur.fetchone()
                if result:
                    return {
                        "description_data": json.loads(result[0]),
                        "version_number": result[1],
                        "created_at": result[2].isoformat() if result[2] else None,
                        "version_notes": result[3]
                    }
        
        return None
        
    except Exception as e:
        logger.warning("get_description_version_error", 
                      product_id=product_id, 
                      error=str(e))
        return None


def analyze_keyword_competition(
    keywords: List[str],
    product_category: str = "general"
) -> Dict[str, Any]:
    """
    Analiza la competencia de palabras clave.
    
    Args:
        keywords: Lista de palabras clave a analizar
        product_category: Categor√≠a del producto
    
    Returns:
        Dict con an√°lisis de competencia de keywords
    """
    # Simulaci√≥n de an√°lisis (en producci√≥n usar√≠a APIs como Google Keyword Planner)
    keyword_analysis = []
    
    for keyword in keywords:
        # Simular dificultad (en producci√≥n vendr√≠a de API)
        difficulty = hash(keyword) % 100  # Simulaci√≥n
        
        keyword_analysis.append({
            "keyword": keyword,
            "difficulty": difficulty,
            "competition_level": (
                "high" if difficulty > 70 else
                "medium" if difficulty > 40 else
                "low"
            ),
            "estimated_monthly_searches": hash(keyword + "searches") % 10000,
            "recommendation": (
                "Usar como keyword secundario" if difficulty > 70 else
                "Excelente keyword principal" if difficulty < 30 else
                "Keyword viable con optimizaci√≥n"
            )
        })
    
    # Ordenar por dificultad
    keyword_analysis.sort(key=lambda x: x["difficulty"])
    
    return {
        "keywords_analyzed": len(keywords),
        "keyword_analysis": keyword_analysis,
        "recommended_primary_keywords": [k["keyword"] for k in keyword_analysis[:3] if k["difficulty"] < 50],
        "recommended_secondary_keywords": [k["keyword"] for k in keyword_analysis if 30 <= k["difficulty"] <= 70],
        "metadata": {
            "category": product_category,
            "analyzed_at": datetime.utcnow().isoformat()
        }
    }


# ============================================================================
# FUNCIONALIDADES AVANZADAS FINALES
# ============================================================================

def translate_product_description(
    description_data: Dict[str, Any],
    target_language: str = "en",
    source_language: str = "es",
    use_llm: bool = True
) -> Dict[str, Any]:
    """
    Traduce descripciones de productos a m√∫ltiples idiomas.
    
    Args:
        description_data: Datos de descripci√≥n a traducir
        target_language: Idioma destino (es, en, pt, fr, de, it, etc.)
        source_language: Idioma origen (default: es)
        use_llm: Usar LLM para traducci√≥n (m√°s precisa)
    
    Returns:
        Dict con descripci√≥n traducida
    """
    try:
        if use_llm:
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if openai_api_key:
                translated = _translate_with_llm(
                    description_data, target_language, source_language, openai_api_key
                )
                if translated:
                    return translated
        
        # Fallback a traducci√≥n b√°sica
        return _translate_basic(description_data, target_language, source_language)
        
    except Exception as e:
        logger.error("translation_error", error=str(e))
        return description_data  # Retornar original en caso de error


def _translate_with_llm(
    description_data: Dict[str, Any],
    target_language: str,
    source_language: str,
    api_key: str
) -> Optional[Dict[str, Any]]:
    """Traduce usando LLM."""
    try:
        language_names = {
            "es": "espa√±ol", "en": "ingl√©s", "pt": "portugu√©s", "fr": "franc√©s",
            "de": "alem√°n", "it": "italiano", "zh": "chino", "ja": "japon√©s"
        }
        
        target_name = language_names.get(target_language, target_language)
        source_name = language_names.get(source_language, source_language)
        
        prompt = f"""Traduce el siguiente contenido de producto de {source_name} a {target_name}.
Mant√©n el tono, estilo y formato. Preserva las palabras clave importantes.

Contenido a traducir:
- Descripci√≥n: {description_data.get('description', '')}
- Descripci√≥n corta: {description_data.get('short_description', '')}
- Call to action: {description_data.get('call_to_action', '')}

Formato JSON:
{{
    "description": "...",
    "short_description": "...",
    "call_to_action": "..."
}}"""
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            "messages": [
                {
                    "role": "system",
                    "content": f"Eres un traductor profesional especializado en e-commerce. Traduce manteniendo el tono persuasivo y las palabras clave SEO. Responde siempre en formato JSON v√°lido."
                },
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.3,
            "max_tokens": 1000
        }
        
        response = httpx.post(
            "https://api.openai.com/v1/chat/completions",
            json=payload,
            headers=headers,
            timeout=30.0
        )
        
        if response.status_code == 200:
            data = response.json()
            content = data["choices"][0]["message"]["content"]
            
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                translated = json.loads(json_match.group())
                return {
                    **description_data,
                    "description": translated.get("description", description_data.get("description", "")),
                    "short_description": translated.get("short_description", description_data.get("short_description", "")),
                    "call_to_action": translated.get("call_to_action", description_data.get("call_to_action", "")),
                    "translation_metadata": {
                        "source_language": source_language,
                        "target_language": target_language,
                        "translated_at": datetime.utcnow().isoformat(),
                        "method": "llm"
                    }
                }
        
        return None
        
    except Exception as e:
        logger.warning("llm_translation_failed", error=str(e))
        return None


def _translate_basic(
    description_data: Dict[str, Any],
    target_language: str,
    source_language: str
) -> Dict[str, Any]:
    """Traducci√≥n b√°sica (placeholder - en producci√≥n usar√≠a servicio real)."""
    # En producci√≥n, aqu√≠ se usar√≠a un servicio de traducci√≥n real
    return {
        **description_data,
        "translation_metadata": {
            "source_language": source_language,
            "target_language": target_language,
            "translated_at": datetime.utcnow().isoformat(),
            "method": "basic",
            "note": "Traducci√≥n b√°sica - usar LLM para mejor calidad"
        }
    }


def generate_ad_content(
    product_name: str,
    description_data: Dict[str, Any],
    ad_platform: str = "google_ads",
    buyer_type: str = "general"
) -> Dict[str, Any]:
    """
    Genera contenido optimizado para plataformas de anuncios.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        ad_platform: Plataforma (google_ads, facebook_ads, instagram_ads, linkedin_ads)
        buyer_type: Tipo de comprador
    
    Returns:
        Dict con contenido de anuncio generado
    """
    short_desc = description_data.get("short_description", "")
    benefits = description_data.get("benefits", [])
    keywords = description_data.get("seo_keywords", [])
    
    platform_configs = {
        "google_ads": {
            "headline_max_length": 30,
            "description_max_length": 90,
            "headlines_count": 3,
            "descriptions_count": 2,
            "path1_max": 15,
            "path2_max": 15
        },
        "facebook_ads": {
            "headline_max_length": 40,
            "description_max_length": 125,
            "primary_text_max": 125,
            "link_description_max": 30
        },
        "instagram_ads": {
            "headline_max_length": 40,
            "description_max_length": 125,
            "primary_text_max": 125
        },
        "linkedin_ads": {
            "headline_max_length": 75,
            "description_max_length": 75,
            "intro_max": 150
        }
    }
    
    config = platform_configs.get(ad_platform, platform_configs["google_ads"])
    
    # Generar headlines
    headlines = []
    if ad_platform == "google_ads":
        headlines = [
            f"{product_name} - {benefits[0] if benefits else 'Calidad Premium'}",
            f"Descubre {product_name}",
            f"{product_name} - Oferta Especial"
        ]
    elif ad_platform in ["facebook_ads", "instagram_ads"]:
        headlines = [
            f"‚ú® {product_name} ‚ú®",
            f"{product_name} - {benefits[0] if benefits else 'Nuevo'}",
            f"Descubre {product_name}"
        ]
    else:  # linkedin
        headlines = [
            f"{product_name} - Soluci√≥n Profesional",
            f"Mejora tu negocio con {product_name}"
        ]
    
    # Truncar headlines seg√∫n l√≠mites
    headlines = [h[:config["headline_max_length"]] for h in headlines[:config.get("headlines_count", 3)]]
    
    # Generar descripciones
    descriptions = []
    if ad_platform == "google_ads":
        descriptions = [
            short_desc[:config["description_max_length"]],
            f"{benefits[0] if benefits else 'Calidad garantizada'}. {short_desc[:60]}"
        ]
    else:
        descriptions = [
            short_desc[:config["description_max_length"]],
            f"{', '.join(benefits[:2]) if benefits else 'Beneficios exclusivos'}"
        ]
    
    descriptions = [d[:config["description_max_length"]] for d in descriptions[:config.get("descriptions_count", 2)]]
    
    # Generar paths (solo Google Ads)
    paths = []
    if ad_platform == "google_ads":
        if keywords:
            paths = [
                keywords[0][:config["path1_max"]] if len(keywords[0]) <= config["path1_max"] else keywords[0][:config["path1_max"]-3] + "...",
                "Comprar"  # En producci√≥n se detectar√≠a el idioma
            ]
    
    return {
        "platform": ad_platform,
        "headlines": headlines,
        "descriptions": descriptions,
        "paths": paths if paths else None,
        "keywords": keywords[:10],
        "final_url": "#",  # En producci√≥n vendr√≠a del producto
        "metadata": {
            "product_name": product_name,
            "buyer_type": buyer_type,
            "generated_at": datetime.utcnow().isoformat()
        }
    }


def analyze_market_trends(
    product_category: str,
    keywords: List[str],
    time_period: str = "30d"
) -> Dict[str, Any]:
    """
    Analiza tendencias de mercado para productos.
    
    Args:
        product_category: Categor√≠a del producto
        keywords: Lista de palabras clave relacionadas
        time_period: Per√≠odo de an√°lisis (7d, 30d, 90d, 1y)
    
    Returns:
        Dict con an√°lisis de tendencias
    """
    # Simulaci√≥n de an√°lisis de tendencias (en producci√≥n usar√≠a APIs reales)
    trends = {
        "category": product_category,
        "time_period": time_period,
        "trend_direction": "up",  # up, down, stable
        "trend_strength": 75,  # 0-100
        "popular_keywords": keywords[:5],
        "emerging_keywords": keywords[5:10] if len(keywords) > 5 else [],
        "seasonality": "none",  # none, high, medium, low
        "competition_level": "medium",
        "recommendations": []
    }
    
    # Simular an√°lisis b√°sico
    if "premium" in " ".join(keywords).lower():
        trends["trend_direction"] = "up"
        trends["trend_strength"] = 85
        trends["recommendations"].append("El mercado premium est√° en crecimiento")
    
    if "eco" in " ".join(keywords).lower() or "sostenible" in " ".join(keywords).lower():
        trends["trend_direction"] = "up"
        trends["trend_strength"] = 90
        trends["recommendations"].append("Productos ecol√≥gicos tienen alta demanda")
        trends["seasonality"] = "high"
    
    return {
        **trends,
        "metadata": {
            "analyzed_at": datetime.utcnow().isoformat(),
            "analysis_method": "simulated"  # En producci√≥n: "google_trends", "semrush", etc.
        }
    }


def personalize_description_for_audience(
    description_data: Dict[str, Any],
    audience_profile: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Personaliza descripci√≥n bas√°ndose en perfil de audiencia.
    
    Args:
        description_data: Datos de descripci√≥n base
        audience_profile: Perfil de audiencia con:
            - age_range: "18-25", "26-35", "36-50", "50+"
            - interests: Lista de intereses
            - location: Ubicaci√≥n
            - language: Idioma
            - buying_behavior: "impulse", "researched", "price_sensitive"
    
    Returns:
        Dict con descripci√≥n personalizada
    """
    age_range = audience_profile.get("age_range", "26-35")
    interests = audience_profile.get("interests", [])
    buying_behavior = audience_profile.get("buying_behavior", "researched")
    
    description = description_data.get("description", "")
    short_desc = description_data.get("short_description", "")
    
    # Personalizar seg√∫n edad
    if age_range in ["18-25", "26-35"]:
        description = description.replace("Descubre", "Explora")
        description = description.replace("producto", "producto genial")
    elif age_range == "50+":
        description = description.replace("innovador", "confiable")
        description = description.replace("moderno", "probado")
    
    # Personalizar seg√∫n comportamiento de compra
    if buying_behavior == "price_sensitive":
        if "econ√≥mico" not in description.lower():
            description = f"Precio especial: {description}"
    elif buying_behavior == "impulse":
        if "ahora" not in description.lower():
            description = f"{description} ¬°Compra ahora!"
    
    # Personalizar seg√∫n intereses
    if "tecnolog√≠a" in interests or "tech" in interests:
        description = description.replace("caracter√≠sticas", "especificaciones t√©cnicas")
    
    return {
        **description_data,
        "description": description,
        "short_description": short_desc,
        "personalization": {
            "audience_profile": audience_profile,
            "personalized_at": datetime.utcnow().isoformat(),
            "changes_applied": [
                f"Personalizado para edad: {age_range}",
                f"Comportamiento: {buying_behavior}",
                f"Intereses: {', '.join(interests[:3])}"
            ]
        }
    }


def generate_brand_voice_analysis(
    descriptions: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    Analiza la voz de marca a partir de m√∫ltiples descripciones.
    
    Args:
        descriptions: Lista de descripciones de productos
    
    Returns:
        Dict con an√°lisis de voz de marca
    """
    all_text = " ".join([d.get("description", "") for d in descriptions])
    all_text_lower = all_text.lower()
    
    # Analizar tono
    formal_words = ["usted", "garantizamos", "ofrecemos", "proporcionamos"]
    casual_words = ["t√∫", "genial", "incre√≠ble", "s√∫per"]
    technical_words = ["especificaciones", "tecnolog√≠a", "algoritmo", "protocolo"]
    emotional_words = ["amor", "pasi√≥n", "sue√±o", "felicidad"]
    
    formal_count = sum(1 for w in formal_words if w in all_text_lower)
    casual_count = sum(1 for w in casual_words if w in all_text_lower)
    technical_count = sum(1 for w in technical_words if w in all_text_lower)
    emotional_count = sum(1 for w in emotional_words if w in all_text_lower)
    
    # Determinar voz dominante
    voice_scores = {
        "formal": formal_count,
        "casual": casual_count,
        "technical": technical_count,
        "emotional": emotional_count
    }
    
    dominant_voice = max(voice_scores.items(), key=lambda x: x[1])[0]
    
    # Analizar palabras m√°s usadas
    words = all_text_lower.split()
    word_freq = {}
    for word in words:
        if len(word) > 4:  # Palabras significativas
            word_freq[word] = word_freq.get(word, 0) + 1
    
    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
    
    return {
        "voice_analysis": {
            "dominant_voice": dominant_voice,
            "voice_scores": voice_scores,
            "voice_consistency": "high" if max(voice_scores.values()) > sum(voice_scores.values()) * 0.4 else "medium"
        },
        "top_words": [{"word": w[0], "frequency": w[1]} for w in top_words],
        "recommendations": _get_brand_voice_recommendations(voice_scores, dominant_voice),
        "metadata": {
            "descriptions_analyzed": len(descriptions),
            "analyzed_at": datetime.utcnow().isoformat()
        }
    }


def _get_brand_voice_recommendations(
    voice_scores: Dict[str, int],
    dominant_voice: str
) -> List[str]:
    """Genera recomendaciones basadas en an√°lisis de voz."""
    recommendations = []
    
    total = sum(voice_scores.values())
    if total == 0:
        return ["No se pudo analizar la voz de marca"]
    
    dominant_percentage = (voice_scores[dominant_voice] / total) * 100
    
    if dominant_percentage < 30:
        recommendations.append("La voz de marca no es consistente. Considera estandarizar el tono.")
    
    if voice_scores["formal"] > voice_scores["casual"] * 2:
        recommendations.append("Voz muy formal. Considera hacerla m√°s accesible para audiencias j√≥venes.")
    
    if voice_scores["technical"] > voice_scores["emotional"] * 3:
        recommendations.append("Enfoque muy t√©cnico. Considera agregar m√°s elementos emocionales.")
    
    recommendations.append(f"Voz dominante: {dominant_voice}. Mantener consistencia en este tono.")
    
    return recommendations


def generate_synthetic_reviews(
    product_name: str,
    product_features: List[str],
    num_reviews: int = 5,
    rating_distribution: Dict[int, int] = None
) -> Dict[str, Any]:
    """
    Genera reviews/testimonios sint√©ticos para productos.
    
    Args:
        product_name: Nombre del producto
        product_features: Lista de caracter√≠sticas
        num_reviews: N√∫mero de reviews a generar
        rating_distribution: Distribuci√≥n de ratings {5: 3, 4: 1, 3: 1} (opcional)
    
    Returns:
        Dict con reviews generadas
    """
    if not rating_distribution:
        # Distribuci√≥n por defecto: mayor√≠a positivos
        rating_distribution = {5: num_reviews - 1, 4: 1}
    
    reviews = []
    review_templates = {
        5: [
            f"Excelente producto. {product_name} super√≥ mis expectativas. {product_features[0] if product_features else 'Muy recomendado'}.",
            f"¬°Incre√≠ble! {product_name} es exactamente lo que necesitaba. Calidad premium.",
            f"5 estrellas sin duda. {product_name} es perfecto para lo que buscaba."
        ],
        4: [
            f"Buen producto. {product_name} cumple con lo esperado. {product_features[0] if product_features else 'Recomendado'}.",
            f"Me gusta {product_name}. Buena relaci√≥n calidad-precio."
        ],
        3: [
            f"{product_name} est√° bien, pero esperaba m√°s. Funciona correctamente.",
            f"Producto aceptable. {product_name} cumple funciones b√°sicas."
        ]
    }
    
    for rating, count in rating_distribution.items():
        templates = review_templates.get(rating, review_templates[4])
        for i in range(min(count, num_reviews)):
            review_text = templates[i % len(templates)]
            reviews.append({
                "rating": rating,
                "review_text": review_text,
                "author": f"Cliente {i+1}",
                "verified_purchase": True,
                "helpful_count": hash(review_text) % 50
            })
    
    # Ordenar por rating (m√°s altos primero)
    reviews.sort(key=lambda x: x["rating"], reverse=True)
    
    return {
        "reviews": reviews[:num_reviews],
        "total_reviews": len(reviews),
        "average_rating": sum(r["rating"] for r in reviews) / len(reviews) if reviews else 0,
        "rating_distribution": rating_distribution,
        "metadata": {
            "product_name": product_name,
            "generated_at": datetime.utcnow().isoformat(),
            "note": "Reviews sint√©ticas generadas autom√°ticamente"
        }
    }


# ============================================================================
# FUNCIONALIDADES AVANZADAS ADICIONALES FINALES
# ============================================================================

def generate_chatbot_content(
    product_name: str,
    description_data: Dict[str, Any],
    chatbot_type: str = "sales"
) -> Dict[str, Any]:
    """
    Genera contenido optimizado para chatbots y asistentes de IA.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        chatbot_type: Tipo de chatbot (sales, support, faq)
    
    Returns:
        Dict con contenido para chatbot
    """
    short_desc = description_data.get("short_description", "")
    benefits = description_data.get("benefits", [])
    features = description_data.get("benefits", [])  # Usar benefits como features
    
    chatbot_templates = {
        "sales": {
            "greeting": f"¬°Hola! üëã ¬øTe interesa conocer m√°s sobre {product_name}?",
            "product_intro": f"{product_name} es {short_desc}",
            "key_benefits": f"Los beneficios principales son: {', '.join(benefits[:3])}",
            "cta": "¬øTe gustar√≠a saber m√°s detalles o hacer una pregunta espec√≠fica?",
            "pricing_inquiry": f"¬øTe gustar√≠a conocer el precio de {product_name}?",
            "comparison": f"¬øQuieres comparar {product_name} con otros productos?"
        },
        "support": {
            "greeting": f"Hola, ¬øen qu√© puedo ayudarte con {product_name}?",
            "troubleshooting": f"Para {product_name}, los problemas m√°s comunes son...",
            "features_help": f"{product_name} incluye: {', '.join(features[:3])}",
            "warranty": f"{product_name} incluye garant√≠a del fabricante.",
            "return_policy": "Ofrecemos pol√≠tica de devoluci√≥n dentro de 30 d√≠as."
        },
        "faq": {
            "common_questions": [
                f"¬øQu√© es {product_name}?",
                f"¬øCu√°les son las caracter√≠sticas de {product_name}?",
                f"¬øC√≥mo funciona {product_name}?",
                f"¬øCu√°l es el precio de {product_name}?",
                f"¬øOfrecen garant√≠a para {product_name}?"
            ],
            "answers": [
                f"{product_name} es {short_desc}",
                f"{product_name} incluye: {', '.join(features[:3])}",
                f"{product_name} funciona de manera simple e intuitiva.",
                "El precio var√≠a seg√∫n el plan. ¬øTe gustar√≠a conocer m√°s detalles?",
                f"S√≠, {product_name} incluye garant√≠a del fabricante."
            ]
        }
    }
    
    template = chatbot_templates.get(chatbot_type, chatbot_templates["sales"])
    
    return {
        "chatbot_type": chatbot_type,
        "content": template,
        "quick_replies": [
            "M√°s informaci√≥n",
            "Precio",
            "Comparar",
            "Contactar ventas"
        ],
        "metadata": {
            "product_name": product_name,
            "generated_at": datetime.utcnow().isoformat()
        }
    }


def analyze_description_roi(
    description_data: Dict[str, Any],
    performance_metrics: Dict[str, Any],
    cost_data: Dict[str, Any] = None
) -> Dict[str, Any]:
    """
    Analiza el ROI (Return on Investment) de una descripci√≥n.
    
    Args:
        description_data: Datos de descripci√≥n
        performance_metrics: M√©tricas de rendimiento:
            - views: Vistas de la p√°gina
            - conversions: Conversiones
            - revenue: Ingresos generados
            - avg_order_value: Valor promedio de orden
        cost_data: Datos de costos (opcional):
            - generation_cost: Costo de generaci√≥n
            - maintenance_cost: Costo de mantenimiento
    
    Returns:
        Dict con an√°lisis de ROI
    """
    views = performance_metrics.get("views", 0)
    conversions = performance_metrics.get("conversions", 0)
    revenue = performance_metrics.get("revenue", 0)
    avg_order_value = performance_metrics.get("avg_order_value", 0)
    
    # Calcular m√©tricas b√°sicas
    conversion_rate = (conversions / views * 100) if views > 0 else 0
    revenue_per_view = (revenue / views) if views > 0 else 0
    
    # Calcular costos
    generation_cost = cost_data.get("generation_cost", 0) if cost_data else 0
    maintenance_cost = cost_data.get("maintenance_cost", 0) if cost_data else 0
    total_cost = generation_cost + maintenance_cost
    
    # Calcular ROI
    roi = ((revenue - total_cost) / total_cost * 100) if total_cost > 0 else float('inf')
    
    # Calcular score de efectividad
    effectiveness_score = min(100, (
        (conversion_rate * 0.4) +
        (min(revenue_per_view * 10, 30)) +
        (30 if conversion_rate > 2 else 0)
    ))
    
    return {
        "performance_metrics": {
            "views": views,
            "conversions": conversions,
            "conversion_rate": round(conversion_rate, 2),
            "revenue": revenue,
            "revenue_per_view": round(revenue_per_view, 2),
            "avg_order_value": avg_order_value
        },
        "cost_analysis": {
            "generation_cost": generation_cost,
            "maintenance_cost": maintenance_cost,
            "total_cost": total_cost
        },
        "roi_analysis": {
            "roi_percentage": round(roi, 2) if roi != float('inf') else "N/A",
            "net_profit": revenue - total_cost,
            "roi_category": (
                "excellent" if roi > 500 else
                "very_good" if roi > 200 else
                "good" if roi > 100 else
                "acceptable" if roi > 50 else
                "poor" if roi > 0 else
                "negative"
            )
        },
        "effectiveness_score": round(effectiveness_score, 2),
        "recommendations": _get_roi_recommendations(conversion_rate, revenue_per_view, roi),
        "metadata": {
            "analyzed_at": datetime.utcnow().isoformat()
        }
    }


def _get_roi_recommendations(
    conversion_rate: float,
    revenue_per_view: float,
    roi: float
) -> List[str]:
    """Genera recomendaciones basadas en ROI."""
    recommendations = []
    
    if conversion_rate < 1.0:
        recommendations.append("Baja tasa de conversi√≥n. Considera optimizar el CTA y agregar elementos de urgencia.")
    
    if revenue_per_view < 0.10:
        recommendations.append("Bajo ingreso por vista. Considera aumentar el valor promedio de orden.")
    
    if isinstance(roi, float) and roi < 100:
        recommendations.append("ROI bajo. Considera optimizar la descripci√≥n o reducir costos.")
    
    if conversion_rate > 3.0 and revenue_per_view > 0.50:
        recommendations.append("Excelente rendimiento. Considera escalar esta descripci√≥n a m√°s productos.")
    
    return recommendations


def generate_landing_page_content(
    product_name: str,
    description_data: Dict[str, Any],
    page_type: str = "product"
) -> Dict[str, Any]:
    """
    Genera contenido completo para landing pages.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        page_type: Tipo de p√°gina (product, campaign, lead_magnet)
    
    Returns:
        Dict con contenido de landing page
    """
    short_desc = description_data.get("short_description", "")
    full_desc = description_data.get("description", "")
    benefits = description_data.get("benefits", [])
    cta = description_data.get("call_to_action", "Descubre m√°s")
    keywords = description_data.get("seo_keywords", [])
    
    hero_section = {
        "headline": f"Descubre {product_name}",
        "subheadline": short_desc,
        "cta_primary": cta,
        "cta_secondary": "Ver m√°s informaci√≥n",
        "trust_badges": ["Garant√≠a de satisfacci√≥n", "Env√≠o gratis", "Soporte 24/7"]
    }
    
    features_section = {
        "title": "Caracter√≠sticas principales",
        "features": benefits[:6],
        "layout": "grid"  # grid, list, cards
    }
    
    benefits_section = {
        "title": "Beneficios",
        "benefits": [
            {
                "icon": "‚úì",
                "title": benefit,
                "description": f"{benefit} para mejorar tu experiencia."
            }
            for benefit in benefits[:4]
        ]
    }
    
    social_proof = {
        "testimonials": [
            {
                "quote": f"Excelente producto. {product_name} super√≥ mis expectativas.",
                "author": "Cliente satisfecho",
                "rating": 5
            }
        ],
        "stats": [
            {"label": "Clientes satisfechos", "value": "10,000+"},
            {"label": "Rating promedio", "value": "4.8/5"},
            {"label": "A√±os de experiencia", "value": "5+"}
        ]
    }
    
    faq_section = {
        "title": "Preguntas frecuentes",
        "faqs": [
            {
                "question": f"¬øQu√© incluye {product_name}?",
                "answer": f"{product_name} incluye {', '.join(benefits[:3])}."
            },
            {
                "question": "¬øOfrecen garant√≠a?",
                "answer": "S√≠, ofrecemos garant√≠a del fabricante."
            }
        ]
    }
    
    return {
        "page_type": page_type,
        "hero_section": hero_section,
        "features_section": features_section,
        "benefits_section": benefits_section,
        "social_proof": social_proof,
        "faq_section": faq_section,
        "seo_optimization": {
            "meta_title": f"{product_name} | {keywords[0] if keywords else 'Producto'}",
            "meta_description": short_desc[:160],
            "h1": f"Descubre {product_name}",
            "keywords": keywords
        },
        "metadata": {
            "product_name": product_name,
            "generated_at": datetime.utcnow().isoformat()
        }
    }


def setup_ab_test_descriptions(
    product_id: str,
    descriptions: List[Dict[str, Any]],
    test_name: str = None,
    conn_id: str = None
) -> Dict[str, Any]:
    """
    Configura un test A/B para m√∫ltiples descripciones.
    
    Args:
        product_id: ID del producto
        descriptions: Lista de descripciones a testear
        test_name: Nombre del test (opcional)
        conn_id: ID de conexi√≥n PostgreSQL
    
    Returns:
        Dict con configuraci√≥n del test A/B
    """
    try:
        if not test_name:
            test_name = f"ab_test_{product_id}_{datetime.utcnow().strftime('%Y%m%d')}"
        
        variants = []
        for i, desc in enumerate(descriptions, 1):
            score = score_product_description(
                desc.get("description", ""),
                buyer_type="general"
            )
            variants.append({
                "variant_id": f"variant_{i}",
                "description": desc,
                "initial_score": score["overall_score"],
                "traffic_percentage": 100 / len(descriptions),  # Distribuci√≥n equitativa
                "conversions": 0,
                "views": 0
            })
        
        ab_test_config = {
            "test_id": test_name,
            "product_id": product_id,
            "variants": variants,
            "status": "active",
            "start_date": datetime.utcnow().isoformat(),
            "end_date": None,
            "traffic_split": "equal"  # equal, weighted
        }
        
        # Guardar en BD si hay conn_id
        if conn_id:
            try:
                hook = PostgresHook(postgres_conn_id=conn_id)
                with hook.get_conn() as conn:
                    with conn.cursor() as cur:
                        cur.execute("""
                            CREATE TABLE IF NOT EXISTS ab_test_descriptions (
                                test_id VARCHAR(255) PRIMARY KEY,
                                product_id VARCHAR(255) NOT NULL,
                                test_config JSONB NOT NULL,
                                status VARCHAR(50) DEFAULT 'active',
                                created_at TIMESTAMPTZ DEFAULT NOW(),
                                updated_at TIMESTAMPTZ DEFAULT NOW()
                            );
                            CREATE INDEX IF NOT EXISTS idx_ab_test_product 
                            ON ab_test_descriptions(product_id);
                        """)
                        
                        cur.execute("""
                            INSERT INTO ab_test_descriptions
                            (test_id, product_id, test_config, status)
                            VALUES (%s, %s, %s, %s)
                            ON CONFLICT (test_id) DO UPDATE SET
                                test_config = EXCLUDED.test_config,
                                status = EXCLUDED.status,
                                updated_at = NOW()
                        """, (test_name, product_id, json.dumps(ab_test_config), "active"))
                        
                        conn.commit()
            except Exception as e:
                logger.warning("ab_test_save_failed", error=str(e))
        
        return {
            "success": True,
            "test_config": ab_test_config,
            "metadata": {
                "created_at": datetime.utcnow().isoformat()
            }
        }
        
    except Exception as e:
        logger.error("ab_test_setup_error", error=str(e))
        return {"success": False, "error": str(e)}


def generate_sms_whatsapp_content(
    product_name: str,
    description_data: Dict[str, Any],
    platform: str = "whatsapp"
) -> Dict[str, Any]:
    """
    Genera contenido optimizado para SMS y WhatsApp.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        platform: Plataforma (sms, whatsapp)
    
    Returns:
        Dict con contenido para mensajer√≠a
    """
    short_desc = description_data.get("short_description", "")
    benefits = description_data.get("benefits", [])
    cta = description_data.get("call_to_action", "Descubre m√°s")
    
    if platform == "sms":
        # SMS: m√°ximo 160 caracteres
        message = f"{product_name}: {short_desc[:100]}. {cta}. M√°s info: [link]"
        if len(message) > 160:
            message = f"{product_name}: {short_desc[:80]}. {cta} [link]"
    else:  # whatsapp
        # WhatsApp: m√°s flexible, puede incluir emojis
        message = f"‚ú® {product_name} ‚ú®\n\n{short_desc}\n\n"
        message += "Beneficios:\n"
        for i, benefit in enumerate(benefits[:3], 1):
            message += f"{i}. {benefit}\n"
        message += f"\n{cta}: [link]"
    
    return {
        "platform": platform,
        "message": message,
        "character_count": len(message),
        "max_length": 160 if platform == "sms" else 4096,
        "includes_link": True,
        "quick_replies": [
            "M√°s informaci√≥n",
            "Ver precio",
            "Comprar ahora"
        ] if platform == "whatsapp" else None,
        "metadata": {
            "product_name": product_name,
            "generated_at": datetime.utcnow().isoformat()
        }
    }


def analyze_accessibility(
    description_data: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Analiza la accesibilidad de una descripci√≥n.
    
    Args:
        description_data: Datos de descripci√≥n
    
    Returns:
        Dict con an√°lisis de accesibilidad
    """
    description = description_data.get("description", "")
    short_desc = description_data.get("short_description", "")
    
    # An√°lisis de legibilidad
    word_count = len(description.split())
    sentence_count = max(description.count('.'), 1)
    avg_sentence_length = word_count / sentence_count
    
    # An√°lisis de complejidad
    complex_words = [w for w in description.split() if len(w) > 6]
    complexity_ratio = len(complex_words) / max(word_count, 1)
    
    # An√°lisis de estructura
    has_headings = any(marker in description for marker in ["##", "###", "**"])
    has_bullets = "‚Ä¢" in description or "-" in description
    has_numbers = any(char.isdigit() for char in description)
    
    # Score de accesibilidad
    accessibility_score = 100
    issues = []
    
    if avg_sentence_length > 25:
        accessibility_score -= 20
        issues.append("Oraciones muy largas. Ideal: 15-20 palabras.")
    
    if complexity_ratio > 0.3:
        accessibility_score -= 15
        issues.append("Demasiadas palabras complejas. Simplifica el lenguaje.")
    
    if not has_bullets and word_count > 100:
        accessibility_score -= 10
        issues.append("Considera usar bullets para mejor legibilidad.")
    
    if not has_numbers and "beneficio" in description.lower():
        accessibility_score -= 5
        issues.append("Considera agregar n√∫meros/estad√≠sticas para credibilidad.")
    
    recommendations = []
    if accessibility_score < 80:
        recommendations.append("Simplifica el lenguaje para mayor accesibilidad")
        recommendations.append("Usa bullets y listas para mejor estructura")
        recommendations.append("Mant√©n oraciones cortas (15-20 palabras)")
    
    return {
        "accessibility_score": max(0, accessibility_score),
        "readability_analysis": {
            "word_count": word_count,
            "sentence_count": sentence_count,
            "avg_sentence_length": round(avg_sentence_length, 1),
            "complexity_ratio": round(complexity_ratio, 2)
        },
        "structure_analysis": {
            "has_headings": has_headings,
            "has_bullets": has_bullets,
            "has_numbers": has_numbers
        },
        "issues": issues,
        "recommendations": recommendations,
        "grade": (
            "A" if accessibility_score >= 90 else
            "B" if accessibility_score >= 75 else
            "C" if accessibility_score >= 60 else
            "D" if accessibility_score >= 40 else "F"
        ),
        "metadata": {
            "analyzed_at": datetime.utcnow().isoformat()
        }
    }


def generate_podcast_video_script(
    product_name: str,
    description_data: Dict[str, Any],
    format_type: str = "video",
    duration_minutes: int = 2
) -> Dict[str, Any]:
    """
    Genera scripts para podcasts y videos de productos.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        format_type: Tipo (video, podcast, short_video)
        duration_minutes: Duraci√≥n en minutos
    
    Returns:
        Dict con script generado
    """
    short_desc = description_data.get("short_description", "")
    benefits = description_data.get("benefits", [])
    
    # Calcular palabras por minuto (promedio: 150 palabras/minuto para habla)
    words_per_minute = 150
    target_words = duration_minutes * words_per_minute
    
    if format_type == "short_video":
        # Short video: 30-60 segundos
        script = f"""
[HOOK - 5 segundos]
¬øBuscas {product_name}? Esto te va a interesar.

[INTRO - 10 segundos]
Hoy te presento {product_name}: {short_desc}

[BENEFICIOS - 30 segundos]
Los 3 beneficios principales son:
1. {benefits[0] if benefits else 'Calidad premium'}
2. {benefits[1] if len(benefits) > 1 else 'Dise√±o innovador'}
3. {benefits[2] if len(benefits) > 2 else 'Garant√≠a incluida'}

[CTA - 5 segundos]
¬øQuieres saber m√°s? Link en la descripci√≥n.
"""
    elif format_type == "podcast":
        script = f"""
[INTRO]
Bienvenidos. Hoy hablamos de {product_name}.

[DESCRIPCI√ìN]
{product_name} es {short_desc}. Es un producto que realmente destaca en su categor√≠a.

[BENEFICIOS DETALLADOS]
D√©jame contarte por qu√© {product_name} es especial:
Primero, {benefits[0] if benefits else 'ofrece calidad excepcional'}.
Segundo, {benefits[1] if len(benefits) > 1 else 'tiene un dise√±o innovador'}.
Y tercero, {benefits[2] if len(benefits) > 2 else 'incluye garant√≠a completa'}.

[CONCLUSI√ìN]
En resumen, {product_name} es una excelente opci√≥n si buscas calidad y confiabilidad.
"""
    else:  # video normal
        script = f"""
[INTRO - 15 segundos]
Hola, en este video te presento {product_name}.

[PROBLEMA - 20 segundos]
Si est√°s buscando [problema que resuelve], {product_name} es la soluci√≥n.

[SOLUCI√ìN - 60 segundos]
{product_name} es {short_desc}.
Los beneficios principales son:
‚Ä¢ {benefits[0] if benefits else 'Calidad premium'}
‚Ä¢ {benefits[1] if len(benefits) > 1 else 'Dise√±o innovador'}
‚Ä¢ {benefits[2] if len(benefits) > 2 else 'Garant√≠a incluida'}

[DEMOSTRACI√ìN - 30 segundos]
[Muestra el producto en acci√≥n]

[CTA - 15 segundos]
Si te interesa {product_name}, link en la descripci√≥n. ¬°No olvides suscribirte!
"""
    
    return {
        "format_type": format_type,
        "duration_minutes": duration_minutes,
        "estimated_words": len(script.split()),
        "script": script.strip(),
        "sections": {
            "hook": "Presentaci√≥n inicial",
            "intro": "Introducci√≥n del producto",
            "body": "Contenido principal",
            "cta": "Llamado a la acci√≥n"
        },
        "metadata": {
            "product_name": product_name,
            "generated_at": datetime.utcnow().isoformat()
        }
    }


# ============================================================================
# FUNCIONALIDADES AVANZADAS FINALES ADICIONALES
# ============================================================================

def generate_description_analytics_dashboard(
    product_id: str,
    description_data: Dict[str, Any],
    performance_data: Dict[str, Any],
    time_period: str = "30d"
) -> Dict[str, Any]:
    """
    Genera dashboard completo de analytics para descripciones.
    
    Args:
        product_id: ID del producto
        description_data: Datos de descripci√≥n
        performance_data: Datos de rendimiento
        time_period: Per√≠odo de an√°lisis
    
    Returns:
        Dict con dashboard de analytics
    """
    views = performance_data.get("views", 0)
    conversions = performance_data.get("conversions", 0)
    revenue = performance_data.get("revenue", 0)
    engagement_rate = performance_data.get("engagement_rate", 0)
    bounce_rate = performance_data.get("bounce_rate", 0)
    avg_time_on_page = performance_data.get("avg_time_on_page", 0)
    
    # Calcular m√©tricas derivadas
    conversion_rate = (conversions / views * 100) if views > 0 else 0
    revenue_per_view = (revenue / views) if views > 0 else 0
    
    # Score de descripci√≥n
    desc_score = score_product_description(
        description_data.get("description", ""),
        buyer_type="general"
    )
    
    # An√°lisis de sentimiento
    sentiment = analyze_description_sentiment(
        description_data.get("description", "")
    )
    
    # An√°lisis de accesibilidad
    accessibility = analyze_accessibility(description_data)
    
    dashboard = {
        "product_id": product_id,
        "time_period": time_period,
        "overview": {
            "total_views": views,
            "total_conversions": conversions,
            "total_revenue": revenue,
            "conversion_rate": round(conversion_rate, 2),
            "revenue_per_view": round(revenue_per_view, 2)
        },
        "engagement_metrics": {
            "engagement_rate": round(engagement_rate, 2),
            "bounce_rate": round(bounce_rate, 2),
            "avg_time_on_page": round(avg_time_on_page, 2),
            "pages_per_session": performance_data.get("pages_per_session", 1)
        },
        "quality_scores": {
            "description_score": desc_score["overall_score"],
            "description_grade": desc_score["grade"],
            "sentiment_score": sentiment["sentiment_score"],
            "sentiment": sentiment["sentiment"],
            "accessibility_score": accessibility["accessibility_score"],
            "accessibility_grade": accessibility["grade"]
        },
        "performance_trends": {
            "conversion_trend": "up" if conversion_rate > 2.0 else "stable" if conversion_rate > 1.0 else "down",
            "engagement_trend": "up" if engagement_rate > 50 else "stable" if engagement_rate > 30 else "down",
            "revenue_trend": "up" if revenue_per_view > 0.50 else "stable" if revenue_per_view > 0.20 else "down"
        },
        "recommendations": _generate_dashboard_recommendations(
            conversion_rate, engagement_rate, bounce_rate, desc_score, accessibility
        ),
        "metadata": {
            "generated_at": datetime.utcnow().isoformat(),
            "data_source": "performance_tracking"
        }
    }
    
    return dashboard


def _generate_dashboard_recommendations(
    conversion_rate: float,
    engagement_rate: float,
    bounce_rate: float,
    desc_score: Dict[str, Any],
    accessibility: Dict[str, Any]
) -> List[str]:
    """Genera recomendaciones para el dashboard."""
    recommendations = []
    
    if conversion_rate < 1.5:
        recommendations.append("üö® Conversi√≥n baja. Optimiza el CTA y agrega elementos de urgencia.")
    
    if engagement_rate < 40:
        recommendations.append("üìä Engagement bajo. Mejora la introducci√≥n para captar atenci√≥n.")
    
    if bounce_rate > 60:
        recommendations.append("‚ö†Ô∏è Bounce rate alto. Revisa la relevancia del contenido.")
    
    if desc_score["overall_score"] < 70:
        recommendations.append(f"üìù Score de descripci√≥n: {desc_score['grade']}. Mejora: {', '.join(desc_score['recommendations'][:2])}")
    
    if accessibility["accessibility_score"] < 75:
        recommendations.append(f"‚ôø Accesibilidad: {accessibility['grade']}. Simplifica el lenguaje.")
    
    if conversion_rate > 3.0 and engagement_rate > 60:
        recommendations.append("‚úÖ Excelente rendimiento. Considera escalar esta estrategia.")
    
    return recommendations


def generate_voice_assistant_content(
    product_name: str,
    description_data: Dict[str, Any],
    platform: str = "alexa"
) -> Dict[str, Any]:
    """
    Genera contenido optimizado para asistentes de voz.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        platform: Plataforma (alexa, google_assistant, siri)
    
    Returns:
        Dict con contenido para voice assistant
    """
    short_desc = description_data.get("short_description", "")
    benefits = description_data.get("benefits", [])
    
    # Contenido optimizado para voz (frases cortas, claras)
    voice_intro = f"{product_name} es {short_desc}"
    
    voice_benefits = []
    for benefit in benefits[:3]:
        # Simplificar para voz
        benefit_voice = benefit.replace("Acceso a", "").replace("Incluye", "")
        voice_benefits.append(benefit_voice)
    
    # Respuestas cortas para preguntas comunes
    voice_responses = {
        "what_is": f"{product_name} es {short_desc}",
        "benefits": f"Los beneficios principales son: {', '.join(voice_benefits)}",
        "price": f"El precio de {product_name} var√≠a seg√∫n el plan. ¬øTe gustar√≠a conocer m√°s detalles?",
        "availability": f"{product_name} est√° disponible ahora. ¬øTe gustar√≠a m√°s informaci√≥n?",
        "comparison": f"{product_name} se destaca por {benefits[0] if benefits else 'su calidad'}"
    }
    
    # Invocaciones (para Alexa Skills)
    if platform == "alexa":
        invocations = [
            f"Alexa, abre {product_name.lower().replace(' ', '')}",
            f"Alexa, pregunta sobre {product_name}",
            f"Alexa, informaci√≥n de {product_name}"
        ]
    elif platform == "google_assistant":
        invocations = [
            f"Ok Google, informaci√≥n sobre {product_name}",
            f"Hey Google, qu√© es {product_name}"
        ]
    else:  # siri
        invocations = [
            f"Hey Siri, informaci√≥n de {product_name}",
            f"Siri, cu√©ntame sobre {product_name}"
        ]
    
    return {
        "platform": platform,
        "voice_intro": voice_intro,
        "voice_benefits": voice_benefits,
        "voice_responses": voice_responses,
        "invocations": invocations,
        "metadata": {
            "product_name": product_name,
            "generated_at": datetime.utcnow().isoformat(),
            "optimized_for": "voice_interaction"
        }
    }


def analyze_compliance_legal(
    description_data: Dict[str, Any],
    region: str = "US"
) -> Dict[str, Any]:
    """
    Analiza compliance y aspectos legales de descripciones.
    
    Args:
        description_data: Datos de descripci√≥n
        region: Regi√≥n (US, EU, LATAM)
    
    Returns:
        Dict con an√°lisis de compliance
    """
    description = description_data.get("description", "").lower()
    short_desc = description_data.get("short_description", "").lower()
    
    # Palabras problem√°ticas por regi√≥n
    problematic_words = {
        "US": ["garantizado", "mejor", "n√∫mero uno", "sin riesgo"],
        "EU": ["garantizado", "mejor", "√∫nico", "exclusivo"],
        "LATAM": ["garantizado", "mejor", "√∫nico"]
    }
    
    # Claims que requieren evidencia
    claims_requiring_evidence = [
        "mejor", "n√∫mero uno", "√∫nico", "exclusivo", "garantizado",
        "100%", "siempre", "nunca", "todos", "ninguno"
    ]
    
    issues = []
    warnings = []
    compliance_score = 100
    
    # Verificar palabras problem√°ticas
    problematic = problematic_words.get(region, problematic_words["US"])
    for word in problematic:
        if word in description or word in short_desc:
            issues.append(f"Palabra potencialmente problem√°tica: '{word}' (puede requerir evidencia)")
            compliance_score -= 10
    
    # Verificar claims sin evidencia
    for claim in claims_requiring_evidence:
        if claim in description or claim in short_desc:
            warnings.append(f"Claim '{claim}' requiere evidencia o disclaimer")
            compliance_score -= 5
    
    # Verificar presencia de disclaimers
    has_disclaimer = any(word in description for word in ["*", "t√©rminos", "condiciones", "sujeto a"])
    if not has_disclaimer and any(word in description for word in ["garant√≠a", "garantizado"]):
        warnings.append("Considera agregar disclaimer para garant√≠as")
        compliance_score -= 5
    
    # Verificar informaci√≥n de contacto/empresa
    has_company_info = any(word in description for word in ["empresa", "compa√±√≠a", "nosotros", "nuestro"])
    
    recommendations = []
    if compliance_score < 80:
        recommendations.append("Revisa claims que requieren evidencia")
        recommendations.append("Agrega disclaimers donde sea necesario")
        recommendations.append("Considera consultar con legal antes de publicar")
    
    return {
        "compliance_score": max(0, compliance_score),
        "region": region,
        "issues": issues,
        "warnings": warnings,
        "has_disclaimer": has_disclaimer,
        "has_company_info": has_company_info,
        "recommendations": recommendations,
        "grade": (
            "A" if compliance_score >= 90 else
            "B" if compliance_score >= 75 else
            "C" if compliance_score >= 60 else
            "D" if compliance_score >= 40 else "F"
        ),
        "metadata": {
            "analyzed_at": datetime.utcnow().isoformat(),
            "region": region
        }
    }


def generate_marketplace_specific_content(
    product_name: str,
    description_data: Dict[str, Any],
    marketplace: str = "amazon"
) -> Dict[str, Any]:
    """
    Genera contenido optimizado para marketplaces espec√≠ficos.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        marketplace: Marketplace (amazon, ebay, walmart, alibaba, etsy)
    
    Returns:
        Dict con contenido optimizado para marketplace
    """
    base_desc = generate_platform_specific_description(
        product_name=product_name,
        product_features=description_data.get("benefits", []),
        platform=marketplace,
        buyer_type="general"
    )
    
    marketplace_specifics = {
        "amazon": {
            "bullet_points": base_desc.get("bullets", []),
            "a_plus_content": {
                "modules": [
                    {
                        "type": "comparison_chart",
                        "title": "Comparaci√≥n de caracter√≠sticas",
                        "content": "Tabla comparativa con competidores"
                    },
                    {
                        "type": "image_text",
                        "title": "Beneficios principales",
                        "content": description_data.get("description", "")
                    }
                ]
            },
            "search_terms": description_data.get("seo_keywords", [])[:10]
        },
        "ebay": {
            "item_specifics": {
                "Brand": "Tu marca",
                "Condition": "New",
                "Type": description_data.get("product_category", "General")
            },
            "description_html": f"<p>{base_desc.get('description', '')}</p>",
            "return_policy": "30 d√≠as de devoluci√≥n"
        },
        "walmart": {
            "short_description": base_desc.get("short_description", ""),
            "long_description": base_desc.get("description", ""),
            "key_features": base_desc.get("benefits", [])[:5]
        },
        "alibaba": {
            "product_description": base_desc.get("description", ""),
            "specifications": {
                "Place of Origin": "Tu pa√≠s",
                "Brand Name": "Tu marca",
                "Model Number": "MODEL-001"
            },
            "packaging_details": "Embalaje est√°ndar"
        },
        "etsy": {
            "description": base_desc.get("description", ""),
            "tags": description_data.get("seo_keywords", [])[:13],  # Etsy permite 13 tags
            "materials": ["Material 1", "Material 2"],
            "processing_time": "1-2 business days"
        }
    }
    
    content = marketplace_specifics.get(marketplace, marketplace_specifics["amazon"])
    
    return {
        "marketplace": marketplace,
        "content": content,
        "base_description": base_desc,
        "optimization_tips": _get_marketplace_tips(marketplace),
        "metadata": {
            "product_name": product_name,
            "generated_at": datetime.utcnow().isoformat()
        }
    }


def _get_marketplace_tips(marketplace: str) -> List[str]:
    """Genera tips de optimizaci√≥n por marketplace."""
    tips_map = {
        "amazon": [
            "Usa bullets con beneficios espec√≠ficos",
            "Incluye palabras clave en los primeros 3 bullets",
            "Optimiza para b√∫squeda por voz",
            "Usa A+ Content para destacar"
        ],
        "ebay": [
            "Incluye item specifics completos",
            "Usa HTML para formato",
            "Menciona pol√≠tica de devoluci√≥n",
            "Incluye informaci√≥n de env√≠o"
        ],
        "walmart": [
            "Mant√©n descripci√≥n corta clara",
            "Destaca key features",
            "Incluye informaci√≥n de garant√≠a"
        ],
        "alibaba": [
            "Especificaciones t√©cnicas detalladas",
            "Informaci√≥n de packaging",
            "Certificaciones si aplican"
        ],
        "etsy": [
            "Story personal/artesanal",
            "Tags relevantes (m√°x 13)",
            "Materials y processing time"
        ]
    }
    
    return tips_map.get(marketplace, ["Optimiza seg√∫n mejores pr√°cticas del marketplace"])


def generate_newsletter_content(
    product_name: str,
    description_data: Dict[str, Any],
    newsletter_type: str = "product_launch"
) -> Dict[str, Any]:
    """
    Genera contenido para newsletters.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        newsletter_type: Tipo (product_launch, feature_update, seasonal, general)
    
    Returns:
        Dict con contenido de newsletter
    """
    short_desc = description_data.get("short_description", "")
    benefits = description_data.get("benefits", [])
    
    newsletter_templates = {
        "product_launch": {
            "subject": f"üéâ ¬°Nuevo! {product_name} ya est√° disponible",
            "preheader": f"Descubre {product_name}",
            "header": "¬°Tenemos algo nuevo para ti!",
            "body": f"""
            <h2>{product_name}</h2>
            <p>{short_desc}</p>
            <p>Caracter√≠sticas destacadas:</p>
            <ul>
            """ + "\n".join([f"<li>{b}</li>" for b in benefits[:4]]) + """
            </ul>
            <p><strong>Lanzamiento especial:</strong> Obt√©n un descuento exclusivo por tiempo limitado.</p>
            """,
            "cta": "Ver producto",
            "footer": "Gracias por ser parte de nuestra comunidad."
        },
        "feature_update": {
            "subject": f"‚ú® {product_name} - Nuevas funcionalidades",
            "preheader": "Actualizaciones importantes",
            "header": "Mejoras en {product_name}",
            "body": f"""
            <h2>Nuevas funcionalidades en {product_name}</h2>
            <p>Hemos agregado mejoras importantes:</p>
            <ul>
            """ + "\n".join([f"<li>{b}</li>" for b in benefits[:3]]) + """
            </ul>
            <p>Estas actualizaciones est√°n disponibles ahora.</p>
            """,
            "cta": "Ver actualizaciones",
            "footer": "Seguimos mejorando para ti."
        },
        "seasonal": {
            "subject": f"üéÅ {product_name} - Oferta especial de temporada",
            "preheader": "Ofertas limitadas",
            "header": "Oferta especial de temporada",
            "body": f"""
            <h2>{product_name} con descuento especial</h2>
            <p>{short_desc}</p>
            <p>Por tiempo limitado, obt√©n {product_name} con un descuento exclusivo.</p>
            <p>No te pierdas esta oportunidad √∫nica.</p>
            """,
            "cta": "Aprovechar oferta",
            "footer": "Oferta v√°lida por tiempo limitado."
        },
        "general": {
            "subject": f"üì∞ Actualizaci√≥n: {product_name}",
            "preheader": "Novedades",
            "header": "Novedades",
            "body": f"""
            <h2>{product_name}</h2>
            <p>{short_desc}</p>
            <p>Descubre m√°s sobre este producto y c√≥mo puede ayudarte.</p>
            """,
            "cta": "Saber m√°s",
            "footer": "Gracias por tu inter√©s."
        }
    }
    
    template = newsletter_templates.get(newsletter_type, newsletter_templates["general"])
    
    return {
        "newsletter_type": newsletter_type,
        "subject": template["subject"],
        "preheader": template["preheader"],
        "content": {
            "header": template["header"],
            "body": template["body"],
            "cta": {
                "text": template["cta"],
                "url": "#"
            },
            "footer": template["footer"]
        },
        "metadata": {
            "product_name": product_name,
            "generated_at": datetime.utcnow().isoformat()
        }
    }


def generate_event_webinar_content(
    product_name: str,
    description_data: Dict[str, Any],
    event_type: str = "webinar"
) -> Dict[str, Any]:
    """
    Genera contenido para eventos y webinars.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        event_type: Tipo (webinar, workshop, demo, launch_event)
    
    Returns:
        Dict con contenido para eventos
    """
    short_desc = description_data.get("short_description", "")
    benefits = description_data.get("benefits", [])
    
    event_templates = {
        "webinar": {
            "title": f"Webinar: Descubre {product_name}",
            "description": f"√önete a nuestro webinar donde presentaremos {product_name}: {short_desc}",
            "agenda": [
                "Introducci√≥n y bienvenida (5 min)",
                f"Presentaci√≥n de {product_name} (20 min)",
                "Demo en vivo (15 min)",
                "Q&A y preguntas (10 min)"
            ],
            "key_points": benefits[:5],
            "cta": "Reg√≠strate gratis"
        },
        "workshop": {
            "title": f"Workshop: Aprende a usar {product_name}",
            "description": f"Workshop pr√°ctico sobre {product_name}. Aprende todo lo que necesitas saber.",
            "agenda": [
                "Introducci√≥n (10 min)",
                "Conceptos b√°sicos (30 min)",
                "Pr√°ctica guiada (40 min)",
                "Casos de uso avanzados (20 min)"
            ],
            "key_points": benefits[:5],
            "cta": "Reservar lugar"
        },
        "demo": {
            "title": f"Demo en vivo: {product_name}",
            "description": f"Ver√°s {product_name} en acci√≥n. {short_desc}",
            "agenda": [
                "Presentaci√≥n (5 min)",
                "Demo interactiva (25 min)",
                "Preguntas (10 min)"
            ],
            "key_points": benefits[:3],
            "cta": "Agendar demo"
        },
        "launch_event": {
            "title": f"Lanzamiento oficial: {product_name}",
            "description": f"√önete al lanzamiento oficial de {product_name}. {short_desc}",
            "agenda": [
                "Recepci√≥n (15 min)",
                "Presentaci√≥n del producto (30 min)",
                "Networking (30 min)",
                "Q&A (15 min)"
            ],
            "key_points": benefits[:5],
            "cta": "Confirmar asistencia"
        }
    }
    
    template = event_templates.get(event_type, event_templates["webinar"])
    
    return {
        "event_type": event_type,
        "title": template["title"],
        "description": template["description"],
        "agenda": template["agenda"],
        "key_points": template["key_points"],
        "cta": template["cta"],
        "metadata": {
            "product_name": product_name,
            "generated_at": datetime.utcnow().isoformat()
        }
    }


def generate_influencer_affiliate_content(
    product_name: str,
    description_data: Dict[str, Any],
    influencer_type: str = "macro"
) -> Dict[str, Any]:
    """
    Genera contenido optimizado para influencers y programas de afiliados.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        influencer_type: Tipo (micro, macro, celebrity, niche)
    
    Returns:
        Dict con contenido para influencers
    """
    short_desc = description_data.get("short_description", "")
    benefits = description_data.get("benefits", [])
    
    influencer_templates = {
        "micro": {
            "caption": f"¬°Hola! Les quiero compartir {product_name} üéâ\n\n{short_desc}\n\nLo que m√°s me gusta:\n" + "\n".join([f"‚ú® {b}" for b in benefits[:3]]),
            "story_text": f"Descubre {product_name} üëÜ",
            "video_script": f"En este video les muestro {product_name}. {short_desc}",
            "hashtags": ["#producto", "#review", "#recomendacion"],
            "affiliate_link": f"https://example.com/{product_name.lower().replace(' ', '-')}?ref=influencer",
            "discount_code": "INFLUENCER10"
        },
        "macro": {
            "caption": f"üåü PARTNERSHIP: {product_name}\n\n{short_desc}\n\nBeneficios clave:\n" + "\n".join([f"‚úÖ {b}" for b in benefits[:4]]),
            "story_text": f"Link en bio para {product_name} üîó",
            "video_script": f"En colaboraci√≥n con {product_name}. {short_desc}. Beneficios: {', '.join(benefits[:3])}",
            "hashtags": ["#ad", "#sponsored", "#partnership", "#producto"],
            "affiliate_link": f"https://example.com/{product_name.lower().replace(' ', '-')}?ref=macro",
            "discount_code": "MACRO15"
        },
        "celebrity": {
            "caption": f"{product_name} - La elecci√≥n profesional\n\n{short_desc}",
            "story_text": f"Descubre m√°s sobre {product_name}",
            "video_script": f"Presentando {product_name}. {short_desc}",
            "hashtags": ["#brandpartner", "#producto"],
            "affiliate_link": f"https://example.com/{product_name.lower().replace(' ', '-')}?ref=celebrity",
            "discount_code": "CELEBRITY20"
        },
        "niche": {
            "caption": f"Para los que buscan calidad: {product_name}\n\n{short_desc}\n\nEspec√≠ficamente:\n" + "\n".join([f"‚Ä¢ {b}" for b in benefits[:5]]),
            "story_text": f"Detalles t√©cnicos de {product_name}",
            "video_script": f"An√°lisis detallado de {product_name}. {short_desc}",
            "hashtags": ["#niche", "#tecnico", "#review", "#producto"],
            "affiliate_link": f"https://example.com/{product_name.lower().replace(' ', '-')}?ref=niche",
            "discount_code": "NICHE12"
        }
    }
    
    content = influencer_templates.get(influencer_type, influencer_templates["macro"])
    
    return {
        "influencer_type": influencer_type,
        "content": content,
        "tracking": {
            "campaign_id": f"influencer_{influencer_type}_{product_name.lower().replace(' ', '_')}",
            "utm_source": "influencer",
            "utm_medium": influencer_type,
            "utm_campaign": product_name.lower().replace(" ", "_")
        },
        "metadata": {
            "product_name": product_name,
            "generated_at": datetime.utcnow().isoformat()
        }
    }


def predict_engagement(
    description_data: Dict[str, Any],
    platform: str = "instagram",
    audience_size: int = 10000
) -> Dict[str, Any]:
    """
    Predice m√©tricas de engagement basado en caracter√≠sticas de la descripci√≥n.
    
    Args:
        description_data: Datos de descripci√≥n
        platform: Plataforma social
        audience_size: Tama√±o de audiencia
    
    Returns:
        Dict con predicciones de engagement
    """
    description = description_data.get("description", "")
    short_desc = description_data.get("short_description", "")
    
    # Caracter√≠sticas que afectan engagement
    has_emojis = any(ord(char) > 127 for char in description)
    has_numbers = any(char.isdigit() for char in description)
    has_questions = "?" in description
    has_cta = any(word in description.lower() for word in ["comprar", "descubre", "obt√©n", "ahora", "click"])
    word_count = len(description.split())
    
    # Factores de engagement por plataforma
    platform_factors = {
        "instagram": {"base": 0.03, "emoji_bonus": 0.01, "hashtag_bonus": 0.005},
        "facebook": {"base": 0.02, "emoji_bonus": 0.005, "hashtag_bonus": 0.003},
        "twitter": {"base": 0.015, "emoji_bonus": 0.008, "hashtag_bonus": 0.01},
        "linkedin": {"base": 0.01, "emoji_bonus": 0.002, "hashtag_bonus": 0.001},
        "tiktok": {"base": 0.05, "emoji_bonus": 0.015, "hashtag_bonus": 0.01}
    }
    
    factors = platform_factors.get(platform, platform_factors["instagram"])
    
    # Calcular engagement rate base
    engagement_rate = factors["base"]
    if has_emojis:
        engagement_rate += factors["emoji_bonus"]
    if has_numbers:
        engagement_rate += 0.002
    if has_questions:
        engagement_rate += 0.003
    if has_cta:
        engagement_rate += 0.002
    
    # Ajustar por longitud (√≥ptimo: 100-200 palabras)
    if 100 <= word_count <= 200:
        engagement_rate += 0.005
    elif word_count > 300:
        engagement_rate -= 0.003
    
    # Predicciones
    predicted_engagement = int(audience_size * engagement_rate)
    predicted_clicks = int(predicted_engagement * 0.15)
    predicted_shares = int(predicted_engagement * 0.05)
    predicted_comments = int(predicted_engagement * 0.20)
    predicted_likes = int(predicted_engagement * 0.60)
    
    engagement_score = min(100, engagement_rate * 1000)  # Score 0-100
    
    recommendations = _get_engagement_recommendations(
        has_emojis, has_numbers, has_questions, has_cta, word_count, platform
    )
    
    return {
        "platform": platform,
        "audience_size": audience_size,
        "engagement_score": round(engagement_score, 2),
        "predicted_metrics": {
            "engagement_rate": round(engagement_rate * 100, 2),
            "total_engagement": predicted_engagement,
            "likes": predicted_likes,
            "comments": predicted_comments,
            "shares": predicted_shares,
            "clicks": predicted_clicks
        },
        "content_features": {
            "has_emojis": has_emojis,
            "has_numbers": has_numbers,
            "has_questions": has_questions,
            "has_cta": has_cta,
            "word_count": word_count
        },
        "recommendations": recommendations,
        "metadata": {
            "generated_at": datetime.utcnow().isoformat()
        }
    }


def _get_engagement_recommendations(
    has_emojis: bool,
    has_numbers: bool,
    has_questions: bool,
    has_cta: bool,
    word_count: int,
    platform: str
) -> List[str]:
    """Genera recomendaciones para mejorar engagement."""
    recommendations = []
    
    if not has_emojis and platform in ["instagram", "tiktok", "twitter"]:
        recommendations.append("Agrega emojis relevantes para aumentar engagement visual")
    
    if not has_numbers:
        recommendations.append("Incluye n√∫meros o estad√≠sticas para mayor credibilidad")
    
    if not has_questions:
        recommendations.append("Agrega preguntas para fomentar interacci√≥n")
    
    if not has_cta:
        recommendations.append("Incluye un call-to-action claro")
    
    if word_count < 50:
        recommendations.append("Ampl√≠a el contenido para mayor valor")
    elif word_count > 300:
        recommendations.append("Reduce la longitud para mantener atenci√≥n")
    
    if platform == "instagram":
        recommendations.append("Usa 5-10 hashtags relevantes")
    elif platform == "twitter":
        recommendations.append("Mant√©n el contenido conciso (280 caracteres)")
    
    return recommendations


def generate_comparison_content(
    product_name: str,
    product_features: List[str],
    competitor_products: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    Genera contenido de comparaci√≥n con productos competidores.
    
    Args:
        product_name: Nombre del producto
        product_features: Lista de caracter√≠sticas
        competitor_products: Lista de competidores con sus caracter√≠sticas
    
    Returns:
        Dict con contenido de comparaci√≥n
    """
    comparison_table = {
        "feature": "Caracter√≠stica",
        "our_product": product_name,
        "competitors": {}
    }
    
    for comp in competitor_products:
        comp_name = comp.get("name", "Competidor")
        comparison_table["competitors"][comp_name] = comp.get("features", [])
    
    # Identificar ventajas
    advantages = []
    for feature in product_features:
        has_advantage = True
        for comp in competitor_products:
            comp_features = comp.get("features", [])
            if feature in comp_features:
                has_advantage = False
                break
        
        if has_advantage:
            advantages.append(feature)
    
    comparison_text = f"Comparaci√≥n: {product_name} vs Competidores\n\n"
    comparison_text += f"{product_name} se destaca por:\n"
    comparison_text += "\n".join([f"‚úÖ {adv}" for adv in advantages[:5]])
    
    comparison_text += "\n\nTabla comparativa:\n"
    comparison_text += f"| Caracter√≠stica | {product_name} | "
    comparison_text += " | ".join([comp.get("name", "Comp") for comp in competitor_products]) + " |\n"
    comparison_text += "|" + "---|" * (len(competitor_products) + 2) + "\n"
    
    for feature in product_features[:10]:
        comparison_text += f"| {feature} | ‚úÖ | "
        comparison_text += " | ".join([
            "‚úÖ" if feature in comp.get("features", []) else "‚ùå"
            for comp in competitor_products
        ]) + " |\n"
    
    return {
        "product_name": product_name,
        "comparison_table": comparison_table,
        "comparison_text": comparison_text,
        "advantages": advantages,
        "metadata": {
            "generated_at": datetime.utcnow().isoformat(),
            "competitors_count": len(competitor_products)
        }
    }


def generate_use_cases_content(
    product_name: str,
    description_data: Dict[str, Any],
    num_cases: int = 5
) -> Dict[str, Any]:
    """
    Genera casos de uso del producto.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        num_cases: N√∫mero de casos de uso a generar
    
    Returns:
        Dict con casos de uso
    """
    category = description_data.get("product_category", "general")
    benefits = description_data.get("benefits", [])
    
    use_case_templates = {
        "software": [
            {"scenario": "Equipos remotos", "benefit": "Colaboraci√≥n en tiempo real"},
            {"scenario": "Startups", "benefit": "Escalabilidad r√°pida"},
            {"scenario": "Empresas grandes", "benefit": "Integraci√≥n con sistemas existentes"},
            {"scenario": "Freelancers", "benefit": "Gesti√≥n de proyectos simple"},
            {"scenario": "Agencias", "benefit": "Gesti√≥n de m√∫ltiples clientes"}
        ],
        "electronics": [
            {"scenario": "Hogar inteligente", "benefit": "Automatizaci√≥n completa"},
            {"scenario": "Oficina", "benefit": "Productividad mejorada"},
            {"scenario": "Viajes", "benefit": "Portabilidad y durabilidad"},
            {"scenario": "Entretenimiento", "benefit": "Experiencia inmersiva"},
            {"scenario": "Fitness", "benefit": "Monitoreo de salud"}
        ],
        "general": [
            {"scenario": "Uso diario", "benefit": benefits[0] if benefits else "Beneficio principal"},
            {"scenario": "Uso profesional", "benefit": benefits[1] if len(benefits) > 1 else "Calidad profesional"},
            {"scenario": "Uso especializado", "benefit": benefits[2] if len(benefits) > 2 else "Especializaci√≥n"},
            {"scenario": "Uso en equipo", "benefit": benefits[3] if len(benefits) > 3 else "Colaboraci√≥n"},
            {"scenario": "Uso personalizado", "benefit": benefits[4] if len(benefits) > 4 else "Personalizaci√≥n"}
        ]
    }
    
    templates = use_case_templates.get(category, use_case_templates["general"])
    use_cases = templates[:num_cases]
    
    use_cases_content = []
    for i, case in enumerate(use_cases, 1):
        use_cases_content.append({
            "case_number": i,
            "title": f"Caso de uso {i}: {case['scenario']}",
            "scenario": case["scenario"],
            "benefit": case["benefit"],
            "description": f"Para {case['scenario'].lower()}, {product_name} ofrece {case['benefit'].lower()}."
        })
    
    return {
        "product_name": product_name,
        "use_cases": use_cases_content,
        "target_audience": category,
        "metadata": {
            "generated_at": datetime.utcnow().isoformat(),
            "num_cases": len(use_cases_content)
        }
    }


def generate_referral_program_content(
    product_name: str,
    description_data: Dict[str, Any],
    referral_type: str = "customer"
) -> Dict[str, Any]:
    """
    Genera contenido para programas de referidos.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        referral_type: Tipo (customer, affiliate, partner)
    
    Returns:
        Dict con contenido para referidos
    """
    short_desc = description_data.get("short_description", "")
    
    referral_templates = {
        "customer": {
            "email_subject": f"Invita a tus amigos a probar {product_name}",
            "email_body": f"¬°Hola!\n\nTe encanta {product_name}? {short_desc}\n\nInvita a tus amigos y ambos obtendr√°n beneficios exclusivos.\n\nTu c√≥digo: REFERIDO123",
            "social_post": f"¬øConoces a alguien que necesite {product_name}? {short_desc} Inv√≠talos y ambos ganan üéÅ",
            "incentive": "20% de descuento para ambos",
            "tracking_code": "REFERIDO123"
        },
        "affiliate": {
            "email_subject": f"√önete al programa de afiliados de {product_name}",
            "email_body": f"Programa de afiliados de {product_name}\n\n{short_desc}\n\nGana comisiones por cada venta que refieras.\n\nReg√≠strate: [link]",
            "social_post": f"Programa de afiliados disponible para {product_name}. Gana comisiones üí∞",
            "incentive": "15% de comisi√≥n por venta",
            "tracking_code": "AFFILIATE_{product_name.upper().replace(' ', '_')}"
        },
        "partner": {
            "email_subject": f"Partnership con {product_name}",
            "email_body": f"Oportunidad de partnership con {product_name}\n\n{short_desc}\n\nBeneficios mutuos para ambas partes.\n\nContacta: [email]",
            "social_post": f"Buscamos partners para {product_name}. {short_desc}",
            "incentive": "T√©rminos personalizados",
            "tracking_code": "PARTNER_{product_name.upper().replace(' ', '_')}"
        }
    }
    
    content = referral_templates.get(referral_type, referral_templates["customer"])
    
    return {
        "referral_type": referral_type,
        "content": content,
        "product_name": product_name,
        "metadata": {
            "generated_at": datetime.utcnow().isoformat()
        }
    }


def analyze_demographic_fit(
    description_data: Dict[str, Any],
    demographics: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Analiza qu√© tan bien se ajusta una descripci√≥n a un perfil demogr√°fico.
    
    Args:
        description_data: Datos de descripci√≥n
        demographics: Perfil demogr√°fico (age_range, gender, location, income_level, education)
    
    Returns:
        Dict con an√°lisis de fit demogr√°fico
    """
    description = description_data.get("description", "").lower()
    
    age_range = demographics.get("age_range", "general")
    location = demographics.get("location", "general")
    income_level = demographics.get("income_level", "general")
    education = demographics.get("education", "general")
    
    # An√°lisis de lenguaje por edad
    age_fit = 70  # Base
    if age_range == "18-25":
        if any(word in description for word in ["genial", "incre√≠ble", "√©pico", "üî•"]):
            age_fit += 10
    elif age_range == "26-35":
        if any(word in description for word in ["profesional", "eficiente", "productivo"]):
            age_fit += 10
    elif age_range == "36-50":
        if any(word in description for word in ["confiable", "estable", "comprobado"]):
            age_fit += 10
    
    # An√°lisis por ubicaci√≥n
    location_fit = 70
    if location == "urban":
        if any(word in description for word in ["moderno", "innovador", "tecnol√≥gico"]):
            location_fit += 10
    elif location == "rural":
        if any(word in description for word in ["durable", "resistente", "pr√°ctico"]):
            location_fit += 10
    
    # An√°lisis por nivel de ingreso
    income_fit = 70
    if income_level == "high":
        if any(word in description for word in ["premium", "exclusivo", "lujo", "calidad superior"]):
            income_fit += 15
    elif income_level == "low":
        if any(word in description for word in ["accesible", "econ√≥mico", "valor", "precio"]):
            income_fit += 15
    
    # An√°lisis por educaci√≥n
    education_fit = 70
    if education == "high":
        if any(word in description for word in ["avanzado", "sofisticado", "t√©cnico", "an√°lisis"]):
            education_fit += 10
    elif education == "low":
        if any(word in description for word in ["simple", "f√°cil", "intuitivo", "sencillo"]):
            education_fit += 10
    
    overall_fit = (age_fit + location_fit + income_fit + education_fit) / 4
    
    recommendations = []
    if overall_fit < 70:
        recommendations.append("Ajusta el lenguaje para mejor alineaci√≥n demogr√°fica")
    if age_fit < 70:
        recommendations.append(f"Adapta el tono para el rango de edad {age_range}")
    if income_fit < 70:
        recommendations.append(f"Enf√≥cate en el nivel de ingreso {income_level}")
    
    return {
        "demographics": demographics,
        "fit_scores": {
            "age_fit": round(age_fit, 2),
            "location_fit": round(location_fit, 2),
            "income_fit": round(income_fit, 2),
            "education_fit": round(education_fit, 2),
            "overall_fit": round(overall_fit, 2)
        },
        "grade": "A" if overall_fit >= 85 else "B" if overall_fit >= 75 else "C" if overall_fit >= 65 else "D",
        "recommendations": recommendations,
        "metadata": {
            "generated_at": datetime.utcnow().isoformat()
        }
    }


def generate_pricing_strategy_content(
    product_name: str,
    description_data: Dict[str, Any],
    pricing_tier: str = "value"
) -> Dict[str, Any]:
    """
    Genera contenido optimizado para diferentes estrategias de pricing.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        pricing_tier: Tier (budget, value, premium, luxury)
    
    Returns:
        Dict con contenido optimizado para pricing
    """
    short_desc = description_data.get("short_description", "")
    benefits = description_data.get("benefits", [])
    
    pricing_templates = {
        "budget": {
            "value_proposition": f"{product_name} - La mejor relaci√≥n calidad-precio",
            "key_message": f"Accesible sin comprometer calidad. {short_desc}",
            "benefits_focus": ["Precio accesible", "Excelente valor", "Sin costos ocultos"],
            "cta": "Obt√©n el mejor precio ahora"
        },
        "value": {
            "value_proposition": f"{product_name} - M√°ximo valor por tu inversi√≥n",
            "key_message": f"Todo lo que necesitas a un precio justo. {short_desc}",
            "benefits_focus": ["Gran valor", "Caracter√≠sticas completas", "Precio competitivo"],
            "cta": "Descubre el valor"
        },
        "premium": {
            "value_proposition": f"{product_name} - Calidad superior que marca la diferencia",
            "key_message": f"Inversi√≥n en excelencia. {short_desc}",
            "benefits_focus": ["Calidad premium", "Rendimiento superior", "Experiencia excepcional"],
            "cta": "Invierte en calidad"
        },
        "luxury": {
            "value_proposition": f"{product_name} - Exclusividad y excelencia",
            "key_message": f"Para quienes buscan lo mejor. {short_desc}",
            "benefits_focus": ["Exclusividad", "M√°xima calidad", "Experiencia √∫nica"],
            "cta": "Accede a la exclusividad"
        }
    }
    
    content = pricing_templates.get(pricing_tier, pricing_templates["value"])
    
    return {
        "pricing_tier": pricing_tier,
        "content": content,
        "product_name": product_name,
        "metadata": {
            "generated_at": datetime.utcnow().isoformat()
        }
    }


def generate_seasonal_content(
    product_name: str,
    description_data: Dict[str, Any],
    season: str = "general",
    holiday: Optional[str] = None
) -> Dict[str, Any]:
    """
    Genera contenido optimizado para temporadas y festividades.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        season: Temporada (spring, summer, fall, winter, general)
        holiday: Festividad espec√≠fica (christmas, valentines, black_friday, etc.)
    
    Returns:
        Dict con contenido estacional optimizado
    """
    short_desc = description_data.get("short_description", "")
    benefits = description_data.get("benefits", [])
    
    seasonal_messages = {
        "spring": {
            "theme": "Renovaci√≥n y crecimiento",
            "message": f"Renueva tu experiencia con {product_name}. {short_desc}",
            "keywords": ["renovaci√≥n", "fresco", "nuevo comienzo", "crecimiento"]
        },
        "summer": {
            "theme": "Energ√≠a y aventura",
            "message": f"Perfecto para el verano. {short_desc}",
            "keywords": ["verano", "energ√≠a", "aventura", "diversi√≥n"]
        },
        "fall": {
            "theme": "Comodidad y preparaci√≥n",
            "message": f"Prep√°rate para el oto√±o con {product_name}. {short_desc}",
            "keywords": ["oto√±o", "comodidad", "preparaci√≥n", "calidez"]
        },
        "winter": {
            "theme": "Calidez y celebraci√≥n",
            "message": f"Ideal para el invierno. {short_desc}",
            "keywords": ["invierno", "calidez", "celebraci√≥n", "hogar"]
        },
        "general": {
            "theme": "Siempre disponible",
            "message": short_desc,
            "keywords": []
        }
    }
    
    holiday_messages = {
        "christmas": {
            "message": f"El regalo perfecto. {short_desc}",
            "cta": "Regala calidad esta Navidad",
            "urgency": "Oferta limitada de temporada"
        },
        "valentines": {
            "message": f"Demuestra tu amor con {product_name}. {short_desc}",
            "cta": "Sorprende a tu ser querido",
            "urgency": "Oferta especial de San Valent√≠n"
        },
        "black_friday": {
            "message": f"Oferta exclusiva Black Friday. {short_desc}",
            "cta": "Aprovecha el descuento ahora",
            "urgency": "Oferta v√°lida solo hoy"
        },
        "mothers_day": {
            "message": f"El regalo perfecto para mam√°. {short_desc}",
            "cta": "Celebra a mam√°",
            "urgency": "Oferta especial D√≠a de la Madre"
        },
        "fathers_day": {
            "message": f"El regalo ideal para pap√°. {short_desc}",
            "cta": "Sorprende a pap√°",
            "urgency": "Oferta especial D√≠a del Padre"
        }
    }
    
    seasonal = seasonal_messages.get(season, seasonal_messages["general"])
    holiday_content = holiday_messages.get(holiday) if holiday else None
    
    return {
        "season": season,
        "holiday": holiday,
        "content": {
            "theme": seasonal["theme"],
            "message": holiday_content["message"] if holiday_content else seasonal["message"],
            "cta": holiday_content["cta"] if holiday_content else f"Descubre {product_name}",
            "urgency": holiday_content["urgency"] if holiday_content else None,
            "keywords": seasonal["keywords"]
        },
        "product_name": product_name,
        "metadata": {
            "generated_at": datetime.utcnow().isoformat()
        }
    }


def generate_urgency_scarcity_content(
    product_name: str,
    description_data: Dict[str, Any],
    urgency_type: str = "limited_time",
    scarcity_level: str = "moderate"
) -> Dict[str, Any]:
    """
    Genera contenido con urgencia y escasez para aumentar conversiones.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        urgency_type: Tipo de urgencia (limited_time, limited_stock, limited_offer, flash_sale)
        scarcity_level: Nivel de escasez (low, moderate, high, extreme)
    
    Returns:
        Dict con contenido de urgencia y escasez
    """
    short_desc = description_data.get("short_description", "")
    
    urgency_messages = {
        "limited_time": {
            "message": f"Oferta por tiempo limitado. {short_desc}",
            "cta": "Aprovecha ahora antes de que termine",
            "badge": "Oferta limitada"
        },
        "limited_stock": {
            "message": f"Solo quedan pocas unidades. {short_desc}",
            "cta": "Asegura el tuyo ahora",
            "badge": "√öltimas unidades"
        },
        "limited_offer": {
            "message": f"Oferta especial disponible por tiempo limitado. {short_desc}",
            "cta": "Aprovecha esta oferta √∫nica",
            "badge": "Oferta especial"
        },
        "flash_sale": {
            "message": f"Flash Sale - Precios incre√≠bles. {short_desc}",
            "cta": "Compra ahora en Flash Sale",
            "badge": "Flash Sale"
        }
    }
    
    scarcity_messages = {
        "low": "Disponible",
        "moderate": "Solo quedan algunos disponibles",
        "high": "Quedan muy pocas unidades",
        "extreme": "√öltimas unidades disponibles"
    }
    
    urgency = urgency_messages.get(urgency_type, urgency_messages["limited_time"])
    scarcity = scarcity_messages.get(scarcity_level, scarcity_messages["moderate"])
    
    return {
        "urgency_type": urgency_type,
        "scarcity_level": scarcity_level,
        "content": {
            "message": urgency["message"],
            "cta": urgency["cta"],
            "badge": urgency["badge"],
            "scarcity_message": scarcity,
            "urgency_indicators": [
                "‚è∞ Tiempo limitado",
                "üî• Oferta especial",
                "‚ö° No te lo pierdas"
            ]
        },
        "product_name": product_name,
        "metadata": {
            "generated_at": datetime.utcnow().isoformat()
        }
    }


def generate_testimonial_integration_content(
    product_name: str,
    description_data: Dict[str, Any],
    testimonials: List[Dict[str, Any]],
    integration_style: str = "seamless"
) -> Dict[str, Any]:
    """
    Integra testimonios de manera natural en las descripciones.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        testimonials: Lista de testimonios con {name, text, rating, role}
        integration_style: Estilo (seamless, highlighted, social_proof)
    
    Returns:
        Dict con descripci√≥n mejorada con testimonios
    """
    short_desc = description_data.get("short_description", "")
    benefits = description_data.get("benefits", [])
    
    if not testimonials:
        return {
            "description": short_desc,
            "testimonials": [],
            "integration_style": integration_style
        }
    
    top_testimonial = max(testimonials, key=lambda t: t.get("rating", 0))
    
    integration_templates = {
        "seamless": {
            "format": f"{short_desc}\n\nComo dice {top_testimonial.get('name', 'nuestro cliente')}: \"{top_testimonial.get('text', '')[:100]}...\"",
            "testimonial_position": "embedded"
        },
        "highlighted": {
            "format": f"{short_desc}\n\n‚≠ê Testimonio destacado:\n\"{top_testimonial.get('text', '')}\" - {top_testimonial.get('name', 'Cliente')}",
            "testimonial_position": "separated"
        },
        "social_proof": {
            "format": f"{short_desc}\n\n√önete a {len(testimonials)} clientes satisfechos:\n\"{top_testimonial.get('text', '')[:80]}...\" - {top_testimonial.get('name', 'Cliente')} ‚≠ê{top_testimonial.get('rating', 5)}/5",
            "testimonial_position": "prominent"
        }
    }
    
    template = integration_templates.get(integration_style, integration_templates["seamless"])
    
    return {
        "description": template["format"],
        "testimonials": testimonials,
        "integration_style": integration_style,
        "testimonial_position": template["testimonial_position"],
        "product_name": product_name,
        "metadata": {
            "generated_at": datetime.utcnow().isoformat(),
            "testimonials_count": len(testimonials),
            "average_rating": sum(t.get("rating", 0) for t in testimonials) / len(testimonials) if testimonials else 0
        }
    }


def generate_cross_sell_upsell_content(
    product_name: str,
    description_data: Dict[str, Any],
    related_products: List[Dict[str, Any]],
    strategy: str = "complementary"
) -> Dict[str, Any]:
    """
    Genera contenido para cross-sell y upsell.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        related_products: Lista de productos relacionados
        strategy: Estrategia (complementary, upgrade, bundle, frequently_bought)
    
    Returns:
        Dict con contenido de cross-sell/upsell
    """
    short_desc = description_data.get("short_description", "")
    
    strategy_templates = {
        "complementary": {
            "message": f"{short_desc}\n\nCompleta tu experiencia con productos complementarios:",
            "cta": "Ver productos complementarios",
            "rationale": "Mejora tu experiencia"
        },
        "upgrade": {
            "message": f"{short_desc}\n\n¬øBuscas m√°s? Descubre nuestra versi√≥n premium:",
            "cta": "Ver versi√≥n premium",
            "rationale": "M√°s caracter√≠sticas y beneficios"
        },
        "bundle": {
            "message": f"{short_desc}\n\nAhorra comprando en paquete:",
            "cta": "Ver paquete completo",
            "rationale": "Ahorra hasta un 20%"
        },
        "frequently_bought": {
            "message": f"{short_desc}\n\nOtros clientes tambi√©n compraron:",
            "cta": "Ver productos relacionados",
            "rationale": "Basado en compras de otros clientes"
        }
    }
    
    template = strategy_templates.get(strategy, strategy_templates["complementary"])
    
    return {
        "strategy": strategy,
        "main_product": product_name,
        "related_products": related_products,
        "content": {
            "message": template["message"],
            "cta": template["cta"],
            "rationale": template["rationale"],
            "product_suggestions": [
                {
                    "name": p.get("name", ""),
                    "reason": p.get("reason", "Producto relacionado"),
                    "discount": p.get("discount", 0)
                }
                for p in related_products[:3]
            ]
        },
        "metadata": {
            "generated_at": datetime.utcnow().isoformat()
        }
    }


def generate_mobile_optimized_content(
    product_name: str,
    description_data: Dict[str, Any],
    mobile_format: str = "compact"
) -> Dict[str, Any]:
    """
    Genera contenido optimizado para dispositivos m√≥viles.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        mobile_format: Formato (compact, swipeable, voice_optimized)
    
    Returns:
        Dict con contenido optimizado para m√≥vil
    """
    short_desc = description_data.get("short_description", "")
    benefits = description_data.get("benefits", [])
    
    mobile_templates = {
        "compact": {
            "description": short_desc[:150] + "..." if len(short_desc) > 150 else short_desc,
            "benefits": [b[:50] for b in benefits[:3]],
            "cta": "Comprar ahora",
            "format": "single_column"
        },
        "swipeable": {
            "description": short_desc,
            "benefits": benefits[:5],
            "cta": "Desliza para ver m√°s",
            "format": "carousel",
            "cards": [
                {"title": "Caracter√≠sticas", "content": short_desc},
                {"title": "Beneficios", "content": "\n".join(benefits[:3])},
                {"title": "Especificaciones", "content": "Ver detalles t√©cnicos"}
            ]
        },
        "voice_optimized": {
            "description": short_desc,
            "benefits": benefits,
            "cta": "Escuchar descripci√≥n",
            "format": "voice",
            "voice_script": f"{product_name}. {short_desc}. Beneficios principales: {', '.join(benefits[:3])}."
        }
    }
    
    template = mobile_templates.get(mobile_format, mobile_templates["compact"])
    
    return {
        "mobile_format": mobile_format,
        "content": template,
        "product_name": product_name,
        "metadata": {
            "generated_at": datetime.utcnow().isoformat(),
            "optimized_for": "mobile",
            "character_limit": 150 if mobile_format == "compact" else None
        }
    }


def generate_accessibility_enhanced_content(
    product_name: str,
    description_data: Dict[str, Any],
    accessibility_features: List[str] = None
) -> Dict[str, Any]:
    """
    Genera contenido mejorado para accesibilidad.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        accessibility_features: Lista de caracter√≠sticas de accesibilidad
    
    Returns:
        Dict con contenido accesible
    """
    if accessibility_features is None:
        accessibility_features = ["screen_reader", "high_contrast", "large_text"]
    
    short_desc = description_data.get("short_description", "")
    benefits = description_data.get("benefits", [])
    
    # Contenido con estructura clara para screen readers
    structured_content = {
        "heading": f"Descripci√≥n de {product_name}",
        "main_description": short_desc,
        "benefits_list": benefits,
        "alt_text": f"Producto {product_name}: {short_desc}",
        "aria_labels": {
            "description": f"Descripci√≥n del producto {product_name}",
            "benefits": f"Lista de beneficios de {product_name}",
            "cta": f"Bot√≥n para comprar {product_name}"
        }
    }
    
    # Versi√≥n simplificada para lectura f√°cil
    easy_read_version = {
        "title": product_name,
        "description": short_desc,
        "key_points": benefits[:3],
        "reading_level": "simple"
    }
    
    return {
        "accessibility_features": accessibility_features,
        "structured_content": structured_content,
        "easy_read_version": easy_read_version,
        "product_name": product_name,
        "metadata": {
            "generated_at": datetime.utcnow().isoformat(),
            "wcag_compliant": True,
            "screen_reader_optimized": "screen_reader" in accessibility_features
        }
    }


def generate_ar_vr_content(
    product_name: str,
    description_data: Dict[str, Any],
    ar_vr_type: str = "ar_preview"
) -> Dict[str, Any]:
    """
    Genera contenido para experiencias AR/VR.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        ar_vr_type: Tipo (ar_preview, vr_experience, 3d_model)
    
    Returns:
        Dict con contenido AR/VR
    """
    short_desc = description_data.get("short_description", "")
    benefits = description_data.get("benefits", [])
    
    ar_vr_templates = {
        "ar_preview": {
            "message": f"Visualiza {product_name} en tu espacio con AR",
            "cta": "Ver en AR",
            "instructions": "Apunta tu c√°mara para ver el producto en tu espacio",
            "features": ["Visualizaci√≥n 3D", "Tama√±o real", "Colores disponibles"]
        },
        "vr_experience": {
            "message": f"Experimenta {product_name} en realidad virtual",
            "cta": "Iniciar experiencia VR",
            "instructions": "Usa tus gafas VR para una experiencia inmersiva",
            "features": ["Experiencia inmersiva", "Interacci√≥n 360¬∞", "Detalles realistas"]
        },
        "3d_model": {
            "message": f"Explora {product_name} en 3D",
            "cta": "Ver modelo 3D",
            "instructions": "Gira y zoom para ver todos los detalles",
            "features": ["Rotaci√≥n 360¬∞", "Zoom interactivo", "Vista detallada"]
        }
    }
    
    template = ar_vr_templates.get(ar_vr_type, ar_vr_templates["ar_preview"])
    
    return {
        "ar_vr_type": ar_vr_type,
        "content": {
            "message": template["message"],
            "cta": template["cta"],
            "instructions": template["instructions"],
            "features": template["features"],
            "description": short_desc,
            "benefits": benefits
        },
        "product_name": product_name,
        "metadata": {
            "generated_at": datetime.utcnow().isoformat(),
            "requires_ar_vr": True
        }
    }


def generate_interactive_content(
    product_name: str,
    description_data: Dict[str, Any],
    interactivity_type: str = "quiz"
) -> Dict[str, Any]:
    """
    Genera contenido interactivo para engagement.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        interactivity_type: Tipo (quiz, calculator, configurator, comparison_tool)
    
    Returns:
        Dict con contenido interactivo
    """
    short_desc = description_data.get("short_description", "")
    benefits = description_data.get("benefits", [])
    
    interactive_templates = {
        "quiz": {
            "title": f"¬øEs {product_name} ideal para ti?",
            "description": "Responde nuestro quiz para descubrirlo",
            "questions": [
                "¬øQu√© buscas en un producto como este?",
                "¬øCu√°l es tu presupuesto?",
                "¬øQu√© caracter√≠sticas son m√°s importantes para ti?"
            ],
            "cta": "Comenzar quiz"
        },
        "calculator": {
            "title": f"Calcula tu ahorro con {product_name}",
            "description": "Descubre cu√°nto puedes ahorrar",
            "inputs": ["Uso mensual", "Costo actual", "Per√≠odo"],
            "cta": "Calcular ahorro"
        },
        "configurator": {
            "title": f"Personaliza tu {product_name}",
            "description": "Elige caracter√≠sticas y opciones",
            "options": ["Color", "Tama√±o", "Caracter√≠sticas adicionales"],
            "cta": "Configurar producto"
        },
        "comparison_tool": {
            "title": f"Compara {product_name} con otras opciones",
            "description": "Ve las diferencias lado a lado",
            "comparison_points": benefits,
            "cta": "Comparar productos"
        }
    }
    
    template = interactive_templates.get(interactivity_type, interactive_templates["quiz"])
    
    return {
        "interactivity_type": interactivity_type,
        "content": template,
        "product_name": product_name,
        "description": short_desc,
        "metadata": {
            "generated_at": datetime.utcnow().isoformat(),
            "engagement_tool": True
        }
    }


def generate_live_shopping_content(
    product_name: str,
    description_data: Dict[str, Any],
    live_event_type: str = "live_demo"
) -> Dict[str, Any]:
    """
    Genera contenido para eventos de live shopping.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        live_event_type: Tipo (live_demo, qna, unboxing, tutorial)
    
    Returns:
        Dict con contenido para live shopping
    """
    short_desc = description_data.get("short_description", "")
    benefits = description_data.get("benefits", [])
    
    live_templates = {
        "live_demo": {
            "title": f"Demo en vivo: {product_name}",
            "description": "√önete a nuestra demostraci√≥n en vivo",
            "highlights": ["Ver el producto en acci√≥n", "Preguntas en tiempo real", "Ofertas exclusivas"],
            "cta": "√önete al live",
            "duration": "30 minutos"
        },
        "qna": {
            "title": f"Pregunta sobre {product_name}",
            "description": "Sesi√≥n de preguntas y respuestas en vivo",
            "highlights": ["Respuestas directas", "Especificaciones t√©cnicas", "Casos de uso"],
            "cta": "Hacer una pregunta",
            "duration": "20 minutos"
        },
        "unboxing": {
            "title": f"Unboxing en vivo: {product_name}",
            "description": "Abre el producto con nosotros",
            "highlights": ["Primera impresi√≥n", "Contenido del paquete", "Ofertas especiales"],
            "cta": "Ver unboxing",
            "duration": "15 minutos"
        },
        "tutorial": {
            "title": f"Tutorial en vivo: {product_name}",
            "description": "Aprende a usar el producto",
            "highlights": ["Gu√≠a paso a paso", "Tips y trucos", "Soporte en vivo"],
            "cta": "Unirse al tutorial",
            "duration": "45 minutos"
        }
    }
    
    template = live_templates.get(live_event_type, live_templates["live_demo"])
    
    return {
        "live_event_type": live_event_type,
        "content": template,
        "product_name": product_name,
        "description": short_desc,
        "benefits": benefits,
        "metadata": {
            "generated_at": datetime.utcnow().isoformat(),
            "live_event": True
        }
    }


def generate_sustainability_content(
    product_name: str,
    description_data: Dict[str, Any],
    sustainability_focus: List[str] = None
) -> Dict[str, Any]:
    """
    Genera contenido enfocado en sostenibilidad y responsabilidad ambiental.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        sustainability_focus: Enfoques (eco_friendly, carbon_neutral, recyclable, ethical)
    
    Returns:
        Dict con contenido de sostenibilidad
    """
    if sustainability_focus is None:
        sustainability_focus = ["eco_friendly", "recyclable"]
    
    short_desc = description_data.get("short_description", "")
    benefits = description_data.get("benefits", [])
    
    sustainability_messages = {
        "eco_friendly": {
            "badge": "üå± Eco-friendly",
            "message": f"{product_name} est√° dise√±ado con el medio ambiente en mente",
            "details": "Materiales sostenibles y procesos respetuosos con el planeta"
        },
        "carbon_neutral": {
            "badge": "‚ôªÔ∏è Carbono neutral",
            "message": f"{product_name} es carbono neutral",
            "details": "Compensamos todas las emisiones de carbono"
        },
        "recyclable": {
            "badge": "‚ôªÔ∏è Reciclable",
            "message": f"{product_name} es 100% reciclable",
            "details": "Todos los materiales pueden ser reciclados"
        },
        "ethical": {
            "badge": "‚ú® Comercio √©tico",
            "message": f"{product_name} es producido de manera √©tica",
            "details": "Fair trade y condiciones laborales justas"
        }
    }
    
    sustainability_content = []
    for focus in sustainability_focus:
        if focus in sustainability_messages:
            sustainability_content.append(sustainability_messages[focus])
    
    return {
        "sustainability_focus": sustainability_focus,
        "content": {
            "description": f"{short_desc}\n\nComprometidos con la sostenibilidad:",
            "sustainability_features": sustainability_content,
            "benefits": benefits,
            "cta": "Elige sostenibilidad"
        },
        "product_name": product_name,
        "metadata": {
            "generated_at": datetime.utcnow().isoformat(),
            "sustainability_certified": True
        }
    }


def generate_gamification_content(
    product_name: str,
    description_data: Dict[str, Any],
    gamification_type: str = "points"
) -> Dict[str, Any]:
    """
    Genera contenido con elementos de gamificaci√≥n.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        gamification_type: Tipo (points, badges, challenges, rewards)
    
    Returns:
        Dict con contenido gamificado
    """
    short_desc = description_data.get("short_description", "")
    
    gamification_templates = {
        "points": {
            "message": f"Gana puntos al comprar {product_name}",
            "points_value": 100,
            "benefit": "Canjea puntos por descuentos",
            "cta": "Compra y gana puntos"
        },
        "badges": {
            "message": f"Desbloquea la insignia '{product_name} Explorer'",
            "badge_name": f"{product_name} Explorer",
            "benefit": "Muestra tu logro",
            "cta": "Desbloquear insignia"
        },
        "challenges": {
            "message": f"Completa el desaf√≠o de {product_name}",
            "challenge": "Compra y comparte tu experiencia",
            "reward": "Descuento del 15%",
            "cta": "Aceptar desaf√≠o"
        },
        "rewards": {
            "message": f"Obt√©n recompensas exclusivas con {product_name}",
            "rewards": ["Descuento del 10%", "Env√≠o gratis", "Producto adicional"],
            "cta": "Ver recompensas"
        }
    }
    
    template = gamification_templates.get(gamification_type, gamification_templates["points"])
    
    return {
        "gamification_type": gamification_type,
        "content": {
            "message": template["message"],
            "description": short_desc,
            "gamification_details": template,
            "cta": template["cta"]
        },
        "product_name": product_name,
        "metadata": {
            "generated_at": datetime.utcnow().isoformat(),
            "engagement_boost": True
        }
    }


def generate_community_driven_content(
    product_name: str,
    description_data: Dict[str, Any],
    community_type: str = "user_generated"
) -> Dict[str, Any]:
    """
    Genera contenido basado en la comunidad.
    
    Args:
        product_name: Nombre del producto
        description_data: Datos de descripci√≥n
        community_type: Tipo (user_generated, reviews, forums, social_proof)
    
    Returns:
        Dict con contenido de comunidad
    """
    short_desc = description_data.get("short_description", "")
    benefits = description_data.get("benefits", [])
    
    community_templates = {
        "user_generated": {
            "message": f"√önete a miles de usuarios de {product_name}",
            "content_source": "Contenido creado por usuarios",
            "cta": "Ver contenido de usuarios",
            "social_proof": "10,000+ usuarios activos"
        },
        "reviews": {
            "message": f"Lee lo que dicen nuestros usuarios sobre {product_name}",
            "content_source": "Rese√±as verificadas",
            "cta": "Leer rese√±as",
            "social_proof": "4.8/5 estrellas"
        },
        "forums": {
            "message": f"√önete a la discusi√≥n sobre {product_name}",
            "content_source": "Foro de la comunidad",
            "cta": "Unirse al foro",
            "social_proof": "5,000+ discusiones activas"
        },
        "social_proof": {
            "message": f"{product_name} - Elegido por miles",
            "content_source": "Testimonios de la comunidad",
            "cta": "Ver testimonios",
            "social_proof": "50,000+ clientes satisfechos"
        }
    }
    
    template = community_templates.get(community_type, community_templates["user_generated"])
    
    return {
        "community_type": community_type,
        "content": {
            "message": template["message"],
            "description": short_desc,
            "benefits": benefits,
            "content_source": template["content_source"],
            "social_proof": template["social_proof"],
            "cta": template["cta"]
        },
        "product_name": product_name,
        "metadata": {
            "generated_at": datetime.utcnow().isoformat(),
            "community_driven": True
        }
    }

# Advanced API Versioning
def _handle_api_versioning(endpoint: str, requested_version: str = None, default_version: str = "v1", conn_id: str = None) -> Dict[str, Any]:
    """Maneja versionado de API con backward compatibility."""
    if requested_version is None:
        requested_version = default_version
    
    # Obtener versi√≥n soportada
    supported_versions = ["v1", "v2", "v3"]
    
    if requested_version not in supported_versions:
        # Intentar backward compatibility
        major_version = requested_version.split(".")[0] if "." in requested_version else requested_version
        
        if major_version in supported_versions:
            requested_version = major_version
        else:
            # Usar versi√≥n por defecto
            requested_version = default_version
    
    version_info = {
        "requested_version": requested_version,
        "supported_versions": supported_versions,
        "using_version": requested_version,
        "is_latest": requested_version == supported_versions[-1],
        "is_deprecated": requested_version in ["v1"],  # Ejemplo
        "endpoint": endpoint
    }
    
    # Construir endpoint versionado
    versioned_endpoint = f"/{requested_version}{endpoint}" if not endpoint.startswith("/") else f"/{requested_version}{endpoint}"
    version_info["versioned_endpoint"] = versioned_endpoint
    
    return version_info

# Advanced Token Bucket Rate Limiting
def _check_token_bucket_advanced(identifier: str, tokens: int = 1, capacity: int = 10, refill_rate: float = 1.0, conn_id: str = None) -> Tuple[bool, Optional[float], Dict[str, Any]]:
    """Token bucket rate limiting avanzado con tracking detallado."""
    try:
        if VARIABLES_AVAILABLE:
            bucket_key = f"token_bucket_{identifier}"
            bucket_data_str = Variable.get(bucket_key, default_var="{}")
            bucket_data = json.loads(bucket_data_str) if bucket_data_str else {}
            
            current_tokens = bucket_data.get("tokens", capacity)
            last_refill = bucket_data.get("last_refill", time.time())
            capacity = bucket_data.get("capacity", capacity)
            refill_rate = bucket_data.get("refill_rate", refill_rate)
            
            # Calcular tokens a agregar basado en tiempo transcurrido
            now = time.time()
            time_elapsed = now - last_refill
            tokens_to_add = time_elapsed * refill_rate
            current_tokens = min(capacity, current_tokens + tokens_to_add)
            
            # Verificar si hay suficientes tokens
            if current_tokens >= tokens:
                current_tokens -= tokens
                allowed = True
                wait_time = None
            else:
                allowed = False
                # Calcular tiempo de espera
                tokens_needed = tokens - current_tokens
                wait_time = tokens_needed / refill_rate
            
            # Actualizar bucket
            bucket_data.update({
                "tokens": current_tokens,
                "last_refill": now,
                "capacity": capacity,
                "refill_rate": refill_rate
            })
            
            Variable.set(bucket_key, json.dumps(bucket_data))
            
            return allowed, wait_time, {
                "tokens_remaining": current_tokens,
                "capacity": capacity,
                "refill_rate": refill_rate
            }
    except Exception as e:
        logger.warning("token_bucket_check_failed", identifier=identifier, error=str(e))
    
    # Fallback: permitir si falla
    return True, None, {}

# Advanced Write-Through Caching
def _cache_write_through(key: str, value: Any, write_func: Callable, ttl: int = 3600, conn_id: str = None) -> bool:
    """Cache write-through: escribe en cache y almacenamiento persistente simult√°neamente."""
    try:
        # Escribir en almacenamiento persistente primero
        write_success = write_func(value) if callable(write_func) else False
        
        if write_success:
            # Escribir en cache
            cache_success = _cache_with_redis(key, value, ttl, conn_id) if redis_client else False
            
            if not cache_success and enrichment_cache:
                enrichment_cache[key] = value
                cache_success = True
            
            return cache_success
        
        return False
    except Exception as e:
        logger.error("write_through_cache_failed", key=key, error=str(e))
        return False

# Advanced Write-Back Caching
def _cache_write_back(key: str, value: Any, write_func: Callable, ttl: int = 3600, conn_id: str = None) -> bool:
    """Cache write-back: escribe en cache primero, luego en almacenamiento persistente de forma as√≠ncrona."""
    try:
        # Escribir en cache primero
        cache_success = _cache_with_redis(key, value, ttl, conn_id) if redis_client else False
        
        if not cache_success and enrichment_cache:
            enrichment_cache[key] = value
            cache_success = True
        
        if cache_success:
            # Marcar para escritura as√≠ncrona
            if VARIABLES_AVAILABLE:
                write_back_key = f"write_back_queue_{key}"
                Variable.set(write_back_key, json.dumps({
                    "key": key,
                    "value": value,
                    "write_func": str(write_func),
                    "timestamp": datetime.utcnow().isoformat()
                }))
            
            # En producci√≥n, esto se ejecutar√≠a de forma as√≠ncrona
            # Por ahora, ejecutar de forma s√≠ncrona
            try:
                if callable(write_func):
                    write_func(value)
            except Exception as e:
                logger.warning("write_back_async_failed", key=key, error=str(e))
        
        return cache_success
    except Exception as e:
        logger.error("write_back_cache_failed", key=key, error=str(e))
        return False

# Advanced Error Classification
def _classify_error_advanced(error: Exception, context: Dict[str, Any] = None) -> Dict[str, Any]:
    """Clasificaci√≥n avanzada de errores con categorizaci√≥n detallada."""
    error_type = type(error).__name__
    error_message = str(error)
    
    classification = {
        "error_type": error_type,
        "error_message": error_message,
        "category": "unknown",
        "severity": "medium",
        "retryable": False,
        "transient": False,
        "user_error": False,
        "system_error": False,
        "tags": []
    }
    
    # Clasificar por tipo
    if error_type in ["ConnectionError", "TimeoutError", "HTTPError"]:
        classification["category"] = "network"
        classification["severity"] = "medium"
        classification["retryable"] = True
        classification["transient"] = True
        classification["system_error"] = True
        classification["tags"].append("network")
    
    elif error_type in ["ValueError", "KeyError", "TypeError"]:
        classification["category"] = "validation"
        classification["severity"] = "low"
        classification["retryable"] = False
        classification["user_error"] = True
        classification["tags"].append("validation")
    
    elif error_type in ["PermissionError", "AuthenticationError"]:
        classification["category"] = "security"
        classification["severity"] = "high"
        classification["retryable"] = False
        classification["user_error"] = True
        classification["tags"].append("security")
    
    elif error_type in ["MemoryError", "OSError"]:
        classification["category"] = "resource"
        classification["severity"] = "high"
        classification["retryable"] = True
        classification["transient"] = True
        classification["system_error"] = True
        classification["tags"].append("resource")
    
    # Clasificar por mensaje
    error_lower = error_message.lower()
    if "timeout" in error_lower or "timed out" in error_lower:
        classification["category"] = "timeout"
        classification["retryable"] = True
        classification["transient"] = True
        classification["tags"].append("timeout")
    
    elif "rate limit" in error_lower or "too many requests" in error_lower:
        classification["category"] = "rate_limit"
        classification["retryable"] = True
        classification["transient"] = True
        classification["tags"].append("rate_limit")
    
    return classification

# Advanced Structured Logging
def _log_with_context(event_name: str, level: str = "info", **kwargs) -> None:
    """Logging estructurado avanzado con contexto enriquecido."""
    log_data = {
        "event": event_name,
        "timestamp": datetime.utcnow().isoformat(),
        "level": level,
        **kwargs
    }
    
    # Agregar contexto adicional si est√° disponible
    try:
        ctx = get_current_context()
        log_data["dag_run_id"] = ctx.get("dag_run_id")
        log_data["task_id"] = ctx.get("task_instance", {}).get("task_id")
    except Exception:
        pass
    
    # Log seg√∫n nivel
    if level == "debug":
        logger.debug(event_name, **log_data)
    elif level == "info":
        logger.info(event_name, **log_data)
    elif level == "warning":
        logger.warning(event_name, **log_data)
    elif level == "error":
        logger.error(event_name, **log_data)
    else:
        logger.info(event_name, **log_data)

# ============================================================================
# FUNCIONES HELPER ULTRA AVANZADAS - ENTERPRISE SUPREME FEATURES
# ============================================================================

# Advanced Data Transformation Pipeline Builder
def _build_transformation_pipeline(transformations: List[Dict[str, Any]]) -> Callable:
    """Construye pipeline de transformaci√≥n de datos reutilizable."""
    def pipeline(data: Dict[str, Any]) -> Dict[str, Any]:
        result = data.copy()
        
        for transform in transformations:
            transform_type = transform.get("type")
            config = transform.get("config", {})
            
            try:
                if transform_type == "map":
                    # Mapear campos
                    field_mapping = config.get("mapping", {})
                    for old_field, new_field in field_mapping.items():
                        if old_field in result:
                            result[new_field] = result.pop(old_field)
                
                elif transform_type == "filter":
                    # Filtrar campos
                    keep_fields = config.get("keep", [])
                    if keep_fields:
                        result = {k: v for k, v in result.items() if k in keep_fields}
                
                elif transform_type == "normalize":
                    # Normalizar valores
                    fields = config.get("fields", [])
                    for field in fields:
                        if field in result and isinstance(result[field], str):
                            result[field] = result[field].strip().lower()
                
                elif transform_type == "enrich":
                    # Enriquecer con datos externos
                    sources = config.get("sources", [])
                    result = _enrich_from_multiple_sources(result, sources)
                
                elif transform_type == "validate":
                    # Validar datos
                    schema = config.get("schema")
                    if schema:
                        is_valid, errors, warnings = _validate_with_dynamic_schema(result, schema)
                        if not is_valid:
                            result["validation_errors"] = errors
                            result["validation_warnings"] = warnings
                
                elif transform_type == "custom":
                    # Transformaci√≥n personalizada
                    custom_func = config.get("function")
                    if custom_func and callable(custom_func):
                        result = custom_func(result)
            except Exception as e:
                logger.warning("transformation_step_failed", transform_type=transform_type, error=str(e))
                result["transformation_errors"] = result.get("transformation_errors", []) + [str(e)]
        
        return result
    
    return pipeline

# Advanced Monitoring Dashboard Generator
def _generate_monitoring_dashboard(metrics: Dict[str, Any], dashboard_type: str = "real_time", time_range: str = "24h", conn_id: str = None) -> Dict[str, Any]:
    """Genera dashboard de monitoreo avanzado con m√∫ltiples widgets."""
    dashboard = {
        "dashboard_id": f"dashboard_{datetime.utcnow().timestamp()}",
        "dashboard_type": dashboard_type,
        "time_range": time_range,
        "generated_at": datetime.utcnow().isoformat(),
        "widgets": []
    }
    
    # Widget: M√©tricas clave
    dashboard["widgets"].append({
        "type": "key_metrics",
        "title": "Key Metrics",
        "metrics": {
            "total_leads": metrics.get("total_leads", 0),
            "qualified_leads": metrics.get("qualified_leads", 0),
            "avg_score": metrics.get("avg_score", 0),
            "conversion_rate": metrics.get("conversion_rate", 0)
        }
    })
    
    # Widget: Gr√°fico de tendencias
    if metrics.get("trends"):
        dashboard["widgets"].append({
            "type": "trend_chart",
            "title": "Trends",
            "data": metrics.get("trends", {})
        })
    
    # Widget: Alertas activas
    if metrics.get("active_alerts"):
        dashboard["widgets"].append({
            "type": "alerts",
            "title": "Active Alerts",
            "alerts": metrics.get("active_alerts", [])
        })
    
    # Widget: Performance metrics
    if metrics.get("performance"):
        dashboard["widgets"].append({
            "type": "performance",
            "title": "Performance",
            "metrics": metrics.get("performance", {})
        })
    
    # Widget: Error rates
    if metrics.get("errors"):
        dashboard["widgets"].append({
            "type": "error_rates",
            "title": "Error Rates",
            "errors": metrics.get("errors", {})
        })
    
    return dashboard

# Advanced Security Scanning
def _perform_advanced_security_scan(data: Dict[str, Any], scan_level: str = "comprehensive") -> Dict[str, Any]:
    """Escaneo de seguridad avanzado con m√∫ltiples checks."""
    scan_results = {
        "scan_id": f"scan_{datetime.utcnow().timestamp()}",
        "scan_level": scan_level,
        "timestamp": datetime.utcnow().isoformat(),
        "vulnerabilities": [],
        "threats": [],
        "risk_score": 0,
        "recommendations": []
    }
    
    # Check 1: SQL Injection
    sql_patterns = ["'", '"', ";", "--", "/*", "*/", "xp_", "exec", "union", "select"]
    for field, value in data.items():
        if isinstance(value, str):
            for pattern in sql_patterns:
                if pattern.lower() in value.lower():
                    scan_results["vulnerabilities"].append({
                        "type": "sql_injection",
                        "field": field,
                        "severity": "high",
                        "pattern": pattern
                    })
                    scan_results["risk_score"] += 10
    
    # Check 2: XSS
    xss_patterns = ["<script", "javascript:", "onerror=", "onload=", "eval("]
    for field, value in data.items():
        if isinstance(value, str):
            for pattern in xss_patterns:
                if pattern.lower() in value.lower():
                    scan_results["vulnerabilities"].append({
                        "type": "xss",
                        "field": field,
                        "severity": "high",
                        "pattern": pattern
                    })
                    scan_results["risk_score"] += 10
    
    # Check 3: Command Injection
    cmd_patterns = ["|", "&", ";", "`", "$(", "<", ">"]
    for field, value in data.items():
        if isinstance(value, str):
            for pattern in cmd_patterns:
                if pattern in value:
                    scan_results["vulnerabilities"].append({
                        "type": "command_injection",
                        "field": field,
                        "severity": "critical",
                        "pattern": pattern
                    })
                    scan_results["risk_score"] += 15
    
    # Check 4: Path Traversal
    path_patterns = ["../", "..\\", "/etc/", "C:\\"]
    for field, value in data.items():
        if isinstance(value, str):
            for pattern in path_patterns:
                if pattern in value:
                    scan_results["threats"].append({
                        "type": "path_traversal",
                        "field": field,
                        "severity": "high",
                        "pattern": pattern
                    })
                    scan_results["risk_score"] += 12
    
    # Check 5: Sensitive Data Exposure
    sensitive_patterns = ["password", "secret", "api_key", "token", "ssn", "credit_card"]
    for field, value in data.items():
        if isinstance(value, str):
            for pattern in sensitive_patterns:
                if pattern.lower() in field.lower() or pattern.lower() in value.lower():
                    scan_results["threats"].append({
                        "type": "sensitive_data_exposure",
                        "field": field,
                        "severity": "critical",
                        "pattern": pattern
                    })
                    scan_results["risk_score"] += 20
    
    # Calcular nivel de riesgo
    if scan_results["risk_score"] >= 50:
        risk_level = "critical"
    elif scan_results["risk_score"] >= 30:
        risk_level = "high"
    elif scan_results["risk_score"] >= 15:
        risk_level = "medium"
    else:
        risk_level = "low"
    
    scan_results["risk_level"] = risk_level
    
    # Generar recomendaciones
    if scan_results["vulnerabilities"]:
        scan_results["recommendations"].append("Sanitize all user inputs")
    if scan_results["threats"]:
        scan_results["recommendations"].append("Implement input validation and output encoding")
    
    return scan_results

# Advanced Data Lineage Tracking
def _track_data_lineage_advanced(source: str, transformation: str, destination: str, metadata: Dict[str, Any] = None, conn_id: str = None) -> str:
    """Tracking avanzado de linaje de datos con metadata completa."""
    lineage_id = f"lineage_{datetime.utcnow().timestamp()}_{hashlib.md5(f'{source}{transformation}{destination}'.encode()).hexdigest()[:8]}"
    
    lineage_record = {
        "lineage_id": lineage_id,
        "source": source,
        "transformation": transformation,
        "destination": destination,
        "metadata": metadata or {},
        "timestamp": datetime.utcnow().isoformat(),
        "dag_run_id": None,
        "task_id": None
    }
    
    try:
        ctx = get_current_context()
        lineage_record["dag_run_id"] = ctx.get("dag_run_id")
        lineage_record["task_id"] = ctx.get("task_instance", {}).get("task_id")
    except Exception:
        pass
    
    try:
        if VARIABLES_AVAILABLE:
            lineage_key = f"data_lineage_{lineage_id}"
            Variable.set(lineage_key, json.dumps(lineage_record))
            
            # Agregar a √≠ndice de linaje
            lineage_index_key = "data_lineage_index"
            index_data = Variable.get(lineage_index_key, default_var="[]")
            index = json.loads(index_data) if index_data else []
            index.append(lineage_id)
            
            # Mantener solo √∫ltimos 10000 registros
            if len(index) > 10000:
                index = index[-10000:]
            
            Variable.set(lineage_index_key, json.dumps(index))
        
        logger.debug("data_lineage_tracked", lineage_id=lineage_id, source=source, destination=destination)
        return lineage_id
    except Exception as e:
        logger.warning("data_lineage_tracking_failed", error=str(e))
        return ""

# Advanced Performance Optimization
def _optimize_performance_advanced(operation_name: str, func: Callable, optimization_strategies: List[str] = None, *args, **kwargs) -> Tuple[Any, Dict[str, Any]]:
    """Optimizaci√≥n avanzada de performance con m√∫ltiples estrategias."""
    if optimization_strategies is None:
        optimization_strategies = ["caching", "memoization", "parallelization"]
    
    optimization_results = {
        "operation": operation_name,
        "strategies_applied": [],
        "performance_improvement": 0.0,
        "baseline_duration": 0.0,
        "optimized_duration": 0.0
    }
    
    # Baseline
    baseline_start = time.time()
    try:
        baseline_result = func(*args, **kwargs)
        baseline_duration = time.time() - baseline_start
        optimization_results["baseline_duration"] = baseline_duration
    except Exception as e:
        raise
    
    # Aplicar optimizaciones
    optimized_result = baseline_result
    optimized_start = time.time()
    
    for strategy in optimization_strategies:
        try:
            if strategy == "caching":
                # Intentar obtener de cache
                cache_key = f"opt_cache_{operation_name}_{hashlib.md5(str(args).encode()).hexdigest()[:16]}"
                cached_result = _get_from_redis_cache(cache_key, conn_id=None)
                if cached_result:
                    optimized_result = cached_result
                    optimization_results["strategies_applied"].append("cache_hit")
                    break
                else:
                    # Guardar en cache
                    _cache_with_redis(cache_key, baseline_result, ttl=3600, conn_id=None)
                    optimization_results["strategies_applied"].append("cache_miss")
            
            elif strategy == "memoization":
                # Memoization (similar a caching pero m√°s espec√≠fico)
                memo_key = f"memo_{operation_name}_{hashlib.md5(str(args).encode()).hexdigest()[:16]}"
                memo_result = _get_from_redis_cache(memo_key, conn_id=None)
                if memo_result:
                    optimized_result = memo_result
                    optimization_results["strategies_applied"].append("memoization_hit")
                    break
            
            elif strategy == "parallelization":
                # Si es una lista, procesar en paralelo
                if isinstance(baseline_result, list) and len(baseline_result) > 1:
                    optimized_result = _process_async(baseline_result, lambda x: x, max_workers=4, conn_id=None)
                    optimization_results["strategies_applied"].append("parallelization")
        except Exception as e:
            logger.warning("optimization_strategy_failed", strategy=strategy, error=str(e))
    
    optimized_duration = time.time() - optimized_start
    optimization_results["optimized_duration"] = optimized_duration
    
    if baseline_duration > 0:
        improvement = ((baseline_duration - optimized_duration) / baseline_duration) * 100
        optimization_results["performance_improvement"] = improvement
    
    return optimized_result, optimization_results

# Advanced Integration Pattern: API Gateway with Circuit Breaker
def _call_api_with_circuit_breaker_gateway(endpoint: str, method: str = "POST", payload: Dict[str, Any] = None, circuit_breaker: CircuitBreaker = None, retry_config: Dict[str, Any] = None, conn_id: str = None) -> Dict[str, Any]:
    """Llamada a API a trav√©s de gateway con circuit breaker y retry."""
    if circuit_breaker is None:
        circuit_breaker = enrichment_circuit_breaker
    
    if retry_config is None:
        retry_config = {"max_retries": 3, "backoff": "exponential"}
    
    def make_request():
        with httpx.Client(timeout=10.0) as client:
            headers = {"Content-Type": "application/json"}
            
            if method.upper() == "POST":
                response = client.post(endpoint, json=payload or {}, headers=headers)
            elif method.upper() == "GET":
                response = client.get(endpoint, params=payload or {}, headers=headers)
            else:
                raise ValueError(f"Unsupported method: {method}")
            
            response.raise_for_status()
            return response.json()
    
    try:
        # Usar circuit breaker
        result = circuit_breaker.call(make_request)
        return {"success": True, "data": result}
    except Exception as e:
        # Intentar retry si est√° configurado
        if retry_config.get("max_retries", 0) > 0:
            try:
                result = _retry_with_advanced_backoff(
                    make_request,
                    max_retries=retry_config.get("max_retries", 3),
                    initial_delay=1.0
                )
                return {"success": True, "data": result, "retried": True}
            except Exception as retry_error:
                return {"success": False, "error": str(retry_error), "retried": True}
        
        return {"success": False, "error": str(e)}

# Advanced Testing Utilities
def _create_test_mock(service_name: str, mock_responses: Dict[str, Any] = None, mock_behavior: str = "static") -> Callable:
    """Crea mock avanzado para testing con comportamiento configurable."""
    if mock_responses is None:
        mock_responses = {}
    
    call_count = {"count": 0}
    
    def mock_service(*args, **kwargs):
        call_count["count"] += 1
        
        if mock_behavior == "static":
            # Respuesta est√°tica
            endpoint = kwargs.get("endpoint", "default")
            return mock_responses.get(endpoint, mock_responses.get("default", {}))
        
        elif mock_behavior == "sequence":
            # Respuestas en secuencia
            responses = mock_responses.get("sequence", [])
            if responses:
                return responses[call_count["count"] % len(responses)]
            return {}
        
        elif mock_behavior == "random":
            # Respuesta aleatoria
            responses = mock_responses.get("random", [])
            if responses:
                import random
                return random.choice(responses)
            return {}
        
        elif mock_behavior == "error_after":
            # Error despu√©s de N llamadas
            error_after = mock_responses.get("error_after", 5)
            if call_count["count"] > error_after:
                raise Exception("Mock error after threshold")
            return mock_responses.get("success", {})
        
        return {}
    
    return mock_service

# Advanced Configuration Management with Validation
def _load_and_validate_config(config_path: str, schema: Dict[str, Any] = None, required_keys: List[str] = None) -> Dict[str, Any]:
    """Carga y valida configuraci√≥n con schema."""
    config = _load_config_from_file(config_path)
    
    if not config:
        return {}
    
    # Validar keys requeridas
    if required_keys:
        missing_keys = [key for key in required_keys if key not in config]
        if missing_keys:
            logger.warning("config_missing_keys", missing_keys=missing_keys)
    
    # Validar contra schema
    if schema:
        is_valid, errors, warnings = _validate_with_dynamic_schema(config, schema)
        if not is_valid:
            logger.error("config_validation_failed", errors=errors)
            return {}
        if warnings:
            logger.warning("config_validation_warnings", warnings=warnings)
    
    return config

# Advanced Error Handling with Recovery Strategies
def _handle_error_with_recovery_strategies(error: Exception, context: Dict[str, Any], recovery_strategies: List[Dict[str, Any]] = None) -> Dict[str, Any]:
    """Manejo avanzado de errores con estrategias de recuperaci√≥n configurables."""
    if recovery_strategies is None:
        recovery_strategies = [
            {"strategy": "retry", "max_attempts": 3, "backoff": "exponential"},
            {"strategy": "fallback", "fallback_value": None},
            {"strategy": "dlq", "save_to_dlq": True}
        ]
    
    recovery_result = {
        "error_type": type(error).__name__,
        "error_message": str(error),
        "recovery_attempted": False,
        "recovery_successful": False,
        "strategies_tried": []
    }
    
    for strategy_config in recovery_strategies:
        strategy = strategy_config.get("strategy")
        recovery_result["strategies_tried"].append(strategy)
        
        try:
            if strategy == "retry":
                func = context.get("function")
                if func and callable(func):
                    max_attempts = strategy_config.get("max_attempts", 3)
                    backoff = strategy_config.get("backoff", "exponential")
                    
                    result = _retry_with_advanced_backoff(
                        func,
                        max_retries=max_attempts,
                        *context.get("args", []),
                        **context.get("kwargs", {})
                    )
                    
                    recovery_result["recovery_attempted"] = True
                    recovery_result["recovery_successful"] = True
                    recovery_result["recovered_with"] = "retry"
                    recovery_result["result"] = result
                    break
            
            elif strategy == "fallback":
                fallback_value = strategy_config.get("fallback_value")
                if fallback_value is not None:
                    recovery_result["recovery_attempted"] = True
                    recovery_result["recovery_successful"] = True
                    recovery_result["recovered_with"] = "fallback"
                    recovery_result["result"] = fallback_value
                    break
            
            elif strategy == "dlq":
                if strategy_config.get("save_to_dlq", True):
                    item = context.get("item")
                    if item:
                        save_to_dlq(item, str(error), context)
                        recovery_result["recovery_attempted"] = True
                        recovery_result["recovered_with"] = "dlq"
                        # No es √©xito completo, pero se guard√≥
        except Exception as recovery_error:
            logger.warning("recovery_strategy_failed", strategy=strategy, error=str(recovery_error))
            continue
    
    return recovery_result

# Advanced Observability with Metrics Aggregation
def _aggregate_metrics_advanced(metrics_list: List[Dict[str, Any]], aggregation_type: str = "time_series", time_window: str = "1h") -> Dict[str, Any]:
    """Agregaci√≥n avanzada de m√©tricas con m√∫ltiples tipos."""
    if not metrics_list:
        return {}
    
    aggregated = {
        "aggregation_type": aggregation_type,
        "time_window": time_window,
        "metrics_count": len(metrics_list),
        "aggregated_at": datetime.utcnow().isoformat()
    }
    
    if aggregation_type == "time_series":
        # Agregar por serie de tiempo
        aggregated["time_series"] = {}
        for metric in metrics_list:
            metric_name = metric.get("name")
            if metric_name:
                if metric_name not in aggregated["time_series"]:
                    aggregated["time_series"][metric_name] = []
                aggregated["time_series"][metric_name].append({
                    "value": metric.get("value"),
                    "timestamp": metric.get("timestamp")
                })
    
    elif aggregation_type == "statistical":
        # Agregaci√≥n estad√≠stica
        values = [m.get("value", 0) for m in metrics_list if isinstance(m.get("value"), (int, float))]
        if values:
            aggregated["statistics"] = {
                "count": len(values),
                "sum": sum(values),
                "avg": sum(values) / len(values),
                "min": min(values),
                "max": max(values),
                "median": sorted(values)[len(values) // 2] if values else 0
            }
    
    elif aggregation_type == "grouped":
        # Agregaci√≥n por grupos
        grouped = {}
        for metric in metrics_list:
            group = metric.get("group", "default")
            if group not in grouped:
                grouped[group] = []
            grouped[group].append(metric.get("value", 0))
        
        aggregated["grouped"] = {
            group: {
                "count": len(values),
                "sum": sum(values),
                "avg": sum(values) / len(values) if values else 0
            }
            for group, values in grouped.items()
        }
    
    return aggregated

# ============================================================================
# FUNCIONES HELPER ULTRA AVANZADAS - ENTERPRISE LEGENDARY FEATURES
# ============================================================================

# Advanced Data Pipeline Orchestration
def _orchestrate_data_pipeline(pipeline_stages: List[Dict[str, Any]], data: Dict[str, Any], error_handling: str = "stop_on_error", conn_id: str = None) -> Dict[str, Any]:
    """Orquesta pipeline de datos con m√∫ltiples etapas y manejo de errores avanzado."""
    pipeline_result = {
        "pipeline_id": f"pipeline_{datetime.utcnow().timestamp()}",
        "stages_executed": [],
        "stages_failed": [],
        "stages_skipped": [],
        "final_data": data.copy(),
        "started_at": datetime.utcnow().isoformat(),
        "status": "in_progress"
    }
    
    current_data = data.copy()
    
    for i, stage in enumerate(pipeline_stages):
        stage_id = stage.get("id", f"stage_{i}")
        stage_type = stage.get("type")
        stage_func = stage.get("function")
        stage_condition = stage.get("condition")
        stage_retry = stage.get("retry", {})
        
        try:
            # Verificar condici√≥n si existe
            if stage_condition:
                condition_met = _evaluate_condition(stage_condition, current_data)
                if not condition_met:
                    pipeline_result["stages_skipped"].append(stage_id)
                    continue
            
            # Ejecutar etapa con retry si est√° configurado
            if stage_retry.get("enabled", False):
                result = _retry_with_advanced_backoff(
                    stage_func,
                    max_retries=stage_retry.get("max_retries", 3),
                    *[current_data],
                    **stage_retry.get("kwargs", {})
                )
            else:
                result = stage_func(current_data) if callable(stage_func) else current_data
            
            current_data = result if isinstance(result, dict) else current_data
            pipeline_result["stages_executed"].append(stage_id)
            
        except Exception as e:
            logger.error("pipeline_stage_failed", stage_id=stage_id, error=str(e))
            pipeline_result["stages_failed"].append({
                "stage_id": stage_id,
                "error": str(e),
                "error_type": type(e).__name__
            })
            
            if error_handling == "stop_on_error":
                pipeline_result["status"] = "failed"
                break
            elif error_handling == "continue_on_error":
                continue
            elif error_handling == "rollback":
                # Rollback a estado anterior
                pipeline_result["status"] = "rolled_back"
                break
    
    pipeline_result["final_data"] = current_data
    pipeline_result["status"] = "completed" if len(pipeline_result["stages_failed"]) == 0 else pipeline_result.get("status", "partial")
    pipeline_result["completed_at"] = datetime.utcnow().isoformat()
    
    return pipeline_result

# Advanced Feature Flag System with A/B Testing
def _check_feature_flag_advanced(flag_name: str, user_id: str = None, default: bool = False, ab_test_config: Dict[str, Any] = None, conn_id: str = None) -> Dict[str, Any]:
    """Sistema avanzado de feature flags con A/B testing integrado."""
    flag_result = {
        "flag_name": flag_name,
        "enabled": default,
        "variant": None,
        "ab_test": False,
        "user_id": user_id
    }
    
    try:
        if VARIABLES_AVAILABLE:
            flag_key = f"feature_flag_{flag_name}"
            flag_data_str = Variable.get(flag_key, default_var="{}")
            flag_data = json.loads(flag_data_str) if flag_data_str else {}
            
            # Verificar si est√° habilitado globalmente
            if flag_data.get("enabled", False):
                flag_result["enabled"] = True
                
                # A/B Testing
                if ab_test_config and flag_data.get("ab_test_enabled", False):
                    variants = ab_test_config.get("variants", ["A", "B"])
                    variant_percentages = ab_test_config.get("percentages", [50, 50])
                    
                    # Asignaci√≥n determin√≠stica basada en user_id
                    if user_id:
                        hash_value = hash(f"{flag_name}{user_id}") % 100
                        cumulative = 0
                        for i, percentage in enumerate(variant_percentages):
                            cumulative += percentage
                            if hash_value < cumulative:
                                flag_result["variant"] = variants[i]
                                flag_result["ab_test"] = True
                                break
                    else:
                        flag_result["variant"] = variants[0]
                        flag_result["ab_test"] = True
                
                # Rollout porcentual
                rollout_percentage = flag_data.get("rollout_percentage", 100)
                if rollout_percentage < 100:
                    if user_id:
                        hash_value = hash(f"{flag_name}{user_id}") % 100
                        if hash_value >= rollout_percentage:
                            flag_result["enabled"] = False
                    else:
                        import random
                        if random.randint(0, 100) >= rollout_percentage:
                            flag_result["enabled"] = False
    except Exception as e:
        logger.warning("feature_flag_check_failed", flag_name=flag_name, error=str(e))
    
    return flag_result

# Advanced Data Deduplication with Fuzzy Matching
def _deduplicate_leads_advanced(leads: List[Dict[str, Any]], matching_strategy: str = "fuzzy", similarity_threshold: float = 0.8, conn_id: str = None) -> Dict[str, Any]:
    """Deduplicaci√≥n avanzada de leads con matching fuzzy."""
    deduplication_result = {
        "original_count": len(leads),
        "unique_leads": [],
        "duplicates": [],
        "duplicate_groups": [],
        "deduplication_rate": 0.0
    }
    
    seen = {}
    duplicate_groups = {}
    
    for i, lead in enumerate(leads):
        lead_id = lead.get("email") or lead.get("lead_ext_id") or f"lead_{i}"
        is_duplicate = False
        matched_group = None
        
        # Matching exacto
        if lead_id in seen:
            is_duplicate = True
            matched_group = seen[lead_id]
        else:
            # Fuzzy matching
            if matching_strategy == "fuzzy":
                for existing_id, existing_lead in seen.items():
                    similarity, match_type = _fuzzy_match_leads(lead, existing_lead)
                    if similarity >= similarity_threshold:
                        is_duplicate = True
                        matched_group = existing_id
                        break
        
        if is_duplicate:
            deduplication_result["duplicates"].append({
                "lead": lead,
                "matched_with": matched_group,
                "similarity": similarity if matching_strategy == "fuzzy" else 1.0
            })
            
            if matched_group not in duplicate_groups:
                duplicate_groups[matched_group] = []
            duplicate_groups[matched_group].append(lead)
        else:
            seen[lead_id] = lead_id
            deduplication_result["unique_leads"].append(lead)
    
    deduplication_result["duplicate_groups"] = [
        {"group_id": group_id, "leads": leads_in_group}
        for group_id, leads_in_group in duplicate_groups.items()
    ]
    
    if deduplication_result["original_count"] > 0:
        deduplication_result["deduplication_rate"] = (
            len(deduplication_result["duplicates"]) / deduplication_result["original_count"] * 100
        )
    
    return deduplication_result

# Advanced Rate Limiting with Sliding Window and Token Bucket
def _check_rate_limit_hybrid(identifier: str, limit: int = 100, window_seconds: int = 3600, tokens: int = 1, capacity: int = 10, refill_rate: float = 1.0, conn_id: str = None) -> Tuple[bool, Optional[float], Dict[str, Any]]:
    """Rate limiting h√≠brido combinando sliding window y token bucket."""
    # Sliding window check
    sliding_ok, sliding_wait = _check_rate_limit_advanced(identifier, limit, window_seconds, conn_id)
    
    # Token bucket check
    token_ok, token_wait, token_info = _check_token_bucket_advanced(identifier, tokens, capacity, refill_rate, conn_id)
    
    # Combinar resultados
    allowed = sliding_ok and token_ok
    wait_time = max(sliding_wait or 0, token_wait or 0) if not allowed else None
    
    return allowed, wait_time, {
        "sliding_window": {"allowed": sliding_ok, "wait_time": sliding_wait},
        "token_bucket": {"allowed": token_ok, "wait_time": token_wait, **token_info}
    }

# Advanced Cache Warming Strategy
def _warm_cache_advanced(cache_keys: List[str], loader_func: Callable, strategy: str = "parallel", conn_id: str = None) -> Dict[str, Any]:
    """Cache warming avanzado con m√∫ltiples estrategias."""
    warming_result = {
        "strategy": strategy,
        "keys_to_warm": len(cache_keys),
        "keys_warmed": 0,
        "keys_failed": 0,
        "duration_seconds": 0,
        "started_at": datetime.utcnow().isoformat()
    }
    
    start_time = time.time()
    
    try:
        if strategy == "parallel" and CONCURRENT_AVAILABLE:
            # Warming paralelo
            with ThreadPoolExecutor(max_workers=min(10, len(cache_keys))) as executor:
                futures = {executor.submit(loader_func, key): key for key in cache_keys}
                
                for future in as_completed(futures):
                    key = futures[future]
                    try:
                        value = future.result()
                        if _cache_with_redis(key, value, ttl=3600, conn_id=conn_id) or (enrichment_cache and enrichment_cache.setdefault(key, value)):
                            warming_result["keys_warmed"] += 1
                        else:
                            warming_result["keys_failed"] += 1
                    except Exception as e:
                        logger.warning("cache_warming_failed", key=key, error=str(e))
                        warming_result["keys_failed"] += 1
        
        elif strategy == "sequential":
            # Warming secuencial
            for key in cache_keys:
                try:
                    value = loader_func(key)
                    if _cache_with_redis(key, value, ttl=3600, conn_id=conn_id) or (enrichment_cache and enrichment_cache.setdefault(key, value)):
                        warming_result["keys_warmed"] += 1
                    else:
                        warming_result["keys_failed"] += 1
                except Exception as e:
                    logger.warning("cache_warming_failed", key=key, error=str(e))
                    warming_result["keys_failed"] += 1
        
        elif strategy == "priority":
            # Warming por prioridad (asumiendo que cache_keys est√° ordenado por prioridad)
            for key in cache_keys:
                try:
                    value = loader_func(key)
                    if _cache_with_redis(key, value, ttl=3600, conn_id=conn_id) or (enrichment_cache and enrichment_cache.setdefault(key, value)):
                        warming_result["keys_warmed"] += 1
                    else:
                        warming_result["keys_failed"] += 1
                except Exception as e:
                    logger.warning("cache_warming_failed", key=key, error=str(e))
                    warming_result["keys_failed"] += 1
    except Exception as e:
        logger.error("cache_warming_error", strategy=strategy, error=str(e))
    
    warming_result["duration_seconds"] = time.time() - start_time
    warming_result["completed_at"] = datetime.utcnow().isoformat()
    
    return warming_result

# Advanced Data Validation Pipeline
def _validate_data_pipeline(data: Dict[str, Any], validators: List[Dict[str, Any]], stop_on_error: bool = False) -> Dict[str, Any]:
    """Pipeline de validaci√≥n de datos con m√∫ltiples validadores."""
    validation_result = {
        "is_valid": True,
        "errors": [],
        "warnings": [],
        "validators_run": 0,
        "validators_passed": 0,
        "validators_failed": 0
    }
    
    for validator_config in validators:
        validator_type = validator_config.get("type")
        validator_func = validator_config.get("function")
        validator_schema = validator_config.get("schema")
        validator_rules = validator_config.get("rules", [])
        
        validation_result["validators_run"] += 1
        
        try:
            if validator_type == "schema":
                is_valid, errors, warnings = _validate_with_dynamic_schema(data, validator_schema)
                if not is_valid:
                    validation_result["is_valid"] = False
                    validation_result["errors"].extend(errors)
                    validation_result["validators_failed"] += 1
                    if stop_on_error:
                        break
                else:
                    validation_result["validators_passed"] += 1
                validation_result["warnings"].extend(warnings.get("warnings", []))
            
            elif validator_type == "rules":
                is_valid, errors = _validate_with_rules_engine(data, validator_rules)
                if not is_valid:
                    validation_result["is_valid"] = False
                    validation_result["errors"].extend(errors)
                    validation_result["validators_failed"] += 1
                    if stop_on_error:
                        break
                else:
                    validation_result["validators_passed"] += 1
            
            elif validator_type == "custom" and validator_func:
                if callable(validator_func):
                    is_valid = validator_func(data)
                    if not is_valid:
                        validation_result["is_valid"] = False
                        validation_result["errors"].append(f"Custom validator {validator_type} failed")
                        validation_result["validators_failed"] += 1
                        if stop_on_error:
                            break
                    else:
                        validation_result["validators_passed"] += 1
        except Exception as e:
            logger.error("validator_execution_failed", validator_type=validator_type, error=str(e))
            validation_result["is_valid"] = False
            validation_result["errors"].append(f"Validator {validator_type} raised exception: {str(e)}")
            validation_result["validators_failed"] += 1
            if stop_on_error:
                break
    
    return validation_result

# Advanced Metrics Collection and Export
def _collect_and_export_metrics(metrics: Dict[str, Any], export_formats: List[str] = None, export_destinations: List[str] = None, conn_id: str = None) -> Dict[str, Any]:
    """Recopila y exporta m√©tricas en m√∫ltiples formatos y destinos."""
    if export_formats is None:
        export_formats = ["prometheus", "json", "statsd"]
    
    if export_destinations is None:
        export_destinations = ["airflow_variables", "s3"]
    
    export_result = {
        "metrics_collected": len(metrics),
        "formats_exported": [],
        "destinations_exported": [],
        "exported_at": datetime.utcnow().isoformat()
    }
    
    # Exportar en diferentes formatos
    for format_type in export_formats:
        try:
            if format_type == "prometheus":
                # Exportar a Prometheus
                for metric_name, metric_value in metrics.items():
                    _send_metric_to_monitoring(metric_name, metric_value)
                export_result["formats_exported"].append("prometheus")
            
            elif format_type == "json":
                # Exportar como JSON
                json_data = json.dumps(metrics)
                if "airflow_variables" in export_destinations:
                    if VARIABLES_AVAILABLE:
                        Variable.set("metrics_export_json", json_data)
                        export_result["destinations_exported"].append("airflow_variables")
            
            elif format_type == "statsd":
                # Exportar a StatsD
                for metric_name, metric_value in metrics.items():
                    _send_metric_to_monitoring(metric_name, metric_value, tags={"format": "statsd"})
                export_result["formats_exported"].append("statsd")
        except Exception as e:
            logger.warning("metrics_export_failed", format=format_type, error=str(e))
    
    # Exportar a S3 si est√° configurado
    if "s3" in export_destinations:
        try:
            s3_path = _export_metrics_to_s3(metrics, "metrics_export", conn_id)
            if s3_path:
                export_result["destinations_exported"].append(f"s3:{s3_path}")
        except Exception as e:
            logger.warning("s3_export_failed", error=str(e))
    
    return export_result

# Advanced Circuit Breaker Management
def _manage_circuit_breakers_advanced(breakers: Dict[str, CircuitBreaker] = None, action: str = "status") -> Dict[str, Any]:
    """Gesti√≥n avanzada de circuit breakers con m√∫ltiples acciones."""
    if breakers is None:
        breakers = {
            "crm": crm_circuit_breaker,
            "onboarding": onboarding_circuit_breaker,
            "enrichment": enrichment_circuit_breaker
        }
    
    management_result = {
        "action": action,
        "breakers": {},
        "timestamp": datetime.utcnow().isoformat()
    }
    
    for name, breaker in breakers.items():
        breaker_info = {
            "name": name,
            "fail_counter": breaker.fail_counter if hasattr(breaker, 'fail_counter') else 0,
            "success_counter": breaker.success_counter if hasattr(breaker, 'success_counter') else 0,
            "current_state": breaker.current_state if hasattr(breaker, 'current_state') else "closed"
        }
        
        if action == "reset":
            try:
                breaker.reset()
                breaker_info["action"] = "reset"
            except Exception as e:
                breaker_info["error"] = str(e)
        
        elif action == "open":
            try:
                breaker.open()
                breaker_info["action"] = "opened"
            except Exception as e:
                breaker_info["error"] = str(e)
        
        elif action == "close":
            try:
                breaker.close()
                breaker_info["action"] = "closed"
            except Exception as e:
                breaker_info["error"] = str(e)
        
        management_result["breakers"][name] = breaker_info
    
    return management_result

# ============================================================================
# FUNCIONES HELPER ULTRA AVANZADAS - ENTERPRISE MYTHICAL FEATURES
# ============================================================================

# Advanced Data Streaming with Backpressure
def _stream_data_with_backpressure(stream_name: str, data: Dict[str, Any], max_queue_size: int = 1000, backpressure_strategy: str = "block", conn_id: str = None) -> Dict[str, Any]:
    """Streaming de datos con control de backpressure."""
    stream_result = {
        "stream_name": stream_name,
        "data_size": len(json.dumps(data)),
        "queued": False,
        "backpressure_applied": False,
        "timestamp": datetime.utcnow().isoformat()
    }
    
    try:
        if VARIABLES_AVAILABLE:
            queue_key = f"stream_queue_{stream_name}"
            queue_data_str = Variable.get(queue_key, default_var="[]")
            queue = json.loads(queue_data_str) if queue_data_str else []
            
            # Verificar tama√±o de cola
            if len(queue) >= max_queue_size:
                stream_result["backpressure_applied"] = True
                
                if backpressure_strategy == "block":
                    # Bloquear hasta que haya espacio
                    while len(queue) >= max_queue_size:
                        time.sleep(0.1)
                        queue_data_str = Variable.get(queue_key, default_var="[]")
                        queue = json.loads(queue_data_str) if queue_data_str else []
                
                elif backpressure_strategy == "drop_oldest":
                    # Eliminar el m√°s antiguo
                    queue = queue[1:]
                
                elif backpressure_strategy == "reject":
                    # Rechazar el nuevo
                    stream_result["queued"] = False
                    return stream_result
            
            # Agregar a cola
            queue.append({
                "data": data,
                "timestamp": datetime.utcnow().isoformat()
            })
            
            Variable.set(queue_key, json.dumps(queue))
            stream_result["queued"] = True
    except Exception as e:
        logger.error("stream_with_backpressure_failed", stream_name=stream_name, error=str(e))
        stream_result["error"] = str(e)
    
    return stream_result

# Advanced Distributed Locking with Lease Renewal
def _acquire_lock_with_lease(lock_key: str, lease_duration_seconds: int = 60, renewal_interval_seconds: int = 30, conn_id: str = None) -> Dict[str, Any]:
    """Adquiere lock distribuido con renovaci√≥n autom√°tica de lease."""
    lock_result = {
        "lock_key": lock_key,
        "acquired": False,
        "lease_duration": lease_duration_seconds,
        "renewal_interval": renewal_interval_seconds,
        "lease_id": None
    }
    
    try:
        if VARIABLES_AVAILABLE:
            lease_id = f"lease_{datetime.utcnow().timestamp()}_{hashlib.md5(lock_key.encode()).hexdigest()[:8]}"
            
            # Intentar adquirir lock
            acquired = _acquire_distributed_lock(lock_key, lease_duration_seconds, conn_id)
            
            if acquired:
                lock_result["acquired"] = True
                lock_result["lease_id"] = lease_id
                
                # Guardar informaci√≥n de lease para renovaci√≥n
                lease_info_key = f"lease_info_{lock_key}"
                lease_info = {
                    "lease_id": lease_id,
                    "lock_key": lock_key,
                    "acquired_at": datetime.utcnow().isoformat(),
                    "lease_duration": lease_duration_seconds,
                    "renewal_interval": renewal_interval_seconds,
                    "last_renewal": datetime.utcnow().isoformat()
                }
                Variable.set(lease_info_key, json.dumps(lease_info))
    except Exception as e:
        logger.error("lock_with_lease_failed", lock_key=lock_key, error=str(e))
        lock_result["error"] = str(e)
    
    return lock_result

def _renew_lease(lock_key: str, conn_id: str = None) -> bool:
    """Renueva lease de un lock distribuido."""
    try:
        if VARIABLES_AVAILABLE:
            lease_info_key = f"lease_info_{lock_key}"
            lease_info_str = Variable.get(lease_info_key, default_var="{}")
            lease_info = json.loads(lease_info_str) if lease_info_str else {}
            
            if lease_info:
                # Renovar lock
                lease_duration = lease_info.get("lease_duration", 60)
                renewed = _acquire_distributed_lock(lock_key, lease_duration, conn_id)
                
                if renewed:
                    lease_info["last_renewal"] = datetime.utcnow().isoformat()
                    Variable.set(lease_info_key, json.dumps(lease_info))
                    return True
    except Exception as e:
        logger.warning("lease_renewal_failed", lock_key=lock_key, error=str(e))
    
    return False

# Advanced Dynamic Batch Processing
def _process_dynamic_batch(items: List[Dict[str, Any]], processor: Callable, min_batch_size: int = 1, max_batch_size: int = 100, target_processing_time: float = 1.0, conn_id: str = None) -> List[Dict[str, Any]]:
    """Procesamiento de batch din√°mico que ajusta tama√±o basado en tiempo objetivo."""
    results = []
    current_batch_size = min_batch_size
    performance_history = []
    
    i = 0
    while i < len(items):
        batch = items[i:i + current_batch_size]
        batch_start_time = time.time()
        
        try:
            # Procesar batch
            batch_results = _process_batch_sync(batch, processor)
            results.extend(batch_results)
            
            # Calcular tiempo de procesamiento
            batch_duration = time.time() - batch_start_time
            avg_time_per_item = batch_duration / len(batch) if batch else 0
            
            # Guardar en historial
            performance_history.append({
                "batch_size": len(batch),
                "duration": batch_duration,
                "avg_time_per_item": avg_time_per_item
            })
            
            # Mantener solo √∫ltimos 10 registros
            if len(performance_history) > 10:
                performance_history = performance_history[-10:]
            
            # Ajustar tama√±o de batch din√°micamente
            if len(performance_history) >= 3:
                recent_avg = sum(p["avg_time_per_item"] for p in performance_history[-3:]) / 3
                
                if recent_avg < target_processing_time * 0.5:
                    # Muy r√°pido, aumentar batch
                    current_batch_size = min(max_batch_size, int(current_batch_size * 1.2))
                elif recent_avg > target_processing_time * 1.5:
                    # Muy lento, reducir batch
                    current_batch_size = max(min_batch_size, int(current_batch_size * 0.8))
                else:
                    # √ìptimo, mantener tama√±o
                    pass
            
            logger.debug("dynamic_batch_processed",
                        batch_size=len(batch),
                        duration=batch_duration,
                        avg_time_per_item=avg_time_per_item,
                        new_batch_size=current_batch_size)
            
        except Exception as e:
            logger.error("dynamic_batch_failed", batch_size=len(batch), error=str(e))
            # Reducir batch size en caso de error
            current_batch_size = max(min_batch_size, int(current_batch_size * 0.5))
        
        i += len(batch)
    
    return results

# Advanced Monitoring with Real-time Anomaly Detection
def _monitor_with_anomaly_detection(metric_name: str, current_value: float, historical_values: List[float] = None, detection_method: str = "z_score", threshold: float = 3.0, conn_id: str = None) -> Dict[str, Any]:
    """Monitoreo avanzado con detecci√≥n de anomal√≠as en tiempo real."""
    detection_result = {
        "metric_name": metric_name,
        "current_value": current_value,
        "is_anomaly": False,
        "anomaly_score": 0.0,
        "detection_method": detection_method,
        "timestamp": datetime.utcnow().isoformat()
    }
    
    try:
        if historical_values is None:
            # Obtener valores hist√≥ricos
            if VARIABLES_AVAILABLE:
                history_key = f"metric_history_{metric_name}"
                history_str = Variable.get(history_key, default_var="[]")
                historical_values = json.loads(history_str) if history_str else []
        
        if len(historical_values) >= 10:  # M√≠nimo de datos hist√≥ricos
            if detection_method == "z_score":
                # Z-score method
                mean = sum(historical_values) / len(historical_values)
                variance = sum((x - mean) ** 2 for x in historical_values) / len(historical_values)
                std_dev = variance ** 0.5
                
                if std_dev > 0:
                    z_score = abs(current_value - mean) / std_dev
                    detection_result["anomaly_score"] = z_score
                    detection_result["is_anomaly"] = z_score > threshold
                    detection_result["mean"] = mean
                    detection_result["std_dev"] = std_dev
                    detection_result["z_score"] = z_score
            
            elif detection_method == "iqr":
                # Interquartile Range method
                sorted_values = sorted(historical_values)
                q1_index = len(sorted_values) // 4
                q3_index = 3 * len(sorted_values) // 4
                q1 = sorted_values[q1_index]
                q3 = sorted_values[q3_index]
                iqr = q3 - q1
                
                lower_bound = q1 - threshold * iqr
                upper_bound = q3 + threshold * iqr
                
                detection_result["is_anomaly"] = current_value < lower_bound or current_value > upper_bound
                detection_result["lower_bound"] = lower_bound
                detection_result["upper_bound"] = upper_bound
                detection_result["iqr"] = iqr
            
            # Actualizar historial
            historical_values.append(current_value)
            if len(historical_values) > 1000:
                historical_values = historical_values[-1000:]
            
            if VARIABLES_AVAILABLE:
                history_key = f"metric_history_{metric_name}"
                Variable.set(history_key, json.dumps(historical_values))
            
            # Alerta si es anomal√≠a
            if detection_result["is_anomaly"]:
                _send_advanced_alert(
                    f"anomaly_{metric_name}",
                    "high",
                    f"Anomaly detected in {metric_name}: {current_value}",
                    {"detection_result": detection_result}
                )
    except Exception as e:
        logger.error("anomaly_detection_failed", metric_name=metric_name, error=str(e))
    
    return detection_result

# Advanced Security with Encryption at Rest
def _encrypt_data_at_rest(data: Dict[str, Any], encryption_key: str = None, algorithm: str = "AES256") -> Dict[str, Any]:
    """Encripta datos para almacenamiento en reposo."""
    if encryption_key is None:
        encryption_key = os.getenv("ENCRYPTION_KEY", "default_key_change_in_production")
    
    encrypted_data = {}
    
    for key, value in data.items():
        if isinstance(value, str):
            # Encriptar string
            encrypted_value = _encrypt_with_algorithm(value, algorithm, encryption_key)
            encrypted_data[f"{key}_encrypted"] = encrypted_value
        elif isinstance(value, dict):
            # Encriptar recursivamente
            encrypted_data[f"{key}_encrypted"] = _encrypt_data_at_rest(value, encryption_key, algorithm)
        else:
            # Mantener valor original si no es string o dict
            encrypted_data[key] = value
    
    encrypted_data["_encryption_metadata"] = {
        "algorithm": algorithm,
        "encrypted_at": datetime.utcnow().isoformat(),
        "fields_encrypted": [k for k in encrypted_data.keys() if k.endswith("_encrypted")]
    }
    
    return encrypted_data

# Advanced Data Quality with ML-based Validation
def _validate_with_ml_model(data: Dict[str, Any], model_name: str = "data_quality_model", conn_id: str = None) -> Dict[str, Any]:
    """Validaci√≥n de calidad de datos usando modelo ML."""
    validation_result = {
        "is_valid": True,
        "quality_score": 0.0,
        "ml_confidence": 0.0,
        "model_name": model_name,
        "predictions": {},
        "recommendations": []
    }
    
    try:
        # Obtener predicci√≥n del modelo ML
        ml_prediction = _get_ml_prediction(data, model_endpoint=f"api/{model_name}")
        
        if ml_prediction:
            quality_score = ml_prediction.get("quality_score", 0.0)
            confidence = ml_prediction.get("confidence", 0.0)
            
            validation_result["quality_score"] = quality_score
            validation_result["ml_confidence"] = confidence
            validation_result["is_valid"] = quality_score >= 0.7  # Threshold configurable
            validation_result["predictions"] = ml_prediction.get("predictions", {})
            
            # Generar recomendaciones basadas en predicci√≥n
            if quality_score < 0.7:
                validation_result["recommendations"].append("Data quality below threshold. Review and improve data completeness.")
            if confidence < 0.8:
                validation_result["recommendations"].append("Low ML confidence. Consider manual review.")
    except Exception as e:
        logger.warning("ml_validation_failed", model_name=model_name, error=str(e))
        # Fallback a validaci√≥n tradicional
        quality_score = _calculate_data_quality_score_advanced(data)
        validation_result["quality_score"] = quality_score.get("overall_score", 0) / 100.0
        validation_result["is_valid"] = validation_result["quality_score"] >= 0.7
    
    return validation_result

# Advanced Performance Auto-tuning
def _auto_tune_performance(operation_name: str, func: Callable, tuning_params: Dict[str, Any] = None, *args, **kwargs) -> Tuple[Any, Dict[str, Any]]:
    """Auto-tuning de performance con optimizaci√≥n autom√°tica de par√°metros."""
    if tuning_params is None:
        tuning_params = {
            "batch_size": {"min": 1, "max": 100, "current": 10},
            "parallelism": {"min": 1, "max": 10, "current": 4},
            "cache_ttl": {"min": 60, "max": 3600, "current": 300}
        }
    
    tuning_result = {
        "operation": operation_name,
        "optimized_params": {},
        "performance_improvement": 0.0,
        "iterations": 0
    }
    
    best_performance = float('inf')
    best_params = {}
    
    # Probar diferentes combinaciones de par√°metros
    for iteration in range(5):  # M√°ximo 5 iteraciones
        current_params = {}
        for param_name, param_config in tuning_params.items():
            if param_name == "batch_size":
                current_params[param_name] = param_config["current"]
            elif param_name == "parallelism":
                current_params[param_name] = param_config["current"]
            elif param_name == "cache_ttl":
                current_params[param_name] = param_config["current"]
        
        # Ejecutar con par√°metros actuales
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            duration = time.time() - start_time
            
            # Actualizar mejor performance
            if duration < best_performance:
                best_performance = duration
                best_params = current_params.copy()
            
            # Ajustar par√°metros para siguiente iteraci√≥n
            if iteration < 4:
                for param_name, param_config in tuning_params.items():
                    if param_name == "batch_size":
                        # Ajustar batch size
                        if duration < 0.5:
                            param_config["current"] = min(param_config["max"], int(param_config["current"] * 1.2))
                        elif duration > 2.0:
                            param_config["current"] = max(param_config["min"], int(param_config["current"] * 0.8))
        except Exception as e:
            logger.warning("auto_tune_iteration_failed", iteration=iteration, error=str(e))
            break
        
        tuning_result["iterations"] = iteration + 1
    
    tuning_result["optimized_params"] = best_params
    tuning_result["best_performance"] = best_performance
    
    # Ejecutar con mejores par√°metros
    final_result = func(*args, **kwargs)
    
    return final_result, tuning_result

# Advanced Integration Pattern: Service Mesh
def _call_service_mesh(service_name: str, endpoint: str, method: str = "POST", payload: Dict[str, Any] = None, retry_policy: Dict[str, Any] = None, circuit_breaker_policy: Dict[str, Any] = None, conn_id: str = None) -> Dict[str, Any]:
    """Llamada a servicio a trav√©s de service mesh con pol√≠ticas avanzadas."""
    mesh_result = {
        "service_name": service_name,
        "endpoint": endpoint,
        "method": method,
        "success": False,
        "retries": 0,
        "circuit_breaker_state": "closed"
    }
    
    try:
        # Aplicar circuit breaker policy
        if circuit_breaker_policy:
            cb_name = circuit_breaker_policy.get("name", f"{service_name}_cb")
            cb = CircuitBreaker(
                fail_max=circuit_breaker_policy.get("fail_max", 5),
                timeout_duration=circuit_breaker_policy.get("timeout_duration", 60),
                expected_exception=Exception
            )
        else:
            cb = enrichment_circuit_breaker
        
        def make_request():
            with httpx.Client(timeout=10.0) as client:
                headers = {"Content-Type": "application/json"}
                
                if method.upper() == "POST":
                    response = client.post(endpoint, json=payload or {}, headers=headers)
                elif method.upper() == "GET":
                    response = client.get(endpoint, params=payload or {}, headers=headers)
                else:
                    raise ValueError(f"Unsupported method: {method}")
                
                response.raise_for_status()
                return response.json()
        
        # Ejecutar con circuit breaker
        result = cb.call(make_request)
        mesh_result["success"] = True
        mesh_result["data"] = result
        
        # Aplicar retry policy si falla
        if retry_policy and retry_policy.get("enabled", False):
            try:
                result = _retry_with_advanced_backoff(
                    make_request,
                    max_retries=retry_policy.get("max_retries", 3),
                    initial_delay=retry_policy.get("initial_delay", 1.0)
                )
                mesh_result["success"] = True
                mesh_result["data"] = result
                mesh_result["retries"] = retry_policy.get("max_retries", 3)
            except Exception as retry_error:
                mesh_result["success"] = False
                mesh_result["error"] = str(retry_error)
    except Exception as e:
        logger.error("service_mesh_call_failed", service=service_name, error=str(e))
        mesh_result["success"] = False
        mesh_result["error"] = str(e)
        mesh_result["circuit_breaker_state"] = "open" if hasattr(cb, 'current_state') else "unknown"
    
    return mesh_result

# Advanced Distributed Tracing with Context Propagation
def _create_trace_with_context(operation_name: str, parent_trace_id: str = None, parent_span_id: str = None, metadata: Dict[str, Any] = None, conn_id: str = None) -> Dict[str, Any]:
    """Crea trace distribuido con propagaci√≥n de contexto."""
    trace_context = {
        "trace_id": parent_trace_id or f"trace_{datetime.utcnow().timestamp()}_{hashlib.md5(operation_name.encode()).hexdigest()[:8]}",
        "span_id": f"span_{datetime.utcnow().timestamp()}_{hashlib.md5(f'{operation_name}{time.time()}'.encode()).hexdigest()[:8]}",
        "parent_span_id": parent_span_id,
        "operation": operation_name,
        "start_time": datetime.utcnow().isoformat(),
        "metadata": metadata or {}
    }
    
    # Crear span con OpenTelemetry si est√° disponible
    if tracer:
        span = tracer.start_span(operation_name)
        if parent_span_id:
            span.set_attribute("parent_span_id", parent_span_id)
        for key, value in (metadata or {}).items():
            span.set_attribute(key, str(value))
        trace_context["span"] = span
    
    # Guardar contexto para propagaci√≥n
    if VARIABLES_AVAILABLE:
        context_key = f"trace_context_{trace_context['trace_id']}"
        Variable.set(context_key, json.dumps(trace_context))
    
    return trace_context

# Advanced Property-Based Testing
def _run_property_based_test(test_name: str, property_func: Callable, generator_func: Callable, num_tests: int = 100, conn_id: str = None) -> Dict[str, Any]:
    """Ejecuta property-based testing con generaci√≥n autom√°tica de datos."""
    test_result = {
        "test_name": test_name,
        "num_tests": num_tests,
        "tests_passed": 0,
        "tests_failed": 0,
        "failing_cases": [],
        "started_at": datetime.utcnow().isoformat()
    }
    
    for i in range(num_tests):
        try:
            # Generar datos de prueba
            test_data = generator_func() if callable(generator_func) else {}
            
            # Ejecutar propiedad
            property_result = property_func(test_data) if callable(property_func) else True
            
            if property_result:
                test_result["tests_passed"] += 1
            else:
                test_result["tests_failed"] += 1
                test_result["failing_cases"].append({
                    "test_number": i + 1,
                    "test_data": test_data,
                    "result": property_result
                })
                
                # Limitar casos fallidos guardados
                if len(test_result["failing_cases"]) > 10:
                    test_result["failing_cases"] = test_result["failing_cases"][:10]
        except Exception as e:
            test_result["tests_failed"] += 1
            logger.warning("property_test_failed", test_name=test_name, test_number=i+1, error=str(e))
    
    test_result["completed_at"] = datetime.utcnow().isoformat()
    test_result["success_rate"] = (test_result["tests_passed"] / num_tests * 100) if num_tests > 0 else 0
    
    return test_result

# ============================================================================
# FUNCIONES HELPER ULTRA AVANZADAS - ENTERPRISE TRANSCENDENT FEATURES
# ============================================================================

# Advanced Data Replication and Synchronization
def _replicate_data_advanced(source_data: Dict[str, Any], replication_targets: List[Dict[str, Any]], replication_strategy: str = "async", conflict_resolution: str = "last_write_wins", conn_id: str = None) -> Dict[str, Any]:
    """Replicaci√≥n avanzada de datos a m√∫ltiples destinos con estrategias configurables."""
    replication_result = {
        "replication_id": f"repl_{datetime.utcnow().timestamp()}",
        "source_data_size": len(json.dumps(source_data)),
        "targets": len(replication_targets),
        "replicated_to": [],
        "failed_replications": [],
        "strategy": replication_strategy,
        "started_at": datetime.utcnow().isoformat()
    }
    
    def replicate_to_target(target_config: Dict[str, Any]) -> Dict[str, Any]:
        target_name = target_config.get("name", "unknown")
        target_func = target_config.get("replication_func")
        
        try:
            if callable(target_func):
                result = target_func(source_data)
                return {"target": target_name, "success": True, "result": result}
            else:
                return {"target": target_name, "success": False, "error": "No replication function provided"}
        except Exception as e:
            return {"target": target_name, "success": False, "error": str(e)}
    
    if replication_strategy == "async" and CONCURRENT_AVAILABLE:
        # Replicaci√≥n as√≠ncrona paralela
        with ThreadPoolExecutor(max_workers=min(10, len(replication_targets))) as executor:
            futures = {executor.submit(replicate_to_target, target): target for target in replication_targets}
            
            for future in as_completed(futures):
                result = future.result()
                if result["success"]:
                    replication_result["replicated_to"].append(result["target"])
                else:
                    replication_result["failed_replications"].append(result)
    else:
        # Replicaci√≥n s√≠ncrona
        for target in replication_targets:
            result = replicate_to_target(target)
            if result["success"]:
                replication_result["replicated_to"].append(result["target"])
            else:
                replication_result["failed_replications"].append(result)
    
    replication_result["completed_at"] = datetime.utcnow().isoformat()
    replication_result["success_rate"] = (len(replication_result["replicated_to"]) / len(replication_targets) * 100) if replication_targets else 0
    
    return replication_result

# Advanced Load Balancing Strategies
def _load_balance_request(endpoints: List[str], strategy: str = "round_robin", health_checks: Dict[str, bool] = None, weights: Dict[str, int] = None, conn_id: str = None) -> str:
    """Balanceo de carga avanzado con m√∫ltiples estrategias."""
    if not endpoints:
        return None
    
    # Filtrar endpoints saludables si hay health checks
    healthy_endpoints = endpoints
    if health_checks:
        healthy_endpoints = [ep for ep in endpoints if health_checks.get(ep, True)]
    
    if not healthy_endpoints:
        # Si no hay endpoints saludables, usar todos
        healthy_endpoints = endpoints
    
    if strategy == "round_robin":
        # Round-robin
        if VARIABLES_AVAILABLE:
            counter_key = f"lb_counter_{hashlib.md5(str(endpoints).encode()).hexdigest()[:8]}"
            counter = int(Variable.get(counter_key, default_var="0"))
            selected = healthy_endpoints[counter % len(healthy_endpoints)]
            Variable.set(counter_key, str(counter + 1))
            return selected
        return healthy_endpoints[0]
    
    elif strategy == "weighted_round_robin":
        # Weighted round-robin
        if weights:
            total_weight = sum(weights.get(ep, 1) for ep in healthy_endpoints)
            if VARIABLES_AVAILABLE:
                counter_key = f"lb_weighted_counter_{hashlib.md5(str(endpoints).encode()).hexdigest()[:8]}"
                counter = int(Variable.get(counter_key, default_var="0"))
                
                cumulative = 0
                for ep in healthy_endpoints:
                    weight = weights.get(ep, 1)
                    cumulative += weight
                    if counter % total_weight < cumulative:
                        Variable.set(counter_key, str(counter + 1))
                        return ep
            return healthy_endpoints[0]
        return healthy_endpoints[0]
    
    elif strategy == "least_connections":
        # Least connections (simplificado - usar√≠a tracking real en producci√≥n)
        if VARIABLES_AVAILABLE:
            connections_key = f"lb_connections_{hashlib.md5(str(endpoints).encode()).hexdigest()[:8]}"
            connections_str = Variable.get(connections_key, default_var="{}")
            connections = json.loads(connections_str) if connections_str else {}
            
            # Encontrar endpoint con menos conexiones
            min_connections = min(connections.get(ep, 0) for ep in healthy_endpoints)
            selected = [ep for ep in healthy_endpoints if connections.get(ep, 0) == min_connections][0]
            
            # Incrementar contador
            connections[selected] = connections.get(selected, 0) + 1
            Variable.set(connections_key, json.dumps(connections))
            
            return selected
        return healthy_endpoints[0]
    
    elif strategy == "random":
        # Random selection
        import random
        return random.choice(healthy_endpoints)
    
    elif strategy == "hash":
        # Hash-based (consistent hashing)
        # Usar hash del request para selecci√≥n determin√≠stica
        request_hash = hash(str(datetime.utcnow().timestamp())) % len(healthy_endpoints)
        return healthy_endpoints[request_hash]
    
    return healthy_endpoints[0]

# Advanced Data Compression and Optimization
def _compress_and_optimize_data(data: Dict[str, Any], compression_level: str = "balanced", optimization_strategies: List[str] = None) -> Dict[str, Any]:
    """Compresi√≥n y optimizaci√≥n avanzada de datos."""
    if optimization_strategies is None:
        optimization_strategies = ["remove_nulls", "normalize_types", "compress_strings"]
    
    optimized_data = data.copy()
    
    for strategy in optimization_strategies:
        try:
            if strategy == "remove_nulls":
                # Remover campos None
                optimized_data = {k: v for k, v in optimized_data.items() if v is not None}
            
            elif strategy == "normalize_types":
                # Normalizar tipos de datos
                for key, value in optimized_data.items():
                    if isinstance(value, bool):
                        optimized_data[key] = int(value)  # Convertir bool a int para ahorrar espacio
                    elif isinstance(value, float) and value.is_integer():
                        optimized_data[key] = int(value)  # Convertir float a int si es entero
            
            elif strategy == "compress_strings":
                # Comprimir strings largos
                for key, value in optimized_data.items():
                    if isinstance(value, str) and len(value) > 100:
                        compressed = _compress_data(value, algorithm="gzip")
                        optimized_data[f"{key}_compressed"] = compressed.hex() if isinstance(compressed, bytes) else compressed
                        optimized_data.pop(key, None)
            
            elif strategy == "deduplicate":
                # Eliminar duplicados en listas
                for key, value in optimized_data.items():
                    if isinstance(value, list):
                        optimized_data[key] = list(dict.fromkeys(value))  # Mantener orden
        except Exception as e:
            logger.warning("optimization_strategy_failed", strategy=strategy, error=str(e))
    
    # Aplicar compresi√≥n seg√∫n nivel
    if compression_level in ["high", "maximum"]:
        data_str = json.dumps(optimized_data)
        compressed = _compress_data(data_str, algorithm="gzip")
        return {
            "_compressed": True,
            "_compression_level": compression_level,
            "_data": compressed.hex() if isinstance(compressed, bytes) else compressed
        }
    
    return optimized_data

# Advanced Monitoring with Predictive Analytics
def _monitor_with_predictive_analytics(metric_name: str, current_value: float, historical_values: List[float] = None, prediction_horizon: int = 24, conn_id: str = None) -> Dict[str, Any]:
    """Monitoreo avanzado con analytics predictivos."""
    prediction_result = {
        "metric_name": metric_name,
        "current_value": current_value,
        "predicted_values": [],
        "trend": "stable",
        "forecast_confidence": 0.0,
        "alerts": []
    }
    
    try:
        if historical_values is None:
            if VARIABLES_AVAILABLE:
                history_key = f"metric_history_{metric_name}"
                history_str = Variable.get(history_key, default_var="[]")
                historical_values = json.loads(history_str) if history_str else []
        
        if len(historical_values) >= 20:  # M√≠nimo para predicci√≥n
            # Simple linear regression para predicci√≥n
            n = len(historical_values)
            x = list(range(n))
            y = historical_values
            
            # Calcular pendiente (tendencia)
            x_mean = sum(x) / n
            y_mean = sum(y) / n
            
            numerator = sum((x[i] - x_mean) * (y[i] - y_mean) for i in range(n))
            denominator = sum((x[i] - x_mean) ** 2 for i in range(n))
            
            if denominator > 0:
                slope = numerator / denominator
                intercept = y_mean - slope * x_mean
                
                # Predecir valores futuros
                for i in range(1, prediction_horizon + 1):
                    predicted = slope * (n + i) + intercept
                    prediction_result["predicted_values"].append({
                        "horizon": i,
                        "predicted_value": predicted,
                        "timestamp": (datetime.utcnow() + timedelta(hours=i)).isoformat()
                    })
                
                # Determinar tendencia
                if slope > 0.1:
                    prediction_result["trend"] = "increasing"
                elif slope < -0.1:
                    prediction_result["trend"] = "decreasing"
                else:
                    prediction_result["trend"] = "stable"
                
                # Calcular confianza (simplificado)
                variance = sum((y[i] - (slope * x[i] + intercept)) ** 2 for i in range(n)) / n if n > 0 else 0
                prediction_result["forecast_confidence"] = max(0, min(1, 1 - (variance / (y_mean ** 2 + 1))))
                
                # Alertas predictivas
                if prediction_result["trend"] == "increasing" and prediction_result["predicted_values"][0]["predicted_value"] > current_value * 1.5:
                    prediction_result["alerts"].append({
                        "type": "growth_alert",
                        "message": f"Predicted significant growth in {metric_name}",
                        "severity": "medium"
                    })
                
                if prediction_result["trend"] == "decreasing" and prediction_result["predicted_values"][0]["predicted_value"] < current_value * 0.5:
                    prediction_result["alerts"].append({
                        "type": "decline_alert",
                        "message": f"Predicted significant decline in {metric_name}",
                        "severity": "high"
                    })
    except Exception as e:
        logger.error("predictive_analytics_failed", metric_name=metric_name, error=str(e))
    
    return prediction_result

# Advanced Security with Zero-Trust Architecture
def _apply_zero_trust_security(data: Dict[str, Any], request_context: Dict[str, Any] = None, security_policies: List[Dict[str, Any]] = None) -> Dict[str, Any]:
    """Aplica principios de zero-trust security."""
    security_result = {
        "zero_trust_applied": True,
        "verifications": [],
        "access_granted": False,
        "risk_score": 0,
        "timestamp": datetime.utcnow().isoformat()
    }
    
    if security_policies is None:
        security_policies = [
            {"type": "identity_verification", "required": True},
            {"type": "device_verification", "required": True},
            {"type": "network_verification", "required": True},
            {"type": "data_classification", "required": True}
        ]
    
    for policy in security_policies:
        policy_type = policy.get("type")
        verification_result = {"type": policy_type, "passed": False}
        
        try:
            if policy_type == "identity_verification":
                # Verificar identidad
                user_id = request_context.get("user_id") if request_context else None
                if user_id:
                    verification_result["passed"] = True
                    verification_result["details"] = {"user_id": user_id}
            
            elif policy_type == "device_verification":
                # Verificar dispositivo
                device_id = request_context.get("device_id") if request_context else None
                if device_id:
                    verification_result["passed"] = True
                    verification_result["details"] = {"device_id": device_id}
            
            elif policy_type == "network_verification":
                # Verificar red
                ip_address = request_context.get("ip_address") if request_context else None
                if ip_address:
                    # Verificar que no sea IP privada o bloqueada
                    try:
                        ip = ipaddress.ip_address(ip_address)
                        if not ip.is_private:
                            verification_result["passed"] = True
                    except Exception:
                        pass
            
            elif policy_type == "data_classification":
                # Clasificar datos
                sensitive_fields = ["email", "phone", "ssn", "credit_card"]
                has_sensitive = any(field in data for field in sensitive_fields)
                if has_sensitive:
                    security_result["risk_score"] += 20
                    verification_result["passed"] = True
                    verification_result["details"] = {"sensitive_data_detected": True}
                else:
                    verification_result["passed"] = True
            
            if not verification_result["passed"] and policy.get("required", False):
                security_result["risk_score"] += 30
        except Exception as e:
            logger.warning("zero_trust_verification_failed", policy_type=policy_type, error=str(e))
            verification_result["error"] = str(e)
        
        security_result["verifications"].append(verification_result)
    
    # Determinar acceso
    all_required_passed = all(
        v["passed"] for v in security_result["verifications"]
        if security_policies[security_result["verifications"].index(v)].get("required", False)
    )
    security_result["access_granted"] = all_required_passed and security_result["risk_score"] < 50
    
    return security_result

# Advanced Data Governance
def _apply_data_governance(data: Dict[str, Any], governance_policies: List[Dict[str, Any]] = None, conn_id: str = None) -> Dict[str, Any]:
    """Aplica pol√≠ticas de data governance avanzadas."""
    governance_result = {
        "governance_applied": True,
        "policies_applied": [],
        "compliance_status": "compliant",
        "violations": [],
        "data_classification": "unclassified"
    }
    
    if governance_policies is None:
        governance_policies = [
            {"type": "data_classification", "rules": {"sensitive_fields": ["email", "phone"]}},
            {"type": "retention_policy", "rules": {"default_retention_days": 90}},
            {"type": "access_control", "rules": {"required_permissions": ["read", "write"]}}
        ]
    
    for policy in governance_policies:
        policy_type = policy.get("type")
        rules = policy.get("rules", {})
        
        try:
            if policy_type == "data_classification":
                # Clasificar datos
                sensitive_fields = rules.get("sensitive_fields", [])
                has_sensitive = any(field in data for field in sensitive_fields)
                
                if has_sensitive:
                    governance_result["data_classification"] = "sensitive"
                elif any(key in data for key in ["company", "industry"]):
                    governance_result["data_classification"] = "internal"
                else:
                    governance_result["data_classification"] = "public"
                
                governance_result["policies_applied"].append({
                    "type": policy_type,
                    "classification": governance_result["data_classification"]
                })
            
            elif policy_type == "retention_policy":
                # Aplicar pol√≠tica de retenci√≥n
                retention_days = rules.get("default_retention_days", 90)
                governance_result["policies_applied"].append({
                    "type": policy_type,
                    "retention_days": retention_days
                })
            
            elif policy_type == "access_control":
                # Verificar control de acceso
                required_permissions = rules.get("required_permissions", [])
                governance_result["policies_applied"].append({
                    "type": policy_type,
                    "required_permissions": required_permissions
                })
        except Exception as e:
            logger.warning("governance_policy_failed", policy_type=policy_type, error=str(e))
            governance_result["violations"].append({
                "policy_type": policy_type,
                "error": str(e)
            })
    
    if governance_result["violations"]:
        governance_result["compliance_status"] = "non_compliant"
    
    return governance_result

# Advanced API Rate Limiting with Quotas
def _check_rate_limit_with_quota(identifier: str, quota_type: str = "daily", quota_limit: int = 1000, window_seconds: int = 86400, conn_id: str = None) -> Tuple[bool, Optional[float], Dict[str, Any]]:
    """Rate limiting avanzado con sistema de cuotas."""
    quota_result = {
        "identifier": identifier,
        "quota_type": quota_type,
        "quota_limit": quota_limit,
        "quota_used": 0,
        "quota_remaining": quota_limit,
        "quota_reset_at": None
    }
    
    try:
        if VARIABLES_AVAILABLE:
            quota_key = f"quota_{quota_type}_{identifier}"
            quota_data_str = Variable.get(quota_key, default_var="{}")
            quota_data = json.loads(quota_data_str) if quota_data_str else {}
            
            # Obtener per√≠odo actual
            if quota_type == "daily":
                period_key = datetime.utcnow().strftime("%Y-%m-%d")
            elif quota_type == "hourly":
                period_key = datetime.utcnow().strftime("%Y-%m-%d-%H")
            else:
                period_key = "default"
            
            period_quota = quota_data.get(period_key, {"used": 0, "reset_at": None})
            quota_used = period_quota.get("used", 0)
            
            quota_result["quota_used"] = quota_used
            quota_result["quota_remaining"] = max(0, quota_limit - quota_used)
            
            # Calcular reset time
            if quota_type == "daily":
                reset_time = (datetime.utcnow() + timedelta(days=1)).replace(hour=0, minute=0, second=0)
            elif quota_type == "hourly":
                reset_time = (datetime.utcnow() + timedelta(hours=1)).replace(minute=0, second=0)
            else:
                reset_time = datetime.utcnow() + timedelta(seconds=window_seconds)
            
            quota_result["quota_reset_at"] = reset_time.isoformat()
            
            # Verificar si hay cuota disponible
            if quota_used >= quota_limit:
                # Calcular tiempo hasta reset
                wait_time = (reset_time - datetime.utcnow()).total_seconds()
                return False, wait_time, quota_result
            
            # Incrementar uso
            period_quota["used"] = quota_used + 1
            period_quota["reset_at"] = reset_time.isoformat()
            quota_data[period_key] = period_quota
            
            # Limpiar per√≠odos antiguos
            if len(quota_data) > 10:
                # Mantener solo √∫ltimos 10 per√≠odos
                sorted_keys = sorted(quota_data.keys())
                for key in sorted_keys[:-10]:
                    quota_data.pop(key, None)
            
            Variable.set(quota_key, json.dumps(quota_data))
            
            quota_result["quota_used"] = period_quota["used"]
            quota_result["quota_remaining"] = max(0, quota_limit - period_quota["used"])
            
            return True, None, quota_result
    except Exception as e:
        logger.warning("quota_check_failed", identifier=identifier, error=str(e))
    
    # Fallback: permitir si falla
    return True, None, quota_result

# Advanced Caching with Cache Coherence
def _cache_with_coherence(key: str, value: Any, ttl: int = 3600, coherence_strategy: str = "write_through", invalidation_keys: List[str] = None, conn_id: str = None) -> bool:
    """Cache con coherencia avanzada entre m√∫ltiples niveles."""
    try:
        # Escribir en cache principal
        cache_success = _cache_with_redis(key, value, ttl, conn_id) if redis_client else False
        
        if not cache_success and enrichment_cache:
            enrichment_cache[key] = value
            cache_success = True
        
        if cache_success:
            # Aplicar estrategia de coherencia
            if coherence_strategy == "write_through":
                # Escribir en todos los niveles simult√°neamente
                if redis_client:
                    redis_client.set(key, json.dumps(value), ex=ttl)
                if enrichment_cache:
                    enrichment_cache[key] = value
            
            elif coherence_strategy == "write_back":
                # Escribir en cache primero, luego propagar
                if invalidation_keys:
                    for inv_key in invalidation_keys:
                        _invalidate_cache_by_dependency(inv_key, conn_id)
            
            elif coherence_strategy == "invalidate_on_write":
                # Invalidar caches relacionados
                if invalidation_keys:
                    for inv_key in invalidation_keys:
                        if redis_client:
                            redis_client.delete(inv_key)
                        if enrichment_cache:
                            enrichment_cache.pop(inv_key, None)
        
        return cache_success
    except Exception as e:
        logger.error("cache_coherence_failed", key=key, error=str(e))
        return False

# Advanced Error Recovery with Automatic Remediation
def _recover_with_automatic_remediation(error: Exception, context: Dict[str, Any], remediation_strategies: List[Dict[str, Any]] = None, conn_id: str = None) -> Dict[str, Any]:
    """Recuperaci√≥n avanzada de errores con remediaci√≥n autom√°tica."""
    if remediation_strategies is None:
        remediation_strategies = [
            {"type": "retry", "enabled": True, "max_attempts": 3},
            {"type": "fallback", "enabled": True, "fallback_value": None},
            {"type": "circuit_breaker_reset", "enabled": False},
            {"type": "scale_up", "enabled": False}
        ]
    
    remediation_result = {
        "error_type": type(error).__name__,
        "error_message": str(error),
        "remediation_attempted": False,
        "remediation_successful": False,
        "strategies_applied": [],
        "recovered": False
    }
    
    for strategy in remediation_strategies:
        if not strategy.get("enabled", False):
            continue
        
        strategy_type = strategy.get("type")
        remediation_result["strategies_applied"].append(strategy_type)
        
        try:
            if strategy_type == "retry":
                func = context.get("function")
                if func and callable(func):
                    max_attempts = strategy.get("max_attempts", 3)
                    result = _retry_with_advanced_backoff(
                        func,
                        max_retries=max_attempts,
                        *context.get("args", []),
                        **context.get("kwargs", {})
                    )
                    remediation_result["remediation_attempted"] = True
                    remediation_result["remediation_successful"] = True
                    remediation_result["recovered"] = True
                    remediation_result["recovered_with"] = "retry"
                    break
            
            elif strategy_type == "fallback":
                fallback_value = strategy.get("fallback_value")
                if fallback_value is not None:
                    remediation_result["remediation_attempted"] = True
                    remediation_result["remediation_successful"] = True
                    remediation_result["recovered"] = True
                    remediation_result["recovered_with"] = "fallback"
                    break
            
            elif strategy_type == "circuit_breaker_reset":
                cb_name = context.get("circuit_breaker_name")
                if cb_name:
                    # Resetear circuit breaker
                    _manage_circuit_breakers_advanced({cb_name: context.get("circuit_breaker")}, action="reset")
                    remediation_result["remediation_attempted"] = True
                    remediation_result["remediation_successful"] = True
            
            elif strategy_type == "scale_up":
                # Escalar recursos (placeholder)
                logger.info("scale_up_remediation", context=context)
                remediation_result["remediation_attempted"] = True
        except Exception as e:
            logger.warning("remediation_strategy_failed", strategy=strategy_type, error=str(e))
            continue
    
    return remediation_result

# Advanced Observability with Correlation IDs
def _create_correlation_context(correlation_id: str = None, parent_correlation_id: str = None, metadata: Dict[str, Any] = None) -> Dict[str, Any]:
    """Crea contexto de correlaci√≥n para observabilidad distribuida."""
    if correlation_id is None:
        correlation_id = f"corr_{datetime.utcnow().timestamp()}_{hashlib.md5(str(time.time()).encode()).hexdigest()[:8]}"
    
    correlation_context = {
        "correlation_id": correlation_id,
        "parent_correlation_id": parent_correlation_id,
        "trace_id": correlation_id,  # Usar correlation_id como trace_id
        "span_id": f"span_{hashlib.md5(f'{correlation_id}{time.time()}'.encode()).hexdigest()[:8]}",
        "metadata": metadata or {},
        "created_at": datetime.utcnow().isoformat()
    }
    
    # Guardar contexto
    if VARIABLES_AVAILABLE:
        context_key = f"correlation_context_{correlation_id}"
        Variable.set(context_key, json.dumps(correlation_context))
    
    # Logging con correlation ID
    logger.info("correlation_context_created", correlation_id=correlation_id, parent_correlation_id=parent_correlation_id)
    
    return correlation_context

def _log_with_correlation(correlation_id: str, event_name: str, level: str = "info", **kwargs) -> None:
    """Logging con correlation ID para trazabilidad."""
    log_data = {
        "correlation_id": correlation_id,
        "event": event_name,
        "timestamp": datetime.utcnow().isoformat(),
        "level": level,
        **kwargs
    }
    
    # Agregar contexto de correlaci√≥n si existe
    if VARIABLES_AVAILABLE:
        context_key = f"correlation_context_{correlation_id}"
        context_str = Variable.get(context_key, default_var="{}")
        if context_str:
            context = json.loads(context_str)
            log_data["correlation_context"] = context
    
    if level == "debug":
        logger.debug(event_name, **log_data)
    elif level == "info":
        logger.info(event_name, **log_data)
    elif level == "warning":
        logger.warning(event_name, **log_data)
    elif level == "error":
        logger.error(event_name, **log_data)
    else:
        logger.info(event_name, **log_data)

# ============================================================================
# FUNCIONES HELPER ULTRA AVANZADAS - ENTERPRISE COSMIC FEATURES
# ============================================================================

# Advanced Event-Driven Processing
def _process_event_driven(event: Dict[str, Any], event_handlers: Dict[str, Callable], event_routing: Dict[str, str] = None, conn_id: str = None) -> Dict[str, Any]:
    """Procesamiento basado en eventos con routing y handlers configurables."""
    event_type = event.get("type", "unknown")
    event_result = {
        "event_id": event.get("event_id", f"evt_{datetime.utcnow().timestamp()}"),
        "event_type": event_type,
        "processed": False,
        "handlers_executed": [],
        "handlers_failed": [],
        "routed_to": None,
        "started_at": datetime.utcnow().isoformat()
    }
    
    # Routing de eventos
    if event_routing:
        target_handler = event_routing.get(event_type)
        if target_handler:
            event_result["routed_to"] = target_handler
    
    # Ejecutar handlers
    handler = event_handlers.get(event_type)
    if handler and callable(handler):
        try:
            result = handler(event)
            event_result["processed"] = True
            event_result["handlers_executed"].append(event_type)
            event_result["result"] = result
        except Exception as e:
            logger.error("event_handler_failed", event_type=event_type, error=str(e))
            event_result["handlers_failed"].append({
                "handler": event_type,
                "error": str(e)
            })
    else:
        # Handler por defecto
        default_handler = event_handlers.get("default")
        if default_handler and callable(default_handler):
            try:
                result = default_handler(event)
                event_result["processed"] = True
                event_result["handlers_executed"].append("default")
                event_result["result"] = result
            except Exception as e:
                logger.error("default_event_handler_failed", error=str(e))
    
    event_result["completed_at"] = datetime.utcnow().isoformat()
    return event_result

# Advanced Distributed State Management
def _manage_distributed_state(state_key: str, state_operation: str = "get", state_data: Dict[str, Any] = None, ttl: int = 3600, conn_id: str = None) -> Dict[str, Any]:
    """Gesti√≥n avanzada de estado distribuido con operaciones CRUD."""
    state_result = {
        "state_key": state_key,
        "operation": state_operation,
        "success": False,
        "state_data": None,
        "timestamp": datetime.utcnow().isoformat()
    }
    
    try:
        if VARIABLES_AVAILABLE:
            full_key = f"distributed_state_{state_key}"
            
            if state_operation == "get":
                # Obtener estado
                state_str = Variable.get(full_key, default_var="{}")
                if state_str:
                    state_data = json.loads(state_str)
                    state_result["state_data"] = state_data
                    state_result["success"] = True
            
            elif state_operation == "set":
                # Establecer estado
                if state_data:
                    Variable.set(full_key, json.dumps(state_data))
                    state_result["success"] = True
                    state_result["state_data"] = state_data
            
            elif state_operation == "update":
                # Actualizar estado (merge)
                existing_str = Variable.get(full_key, default_var="{}")
                existing = json.loads(existing_str) if existing_str else {}
                
                if state_data:
                    existing.update(state_data)
                    Variable.set(full_key, json.dumps(existing))
                    state_result["success"] = True
                    state_result["state_data"] = existing
            
            elif state_operation == "delete":
                # Eliminar estado
                try:
                    Variable.delete(full_key)
                    state_result["success"] = True
                except Exception:
                    state_result["success"] = True  # Ya no existe
            
            elif state_operation == "increment":
                # Incrementar valor num√©rico
                existing_str = Variable.get(full_key, default_var="0")
                try:
                    current_value = int(existing_str)
                    increment = state_data.get("increment", 1) if state_data else 1
                    new_value = current_value + increment
                    Variable.set(full_key, str(new_value))
                    state_result["success"] = True
                    state_result["state_data"] = {"value": new_value}
                except ValueError:
                    state_result["success"] = False
                    state_result["error"] = "State is not numeric"
    except Exception as e:
        logger.error("distributed_state_operation_failed", operation=state_operation, error=str(e))
        state_result["error"] = str(e)
    
    return state_result

# Advanced Query Optimization with Execution Plans
def _optimize_query_with_plan(query: str, params: Dict[str, Any] = None, conn_id: str = None) -> Dict[str, Any]:
    """Optimizaci√≥n avanzada de queries con an√°lisis de execution plan."""
    optimization_result = {
        "original_query": query,
        "optimized_query": query,
        "optimizations_applied": [],
        "estimated_cost": 0.0,
        "execution_plan": None,
        "recommendations": []
    }
    
    try:
        hook = PostgresHook(postgres_conn_id=conn_id) if conn_id else None
        if hook:
            with hook.get_conn() as conn:
                with conn.cursor() as cur:
                    # Obtener execution plan
                    explain_query = f"EXPLAIN ANALYZE {query}"
                    try:
                        cur.execute(explain_query, params or {})
                        plan_rows = cur.fetchall()
                        if plan_rows:
                            optimization_result["execution_plan"] = "\n".join(str(row) for row in plan_rows)
                    except Exception:
                        # Si EXPLAIN ANALYZE falla, usar EXPLAIN
                        try:
                            explain_query = f"EXPLAIN {query}"
                            cur.execute(explain_query, params or {})
                            plan_rows = cur.fetchall()
                            if plan_rows:
                                optimization_result["execution_plan"] = "\n".join(str(row) for row in plan_rows)
                        except Exception:
                            pass
                    
                    # Analizar query para optimizaciones
                    query_upper = query.upper()
                    
                    # Optimizaci√≥n 1: SELECT * -> SELECT espec√≠fico
                    if "SELECT *" in query_upper:
                        optimization_result["recommendations"].append("Consider selecting specific columns instead of *")
                    
                    # Optimizaci√≥n 2: LIKE sin √≠ndice
                    if "LIKE" in query_upper and "%" in query:
                        optimization_result["recommendations"].append("Consider using full-text search or indexed columns for LIKE queries")
                    
                    # Optimizaci√≥n 3: Sin WHERE en UPDATE/DELETE
                    if ("UPDATE" in query_upper or "DELETE" in query_upper) and "WHERE" not in query_upper:
                        optimization_result["recommendations"].append("WARNING: UPDATE/DELETE without WHERE clause")
                    
                    # Optimizaci√≥n 4: JOIN sin √≠ndices
                    if "JOIN" in query_upper:
                        optimization_result["recommendations"].append("Ensure JOIN columns are indexed")
                    
                    # Aplicar optimizaciones b√°sicas
                    optimized = query
                    
                    # Remover espacios m√∫ltiples
                    optimized = re.sub(r'\s+', ' ', optimized)
                    optimization_result["optimizations_applied"].append("whitespace_normalization")
                    
                    optimization_result["optimized_query"] = optimized
    except Exception as e:
        logger.warning("query_optimization_failed", error=str(e))
        optimization_result["error"] = str(e)
    
    return optimization_result

# Advanced Data Partitioning Strategy
def _partition_data_advanced(data: List[Dict[str, Any]], partition_strategy: str = "hash", partition_key: str = "email", num_partitions: int = 4) -> Dict[str, List[Dict[str, Any]]]:
    """Particionamiento avanzado de datos con m√∫ltiples estrategias."""
    partitions = {f"partition_{i}": [] for i in range(num_partitions)}
    
    for item in data:
        partition_index = 0
        
        if partition_strategy == "hash":
            # Hash-based partitioning
            key_value = str(item.get(partition_key, ""))
            partition_index = hash(key_value) % num_partitions
        
        elif partition_strategy == "range":
            # Range-based partitioning
            key_value = item.get(partition_key, "")
            if isinstance(key_value, (int, float)):
                partition_index = min(int(key_value / (100 / num_partitions)), num_partitions - 1)
            else:
                partition_index = hash(str(key_value)) % num_partitions
        
        elif partition_strategy == "round_robin":
            # Round-robin partitioning
            partition_index = len([item for p in partitions.values() for item in p]) % num_partitions
        
        elif partition_strategy == "custom":
            # Custom partitioning function
            custom_func = partition_strategy.get("function")
            if custom_func and callable(custom_func):
                partition_index = custom_func(item) % num_partitions
        
        partitions[f"partition_{partition_index}"].append(item)
    
    return partitions

# Advanced Circuit Breaker with Half-Open State
def _call_with_advanced_circuit_breaker(func: Callable, circuit_breaker: CircuitBreaker = None, half_open_requests: int = 3, *args, **kwargs) -> Tuple[Any, Dict[str, Any]]:
    """Llamada con circuit breaker avanzado que maneja estado half-open."""
    if circuit_breaker is None:
        circuit_breaker = enrichment_circuit_breaker
    
    cb_result = {
        "circuit_state": "closed",
        "call_successful": False,
        "result": None,
        "error": None
    }
    
    try:
        # Intentar llamada
        result = circuit_breaker.call(func, *args, **kwargs)
        cb_result["call_successful"] = True
        cb_result["result"] = result
        cb_result["circuit_state"] = "closed"
    except Exception as e:
        cb_result["error"] = str(e)
        cb_result["circuit_state"] = "open" if hasattr(circuit_breaker, 'current_state') else "unknown"
        
        # En estado half-open, permitir algunas llamadas de prueba
        if hasattr(circuit_breaker, 'current_state') and circuit_breaker.current_state == "half_open":
            # Permitir llamadas limitadas en half-open
            pass
    
    return cb_result.get("result"), cb_result

# Advanced Data Validation with Cross-Field Rules
def _validate_with_cross_field_rules(data: Dict[str, Any], cross_field_rules: List[Dict[str, Any]]) -> Tuple[bool, List[str], Dict[str, Any]]:
    """Validaci√≥n avanzada con reglas que involucran m√∫ltiples campos."""
    errors = []
    warnings = []
    validation_details = {}
    
    for rule in cross_field_rules:
        rule_type = rule.get("type")
        fields = rule.get("fields", [])
        condition = rule.get("condition")
        error_message = rule.get("error_message", "Cross-field validation failed")
        
        try:
            if rule_type == "all_required":
                # Todos los campos deben estar presentes
                missing_fields = [f for f in fields if f not in data or data[f] is None]
                if missing_fields:
                    errors.append(f"{error_message}: Missing fields: {missing_fields}")
            
            elif rule_type == "at_least_one":
                # Al menos uno de los campos debe estar presente
                present_fields = [f for f in fields if f in data and data[f] is not None]
                if not present_fields:
                    errors.append(f"{error_message}: At least one of {fields} must be present")
            
            elif rule_type == "mutually_exclusive":
                # Los campos son mutuamente excluyentes
                present_fields = [f for f in fields if f in data and data[f] is not None]
                if len(present_fields) > 1:
                    errors.append(f"{error_message}: Fields {present_fields} are mutually exclusive")
            
            elif rule_type == "conditional":
                # Validaci√≥n condicional
                if condition and callable(condition):
                    if not condition(data):
                        errors.append(error_message)
            
            elif rule_type == "sum_constraint":
                # Restricci√≥n de suma
                target_sum = rule.get("target_sum")
                if target_sum is not None:
                    actual_sum = sum(float(data.get(f, 0)) for f in fields if isinstance(data.get(f), (int, float)))
                    if abs(actual_sum - target_sum) > 0.01:  # Tolerancia para floats
                        errors.append(f"{error_message}: Sum of {fields} should be {target_sum}, got {actual_sum}")
            
            elif rule_type == "range_constraint":
                # Restricci√≥n de rango entre campos
                min_field = rule.get("min_field")
                max_field = rule.get("max_field")
                if min_field and max_field:
                    min_value = data.get(min_field)
                    max_value = data.get(max_field)
                    if min_value is not None and max_value is not None:
                        if isinstance(min_value, (int, float)) and isinstance(max_value, (int, float)):
                            if min_value > max_value:
                                errors.append(f"{error_message}: {min_field} ({min_value}) cannot be greater than {max_field} ({max_value})")
        except Exception as e:
            warnings.append(f"Cross-field rule {rule_type} raised exception: {str(e)}")
    
    validation_details["cross_field_rules_applied"] = len(cross_field_rules)
    validation_details["warnings"] = warnings
    
    return len(errors) == 0, errors, validation_details

# Advanced Retry with Adaptive Strategy
def _retry_with_adaptive_strategy(func: Callable, initial_strategy: str = "exponential", max_retries: int = 3, performance_history: List[float] = None, *args, **kwargs) -> Tuple[Any, Dict[str, Any]]:
    """Retry con estrategia adaptativa basada en historial de performance."""
    retry_result = {
        "strategy_used": initial_strategy,
        "retries": 0,
        "total_duration": 0.0,
        "success": False
    }
    
    current_strategy = initial_strategy
    last_exception = None
    
    # Analizar historial para ajustar estrategia
    if performance_history and len(performance_history) >= 3:
        avg_duration = sum(performance_history) / len(performance_history)
        if avg_duration > 5.0:
            # Si es lento, usar estrategia m√°s agresiva
            current_strategy = "polynomial"
        elif avg_duration < 0.5:
            # Si es r√°pido, usar estrategia m√°s conservadora
            current_strategy = "linear"
    
    for attempt in range(max_retries):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            duration = time.time() - start_time
            
            retry_result["success"] = True
            retry_result["total_duration"] = duration
            retry_result["strategy_used"] = current_strategy
            return result, retry_result
        except Exception as e:
            last_exception = e
            retry_result["retries"] += 1
            
            if attempt < max_retries - 1:
                # Calcular delay seg√∫n estrategia
                if current_strategy == "exponential":
                    delay = 2 ** attempt
                elif current_strategy == "linear":
                    delay = (attempt + 1) * 1
                elif current_strategy == "polynomial":
                    delay = (attempt + 1) ** 2
                elif current_strategy == "fibonacci":
                    fib = [1, 1]
                    for i in range(attempt + 1):
                        fib.append(fib[-1] + fib[-2])
                    delay = fib[min(attempt, len(fib) - 1)]
                else:
                    delay = 2 ** attempt
                
                time.sleep(delay)
    
    retry_result["error"] = str(last_exception)
    raise last_exception

# Advanced Data Sampling for Large Datasets
def _sample_data_advanced(data: List[Dict[str, Any]], sample_size: int = 100, sampling_method: str = "random", seed: int = None) -> List[Dict[str, Any]]:
    """Muestreo avanzado de datos con m√∫ltiples m√©todos."""
    if len(data) <= sample_size:
        return data
    
    if sampling_method == "random":
        import random
        if seed is not None:
            random.seed(seed)
        return random.sample(data, sample_size)
    
    elif sampling_method == "systematic":
        # Muestreo sistem√°tico
        step = len(data) // sample_size
        return [data[i] for i in range(0, len(data), step)][:sample_size]
    
    elif sampling_method == "stratified":
        # Muestreo estratificado (simplificado)
        # Dividir en estratos y muestrear de cada uno
        strata_size = len(data) // sample_size
        sampled = []
        for i in range(0, len(data), strata_size):
            if len(sampled) < sample_size:
                sampled.append(data[i])
        return sampled[:sample_size]
    
    elif sampling_method == "first_n":
        # Primeros N
        return data[:sample_size]
    
    elif sampling_method == "last_n":
        # √öltimos N
        return data[-sample_size:]
    
    return data[:sample_size]

# Advanced API Throttling with Burst Allowance
def _throttle_with_burst_allowance(identifier: str, rate_limit: int = 100, window_seconds: int = 3600, burst_allowance: int = 20, conn_id: str = None) -> Tuple[bool, Optional[float], Dict[str, Any]]:
    """Throttling avanzado con allowance para bursts."""
    throttle_result = {
        "identifier": identifier,
        "rate_limit": rate_limit,
        "burst_allowance": burst_allowance,
        "current_usage": 0,
        "burst_used": 0,
        "allowed": False
    }
    
    try:
        if VARIABLES_AVAILABLE:
            throttle_key = f"throttle_{identifier}"
            throttle_data_str = Variable.get(throttle_key, default_var="{}")
            throttle_data = json.loads(throttle_data_str) if throttle_data_str else {}
            
            window_start = int((datetime.utcnow().timestamp() - window_seconds) / window_seconds) * window_seconds
            window_key = str(window_start)
            
            window_data = throttle_data.get(window_key, {"requests": 0, "burst_used": 0})
            current_usage = window_data.get("requests", 0)
            burst_used = window_data.get("burst_used", 0)
            
            throttle_result["current_usage"] = current_usage
            throttle_result["burst_used"] = burst_used
            
            # Verificar l√≠mite normal
            if current_usage < rate_limit:
                throttle_result["allowed"] = True
                window_data["requests"] = current_usage + 1
            # Verificar burst allowance
            elif burst_used < burst_allowance:
                throttle_result["allowed"] = True
                window_data["burst_used"] = burst_used + 1
                throttle_result["burst_used"] = window_data["burst_used"]
            else:
                # Calcular tiempo de espera
                next_window = window_start + window_seconds
                wait_time = next_window - datetime.utcnow().timestamp()
                throttle_result["allowed"] = False
                return False, wait_time, throttle_result
            
            # Actualizar datos
            throttle_data[window_key] = window_data
            
            # Limpiar ventanas antiguas
            current_window = int(datetime.utcnow().timestamp() / window_seconds) * window_seconds
            for key in list(throttle_data.keys()):
                try:
                    key_window = int(key)
                    if key_window < current_window - window_seconds * 2:
                        throttle_data.pop(key, None)
                except ValueError:
                    pass
            
            Variable.set(throttle_key, json.dumps(throttle_data))
            
            return True, None, throttle_result
    except Exception as e:
        logger.warning("throttle_check_failed", identifier=identifier, error=str(e))
    
    # Fallback: permitir si falla
    return True, None, throttle_result

# Advanced Data Transformation with Schema Evolution
def _transform_with_schema_evolution(data: Dict[str, Any], source_schema: Dict[str, Any], target_schema: Dict[str, Any], evolution_strategy: str = "forward_compatible") -> Dict[str, Any]:
    """Transformaci√≥n de datos con evoluci√≥n de schema."""
    transformation_result = {
        "source_schema_version": source_schema.get("version", "1.0"),
        "target_schema_version": target_schema.get("version", "2.0"),
        "evolution_strategy": evolution_strategy,
        "transformations_applied": [],
        "warnings": [],
        "transformed_data": {}
    }
    
    source_fields = source_schema.get("fields", {})
    target_fields = target_schema.get("fields", {})
    
    transformed = {}
    
    # Mapear campos existentes
    for field_name, field_config in target_fields.items():
        source_field = source_fields.get(field_name)
        
        if source_field:
            # Campo existe en ambos schemas
            source_value = data.get(field_name)
            
            # Aplicar transformaci√≥n si es necesario
            transform_func = field_config.get("transform")
            if transform_func and callable(transform_func):
                try:
                    transformed_value = transform_func(source_value)
                    transformed[field_name] = transformed_value
                    transformation_result["transformations_applied"].append(f"transformed_{field_name}")
                except Exception as e:
                    transformed[field_name] = source_value
                    transformation_result["warnings"].append(f"Transform failed for {field_name}: {str(e)}")
            else:
                transformed[field_name] = source_value
        else:
            # Campo nuevo en target schema
            default_value = field_config.get("default")
            if default_value is not None:
                transformed[field_name] = default_value
                transformation_result["transformations_applied"].append(f"added_default_{field_name}")
            elif field_config.get("required", False):
                transformation_result["warnings"].append(f"Required field {field_name} missing in source")
    
    # Manejar campos obsoletos
    for field_name in data.keys():
        if field_name not in target_fields:
            if evolution_strategy == "forward_compatible":
                # Mantener campos obsoletos
                transformed[field_name] = data[field_name]
                transformation_result["transformations_applied"].append(f"preserved_obsolete_{field_name}")
            elif evolution_strategy == "strict":
                # Eliminar campos obsoletos
                transformation_result["transformations_applied"].append(f"removed_obsolete_{field_name}")
    
    transformation_result["transformed_data"] = transformed
    return transformation_result

# ============================================================================
# FUNCIONES HELPER ULTRA AVANZADAS - ENTERPRISE QUANTUM FEATURES
# ============================================================================

# Advanced Stream Processing with Windowing
def _process_stream_with_windowing(stream_data: List[Dict[str, Any]], window_type: str = "tumbling", window_size: int = 100, window_duration_seconds: int = 60, aggregation_func: Callable = None, conn_id: str = None) -> Dict[str, Any]:
    """Procesamiento de streams con ventanas deslizantes y agregaciones."""
    stream_result = {
        "window_type": window_type,
        "window_size": window_size,
        "windows_processed": 0,
        "aggregations": [],
        "total_items": len(stream_data),
        "started_at": datetime.utcnow().isoformat()
    }
    
    if not stream_data:
        return stream_result
    
    if window_type == "tumbling":
        # Ventanas fijas sin solapamiento
        for i in range(0, len(stream_data), window_size):
            window = stream_data[i:i + window_size]
            if aggregation_func and callable(aggregation_func):
                aggregated = aggregation_func(window)
                stream_result["aggregations"].append({
                    "window_index": stream_result["windows_processed"],
                    "items": len(window),
                    "aggregated_value": aggregated
                })
            stream_result["windows_processed"] += 1
    
    elif window_type == "sliding":
        # Ventanas deslizantes con solapamiento
        step = window_size // 2  # 50% de solapamiento
        for i in range(0, len(stream_data) - window_size + 1, step):
            window = stream_data[i:i + window_size]
            if aggregation_func and callable(aggregation_func):
                aggregated = aggregation_func(window)
                stream_result["aggregations"].append({
                    "window_index": stream_result["windows_processed"],
                    "items": len(window),
                    "aggregated_value": aggregated
                })
            stream_result["windows_processed"] += 1
    
    elif window_type == "session":
        # Ventanas basadas en sesiones (gap-based)
        current_window = []
        for item in stream_data:
            timestamp = item.get("timestamp", datetime.utcnow().timestamp())
            if current_window:
                last_timestamp = current_window[-1].get("timestamp", timestamp)
                gap = timestamp - last_timestamp
                if gap > window_duration_seconds:
                    # Nueva sesi√≥n
                    if aggregation_func and callable(aggregation_func):
                        aggregated = aggregation_func(current_window)
                        stream_result["aggregations"].append({
                            "window_index": stream_result["windows_processed"],
                            "items": len(current_window),
                            "aggregated_value": aggregated
                        })
                    stream_result["windows_processed"] += 1
                    current_window = []
            current_window.append(item)
        
        # Procesar √∫ltima ventana
        if current_window and aggregation_func and callable(aggregation_func):
            aggregated = aggregation_func(current_window)
            stream_result["aggregations"].append({
                "window_index": stream_result["windows_processed"],
                "items": len(current_window),
                "aggregated_value": aggregated
            })
            stream_result["windows_processed"] += 1
    
    stream_result["completed_at"] = datetime.utcnow().isoformat()
    return stream_result

# Advanced Memory Management and Optimization
def _optimize_memory_usage(data: Dict[str, Any], optimization_level: str = "moderate", max_memory_mb: int = 100) -> Dict[str, Any]:
    """Optimizaci√≥n avanzada de uso de memoria."""
    optimization_result = {
        "original_size_mb": 0,
        "optimized_size_mb": 0,
        "optimizations_applied": [],
        "memory_saved_mb": 0
    }
    
    try:
        import sys
        original_size = sys.getsizeof(json.dumps(data))
        optimization_result["original_size_mb"] = original_size / (1024 * 1024)
        
        optimized = data.copy()
        
        if optimization_level in ["moderate", "aggressive"]:
            # Remover campos None
            optimized = {k: v for k, v in optimized.items() if v is not None}
            optimization_result["optimizations_applied"].append("remove_nulls")
            
            # Truncar strings largos
            for key, value in optimized.items():
                if isinstance(value, str) and len(value) > 1000:
                    optimized[key] = value[:1000] + "..."
                    optimization_result["optimizations_applied"].append(f"truncated_{key}")
            
            # Comprimir listas grandes
            for key, value in optimized.items():
                if isinstance(value, list) and len(value) > 100:
                    if optimization_level == "aggressive":
                        # Mantener solo primeros y √∫ltimos elementos
                        optimized[key] = value[:50] + value[-50:]
                        optimization_result["optimizations_applied"].append(f"compressed_list_{key}")
        
        if optimization_level == "aggressive":
            # Comprimir datos completos
            data_str = json.dumps(optimized)
            compressed = _compress_data(data_str, algorithm="gzip")
            optimized = {
                "_compressed": True,
                "_data": compressed.hex() if isinstance(compressed, bytes) else compressed
            }
            optimization_result["optimizations_applied"].append("full_compression")
        
        optimized_size = sys.getsizeof(json.dumps(optimized))
        optimization_result["optimized_size_mb"] = optimized_size / (1024 * 1024)
        optimization_result["memory_saved_mb"] = optimization_result["original_size_mb"] - optimization_result["optimized_size_mb"]
        optimization_result["optimized_data"] = optimized
    except Exception as e:
        logger.warning("memory_optimization_failed", error=str(e))
        optimization_result["error"] = str(e)
        optimization_result["optimized_data"] = data
    
    return optimization_result

# Advanced Network Optimization with Connection Pooling
def _optimize_network_requests(requests: List[Dict[str, Any]], max_concurrent: int = 10, connection_pool_size: int = 5, timeout: int = 30, conn_id: str = None) -> Dict[str, Any]:
    """Optimizaci√≥n de requests de red con connection pooling."""
    network_result = {
        "total_requests": len(requests),
        "successful_requests": 0,
        "failed_requests": 0,
        "total_duration": 0.0,
        "average_duration": 0.0,
        "requests_per_second": 0.0,
        "connection_pool_utilization": 0.0
    }
    
    start_time = time.time()
    
    if CONCURRENT_AVAILABLE and len(requests) > 1:
        # Procesamiento paralelo con pool limitado
        with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
            futures = []
            for request in requests:
                future = executor.submit(_execute_network_request, request, timeout)
                futures.append(future)
            
            for future in as_completed(futures):
                try:
                    result = future.result()
                    if result.get("success"):
                        network_result["successful_requests"] += 1
                    else:
                        network_result["failed_requests"] += 1
                except Exception as e:
                    logger.error("network_request_failed", error=str(e))
                    network_result["failed_requests"] += 1
    else:
        # Procesamiento secuencial
        for request in requests:
            try:
                result = _execute_network_request(request, timeout)
                if result.get("success"):
                    network_result["successful_requests"] += 1
                else:
                    network_result["failed_requests"] += 1
            except Exception as e:
                logger.error("network_request_failed", error=str(e))
                network_result["failed_requests"] += 1
    
    total_duration = time.time() - start_time
    network_result["total_duration"] = total_duration
    network_result["average_duration"] = total_duration / len(requests) if requests else 0
    network_result["requests_per_second"] = len(requests) / total_duration if total_duration > 0 else 0
    network_result["connection_pool_utilization"] = min(1.0, max_concurrent / connection_pool_size) if connection_pool_size > 0 else 0
    
    return network_result

def _execute_network_request(request: Dict[str, Any], timeout: int) -> Dict[str, Any]:
    """Ejecuta un request de red individual."""
    # Placeholder para ejecuci√≥n real de request
    return {"success": True, "response": {}}

# Advanced Data Deduplication with Similarity Scoring
def _deduplicate_with_similarity(data: List[Dict[str, Any]], similarity_threshold: float = 0.8, key_fields: List[str] = None, similarity_algorithm: str = "jaccard") -> Dict[str, Any]:
    """Deduplicaci√≥n avanzada con scoring de similitud."""
    if key_fields is None:
        key_fields = ["email", "name", "phone"]
    
    deduplication_result = {
        "original_count": len(data),
        "unique_count": 0,
        "duplicates_found": 0,
        "duplicate_groups": [],
        "similarity_scores": []
    }
    
    unique_items = []
    processed_indices = set()
    
    for i, item1 in enumerate(data):
        if i in processed_indices:
            continue
        
        duplicate_group = [i]
        max_similarity = 0.0
        
        for j, item2 in enumerate(data[i+1:], start=i+1):
            if j in processed_indices:
                continue
            
            # Calcular similitud
            similarity = _calculate_similarity(item1, item2, key_fields, similarity_algorithm)
            deduplication_result["similarity_scores"].append({
                "item1_index": i,
                "item2_index": j,
                "similarity": similarity
            })
            
            if similarity >= similarity_threshold:
                duplicate_group.append(j)
                max_similarity = max(max_similarity, similarity)
        
        if len(duplicate_group) > 1:
            # Grupo de duplicados encontrado
            deduplication_result["duplicate_groups"].append({
                "group_id": len(deduplication_result["duplicate_groups"]),
                "indices": duplicate_group,
                "max_similarity": max_similarity,
                "representative_item": data[duplicate_group[0]]
            })
            deduplication_result["duplicates_found"] += len(duplicate_group) - 1
            processed_indices.update(duplicate_group)
            # Agregar solo el primer item como √∫nico
            unique_items.append(data[duplicate_group[0]])
        else:
            unique_items.append(item1)
            processed_indices.add(i)
    
    deduplication_result["unique_count"] = len(unique_items)
    deduplication_result["unique_items"] = unique_items
    
    return deduplication_result

def _calculate_similarity(item1: Dict[str, Any], item2: Dict[str, Any], key_fields: List[str], algorithm: str = "jaccard") -> float:
    """Calcula similitud entre dos items."""
    if algorithm == "jaccard":
        # Jaccard similarity
        set1 = set(str(item1.get(f, "")).lower() for f in key_fields if item1.get(f))
        set2 = set(str(item2.get(f, "")).lower() for f in key_fields if item2.get(f))
        
        if not set1 and not set2:
            return 1.0
        if not set1 or not set2:
            return 0.0
        
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        return intersection / union if union > 0 else 0.0
    
    elif algorithm == "cosine":
        # Cosine similarity (simplificado)
        vec1 = [str(item1.get(f, "")).lower() for f in key_fields]
        vec2 = [str(item2.get(f, "")).lower() for f in key_fields]
        
        # Calcular similitud de strings
        matches = sum(1 for v1, v2 in zip(vec1, vec2) if v1 == v2)
        return matches / len(key_fields) if key_fields else 0.0
    
    return 0.0

# Advanced Caching with Predictive Prefetching
def _cache_with_prefetching(cache_key: str, fetch_func: Callable, prefetch_keys: List[str] = None, prefetch_func: Callable = None, ttl: int = 3600, conn_id: str = None) -> Tuple[Any, Dict[str, Any]]:
    """Cache con prefetching predictivo."""
    prefetch_result = {
        "cache_key": cache_key,
        "cache_hit": False,
        "prefetch_keys": prefetch_keys or [],
        "prefetched": 0,
        "prefetch_duration": 0.0
    }
    
    # Intentar obtener de cache
    cached_value = _get_from_cache(cache_key, conn_id)
    if cached_value is not None:
        prefetch_result["cache_hit"] = True
        prefetch_result["cached_value"] = cached_value
        
        # Prefetching en background
        if prefetch_keys and prefetch_func and callable(prefetch_func):
            prefetch_start = time.time()
            for prefetch_key in prefetch_keys[:5]:  # Limitar a 5 para no sobrecargar
                try:
                    prefetch_value = prefetch_func(prefetch_key)
                    _cache_with_redis(prefetch_key, prefetch_value, ttl, conn_id)
                    prefetch_result["prefetched"] += 1
                except Exception as e:
                    logger.warning("prefetch_failed", key=prefetch_key, error=str(e))
            prefetch_result["prefetch_duration"] = time.time() - prefetch_start
        
        return cached_value, prefetch_result
    
    # Cache miss - obtener y cachear
    try:
        value = fetch_func()
        _cache_with_redis(cache_key, value, ttl, conn_id)
        prefetch_result["cached_value"] = value
        return value, prefetch_result
    except Exception as e:
        logger.error("fetch_failed", key=cache_key, error=str(e))
        prefetch_result["error"] = str(e)
        return None, prefetch_result

def _get_from_cache(key: str, conn_id: str = None) -> Any:
    """Obtiene valor del cache."""
    try:
        if redis_client:
            cached = redis_client.get(key)
            if cached:
                return json.loads(cached)
        if enrichment_cache:
            return enrichment_cache.get(key)
    except Exception:
        pass
    return None

# Advanced Error Classification and Routing
def _classify_and_route_error(error: Exception, error_routing: Dict[str, Callable] = None) -> Dict[str, Any]:
    """Clasificaci√≥n avanzada de errores y routing a handlers espec√≠ficos."""
    error_classification = {
        "error_type": type(error).__name__,
        "error_message": str(error),
        "error_category": "unknown",
        "severity": "medium",
        "routed_to": None,
        "handled": False
    }
    
    # Clasificar error
    error_str = str(error).lower()
    
    if "timeout" in error_str or "timed out" in error_str:
        error_classification["error_category"] = "timeout"
        error_classification["severity"] = "medium"
    elif "connection" in error_str or "network" in error_str:
        error_classification["error_category"] = "network"
        error_classification["severity"] = "high"
    elif "rate limit" in error_str or "429" in error_str:
        error_classification["error_category"] = "rate_limit"
        error_classification["severity"] = "low"
    elif "authentication" in error_str or "401" in error_str or "403" in error_str:
        error_classification["error_category"] = "authentication"
        error_classification["severity"] = "high"
    elif "validation" in error_str or "invalid" in error_str:
        error_classification["error_category"] = "validation"
        error_classification["severity"] = "low"
    elif "not found" in error_str or "404" in error_str:
        error_classification["error_category"] = "not_found"
        error_classification["severity"] = "low"
    elif "server" in error_str or "500" in error_str or "502" in error_str or "503" in error_str:
        error_classification["error_category"] = "server_error"
        error_classification["severity"] = "high"
    else:
        error_classification["error_category"] = "unknown"
        error_classification["severity"] = "medium"
    
    # Routing a handler espec√≠fico
    if error_routing:
        handler = error_routing.get(error_classification["error_category"])
        if handler and callable(handler):
            try:
                result = handler(error)
                error_classification["routed_to"] = error_classification["error_category"]
                error_classification["handled"] = True
                error_classification["handler_result"] = result
            except Exception as e:
                logger.error("error_handler_failed", category=error_classification["error_category"], error=str(e))
    
    return error_classification

# Advanced Data Synchronization with Conflict Resolution
def _synchronize_data_with_conflicts(source_data: Dict[str, Any], target_data: Dict[str, Any], conflict_resolution: str = "last_write_wins", timestamp_field: str = "updated_at") -> Dict[str, Any]:
    """Sincronizaci√≥n avanzada de datos con resoluci√≥n de conflictos."""
    sync_result = {
        "conflicts_detected": 0,
        "conflicts_resolved": 0,
        "resolution_strategy": conflict_resolution,
        "synchronized_data": {},
        "conflicts": []
    }
    
    all_keys = set(source_data.keys()) | set(target_data.keys())
    
    for key in all_keys:
        source_value = source_data.get(key)
        target_value = target_data.get(key)
        
        if source_value != target_value:
            # Conflicto detectado
            conflict = {
                "field": key,
                "source_value": source_value,
                "target_value": target_value
            }
            sync_result["conflicts_detected"] += 1
            
            # Resolver conflicto
            resolved_value = None
            
            if conflict_resolution == "last_write_wins":
                # Usar el m√°s reciente basado en timestamp
                source_timestamp = source_data.get(timestamp_field, 0)
                target_timestamp = target_data.get(timestamp_field, 0)
                resolved_value = source_value if source_timestamp >= target_timestamp else target_value
            
            elif conflict_resolution == "source_wins":
                resolved_value = source_value
            
            elif conflict_resolution == "target_wins":
                resolved_value = target_value
            
            elif conflict_resolution == "merge":
                # Merge de valores (para dicts)
                if isinstance(source_value, dict) and isinstance(target_value, dict):
                    resolved_value = {**target_value, **source_value}
                else:
                    resolved_value = source_value
            
            elif conflict_resolution == "custom":
                # Resoluci√≥n personalizada (placeholder)
                resolved_value = source_value
            
            if resolved_value is not None:
                sync_result["synchronized_data"][key] = resolved_value
                conflict["resolved_value"] = resolved_value
                sync_result["conflicts_resolved"] += 1
                sync_result["conflicts"].append(conflict)
        else:
            # Sin conflicto
            sync_result["synchronized_data"][key] = source_value if source_value is not None else target_value
    
    return sync_result

# Advanced Performance Profiling with Bottleneck Detection
def _profile_with_bottleneck_detection(func: Callable, *args, **kwargs) -> Dict[str, Any]:
    """Profiling avanzado con detecci√≥n de cuellos de botella."""
    profile_result = {
        "function_name": func.__name__,
        "total_duration": 0.0,
        "cpu_time": 0.0,
        "memory_usage_mb": 0.0,
        "bottlenecks": [],
        "recommendations": []
    }
    
    try:
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        
        # Medir antes
        cpu_before = process.cpu_times().user + process.cpu_times().system
        memory_before = process.memory_info().rss / (1024 * 1024)
        
        # Ejecutar funci√≥n
        start_time = time.time()
        result = func(*args, **kwargs)
        total_duration = time.time() - start_time
        
        # Medir despu√©s
        cpu_after = process.cpu_times().user + process.cpu_times().system
        memory_after = process.memory_info().rss / (1024 * 1024)
        
        profile_result["total_duration"] = total_duration
        profile_result["cpu_time"] = cpu_after - cpu_before
        profile_result["memory_usage_mb"] = memory_after - memory_before
        
        # Detectar cuellos de botella
        if total_duration > 5.0:
            profile_result["bottlenecks"].append({
                "type": "slow_execution",
                "severity": "high",
                "message": f"Function took {total_duration:.2f}s to execute"
            })
            profile_result["recommendations"].append("Consider optimizing function or using caching")
        
        if profile_result["cpu_time"] / total_duration > 0.8:
            profile_result["bottlenecks"].append({
                "type": "cpu_bound",
                "severity": "medium",
                "message": "Function is CPU-bound"
            })
            profile_result["recommendations"].append("Consider parallel processing or async execution")
        
        if memory_after - memory_before > 100:
            profile_result["bottlenecks"].append({
                "type": "memory_intensive",
                "severity": "medium",
                "message": f"Function used {memory_after - memory_before:.2f}MB of memory"
            })
            profile_result["recommendations"].append("Consider memory optimization or streaming")
        
        profile_result["result"] = result
    except Exception as e:
        logger.warning("profiling_failed", error=str(e))
        profile_result["error"] = str(e)
        # Ejecutar funci√≥n sin profiling
        profile_result["result"] = func(*args, **kwargs)
    
    return profile_result

# M√©tricas Prometheus (si est√° disponible)
try:
    from prometheus_client import Counter, Histogram, Gauge, Summary
    
    # M√©tricas de leads
    leads_qualified = Counter(
        'leads_qualified_total',
        'Total qualified leads',
        ['priority']
    )
    leads_high_priority = Counter(
        'leads_high_priority_total',
        'Total high priority leads'
    )
    leads_medium_priority = Counter(
        'leads_medium_priority_total',
        'Total medium priority leads'
    )
    leads_low_priority = Counter(
        'leads_low_priority_total',
        'Total low priority leads'
    )
    
    lead_metrics = {
        "captured": Counter("lead_capture_total", "Total leads captured", ["source", "status"]),
        "validated": Counter("lead_validation_total", "Total leads validated", ["result"]),
        "spam_detected": Counter("lead_spam_detected_total", "Total spam leads detected"),
        "rate_limited": Counter("lead_rate_limited_total", "Total rate limited leads", ["reason"]),
        "enriched": Counter("lead_enriched_total", "Total leads enriched", ["source"]),
        "scored": Histogram("lead_score", "Lead score distribution", buckets=[0, 20, 40, 60, 80, 100]),
        "onboarding_triggered": Counter("lead_onboarding_triggered_total", "Total onboarding triggers"),
        "contract_generated": Counter("lead_contract_generated_total", "Total contracts generated"),
        "crm_synced": Counter("lead_crm_synced_total", "Total CRM syncs", ["crm_type", "status"]),
        "processing_time": Summary("lead_processing_seconds", "Lead processing time"),
        "dlq_size": Gauge("lead_dlq_size", "Dead letter queue size"),
        "qualified": leads_qualified,
        "high_priority": leads_high_priority,
        "medium_priority": leads_medium_priority,
        "low_priority": leads_low_priority,
    }
    
    PROMETHEUS_AVAILABLE = True
    
    # Contadores de m√©tricas
    leads_processed = Counter(
        'leads_processed_total',
        'Total leads processed',
        ['status', 'source']
    )
    leads_spam_detected = Counter(
        'leads_spam_detected_total',
        'Total spam leads detected'
    )
    leads_enriched = Counter(
        'leads_enriched_total',
        'Total leads enriched',
        ['source']
    )
    crm_sync_duration = Histogram(
        'crm_sync_duration_seconds',
        'CRM sync duration in seconds',
        ['crm_type', 'status']
    )
    lead_score_distribution = Histogram(
        'lead_score_distribution',
        'Lead score distribution',
        buckets=[0, 20, 40, 60, 80, 100]
    )
    active_leads_gauge = Gauge(
        'active_leads_count',
        'Number of active leads in pipeline'
    )
except ImportError:
    # M√©tricas dummy si prometheus no est√° disponible
    class DummyMetric:
        def inc(self, *args, **kwargs): pass
        def observe(self, *args, **kwargs): pass
        def set(self, *args, **kwargs): pass
    
    leads_processed = DummyMetric()
    leads_spam_detected = DummyMetric()
    leads_enriched = DummyMetric()
    crm_sync_duration = DummyMetric()
    leads_qualified = DummyMetric()
    leads_high_priority = DummyMetric()
    leads_medium_priority = DummyMetric()
    leads_low_priority = DummyMetric()
    lead_score_distribution = DummyMetric()
    active_leads_gauge = DummyMetric()



@dag(
    dag_id="web_lead_capture",
    start_date=pendulum.datetime(2025, 1, 1, tz="UTC"),
    schedule=None,  # Triggered manually or via webhook
    catchup=False,
    default_args={
        "owner": "sales",
        "retries": 2,
        "retry_delay": timedelta(minutes=5),
        "depends_on_past": False,
    },
    doc_md="""
    ### Captura Autom√°tica de Leads desde Web - Enterprise Edition
    
    Sistema enterprise que captura leads autom√°ticamente desde formularios web, asigna vendedores,
    actualiza estados y programa seguimientos. Incluye integraci√≥n autom√°tica con onboarding,
    generaci√≥n de contratos, y caracter√≠sticas avanzadas de nivel enterprise.
    
    **Funcionalidades Core:**
    - ‚úÖ Captura de leads desde formularios web (v√≠a webhook o API)
    - ‚úÖ Validaci√≥n y enriquecimiento avanzado de datos
    - ‚úÖ Scoring autom√°tico mejorado con ML
    - ‚úÖ Detecci√≥n inteligente de duplicados
    - ‚úÖ Asignaci√≥n inteligente a vendedores
    - ‚úÖ Sincronizaci√≥n autom√°tica con Salesforce/Pipedrive
    - ‚úÖ Programaci√≥n de seguimientos autom√°ticos
    - ‚úÖ Integraci√≥n autom√°tica con onboarding para leads calificados
    - ‚úÖ Generaci√≥n autom√°tica de contratos para leads convertidos
    - ‚úÖ Actualizaci√≥n de estados en tiempo real
    - ‚úÖ Notificaciones mejoradas
    - ‚úÖ **Automatizaci√≥n de Nurturing**: Secuencias personalizadas basadas en segmento y engagement
    - ‚úÖ **Monitoreo de Calidad de Datos**: M√©tricas de completitud, validez y enriquecimiento
    - ‚úÖ **Scoring Predictivo con ML**: Ajuste de score basado en datos hist√≥ricos y factores predictivos
    - ‚úÖ **Routing Inteligente de Leads**: Asignaci√≥n autom√°tica basada en especializaci√≥n, carga y performance
    - ‚úÖ **An√°lisis de Intenci√≥n de Compra**: Scoring de intenci√≥n basado en comportamiento y se√±ales
    
    **Caracter√≠sticas Enterprise:**
    - üîí **Distributed Locking**: Previene ejecuciones concurrentes usando Redis/PostgreSQL
    - üîÑ **Idempotency Control**: Garantiza procesamiento √∫nico de leads duplicados
    - üíæ **Checkpointing**: Permite reanudar procesamiento desde puntos de control
    - üö© **Feature Flags**: Control din√°mico de caracter√≠sticas v√≠a Airflow Variables
    - üìä **Adaptive Batch Processing**: Ajuste autom√°tico de tama√±os de batch seg√∫n performance
    - üìà **Historical Metrics Comparison**: Comparaci√≥n de m√©tricas con ejecuciones anteriores
    - üìâ **Churn Risk Calculation**: C√°lculo de riesgo de p√©rdida de leads con estrategias de mitigaci√≥n
    - üí∞ **Lifetime Value (LTV) Estimation**: Estimaci√≥n de valor de vida del lead (optimista, realista, pesimista)
    - üìã **Executive Reports**: Reportes ejecutivos con m√©tricas clave y recomendaciones priorizadas
    - ‚òÅÔ∏è **S3 Export**: Exportaci√≥n autom√°tica de m√©tricas y reportes a S3 para an√°lisis a largo plazo
    - üîç **Cohort Analysis**: An√°lisis de grupos de leads basado en fecha de captura
    - üéØ **Multi-channel Attribution**: Atribuci√≥n de conversiones a m√∫ltiples canales
    - ‚è∞ **Contact Timing Optimization**: Optimizaci√≥n de tiempos de contacto basada en datos hist√≥ricos
    - üíµ **Cost Tracking**: Rastreo de costos de APIs externas para optimizaci√≥n de gastos
    - ‚ö° **Jitter/Delays**: Prevenci√≥n de thundering herd con delays aleatorios
    - üìä **Progress Tracking**: Logging peri√≥dico de progreso de operaciones
    - üö® **Predictive Alerts**: Alertas predictivas basadas en tendencias y patrones
    - ü§ñ **ML Scoring Integration**: Integraci√≥n con modelos ML externos para scoring avanzado
    - üìä **Data Quality Scoring**: Sistema de scoring de calidad de datos del lead
    - üîÑ **Workflow Automation**: Disparo autom√°tico de workflows basados en eventos
    - üèÜ **An√°lisis de Competencia Avanzado**: Detecci√≥n de competidores, riesgo competitivo y win probability
    - üéØ **Recomendaciones de Productos Inteligentes**: Recomendaciones autom√°ticas basadas en empresa, industria y necesidades
    - üí≠ **An√°lisis de Sentimiento Avanzado con NLP**: An√°lisis profundo de sentimiento y emociones
    - üìà **Scoring Din√°mico Basado en Comportamiento**: Ajuste de score en tiempo real seg√∫n comportamiento
    - üì± **An√°lisis de Redes Sociales**: Detecci√≥n de presencia y c√°lculo de influence score
    - ‚è±Ô∏è **Predicci√≥n de Tiempo de Respuesta √ìptimo**: Optimizaci√≥n de tiempos para maximizar conversi√≥n
    - üéñÔ∏è **Priorizaci√≥n Inteligente Multi-Factor**: Sistema de priorizaci√≥n basado en m√∫ltiples factores
    - üòä **An√°lisis de Satisfacci√≥n Avanzado**: Scoring de satisfacci√≥n con estimaci√≥n de NPS
    - üìä **An√°lisis de Mercado y Tendencias**: Evaluaci√≥n de oportunidades de mercado y crecimiento
    
    **Par√°metros:**
    - `postgres_conn_id`: Connection ID para Postgres
    - `crm_type`: Tipo de CRM ('salesforce' o 'pipedrive')
    - `crm_config`: Configuraci√≥n del CRM (JSON string)
    - `auto_assign_enabled`: Auto-asignar leads a vendedores (default: true)
    - `auto_sync_crm`: Sincronizar autom√°ticamente con CRM (default: true)
    - `create_followup_tasks`: Crear tareas de seguimiento (default: true)
    - `auto_trigger_onboarding`: Disparar onboarding autom√°tico para leads calificados (default: true)
    - `auto_generate_contract`: Generar contrato autom√°tico tras onboarding (default: true)
    - `lead_score_threshold`: Score m√≠nimo para auto-onboarding (default: 60)
    - `lead_data`: Datos del lead en JSON (cuando se invoca desde webhook)
    - `dry_run`: Modo de prueba sin cambios reales (default: false)
    """,
    params={
        "postgres_conn_id": Param("postgres_default", type="string", minLength=1),
        "crm_type": Param("salesforce", type="string", enum=["salesforce", "pipedrive"]),
        "crm_config": Param("{}", type="string"),
        "auto_assign_enabled": Param(True, type="boolean"),
        "auto_sync_crm": Param(True, type="boolean"),
        "create_followup_tasks": Param(True, type="boolean"),
        "auto_trigger_onboarding": Param(True, type="boolean", description="Disparar onboarding autom√°tico para leads calificados"),
        "auto_generate_contract": Param(True, type="boolean", description="Generar contrato autom√°tico tras onboarding"),
        "lead_score_threshold": Param(60, type="integer", description="Score m√≠nimo para auto-onboarding"),
        "lead_data": Param("{}", type="string"),
        "dry_run": Param(False, type="boolean"),
    },
    default_args={
        "owner": "sales",
        "retries": 2,
        "retry_delay": timedelta(minutes=5),
        "depends_on_past": False,
        "on_failure_callback": on_task_failure,
    },
    tags=["sales", "leads", "crm", "automation", "webhook", "onboarding", "contracts"],
    on_success_callback=lambda context: notify_slack(":white_check_mark: web_lead_capture DAG succeeded"),
    on_failure_callback=lambda context: notify_slack(":x: web_lead_capture DAG failed"),
)
def web_lead_capture() -> None:
    """
    DAG para captura autom√°tica de leads desde web.
    Versi√≥n mejorada con Pydantic, structlog, circuit breakers y tracing.
    
    Pipeline de procesamiento:
    1. Rate limiting - Prevenir spam y abuso
    2. Detecci√≥n de spam - Heur√≠sticas m√∫ltiples
    3. Validaci√≥n de dominio email - Verificaci√≥n DNS (MX, A, AAAA)
    4. Validaci√≥n de datos - Pydantic v2
    5. Detecci√≥n de duplicados - Evitar procesamiento m√∫ltiple
    6. Enriquecimiento - Clearbit, Hunter.io
    7. Scoring - C√°lculo autom√°tico con factores m√∫ltiples
    8. Guardado en BD - PostgreSQL con upsert
    9. Asignaci√≥n - Asignaci√≥n inteligente a vendedores
    10. Sincronizaci√≥n CRM - Salesforce/Pipedrive
    11. Follow-up tasks - Tareas de seguimiento autom√°ticas
    12. Onboarding - Trigger autom√°tico para leads calificados
    
    Configuraci√≥n v√≠a variables de entorno:
    - LEAD_CAPTURE_TIMEOUT: Timeout general (default: 30s)
    - DNS_TIMEOUT: Timeout para consultas DNS (default: 3s)
    - ENRICHMENT_TIMEOUT: Timeout para APIs de enriquecimiento (default: 2s)
    - RATE_LIMIT_EMAIL_MAX: M√°ximo leads por email (default: 5)
    - RATE_LIMIT_EMAIL_PERIOD: Per√≠odo en segundos (default: 3600)
    - SPAM_SCORE_THRESHOLD: Umbral de spam (default: 50)
    - HIGH_VALUE_SCORE_THRESHOLD: Umbral de alto valor (default: 70)
    - QUALIFIED_SCORE_THRESHOLD: Umbral de calificaci√≥n (default: 40)
    """
    
    # ============================================================================
    # HEALTH CHECK TASK
    # ============================================================================
    
    @task(task_id="health_check")
    def health_check() -> Dict[str, Any]:
        """
        Health check del sistema antes de procesar leads.
        Verifica conectividad con servicios cr√≠ticos.
        """
        health_status = {
            "status": "healthy",
            "checks": {},
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # Verificar PostgreSQL
        try:
            ctx = get_current_context()
            params = ctx["params"]
            conn_id = str(params["postgres_conn_id"])
            hook = PostgresHook(postgres_conn_id=conn_id)
            with hook.get_conn() as conn:
                with conn.cursor() as cur:
                    cur.execute("SELECT 1")
                    health_status["checks"]["postgres"] = "ok"
        except Exception as e:
            health_status["checks"]["postgres"] = f"error: {str(e)}"
            health_status["status"] = "degraded"
            logger.warning("health_check_postgres_failed", error=str(e))
        
        # Verificar Redis (si est√° configurado)
        if redis_client:
            try:
                redis_client.ping()
                health_status["checks"]["redis"] = "ok"
            except Exception as e:
                health_status["checks"]["redis"] = f"error: {str(e)}"
                health_status["status"] = "degraded"
                logger.warning("health_check_redis_failed", error=str(e))
        
        # Verificar circuit breakers
        health_status["checks"]["circuit_breakers"] = {
            "crm": "closed" if crm_circuit_breaker.current_state == "closed" else "open",
            "onboarding": "closed" if onboarding_circuit_breaker.current_state == "closed" else "open",
            "enrichment": "closed" if enrichment_circuit_breaker.current_state == "closed" else "open"
        }
        
        # Verificar cache
        health_status["checks"]["cache"] = {
            "size": len(enrichment_cache),
            "maxsize": enrichment_cache.maxsize
        }
        
        logger.info("health_check_completed", status=health_status["status"], checks=health_status["checks"])
        return health_status
    
    @task(task_id="analyze_lead_quality")
    def analyze_lead_quality(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        An√°lisis avanzado de calidad del lead usando m√∫ltiples factores.
        Calcula un score de calidad y predice probabilidad de conversi√≥n.
        """
        span = None
        if tracer:
            span = tracer.start_span("analyze_lead_quality")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            quality_score = 0
            quality_factors = {}
            conversion_probability = 0.0
            
            # Factor 1: Completitud de datos (max 25 puntos)
            completeness = 0
            if lead_data.get("first_name") and lead_data.get("last_name"):
                completeness += 10
                quality_factors["has_full_name"] = True
            if lead_data.get("email"):
                completeness += 5
                quality_factors["has_email"] = True
            if lead_data.get("phone"):
                completeness += 5
                quality_factors["has_phone"] = True
            if lead_data.get("company"):
                completeness += 5
                quality_factors["has_company"] = True
            quality_score += completeness
            quality_factors["completeness_score"] = completeness
            
            # Factor 2: Validaci√≥n de datos (max 30 puntos)
            validation_score = 0
            if lead_data.get("email_validation", {}).get("valid_mx"):
                validation_score += 15
                quality_factors["email_validated"] = True
            if lead_data.get("company_validation", {}).get("validated"):
                validation_score += 10
                quality_factors["company_validated"] = True
            if lead_data.get("enriched_data", {}).get("hunter", {}).get("result") == "deliverable":
                validation_score += 5
                quality_factors["email_deliverable"] = True
            quality_score += validation_score
            quality_factors["validation_score"] = validation_score
            
            # Factor 3: Enriquecimiento (max 20 puntos)
            enrichment_score = 0
            if lead_data.get("is_enriched"):
                enrichment_score += 10
                enriched_data = lead_data.get("enriched_data", {})
                if enriched_data.get("clearbit", {}).get("company"):
                    enrichment_score += 10
                    quality_factors["has_company_data"] = True
            quality_score += enrichment_score
            quality_factors["enrichment_score"] = enrichment_score
            
            # Factor 4: Se√±ales de calidad (max 25 puntos)
            signal_score = 0
            if lead_data.get("utm_campaign"):
                signal_score += 5
                quality_factors["has_campaign"] = True
            if lead_data.get("utm_source") in ["organic", "direct", "referral"]:
                signal_score += 10
                quality_factors["high_quality_source"] = True
            elif lead_data.get("utm_source"):
                signal_score += 5
                quality_factors["has_source"] = True
            
            # Bonus por mensaje personalizado
            message = lead_data.get("message", "")
            if message and len(message) > 20:
                signal_score += 10
                quality_factors["has_detailed_message"] = True
            quality_score += signal_score
            quality_factors["signal_score"] = signal_score
            
            # Penalizaciones
            if lead_data.get("is_duplicate"):
                quality_score -= 15
                quality_factors["duplicate_penalty"] = True
            if lead_data.get("spam_score", 0) > 0:
                quality_score -= min(lead_data.get("spam_score", 0) // 3, 20)
                quality_factors["spam_penalty"] = True
            
            # Normalizar a 0-100
            quality_score = max(0, min(quality_score, 100))
            
            # Calcular probabilidad de conversi√≥n (heur√≠stica simple)
            # Basado en quality_score y score del lead
            lead_score = lead_data.get("score", 0)
            conversion_probability = min(0.95, (quality_score * 0.4 + lead_score * 0.6) / 100)
            
            quality_analysis = {
                "quality_score": quality_score,
                "conversion_probability": round(conversion_probability, 3),
                "factors": quality_factors,
                "grade": (
                    "A" if quality_score >= 80 else
                    "B" if quality_score >= 60 else
                    "C" if quality_score >= 40 else
                    "D" if quality_score >= 20 else "F"
                )
            }
            
            lead_data["quality_analysis"] = quality_analysis
            
            if span:
                span.set_attribute("quality.score", quality_score)
                span.set_attribute("quality.grade", quality_analysis["grade"])
                span.set_attribute("conversion.probability", conversion_probability)
                span.end()
            
            logger.info(
                "lead_quality_analyzed",
                email=lead_data.get("email"),
                quality_score=quality_score,
                grade=quality_analysis["grade"],
                conversion_probability=conversion_probability
            )
            
            return lead_data
            
        except Exception as e:
            logger.error("quality_analysis_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            # Continuar sin an√°lisis de calidad si falla
            return lead_data
    
    @task(task_id="segment_lead")
    def segment_lead(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """Segmenta el lead autom√°ticamente basado en m√∫ltiples factores."""
        span = None
        if tracer:
            span = tracer.start_span("segment_lead")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            score = lead_data.get("score", 0)
            priority = lead_data.get("priority", "low")
            quality_grade = lead_data.get("quality_analysis", {}).get("grade", "F")
            source = lead_data.get("source", "unknown")
            company = lead_data.get("company")
            
            segments = []
            
            if score >= 75:
                segments.append("high_value")
            elif score >= 50:
                segments.append("medium_value")
            else:
                segments.append("low_value")
            
            if quality_grade in ["A", "B"]:
                segments.append("high_quality")
            elif quality_grade == "C":
                segments.append("medium_quality")
            else:
                segments.append("low_quality")
            
            if source in ["organic", "direct", "referral"]:
                segments.append("organic_source")
            elif source in ["paid", "social"]:
                segments.append("paid_source")
            
            if company:
                enriched_data = lead_data.get("enriched_data", {})
                clearbit_company = enriched_data.get("clearbit", {}).get("company", {})
                employees = clearbit_company.get("metrics", {}).get("employees", 0)
                
                if employees > 1000:
                    segments.append("enterprise")
                elif employees > 100:
                    segments.append("mid_market")
                else:
                    segments.append("small_business")
            else:
                segments.append("no_company")
            
            segments.append(f"priority_{priority}")
            primary_segment = segments[0] if segments else "unknown"
            
            lead_data["segmentation"] = {
                "segments": segments,
                "primary_segment": primary_segment,
                "segment_count": len(segments),
            }
            
            if span:
                span.set_attribute("segmentation.primary", primary_segment)
                span.end()
            
            logger.info("lead_segmented", email=lead_data.get("email"), primary_segment=primary_segment)
            return lead_data
            
        except Exception as e:
            logger.error("segmentation_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            return lead_data
    
    @task(task_id="analyze_sentiment")
    def analyze_sentiment(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analiza el sentimiento del mensaje del lead usando heur√≠sticas."""
        span = None
        if tracer:
            span = tracer.start_span("analyze_sentiment")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            message = lead_data.get("message", "")
            if not message:
                return lead_data
            
            positive_words = ["interesado", "interesante", "me gusta", "excelente", "genial", "perfecto", "bueno", "buena", "s√≠", "yes", "definitivamente", "quiero", "necesito", "urgente", "importante", "prioridad", "gracias", "thanks", "espero", "deseo", "me encanta"]
            negative_words = ["no", "not", "nunca", "never", "mala", "bad", "terrible", "problema", "problem", "error", "queja", "complaint", "cancelar", "cancel", "devoluci√≥n", "refund", "insatisfecho"]
            neutral_words = ["informaci√≥n", "info", "informaci√≥n", "consulta", "pregunta", "question", "m√°s", "more", "detalles", "details"]
            
            message_lower = message.lower()
            positive_count = sum(1 for word in positive_words if word in message_lower)
            negative_count = sum(1 for word in negative_words if word in message_lower)
            neutral_count = sum(1 for word in neutral_words if word in message_lower)
            
            total_words = positive_count + negative_count + neutral_count
            sentiment_score = (positive_count - negative_count) / max(total_words, 1) if total_words > 0 else 0.0
            sentiment_score = max(-1.0, min(1.0, sentiment_score))
            
            if sentiment_score > 0.3:
                sentiment = "positive"
            elif sentiment_score < -0.3:
                sentiment = "negative"
            else:
                sentiment = "neutral"
            
            confidence = min(1.0, total_words / 5.0) if total_words > 0 else 0.5
            
            lead_data["sentiment_analysis"] = {
                "sentiment": sentiment,
                "sentiment_score": round(sentiment_score, 3),
                "confidence": round(confidence, 2),
                "positive_count": positive_count,
                "negative_count": negative_count,
                "neutral_count": neutral_count,
                "message_length": len(message)
            }
            
            if sentiment == "positive" and confidence > 0.7:
                current_score = lead_data.get("score", 0)
                lead_data["score"] = min(100, current_score + 5)
            elif sentiment == "negative" and confidence > 0.7:
                current_score = lead_data.get("score", 0)
                lead_data["score"] = max(0, current_score - 5)
            
            if span:
                span.set_attribute("sentiment.value", sentiment)
                span.set_attribute("sentiment.score", sentiment_score)
                span.end()
            
            logger.info("sentiment_analyzed", email=lead_data.get("email"), sentiment=sentiment, sentiment_score=sentiment_score)
            return lead_data
            
        except Exception as e:
            logger.error("sentiment_analysis_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            return lead_data
    
    @task(task_id="calculate_lead_velocity")
    def calculate_lead_velocity(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """Calcula la velocidad del lead (tiempo estimado de respuesta y cierre)."""
        span = None
        if tracer:
            span = tracer.start_span("calculate_lead_velocity")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            velocity_score = 0
            velocity_factors = {}
            
            priority = lead_data.get("priority", "low")
            priority_scores = {"high": 80, "medium": 50, "low": 20, "very_low": 10}
            priority_score = priority_scores.get(priority, 20)
            velocity_score += priority_score * 0.3
            velocity_factors["priority_contribution"] = priority_score * 0.3
            
            sentiment = lead_data.get("sentiment_analysis", {}).get("sentiment", "neutral")
            sentiment_scores = {"positive": 70, "neutral": 40, "negative": 20}
            sentiment_score = sentiment_scores.get(sentiment, 40)
            velocity_score += sentiment_score * 0.2
            velocity_factors["sentiment_contribution"] = sentiment_score * 0.2
            
            lead_score = lead_data.get("score", 0)
            velocity_score += lead_score * 0.3
            velocity_factors["lead_score_contribution"] = lead_score * 0.3
            
            message = lead_data.get("message", "").lower()
            urgency_keywords = ["urgente", "urgent", "r√°pido", "quick", "inmediato", "asap", "pronto"]
            has_urgency = any(keyword in message for keyword in urgency_keywords)
            urgency_score = 60 if has_urgency else 30
            velocity_score += urgency_score * 0.2
            velocity_factors["urgency_contribution"] = urgency_score * 0.2
            
            velocity_score = max(0, min(velocity_score, 100))
            
            if velocity_score >= 70:
                estimated_response_hours = 1
                estimated_close_days = 7
                velocity_category = "fast"
            elif velocity_score >= 50:
                estimated_response_hours = 4
                estimated_close_days = 14
                velocity_category = "medium"
            elif velocity_score >= 30:
                estimated_response_hours = 24
                estimated_close_days = 30
                velocity_category = "slow"
            else:
                estimated_response_hours = 48
                estimated_close_days = 60
                velocity_category = "very_slow"
            
            lead_data["velocity_analysis"] = {
                "velocity_score": round(velocity_score, 2),
                "velocity_category": velocity_category,
                "estimated_response_hours": estimated_response_hours,
                "estimated_close_days": estimated_close_days,
                "factors": velocity_factors,
                "has_urgency": has_urgency
            }
            
            if span:
                span.set_attribute("velocity.score", velocity_score)
                span.set_attribute("velocity.category", velocity_category)
                span.end()
            
            logger.info("lead_velocity_calculated", email=lead_data.get("email"), velocity_score=velocity_score, velocity_category=velocity_category)
            return lead_data
            
        except Exception as e:
            logger.error("velocity_calculation_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            return lead_data
    
    @task(task_id="detect_anomalies")
    def detect_anomalies(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """Detecta anomal√≠as y patrones inusuales en el lead."""
        span = None
        if tracer:
            span = tracer.start_span("detect_anomalies")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            anomalies = []
            anomaly_score = 0
            
            email = lead_data.get("email", "")
            if email and "@" in email:
                domain = email.split("@")[1]
                temp_domains = ["10minutemail.com", "mailinator.com", "guerrillamail.com", "tempmail.com", "throwaway.email", "getnada.com"]
                if any(temp in domain.lower() for temp in temp_domains):
                    anomalies.append({"type": "temporary_email", "severity": "medium", "description": "Email domain appears to be temporary", "domain": domain})
                    anomaly_score += 30
            
            score = lead_data.get("score", 0)
            has_complete_data = (lead_data.get("first_name") and lead_data.get("last_name") and lead_data.get("email") and lead_data.get("phone") and lead_data.get("company"))
            if score >= 80 and not has_complete_data:
                anomalies.append({"type": "high_score_without_data", "severity": "low", "description": "High score but missing complete data", "score": score, "missing_fields": [f for f in ["first_name", "last_name", "phone", "company"] if not lead_data.get(f)]})
                anomaly_score += 10
            
            first_name = lead_data.get("first_name", "").lower()
            last_name = lead_data.get("last_name", "").lower()
            suspicious_patterns = [(len(first_name) == 1 and len(last_name) == 1), (first_name == last_name), (first_name.isdigit() or last_name.isdigit())]
            if any(suspicious_patterns):
                anomalies.append({"type": "suspicious_name_pattern", "severity": "medium", "description": "Name pattern appears suspicious", "first_name": lead_data.get("first_name"), "last_name": lead_data.get("last_name")})
                anomaly_score += 20
            
            message = lead_data.get("message", "")
            if message:
                if len(message) > 5000:
                    anomalies.append({"type": "extremely_long_message", "severity": "low", "description": "Message is extremely long", "length": len(message)})
                    anomaly_score += 5
                url_count = len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', message))
                if url_count > 3:
                    anomalies.append({"type": "multiple_urls", "severity": "medium", "description": "Message contains multiple URLs", "url_count": url_count})
                    anomaly_score += 15
            
            severity = "none"
            if anomaly_score >= 50:
                severity = "high"
            elif anomaly_score >= 30:
                severity = "medium"
            elif anomaly_score > 0:
                severity = "low"
            
            lead_data["anomaly_detection"] = {"anomalies": anomalies, "anomaly_score": anomaly_score, "severity": severity, "anomaly_count": len(anomalies)}
            
            if span:
                span.set_attribute("anomaly.score", anomaly_score)
                span.set_attribute("anomaly.severity", severity)
                span.end()
            
            if anomalies:
                logger.warning("anomalies_detected", email=lead_data.get("email"), anomaly_count=len(anomalies), severity=severity)
            
            return lead_data
            
        except Exception as e:
            logger.error("anomaly_detection_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            return lead_data
    
    @task(task_id="predict_conversion")
    def predict_conversion(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """Predice probabilidad de conversi√≥n usando m√∫ltiples factores."""
        span = None
        if tracer:
            span = tracer.start_span("predict_conversion")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            factors = {}
            conversion_score = 0
            
            lead_score = lead_data.get("score", 0)
            conversion_score += lead_score * 0.4
            factors["lead_score_contribution"] = lead_score * 0.4
            
            quality_score = lead_data.get("quality_analysis", {}).get("quality_score", 0)
            conversion_score += quality_score * 0.3
            factors["quality_score_contribution"] = quality_score * 0.3
            
            primary_segment = lead_data.get("segmentation", {}).get("primary_segment", "unknown")
            segment_scores = {"high_value": 80, "enterprise": 75, "organic_source": 70, "medium_value": 50, "mid_market": 45, "paid_source": 40, "low_value": 20, "small_business": 25, "no_company": 15}
            segment_score = segment_scores.get(primary_segment, 30)
            conversion_score += segment_score * 0.2
            factors["segment_contribution"] = segment_score * 0.2
            
            if lead_data.get("is_enriched"):
                enriched_data = lead_data.get("enriched_data", {})
                if enriched_data.get("clearbit", {}).get("company"):
                    conversion_score += 10
                    factors["enrichment_contribution"] = 10
                else:
                    factors["enrichment_contribution"] = 0
            else:
                factors["enrichment_contribution"] = 0
            
            if lead_data.get("is_duplicate"):
                conversion_score -= 15
                factors["duplicate_penalty"] = -15
            
            if lead_data.get("spam_score", 0) > 0:
                conversion_score -= min(lead_data.get("spam_score", 0) // 3, 20)
                factors["spam_penalty"] = -min(lead_data.get("spam_score", 0) // 3, 20)
            
            if lead_data.get("anomaly_detection", {}).get("severity") == "high":
                conversion_score -= 25
                factors["anomaly_penalty"] = -25
            elif lead_data.get("anomaly_detection", {}).get("severity") == "medium":
                conversion_score -= 10
                factors["anomaly_penalty"] = -10
            
            conversion_score = max(0, min(conversion_score, 100))
            conversion_probability = conversion_score / 100
            
            if conversion_probability >= 0.7:
                conversion_category = "high"
                days_to_convert = 7
            elif conversion_probability >= 0.5:
                conversion_category = "medium"
                days_to_convert = 14
            elif conversion_probability >= 0.3:
                conversion_category = "low"
                days_to_convert = 30
            else:
                conversion_category = "very_low"
                days_to_convert = 60
            
            lead_data["conversion_prediction"] = {
                "conversion_score": round(conversion_score, 2),
                "conversion_probability": round(conversion_probability, 3),
                "conversion_category": conversion_category,
                "estimated_days_to_convert": days_to_convert,
                "factors": factors,
                "confidence": "high" if abs(conversion_probability - lead_data.get("quality_analysis", {}).get("conversion_probability", 0)) < 0.1 else "medium"
            }
            
            if span:
                span.set_attribute("conversion.score", conversion_score)
                span.set_attribute("conversion.probability", conversion_probability)
                span.set_attribute("conversion.category", conversion_category)
                span.end()
            
            logger.info("conversion_predicted", email=lead_data.get("email"), conversion_probability=conversion_probability, conversion_category=conversion_category)
            return lead_data
            
        except Exception as e:
            logger.error("conversion_prediction_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            return lead_data
    
    @task(task_id="generate_recommendations")
    def generate_recommendations(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """Genera recomendaciones de acciones basadas en el an√°lisis del lead."""
        span = None
        if tracer:
            span = tracer.start_span("generate_recommendations")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            recommendations = []
            score = lead_data.get("score", 0)
            priority = lead_data.get("priority", "low")
            quality_grade = lead_data.get("quality_analysis", {}).get("grade", "F")
            primary_segment = lead_data.get("segmentation", {}).get("primary_segment", "unknown")
            
            if score >= 75:
                recommendations.append({"type": "action", "priority": "high", "action": "contact_immediately", "reason": "High score lead", "estimated_value": "high"})
            elif score >= 50:
                recommendations.append({"type": "action", "priority": "medium", "action": "contact_within_24h", "reason": "Medium score lead", "estimated_value": "medium"})
            else:
                recommendations.append({"type": "action", "priority": "low", "action": "nurture_sequence", "reason": "Low score lead", "estimated_value": "low"})
            
            if quality_grade in ["A", "B"]:
                recommendations.append({"type": "enrichment", "priority": "medium", "action": "request_additional_info", "reason": "High quality lead, gather more details"})
            
            if primary_segment == "enterprise":
                recommendations.append({"type": "strategy", "priority": "high", "action": "assign_to_enterprise_rep", "reason": "Enterprise segment requires specialized handling"})
            elif primary_segment == "organic_source":
                recommendations.append({"type": "strategy", "priority": "medium", "action": "prioritize_organic", "reason": "Organic leads have higher conversion rates"})
            
            if not lead_data.get("phone"):
                recommendations.append({"type": "data_collection", "priority": "medium", "action": "request_phone_number", "reason": "Phone number missing, important for contact"})
            
            if not lead_data.get("company"):
                recommendations.append({"type": "data_collection", "priority": "low", "action": "request_company_info", "reason": "Company information missing"})
            
            if not lead_data.get("is_enriched"):
                recommendations.append({"type": "enrichment", "priority": "low", "action": "enrich_with_external_apis", "reason": "Lead data could be enriched"})
            
            priority_order = {"high": 0, "medium": 1, "low": 2}
            recommendations.sort(key=lambda x: priority_order.get(x.get("priority", "low"), 2))
            
            lead_data["recommendations"] = recommendations
            
            if span:
                span.set_attribute("recommendations.count", len(recommendations))
                span.end()
            
            logger.info("recommendations_generated", email=lead_data.get("email"), recommendations_count=len(recommendations))
            return lead_data
            
        except Exception as e:
            logger.error("recommendations_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            return lead_data
    
    @task(task_id="generate_product_descriptions")
    def generate_product_descriptions(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Genera descripciones de productos optimizadas para e-commerce basadas en el lead.
        Esta tarea es opcional y se puede habilitar cuando se necesite generar contenido de productos.
        """
        span = None
        if tracer:
            span = tracer.start_span("generate_product_descriptions")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            # Obtener productos recomendados del an√°lisis de fit
            product_fit = lead_data.get("product_fit_analysis", {})
            recommended_products = product_fit.get("recommended_products", [])
            
            # Si no hay productos recomendados, usar productos por defecto basados en el segmento
            if not recommended_products:
                primary_segment = lead_data.get("segmentation", {}).get("primary_segment", "unknown")
                if primary_segment == "enterprise":
                    recommended_products = ["Enterprise Plan", "API Access", "Premium Support"]
                elif primary_segment == "mid_market":
                    recommended_products = ["Professional Plan", "Standard Features"]
                else:
                    recommended_products = ["Starter Plan", "Basic Features"]
            
            # Determinar tipo de comprador basado en el lead
            buyer_type = "general"
            sentiment = lead_data.get("sentiment_analysis", {}).get("sentiment", "neutral")
            message = lead_data.get("message", "").lower()
            
            # Detectar tipo de comprador desde el mensaje
            if any(kw in message for kw in ["sostenible", "ecol√≥gico", "medio ambiente", "verde"]):
                buyer_type = "eco-friendly"
            elif any(kw in message for kw in ["premium", "exclusivo", "lujo", "alta calidad"]):
                buyer_type = "luxury"
            elif any(kw in message for kw in ["econ√≥mico", "precio", "barato", "presupuesto"]):
                buyer_type = "budget"
            elif any(kw in message for kw in ["tecnolog√≠a", "api", "t√©cnico", "desarrollador"]):
                buyer_type = "tech-savvy"
            elif any(kw in message for kw in ["salud", "bienestar", "natural", "org√°nico"]):
                buyer_type = "health-conscious"
            elif any(kw in message for kw in ["simple", "m√≠nimo", "esencial", "dise√±o limpio"]):
                buyer_type = "minimalist"
            
            # Generar descripciones para cada producto recomendado
            product_descriptions = []
            
            for product_name in recommended_products[:5]:  # Limitar a 5 productos
                try:
                    # Obtener caracter√≠sticas del producto (esto podr√≠a venir de una base de datos)
                    product_features = _get_product_features(product_name)
                    
                    # Determinar rango de precio basado en el producto
                    price_range = "medium"
                    if "enterprise" in product_name.lower() or "premium" in product_name.lower():
                        price_range = "premium"
                    elif "starter" in product_name.lower() or "basic" in product_name.lower():
                        price_range = "low"
                    
                    # Generar descripci√≥n
                    description = generate_product_description(
                        product_name=product_name,
                        product_features=product_features,
                        product_category="software",
                        buyer_type=buyer_type,
                        price_range=price_range,
                        use_llm=os.getenv("ENABLE_LLM_PRODUCT_DESCRIPTIONS", "false").lower() == "true",
                        language="es",
                        max_length=500,
                        include_seo=True,
                        tone="professional"
                    )
                    
                    product_descriptions.append({
                        "product_name": product_name,
                        "description": description,
                        "buyer_type": buyer_type,
                        "price_range": price_range
                    })
                    
                except Exception as e:
                    logger.warning("product_description_generation_failed",
                                 product_name=product_name,
                                 error=str(e))
            
            lead_data["product_descriptions"] = {
                "generated_descriptions": product_descriptions,
                "buyer_type_detected": buyer_type,
                "total_products": len(product_descriptions),
                "generation_timestamp": datetime.utcnow().isoformat()
            }
            
            if span:
                span.set_attribute("product_descriptions.count", len(product_descriptions))
                span.set_attribute("buyer_type", buyer_type)
                span.end()
            
            logger.info("product_descriptions_generated",
                       email=lead_data.get("email"),
                       products_count=len(product_descriptions),
                       buyer_type=buyer_type)
            return lead_data
            
        except Exception as e:
            logger.error("product_descriptions_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            # Continuar sin descripciones si falla
            return lead_data

    @task(task_id="create_audit_trail")
    def create_audit_trail(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """Crea un registro de auditor√≠a completo del procesamiento del lead."""
        span = None
        if tracer:
            span = tracer.start_span("create_audit_trail")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            ctx = get_current_context()
            
            audit_trail = {
                "lead_id": lead_data.get("lead_ext_id"),
                "email": lead_data.get("email"),
                "processing_timestamp": datetime.utcnow().isoformat(),
                "dag_run_id": ctx.get("dag_run_id"),
                "task_id": ctx.get("task_instance", {}).get("task_id"),
                "pipeline_version": "2.0",
                "processing_steps": {
                    "rate_limited": lead_data.get("rate_limit_passed", False),
                    "spam_checked": not lead_data.get("is_spam", False),
                    "email_validated": lead_data.get("email_validation", {}).get("valid_mx") is not None,
                    "data_validated": True,
                    "duplicate_checked": not lead_data.get("is_duplicate", False),
                    "company_validated": lead_data.get("company_validation", {}).get("validated", False),
                    "enriched": lead_data.get("is_enriched", False),
                    "scored": lead_data.get("score") is not None,
                    "quality_analyzed": lead_data.get("quality_analysis") is not None,
                    "segmented": lead_data.get("segmentation") is not None,
                    "sentiment_analyzed": lead_data.get("sentiment_analysis") is not None,
                    "velocity_calculated": lead_data.get("velocity_analysis") is not None,
                    "anomalies_detected": lead_data.get("anomaly_detection") is not None,
                    "conversion_predicted": lead_data.get("conversion_prediction") is not None,
                    "recommendations_generated": lead_data.get("recommendations") is not None,
                },
                "scores": {
                    "lead_score": lead_data.get("score"),
                    "spam_score": lead_data.get("spam_score", 0),
                    "quality_score": lead_data.get("quality_analysis", {}).get("quality_score"),
                    "ml_score": lead_data.get("ml_score"),
                    "ml_confidence": lead_data.get("ml_confidence"),
                    "anomaly_score": lead_data.get("anomaly_detection", {}).get("anomaly_score", 0),
                    "conversion_score": lead_data.get("conversion_prediction", {}).get("conversion_score", 0),
                    "sentiment_score": lead_data.get("sentiment_analysis", {}).get("sentiment_score", 0),
                    "velocity_score": lead_data.get("velocity_analysis", {}).get("velocity_score", 0),
                },
                "classifications": {
                    "priority": lead_data.get("priority"),
                    "quality_grade": lead_data.get("quality_analysis", {}).get("grade"),
                    "primary_segment": lead_data.get("segmentation", {}).get("primary_segment"),
                    "is_qualified": lead_data.get("is_qualified", False),
                    "conversion_category": lead_data.get("conversion_prediction", {}).get("conversion_category"),
                    "sentiment": lead_data.get("sentiment_analysis", {}).get("sentiment"),
                    "velocity_category": lead_data.get("velocity_analysis", {}).get("velocity_category"),
                },
                "business_rules": lead_data.get("business_rules", {}),
                "metadata": {
                    "source": lead_data.get("source"),
                    "utm_campaign": lead_data.get("utm_campaign"),
                    "optimized_at": lead_data.get("optimized_at"),
                }
            }
            
            try:
                ctx = get_current_context()
                params = ctx["params"]
                conn_id = str(params["postgres_conn_id"])
                hook = PostgresHook(postgres_conn_id=conn_id)
                
                with hook.get_conn() as conn:
                    with conn.cursor() as cur:
                        try:
                            cur.execute("""
                                INSERT INTO lead_audit_trail (
                                    lead_id, email, audit_data, created_at
                                ) VALUES (
                                    %s, %s, %s, NOW()
                                )
                                ON CONFLICT (lead_id) DO UPDATE SET
                                    audit_data = EXCLUDED.audit_data,
                                    updated_at = NOW()
                            """, (audit_trail["lead_id"], audit_trail["email"], json.dumps(audit_trail)))
                        except Exception:
                            logger.debug("audit_trail_table_not_found")
            except Exception as e:
                logger.debug("audit_trail_save_failed", error=str(e))
            
            lead_data["audit_trail"] = audit_trail
            
            if span:
                span.set_attribute("audit_trail.created", True)
                span.end()
            
            logger.debug("audit_trail_created", lead_id=audit_trail["lead_id"])
            return lead_data
            
        except Exception as e:
            logger.error("audit_trail_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            return lead_data
    
    @task(task_id="assign_to_ab_test")
    def assign_to_ab_test(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Asigna el lead a un grupo A/B para experimentaci√≥n y optimizaci√≥n.
        """
        span = None
        if tracer:
            span = tracer.start_span("assign_to_ab_test")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            # Determinar si el lead debe participar en A/B testing
            ab_testing_enabled = os.getenv("AB_TESTING_ENABLED", "false").lower() == "true"
            if not ab_testing_enabled:
                return lead_data
            
            # Generar hash determin√≠stico del email para consistencia
            email = lead_data.get("email", "")
            if not email:
                return lead_data
            
            email_hash = int(hashlib.md5(email.encode()).hexdigest(), 16)
            
            # Asignar grupo A/B (50/50 split)
            ab_group = "A" if (email_hash % 2) == 0 else "B"
            
            # Obtener variantes de experimentos activos
            experiments = []
            
            # Experimento 1: Scoring algorithm
            scoring_experiment = os.getenv("AB_EXPERIMENT_SCORING", "")
            if scoring_experiment:
                variant = "new" if ab_group == "B" else "baseline"
                experiments.append({
                    "experiment_id": "scoring_algorithm",
                    "variant": variant,
                    "group": ab_group
                })
            
            # Experimento 2: Enrichment strategy
            enrichment_experiment = os.getenv("AB_EXPERIMENT_ENRICHMENT", "")
            if enrichment_experiment:
                variant = "aggressive" if ab_group == "B" else "conservative"
                experiments.append({
                    "experiment_id": "enrichment_strategy",
                    "variant": variant,
                    "group": ab_group
                })
            
            # Experimento 3: Follow-up timing
            followup_experiment = os.getenv("AB_EXPERIMENT_FOLLOWUP", "")
            if followup_experiment:
                variant = "fast" if ab_group == "B" else "standard"
                experiments.append({
                    "experiment_id": "followup_timing",
                    "variant": variant,
                    "group": ab_group
                })
            
            ab_assignment = {
                "group": ab_group,
                "experiments": experiments,
                "assignment_hash": email_hash % 100,  # 0-99 para percentiles
            }
            
            lead_data["ab_assignment"] = ab_assignment
            
            # Aplicar variantes de experimentos
            if experiments:
                for exp in experiments:
                    if exp["experiment_id"] == "scoring_algorithm" and exp["variant"] == "new":
                        # Aplicar algoritmo de scoring mejorado
                        current_score = lead_data.get("score", 0)
                        # Boost del 5% para grupo B
                        lead_data["score"] = min(100, int(current_score * 1.05))
                        lead_data["score_boosted_by_ab"] = True
                    
                    elif exp["experiment_id"] == "enrichment_strategy" and exp["variant"] == "aggressive":
                        # Marcar para enriquecimiento m√°s agresivo
                        lead_data["aggressive_enrichment"] = True
            
            if span:
                span.set_attribute("ab.group", ab_group)
                span.set_attribute("ab.experiments_count", len(experiments))
                span.end()
            
            logger.info(
                "ab_assignment_created",
                email=email,
                group=ab_group,
                experiments=[exp["experiment_id"] for exp in experiments]
            )
            
            return lead_data
            
        except Exception as e:
            logger.error("ab_testing_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            return lead_data
    
    @task(task_id="calculate_lead_value")
    def calculate_lead_value(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Calcula el valor estimado del lead basado en m√∫ltiples factores.
        """
        span = None
        if tracer:
            span = tracer.start_span("calculate_lead_value")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            value_score = 0
            value_factors = {}
            
            # Factor 1: Tama√±o de empresa (40% peso)
            enriched_data = lead_data.get("enriched_data", {})
            clearbit_company = enriched_data.get("clearbit", {}).get("company", {})
            employees = clearbit_company.get("metrics", {}).get("employees", 0)
            
            if employees > 1000:
                company_value = 100
                value_tier = "enterprise"
            elif employees > 500:
                company_value = 75
                value_tier = "large"
            elif employees > 100:
                company_value = 50
                value_tier = "mid_market"
            elif employees > 10:
                company_value = 30
                value_tier = "small_business"
            else:
                company_value = 10
                value_tier = "micro"
            
            value_score += company_value * 0.4
            value_factors["company_size_contribution"] = company_value * 0.4
            value_factors["company_tier"] = value_tier
            
            # Factor 2: Score del lead (30% peso)
            lead_score = lead_data.get("score", 0)
            value_score += lead_score * 0.3
            value_factors["lead_score_contribution"] = lead_score * 0.3
            
            # Factor 3: Probabilidad de conversi√≥n (20% peso)
            conversion_prob = lead_data.get("conversion_prediction", {}).get("conversion_probability", 0.5)
            conversion_value = conversion_prob * 100
            value_score += conversion_value * 0.2
            value_factors["conversion_prob_contribution"] = conversion_value * 0.2
            
            # Factor 4: Segmentaci√≥n (10% peso)
            primary_segment = lead_data.get("segmentation", {}).get("primary_segment", "unknown")
            segment_values = {
                "high_value": 80,
                "enterprise": 90,
                "organic_source": 70,
                "medium_value": 50,
                "mid_market": 60,
                "paid_source": 40,
                "low_value": 20
            }
            segment_value = segment_values.get(primary_segment, 30)
            value_score += segment_value * 0.1
            value_factors["segment_contribution"] = segment_value * 0.1
            
            # Normalizar a 0-100
            value_score = max(0, min(value_score, 100))
            
            # Estimar valor monetario (si est√° disponible)
            estimated_value = None
            if employees > 0:
                # Heur√≠stica simple: $1000 por empleado en empresas grandes, escalado para otras
                if employees > 1000:
                    estimated_value = employees * 10
                elif employees > 100:
                    estimated_value = employees * 5
                else:
                    estimated_value = employees * 2
            
            # Ajustar por probabilidad de conversi√≥n
            if estimated_value:
                estimated_value = int(estimated_value * conversion_prob)
            
            value_analysis = {
                "value_score": round(value_score, 2),
                "estimated_value_usd": estimated_value,
                "value_tier": value_tier,
                "factors": value_factors,
                "value_category": (
                    "very_high" if value_score >= 80 else
                    "high" if value_score >= 60 else
                    "medium" if value_score >= 40 else
                    "low" if value_score >= 20 else "very_low"
                )
            }
            
            lead_data["value_analysis"] = value_analysis
            
            if span:
                span.set_attribute("value.score", value_score)
                span.set_attribute("value.tier", value_tier)
                span.set_attribute("value.estimated_usd", estimated_value)
                span.end()
            
            logger.info(
                "lead_value_calculated",
                email=lead_data.get("email"),
                value_score=value_score,
                value_tier=value_tier,
                estimated_value=estimated_value
            )
            
            return lead_data
            
        except Exception as e:
            logger.error("value_calculation_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            return lead_data
    
    @task(task_id="generate_lead_insights")
    def generate_lead_insights(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Genera insights y an√°lisis inteligentes sobre el lead.
        """
        span = None
        if tracer:
            span = tracer.start_span("generate_lead_insights")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            insights = []
            
            # Insight 1: Oportunidad de alto valor
            value_score = lead_data.get("value_analysis", {}).get("value_score", 0)
            if value_score >= 80:
                insights.append({
                    "type": "high_value_opportunity",
                    "priority": "high",
                    "title": "Oportunidad de Alto Valor",
                    "description": f"Lead con value score de {value_score}, probablemente empresa enterprise",
                    "recommended_action": "Asignar a representante senior inmediatamente"
                })
            
            # Insight 2: Urgencia detectada
            velocity_analysis = lead_data.get("velocity_analysis", {})
            if velocity_analysis.get("has_urgency") or velocity_analysis.get("velocity_category") == "fast":
                insights.append({
                    "type": "urgency_detected",
                    "priority": "high",
                    "title": "Urgencia Detectada",
                    "description": "Lead muestra se√±ales de urgencia en el mensaje",
                    "recommended_action": f"Contactar en {velocity_analysis.get('estimated_response_hours', 24)} horas"
                })
            
            # Insight 3: Sentimiento positivo
            sentiment = lead_data.get("sentiment_analysis", {})
            if sentiment.get("sentiment") == "positive" and sentiment.get("confidence", 0) > 0.7:
                insights.append({
                    "type": "positive_sentiment",
                    "priority": "medium",
                    "title": "Sentimiento Positivo",
                    "description": "Lead muestra sentimiento muy positivo, alta probabilidad de conversi√≥n",
                    "recommended_action": "Priorizar contacto y ofrecer demo inmediata"
                })
            
            # Insight 4: Datos faltantes cr√≠ticos
            missing_critical = []
            if not lead_data.get("phone"):
                missing_critical.append("tel√©fono")
            if not lead_data.get("company"):
                missing_critical.append("empresa")
            
            if missing_critical:
                insights.append({
                    "type": "missing_critical_data",
                    "priority": "medium",
                    "title": "Datos Cr√≠ticos Faltantes",
                    "description": f"Faltan datos importantes: {', '.join(missing_critical)}",
                    "recommended_action": "Solicitar informaci√≥n faltante en primer contacto"
                })
            
            # Insight 5: Anomal√≠as detectadas
            anomaly_detection = lead_data.get("anomaly_detection", {})
            if anomaly_detection.get("severity") == "high":
                insights.append({
                    "type": "high_anomaly_risk",
                    "priority": "high",
                    "title": "Alto Riesgo de Anomal√≠as",
                    "description": f"Se detectaron {anomaly_detection.get('anomaly_count', 0)} anomal√≠as con severidad alta",
                    "recommended_action": "Revisar manualmente antes de procesar"
                })
            
            # Insight 6: Segmentaci√≥n especial
            segmentation = lead_data.get("segmentation", {})
            if "enterprise" in segmentation.get("segments", []):
                insights.append({
                    "type": "enterprise_segment",
                    "priority": "high",
                    "title": "Lead Enterprise",
                    "description": "Lead clasificado como empresa enterprise",
                    "recommended_action": "Asignar a equipo enterprise con pricing especializado"
                })
            
            # Insight 7: Velocidad de conversi√≥n
            conversion_pred = lead_data.get("conversion_prediction", {})
            if conversion_pred.get("conversion_category") == "high":
                insights.append({
                    "type": "high_conversion_probability",
                    "priority": "medium",
                    "title": "Alta Probabilidad de Conversi√≥n",
                    "description": f"Probabilidad de conversi√≥n: {conversion_pred.get('conversion_probability', 0):.1%}, estimado {conversion_pred.get('estimated_days_to_convert', 0)} d√≠as",
                    "recommended_action": "Acelerar proceso de onboarding"
                })
            
            # Ordenar por prioridad
            priority_order = {"high": 0, "medium": 1, "low": 2}
            insights.sort(key=lambda x: priority_order.get(x.get("priority", "low"), 2))
            
            lead_data["insights"] = insights
            
            if span:
                span.set_attribute("insights.count", len(insights))
                span.set_attribute("insights.high_priority", sum(1 for i in insights if i.get("priority") == "high"))
                span.end()
            
            logger.info(
                "insights_generated",
                email=lead_data.get("email"),
                insights_count=len(insights),
                high_priority_count=sum(1 for i in insights if i.get("priority") == "high")
            )
            
            return lead_data
            
        except Exception as e:
            logger.error("insights_generation_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            return lead_data
    
    @task(task_id="export_metrics")
    def export_metrics(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Exporta m√©tricas a sistemas externos (Prometheus, Datadog, etc.).
        """
        try:
            # Exportar a Prometheus si est√° disponible
            if PROMETHEUS_AVAILABLE:
                email = lead_data.get("email")
                score = lead_data.get("score", 0)
                priority = lead_data.get("priority", "low")
                is_qualified = lead_data.get("is_qualified", False)
                
                # Registrar m√©tricas adicionales
                lead_metrics["captured"].labels(
                    source=lead_data.get("source", "unknown"),
                    status="completed"
                ).inc()
                
                if is_qualified:
                    lead_metrics["qualified"].inc()
                
                # Registrar score por prioridad
                if priority == "high":
                    lead_metrics["high_priority"].inc()
                elif priority == "medium":
                    lead_metrics["medium_priority"].inc()
                else:
                    lead_metrics["low_priority"].inc()
            
            # Exportar a webhook de m√©tricas si est√° configurado
            metrics_webhook = os.getenv("METRICS_WEBHOOK_URL")
            if metrics_webhook:
                try:
                    metrics_payload = {
                        "timestamp": datetime.utcnow().isoformat(),
                        "lead_id": lead_data.get("lead_ext_id"),
                        "email": lead_data.get("email"),
                        "score": lead_data.get("score"),
                        "priority": lead_data.get("priority"),
                        "is_qualified": lead_data.get("is_qualified", False),
                        "quality_grade": lead_data.get("quality_analysis", {}).get("grade"),
                        "conversion_probability": lead_data.get("quality_analysis", {}).get("conversion_probability"),
                        "source": lead_data.get("source"),
                    }
                    
                    with httpx.Client(timeout=2.0) as client:
                        client.post(
                            metrics_webhook,
                            json=metrics_payload,
                            headers={"Content-Type": "application/json"},
                        )
                    logger.debug("metrics_exported", webhook=metrics_webhook)
                except Exception as e:
                    logger.debug("metrics_export_failed", error=str(e))
            
        except Exception as e:
            logger.debug("metrics_export_error", error=str(e))
            # No fallar el pipeline si falla la exportaci√≥n de m√©tricas
        
        return lead_data
    
    @task(task_id="analyze_cohort_performance")
    def analyze_cohort_performance(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analiza el rendimiento de cohortes para identificar tendencias de conversi√≥n.
        """
        span = None
        if tracer:
            span = tracer.start_span("analyze_cohort_performance")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            ctx = get_current_context()
            conn_id = str(ctx["params"]["postgres_conn_id"])
            hook = PostgresHook(postgres_conn_id=conn_id)
            
            cohort_analysis = {}
            
            with hook.get_conn() as conn:
                with conn.cursor() as cur:
                    # An√°lisis por cohorte de fecha (semanal)
                    source = lead_data.get("source", "unknown")
                    created_date = datetime.utcnow().date()
                    week_start = created_date - timedelta(days=created_date.weekday())
                    
                    # Obtener estad√≠sticas de la cohorte actual
                    cur.execute("""
                        SELECT 
                            COUNT(*) as total_leads,
                            AVG(score) as avg_score,
                            COUNT(CASE WHEN score >= 60 THEN 1 END) as qualified_count,
                            COUNT(CASE WHEN status = 'converted' THEN 1 END) as converted_count
                        FROM leads
                        WHERE DATE_TRUNC('week', created_at) = %s
                        AND source = %s
                    """, (week_start, source))
                    
                    cohort_stats = cur.fetchone()
                    if cohort_stats and cohort_stats[0] > 0:
                        cohort_analysis["weekly_cohort"] = {
                            "week_start": week_start.isoformat(),
                            "total_leads": cohort_stats[0],
                            "avg_score": float(cohort_stats[1] or 0),
                            "qualified_rate": (cohort_stats[2] / cohort_stats[0] * 100) if cohort_stats[0] > 0 else 0,
                            "conversion_rate": (cohort_stats[3] / cohort_stats[0] * 100) if cohort_stats[0] > 0 else 0,
                        }
                    
                    # Comparar con cohorte anterior
                    prev_week_start = week_start - timedelta(days=7)
                    cur.execute("""
                        SELECT 
                            COUNT(*) as total_leads,
                            AVG(score) as avg_score,
                            COUNT(CASE WHEN score >= 60 THEN 1 END) as qualified_count,
                            COUNT(CASE WHEN status = 'converted' THEN 1 END) as converted_count
                        FROM leads
                        WHERE DATE_TRUNC('week', created_at) = %s
                        AND source = %s
                    """, (prev_week_start, source))
                    
                    prev_cohort_stats = cur.fetchone()
                    if prev_cohort_stats and prev_cohort_stats[0] > 0 and cohort_stats:
                        cohort_analysis["trend"] = {
                            "conversion_rate_change": (
                                (cohort_stats[3] / cohort_stats[0] * 100) - 
                                (prev_cohort_stats[3] / prev_cohort_stats[0] * 100)
                            ) if cohort_stats[0] > 0 and prev_cohort_stats[0] > 0 else 0,
                            "avg_score_change": (
                                float(cohort_stats[1] or 0) - float(prev_cohort_stats[1] or 0)
                            ),
                        }
            
            lead_data["cohort_analysis"] = cohort_analysis
            
            if span:
                span.set_attribute("cohort.week_start", cohort_analysis.get("weekly_cohort", {}).get("week_start", ""))
                span.end()
            
            logger.info(
                "cohort_analysis_completed",
                email=lead_data.get("email"),
                cohort_week=cohort_analysis.get("weekly_cohort", {}).get("week_start"),
                conversion_rate=cohort_analysis.get("weekly_cohort", {}).get("conversion_rate", 0)
            )
            
            return lead_data
            
        except Exception as e:
            logger.error("cohort_analysis_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            return lead_data
    
    @task(task_id="calculate_multi_channel_attribution")
    def calculate_multi_channel_attribution(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Calcula atribuci√≥n multi-canal para entender qu√© canales contribuyeron a la conversi√≥n.
        """
        span = None
        if tracer:
            span = tracer.start_span("calculate_multi_channel_attribution")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            attribution = {
                "channels": [],
                "primary_channel": None,
                "attribution_model": "first_touch"  # Por defecto
            }
            
            # Recopilar todos los canales de atribuci√≥n
            source = lead_data.get("source", "unknown")
            utm_source = lead_data.get("utm_source")
            utm_medium = lead_data.get("utm_medium")
            utm_campaign = lead_data.get("utm_campaign")
            referrer = lead_data.get("referrer")
            
            channels = []
            
            # Canal principal (source)
            if source and source != "unknown":
                channels.append({
                    "channel": source,
                    "type": "primary",
                    "touchpoint": "first",
                    "weight": 1.0
                })
            
            # UTM parameters
            if utm_source:
                channels.append({
                    "channel": f"{utm_source}/{utm_medium or 'unknown'}",
                    "type": "utm",
                    "campaign": utm_campaign,
                    "touchpoint": "first",
                    "weight": 0.8
                })
            
            # Referrer
            if referrer:
                # Extraer dominio del referrer
                try:
                    from urllib.parse import urlparse
                    parsed = urlparse(referrer)
                    referrer_domain = parsed.netloc
                    if referrer_domain:
                        channels.append({
                            "channel": referrer_domain,
                            "type": "referrer",
                            "touchpoint": "assist",
                            "weight": 0.3
                        })
                except Exception:
                    pass
            
            # Calcular atribuci√≥n ponderada
            if channels:
                total_weight = sum(c.get("weight", 0) for c in channels)
                for channel in channels:
                    channel["attribution_percentage"] = (channel.get("weight", 0) / total_weight * 100) if total_weight > 0 else 0
                
                # Canal primario es el de mayor peso
                primary = max(channels, key=lambda x: x.get("weight", 0))
                attribution["primary_channel"] = primary.get("channel")
                attribution["channels"] = channels
                
                # Modelo de atribuci√≥n mejorado (time-decay)
                attribution["attribution_model"] = "time_decay"
            
            lead_data["attribution"] = attribution
            
            if span:
                span.set_attribute("attribution.primary_channel", attribution.get("primary_channel", ""))
                span.set_attribute("attribution.channels_count", len(attribution.get("channels", [])))
                span.end()
            
            logger.info(
                "attribution_calculated",
                email=lead_data.get("email"),
                primary_channel=attribution.get("primary_channel"),
                channels_count=len(attribution.get("channels", []))
            )
            
            return lead_data
            
        except Exception as e:
            logger.error("attribution_calculation_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            return lead_data
    
    @task(task_id="score_lead_source_quality")
    def score_lead_source_quality(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Eval√∫a la calidad de la fuente del lead basado en historial de conversi√≥n.
        """
        span = None
        if tracer:
            span = tracer.start_span("score_lead_source_quality")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            ctx = get_current_context()
            conn_id = str(ctx["params"]["postgres_conn_id"])
            hook = PostgresHook(postgres_conn_id=conn_id)
            
            source = lead_data.get("source", "unknown")
            source_quality = {
                "source": source,
                "quality_score": 50,  # Default
                "quality_grade": "C",
                "factors": {}
            }
            
            with hook.get_conn() as conn:
                with conn.cursor() as cur:
                    # Obtener estad√≠sticas hist√≥ricas de la fuente
                    cur.execute("""
                        SELECT 
                            COUNT(*) as total_leads,
                            AVG(score) as avg_score,
                            COUNT(CASE WHEN score >= 60 THEN 1 END) as qualified_count,
                            COUNT(CASE WHEN status = 'converted' THEN 1 END) as converted_count,
                            AVG(CASE WHEN status = 'converted' THEN EXTRACT(EPOCH FROM (updated_at - created_at))/86400 ELSE NULL END) as avg_days_to_convert
                        FROM leads
                        WHERE source = %s
                        AND created_at >= NOW() - INTERVAL '90 days'
                    """, (source,))
                    
                    stats = cur.fetchone()
                    if stats and stats[0] > 0:
                        total = stats[0]
                        avg_score = float(stats[1] or 0)
                        qualified_rate = (stats[2] / total * 100) if total > 0 else 0
                        conversion_rate = (stats[3] / total * 100) if total > 0 else 0
                        avg_days = float(stats[4] or 30)
                        
                        # Calcular quality score (0-100)
                        quality_score = 0
                        
                        # Factor 1: Tasa de calificaci√≥n (40%)
                        quality_score += min(qualified_rate * 0.4, 40)
                        source_quality["factors"]["qualification_rate"] = qualified_rate
                        
                        # Factor 2: Tasa de conversi√≥n (40%)
                        quality_score += min(conversion_rate * 0.4, 40)
                        source_quality["factors"]["conversion_rate"] = conversion_rate
                        
                        # Factor 3: Score promedio (15%)
                        quality_score += min(avg_score * 0.15, 15)
                        source_quality["factors"]["avg_score"] = avg_score
                        
                        # Factor 4: Velocidad de conversi√≥n (5%)
                        if avg_days < 7:
                            quality_score += 5
                        elif avg_days < 14:
                            quality_score += 3
                        elif avg_days < 30:
                            quality_score += 1
                        source_quality["factors"]["avg_days_to_convert"] = avg_days
                        
                        source_quality["quality_score"] = round(quality_score, 2)
                        
                        # Asignar grade
                        if quality_score >= 80:
                            source_quality["quality_grade"] = "A"
                        elif quality_score >= 65:
                            source_quality["quality_grade"] = "B"
                        elif quality_score >= 50:
                            source_quality["quality_grade"] = "C"
                        elif quality_score >= 35:
                            source_quality["quality_grade"] = "D"
                        else:
                            source_quality["quality_grade"] = "F"
                        
                        source_quality["stats"] = {
                            "total_leads": total,
                            "qualified_rate": round(qualified_rate, 2),
                            "conversion_rate": round(conversion_rate, 2),
                            "avg_score": round(avg_score, 2),
                            "avg_days_to_convert": round(avg_days, 2)
                        }
            
            lead_data["source_quality"] = source_quality
            
            # Ajustar score del lead basado en calidad de fuente
            if source_quality["quality_grade"] in ["A", "B"]:
                current_score = lead_data.get("score", 0)
                lead_data["score"] = min(100, current_score + 3)
            elif source_quality["quality_grade"] == "F":
                current_score = lead_data.get("score", 0)
                lead_data["score"] = max(0, current_score - 2)
            
            if span:
                span.set_attribute("source_quality.score", source_quality["quality_score"])
                span.set_attribute("source_quality.grade", source_quality["quality_grade"])
                span.end()
            
            logger.info(
                "source_quality_scored",
                email=lead_data.get("email"),
                source=source,
                quality_score=source_quality["quality_score"],
                quality_grade=source_quality["quality_grade"]
            )
            
            return lead_data
            
        except Exception as e:
            logger.error("source_quality_scoring_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            return lead_data
    
    @task(task_id="detect_advanced_fraud")
    def detect_advanced_fraud(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Detecci√≥n avanzada de fraude m√°s all√° de spam b√°sico.
        """
        span = None
        if tracer:
            span = tracer.start_span("detect_advanced_fraud")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            fraud_score = 0
            fraud_indicators = []
            
            # Verificar patrones de fraude conocidos
            email = lead_data.get("email", "").lower()
            ip_address = lead_data.get("ip_address")
            
            # Indicador 1: Emails temporales conocidos
            temp_email_domains = [
                "tempmail.com", "10minutemail.com", "guerrillamail.com",
                "mailinator.com", "throwaway.email", "temp-mail.org"
            ]
            email_domain = email.split("@")[-1] if "@" in email else ""
            if any(temp in email_domain for temp in temp_email_domains):
                fraud_score += 20
                fraud_indicators.append("temporary_email_domain")
            
            # Indicador 2: Patrones de email sospechosos
            if re.match(r'^[a-z]+\d+@', email) or re.match(r'^\d+[a-z]+@', email):
                fraud_score += 10
                fraud_indicators.append("suspicious_email_pattern")
            
            # Indicador 3: IP en lista negra (simulado - en producci√≥n usar servicio real)
            if ip_address:
                # Verificar si IP es privada/localhost (sospechoso para web leads)
                try:
                    ip = ipaddress.ip_address(ip_address)
                    if ip.is_private or ip.is_loopback:
                        fraud_score += 15
                        fraud_indicators.append("private_ip_address")
                except Exception:
                    pass
            
            # Indicador 4: Velocidad de env√≠o (m√∫ltiples leads desde misma IP en corto tiempo)
            if ip_address:
                try:
                    ctx = get_current_context()
                    conn_id = str(ctx["params"]["postgres_conn_id"])
                    hook = PostgresHook(postgres_conn_id=conn_id)
                    
                    with hook.get_conn() as conn:
                        with conn.cursor() as cur:
                            cur.execute("""
                                SELECT COUNT(*) 
                                FROM leads 
                                WHERE ip_address = %s 
                                AND created_at >= NOW() - INTERVAL '1 hour'
                            """, (ip_address,))
                            
                            recent_count = cur.fetchone()[0] or 0
                            if recent_count > 5:
                                fraud_score += 25
                                fraud_indicators.append(f"high_velocity_submission_{recent_count}_in_1h")
                except Exception:
                    pass
            
            # Indicador 5: Coincidencia exacta con leads rechazados recientes
            if email:
                try:
                    ctx = get_current_context()
                    conn_id = str(ctx["params"]["postgres_conn_id"])
                    hook = PostgresHook(postgres_conn_id=conn_id)
                    
                    with hook.get_conn() as conn:
                        with conn.cursor() as cur:
                            cur.execute("""
                                SELECT COUNT(*) 
                                FROM leads 
                                WHERE email = %s 
                                AND (status = 'rejected' OR is_spam = true)
                                AND created_at >= NOW() - INTERVAL '30 days'
                            """, (email,))
                            
                            rejected_count = cur.fetchone()[0] or 0
                            if rejected_count > 0:
                                fraud_score += 30
                                fraud_indicators.append(f"previously_rejected_{rejected_count}_times")
                except Exception:
                    pass
            
            # Clasificar nivel de fraude
            if fraud_score >= 50:
                fraud_level = "high"
            elif fraud_score >= 30:
                fraud_level = "medium"
            elif fraud_score >= 15:
                fraud_level = "low"
            else:
                fraud_level = "none"
            
            fraud_detection = {
                "fraud_score": fraud_score,
                "fraud_level": fraud_level,
                "indicators": fraud_indicators,
                "is_fraud": fraud_level in ["high", "medium"]
            }
            
            lead_data["fraud_detection"] = fraud_detection
            
            # Rechazar si es fraude alto
            if fraud_level == "high":
                lead_data["is_fraud"] = True
                lead_data["status"] = "rejected"
                logger.warning(
                    "high_fraud_detected",
                    email=email,
                    fraud_score=fraud_score,
                    indicators=fraud_indicators
                )
            
            if span:
                span.set_attribute("fraud.score", fraud_score)
                span.set_attribute("fraud.level", fraud_level)
                span.set_attribute("fraud.indicators_count", len(fraud_indicators))
                span.end()
            
            return lead_data
            
        except Exception as e:
            logger.error("fraud_detection_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            return lead_data
    
    @task(task_id="track_lead_lifecycle")
    def track_lead_lifecycle(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Rastrea el ciclo de vida completo del lead para an√°lisis de funnel.
        """
        span = None
        if tracer:
            span = tracer.start_span("track_lead_lifecycle")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            lifecycle = {
                "stage": "captured",
                "stage_timestamp": datetime.utcnow().isoformat(),
                "funnel_position": 1,
                "time_in_stage_seconds": 0,
                "stage_history": []
            }
            
            # Determinar etapa actual
            if lead_data.get("is_qualified", False):
                lifecycle["stage"] = "qualified"
                lifecycle["funnel_position"] = 2
            elif lead_data.get("is_spam", False) or lead_data.get("is_fraud", False):
                lifecycle["stage"] = "rejected"
                lifecycle["funnel_position"] = 0
            elif lead_data.get("status") == "converted":
                lifecycle["stage"] = "converted"
                lifecycle["funnel_position"] = 4
            elif lead_data.get("assigned_sales_rep"):
                lifecycle["stage"] = "assigned"
                lifecycle["funnel_position"] = 3
            
            # Registrar historial de etapas
            lifecycle["stage_history"].append({
                "stage": lifecycle["stage"],
                "timestamp": lifecycle["stage_timestamp"],
                "trigger": "initial_capture"
            })
            
            # Calcular m√©tricas de funnel
            try:
                ctx = get_current_context()
                conn_id = str(ctx["params"]["postgres_conn_id"])
                hook = PostgresHook(postgres_conn_id=conn_id)
                
                with hook.get_conn() as conn:
                    with conn.cursor() as cur:
                        # Obtener estad√≠sticas de funnel
                        cur.execute("""
                            SELECT 
                                COUNT(*) FILTER (WHERE status = 'new') as captured,
                                COUNT(*) FILTER (WHERE is_qualified = true) as qualified,
                                COUNT(*) FILTER (WHERE assigned_sales_rep IS NOT NULL) as assigned,
                                COUNT(*) FILTER (WHERE status = 'converted') as converted
                            FROM leads
                            WHERE created_at >= NOW() - INTERVAL '30 days'
                        """)
                        
                        funnel_stats = cur.fetchone()
                        if funnel_stats:
                            total_captured = funnel_stats[0] or 0
                            total_qualified = funnel_stats[1] or 0
                            total_assigned = funnel_stats[2] or 0
                            total_converted = funnel_stats[3] or 0
                            
                            lifecycle["funnel_metrics"] = {
                                "captured": total_captured,
                                "qualified": total_qualified,
                                "assigned": total_assigned,
                                "converted": total_converted,
                                "qualification_rate": (total_qualified / total_captured * 100) if total_captured > 0 else 0,
                                "conversion_rate": (total_converted / total_captured * 100) if total_captured > 0 else 0,
                                "assignment_rate": (total_assigned / total_qualified * 100) if total_qualified > 0 else 0
                            }
            except Exception:
                pass
            
            lead_data["lifecycle"] = lifecycle
            
            if span:
                span.set_attribute("lifecycle.stage", lifecycle["stage"])
                span.set_attribute("lifecycle.funnel_position", lifecycle["funnel_position"])
                span.end()
            
            logger.info(
                "lifecycle_tracked",
                email=lead_data.get("email"),
                stage=lifecycle["stage"],
                funnel_position=lifecycle["funnel_position"]
            )
            
            return lead_data
            
        except Exception as e:
            logger.error("lifecycle_tracking_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            return lead_data
    
    @task(task_id="validate_ip_address")
    def validate_ip_address(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Valida y analiza la direcci√≥n IP del lead.
        """
        span = None
        if tracer:
            span = tracer.start_span("validate_ip_address")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            ip_address = lead_data.get("ip_address")
            if not ip_address:
                return lead_data
            
            ip_validation = {
                "ip_address": ip_address,
                "is_valid": False,
                "is_private": False,
                "is_vpn": False,
                "country": None,
                "risk_score": 0
            }
            
            try:
                ip = ipaddress.ip_address(ip_address)
                ip_validation["is_valid"] = True
                ip_validation["is_private"] = ip.is_private or ip.is_loopback
                
                if ip_validation["is_private"]:
                    ip_validation["risk_score"] += 10
                
            except ValueError:
                ip_validation["is_valid"] = False
                ip_validation["risk_score"] += 20
                logger.warning("invalid_ip_address", ip=ip_address)
            
            # En producci√≥n, aqu√≠ se podr√≠a integrar con servicios de geolocalizaci√≥n
            # como MaxMind, IPStack, etc.
            
            lead_data["ip_validation"] = ip_validation
            
            if span:
                span.set_attribute("ip.is_valid", ip_validation["is_valid"])
                span.set_attribute("ip.is_private", ip_validation["is_private"])
                span.set_attribute("ip.risk_score", ip_validation["risk_score"])
                span.end()
            
            return lead_data
            
        except Exception as e:
            logger.error("ip_validation_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            return lead_data
    
    # ============================================================================
    # MODELOS PYDANTIC PARA VALIDACI√ìN
    # ============================================================================
    
    class LeadMetadata(BaseModel):
        """Metadatos del lead"""
        ip_address: Optional[str] = None
        user_agent: Optional[str] = None
        landing_page: Optional[str] = None
        timestamp: str = Field(default_factory=lambda: datetime.utcnow().isoformat())
        extra: Dict[str, Any] = Field(default_factory=dict)
    
    class LeadInputModel(BaseModel):
        """Modelo de entrada para validaci√≥n de leads con Pydantic"""
        email: EmailStr
        first_name: Optional[str] = Field(None, min_length=1, max_length=100)
        last_name: Optional[str] = Field(None, min_length=1, max_length=100)
        full_name: Optional[str] = Field(None, min_length=1)
        phone: Optional[str] = Field(None, pattern=r'^\+?[1-9]\d{1,14}$')
        company: Optional[str] = Field(None, max_length=200)
        source: str = Field(default="web", max_length=50)
        utm_source: Optional[str] = None
        utm_campaign: Optional[str] = None
        utm_medium: Optional[str] = None
        referrer: Optional[str] = None
        message: Optional[str] = None
        ip_address: Optional[str] = None
        user_agent: Optional[str] = None
        landing_page: Optional[str] = None
        metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)
        
        @validator('phone', pre=True)
        def normalize_phone(cls, v):
            """Normalizar tel√©fono: solo n√∫meros y +"""
            if not v:
                return v
            import re
            return re.sub(r'[^\d+]', '', str(v).strip())
        
        @validator('first_name', 'last_name', pre=True)
        def normalize_name(cls, v):
            """Normalizar nombres: trim whitespace"""
            if not v:
                return v
            return str(v).strip() or None
        
        def model_post_init(self, __context):
            """Post-procesamiento: extraer first_name/last_name de full_name si no existen"""
            if not self.first_name and not self.last_name and self.full_name:
                parts = str(self.full_name).strip().split(maxsplit=1)
                self.first_name = parts[0] if parts else None
                self.last_name = parts[1] if len(parts) > 1 else None
    
    @task(task_id="check_rate_limit")
    def check_rate_limit() -> Dict[str, Any]:
        """Verifica rate limiting para prevenir spam y abuso."""
        ctx = get_current_context()
        params = ctx["params"]
        lead_data_str = str(params.get("lead_data", "{}"))
        
        span = None
        if tracer:
            span = tracer.start_span("check_rate_limit")
        
        try:
            lead_data = json.loads(lead_data_str) if lead_data_str else {}
            email = lead_data.get("email", "").lower()
            ip_address = lead_data.get("ip_address")
            
            if not email:
                if span:
                    span.end()
                raise ValueError("Email requerido para rate limiting")
            
            # Verificar rate limit por email
            if rate_limiter and redis_client:
                email_key = f"rate_limit:email:{email}"
                ip_key = f"rate_limit:ip:{ip_address}" if ip_address else None
                
                # L√≠mite configurable por email
                allowed = rate_limiter.is_allowed(email_key, max_calls=RATE_LIMIT_EMAIL_MAX, period=RATE_LIMIT_EMAIL_PERIOD)
                
                if not allowed:
                    logger.warning(
                        "rate_limit_exceeded",
                        email=email,
                        ip_address=ip_address,
                        reason="email_limit"
                    )
                    if span:
                        span.set_attribute("rate_limit.exceeded", True)
                        span.set_attribute("rate_limit.reason", "email")
                        span.end()
                    raise AirflowFailException("Rate limit excedido: demasiados leads desde este email")
                
                # L√≠mite configurable por IP
                if ip_key:
                    ip_allowed = rate_limiter.is_allowed(ip_key, max_calls=RATE_LIMIT_IP_MAX, period=RATE_LIMIT_IP_PERIOD)
                    if not ip_allowed:
                        logger.warning(
                            "rate_limit_exceeded",
                            email=email,
                            ip_address=ip_address,
                            reason="ip_limit"
                        )
                        if span:
                            span.set_attribute("rate_limit.exceeded", True)
                            span.set_attribute("rate_limit.reason", "ip")
                            span.end()
                        raise AirflowFailException("Rate limit excedido: demasiados leads desde esta IP")
            
            if span:
                span.set_attribute("rate_limit.passed", True)
                span.end()
            
            return {"lead_data": lead_data_str, "rate_limit_passed": True}
            
        except (json.JSONDecodeError, ValueError, AirflowFailException):
            raise
        except Exception as e:
            logger.error("rate_limit_check_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            # Si falla el rate limit, continuar (fail open)
            return {"lead_data": lead_data_str, "rate_limit_passed": True}
    
    @task(task_id="detect_spam")
    def detect_spam(rate_limited: Dict[str, Any]) -> Dict[str, Any]:
        """Detecta leads spam usando m√∫ltiples heur√≠sticas."""
        lead_data_str = rate_limited.get("lead_data", "{}")
        
        span = None
        if tracer:
            span = tracer.start_span("detect_spam")
        
        try:
            lead_data = json.loads(lead_data_str) if lead_data_str else {}
            
            spam_score = 0
            spam_indicators = []
            
            email = lead_data.get("email", "").lower()
            message = lead_data.get("message", "").lower()
            first_name = (lead_data.get("first_name") or "").lower()
            last_name = (lead_data.get("last_name") or "").lower()
            
            # Honeypot field (campo oculto que solo bots llenan)
            if lead_data.get("website") or lead_data.get("url"):
                spam_score += 100
                spam_indicators.append("honeypot_field")
            
            # Email patterns comunes de spam
            spam_email_patterns = [
                r'^[a-z]\d+@',  # a123@example.com
                r'^test\d*@',  # test@, test123@
                r'@(test|example|fake|spam)\.',
                r'@(mailinator|10minutemail|guerrillamail)\.',
            ]
            
            import re
            for pattern in spam_email_patterns:
                if re.search(pattern, email):
                    spam_score += 20
                    spam_indicators.append(f"spam_email_pattern:{pattern}")
                    break
            
            # Mensaje con links sospechosos
            if message:
                suspicious_domains = ['bit.ly', 'tinyurl.com', 'goo.gl']
                if any(domain in message for domain in suspicious_domains):
                    spam_score += 15
                    spam_indicators.append("suspicious_links")
                
                # Mensajes muy cortos o muy gen√©ricos
                if len(message) < 10:
                    spam_score += 10
                    spam_indicators.append("message_too_short")
                
                spam_words = ['viagra', 'casino', 'loan', 'free money', 'click here']
                if any(word in message for word in spam_words):
                    spam_score += 25
                    spam_indicators.append("spam_keywords")
            
            # Nombres sospechosos
            if first_name and first_name in ['test', 'admin', 'spam', 'bot']:
                spam_score += 15
                spam_indicators.append("suspicious_name")
            
            # Dominio de email temporal
            if email and '@' in email:
                domain = email.split('@')[1]
                temp_domains = ['tempmail.com', 'mailinator.com', '10minutemail.com']
                if domain in temp_domains:
                    spam_score += 30
                    spam_indicators.append("temp_email_domain")
            
            is_spam = spam_score >= SPAM_SCORE_THRESHOLD
            
            if is_spam:
                logger.warning(
                    "spam_detected",
                    email=email,
                    spam_score=spam_score,
                    indicators=spam_indicators
                )
                lead_metrics["spam_detected"].inc()
                # Notificar detecci√≥n de spam
                lead_data_for_notification = {"email": email, "ip_address": rate_limited.get("ip_address")}
                notification_manager.notify_spam_detected(lead_data_for_notification, spam_score, spam_indicators)
            
            if span:
                span.set_attribute("spam.score", spam_score)
                span.set_attribute("spam.is_spam", is_spam)
                span.set_attribute("spam.indicators_count", len(spam_indicators))
                span.end()
            
            return {
                **rate_limited,
                "spam_score": spam_score,
                "is_spam": is_spam,
                "spam_indicators": spam_indicators,
            }
            
        except Exception as e:
            logger.error("spam_detection_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            # Si falla la detecci√≥n, continuar (fail open)
            # Esto previene que errores en detecci√≥n bloqueen leads v√°lidos
            logger.warning("spam_detection_failed_fallback", error=str(e))
            return {**rate_limited, "spam_score": 0, "is_spam": False, "spam_indicators": []}
    
    @task(task_id="validate_email_domain")
    def validate_email_domain(spam_checked: Dict[str, Any]) -> Dict[str, Any]:
        """Valida que el dominio del email tenga registros MX v√°lidos."""
        lead_data_str = spam_checked.get("lead_data", "{}")
        
        span = None
        if tracer:
            span = tracer.start_span("validate_email_domain")
        
        try:
            lead_data = json.loads(lead_data_str) if lead_data_str else {}
            email = lead_data.get("email", "").lower()
            
            if not email or "@" not in email:
                if span:
                    span.end()
                return spam_checked
            
            domain = email.split("@")[1]
            
            # Verificar dominio MX
            try:
                # Intentar con dnspython si est√° disponible
                try:
                    mx_records = dns.resolver.resolve(domain, 'MX')
                    has_mx = len(list(mx_records)) > 0
                    
                    if not has_mx:
                        logger.warning(
                            "email_domain_no_mx",
                            email=email,
                            domain=domain
                        )
                        lead_data["email_validation"] = {
                            "valid_mx": False,
                            "domain": domain,
                            "warning": "No MX records found"
                        }
                        # No rechazar, solo advertir
                except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer):
                    logger.warning(
                        "email_domain_invalid",
                        email=email,
                        domain=domain
                    )
                    lead_data["email_validation"] = {
                        "valid_mx": False,
                        "domain": domain,
                        "warning": "Domain not found"
                    }
                except Exception as e:
                    logger.debug("mx_check_error", domain=domain, error=str(e))
                    # Si falla la verificaci√≥n, continuar (fail open)
                    lead_data["email_validation"] = {
                        "valid_mx": None,
                        "domain": domain,
                        "error": str(e)
                    }
            except ImportError:
                # Si dnspython no est√° disponible, hacer verificaci√≥n b√°sica
                try:
                    socket.gethostbyname(domain)
                    lead_data["email_validation"] = {
                        "valid_mx": None,
                        "domain": domain,
                        "note": "DNS check only (dnspython not available)"
                    }
                except socket.gaierror:
                    logger.warning("email_domain_dns_failed", domain=domain)
                    lead_data["email_validation"] = {
                        "valid_mx": False,
                        "domain": domain,
                        "warning": "DNS lookup failed"
                    }
            
            if span:
                span.set_attribute("email.domain", domain)
                span.set_attribute("email.valid_mx", lead_data.get("email_validation", {}).get("valid_mx"))
                span.end()
            
            return {
                **spam_checked,
                "lead_data": json.dumps(lead_data)
            }
            
        except Exception as e:
            logger.error("email_domain_validation_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            # Si falla, continuar sin validaci√≥n
            return spam_checked
    
    @task(task_id="validate_lead_data")
    def validate_lead_data(email_validated: Dict[str, Any]) -> Dict[str, Any]:
        """
        Valida y normaliza los datos del lead usando Pydantic.
        Mejora: Validaci√≥n robusta con Pydantic v2 en lugar de validaci√≥n manual.
        """
        ctx = get_current_context()
        lead_data_str = email_validated.get("lead_data", "{}")
        
        # Rechazar spam si se detect√≥
        if email_validated.get("is_spam", False):
            spam_score = email_validated.get("spam_score")
            logger.warning(
                "spam_rejected",
                spam_score=spam_score,
                indicators=email_validated.get("spam_indicators", [])
            )
            lead_metrics["validated"].labels(result="spam_rejected").inc()
            
            # Guardar en DLQ para an√°lisis posterior
            try:
                lead_data = json.loads(lead_data_str)
                save_to_dlq(
                    lead_data,
                    f"Spam detected (score: {spam_score})",
                    {"spam_indicators": email_validated.get("spam_indicators", [])}
                )
            except Exception as e:
                logger.debug("dlq_save_failed", error=str(e))
            
            raise AirflowFailException(f"Lead rechazado por spam (score: {spam_score})")
        
        # Tracing
        span = None
        if tracer:
            span = tracer.start_span("validate_lead_data")
        
        try:
            # Parse JSON
            try:
                lead_data = json.loads(lead_data_str) if lead_data_str else {}
            except json.JSONDecodeError as e:
                logger.error(
                    "lead_validation_error",
                    error="invalid_json",
                    message=str(e),
                    exc_info=True
                )
                if span:
                    span.set_attribute("error", True)
                    span.set_attribute("error.type", "json_decode_error")
                    span.record_exception(e)
                    span.end()
                raise ValueError("lead_data debe ser un JSON v√°lido") from e
            
            # Validar con Pydantic
            lead_model = LeadInputModel(**lead_data)
            
            # Construir metadatos
            metadata = LeadMetadata(
                ip_address=lead_model.ip_address,
                user_agent=lead_model.user_agent,
                landing_page=lead_model.landing_page,
                extra=lead_model.metadata or {}
            )
        
            # Datos normalizados
            normalized = {
                "email": str(lead_model.email).lower(),
                "first_name": lead_model.first_name,
                "last_name": lead_model.last_name,
                "phone": lead_model.phone,
                "company": lead_model.company,
                "source": lead_model.source,
                "utm_source": lead_model.utm_source,
                "utm_campaign": lead_model.utm_campaign,
                "utm_medium": lead_model.utm_medium,
                "referrer": lead_model.referrer,
                "message": lead_model.message,
                "metadata": metadata.dict()
            }
            
            logger.info(
                "lead_validated",
                email=normalized["email"],
                has_name=bool(normalized["first_name"] or normalized["last_name"]),
                has_phone=bool(normalized["phone"]),
                has_company=bool(normalized["company"]),
                source=normalized["source"]
            )
            leads_processed.inc(status="validated", source=normalized["source"])
            
            if span:
                span.set_attribute("lead.email", normalized["email"])
                span.set_attribute("lead.source", normalized["source"])
                span.set_attribute("validation.success", True)
                span.end()
            
            lead_metrics["validated"].labels(result="success").inc()
            lead_metrics["captured"].labels(
                source=normalized.get("source", "unknown"),
                status="validated"
            ).inc()
            
            return normalized
            
        except ValidationError as e:
            logger.error(
                "lead_validation_error",
                error="pydantic_validation",
                errors=e.errors(),
                exc_info=True
            )
            if span:
                span.set_attribute("error", True)
                span.set_attribute("error.type", "validation_error")
                span.record_exception(e)
                span.end()
            raise ValueError(f"Datos de lead inv√°lidos: {e}") from e
        except Exception as e:
            logger.error(
                "lead_validation_error",
                error="unexpected_error",
                message=str(e),
                exc_info=True
            )
            if span:
                span.set_attribute("error", True)
                span.set_attribute("error.type", "unexpected")
                span.record_exception(e)
                span.end()
            raise
    
    @task(task_id="check_duplicate_lead")
    def check_duplicate_lead(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Detecta leads duplicados usando m√∫ltiples criterios incluyendo fuzzy matching.
        Mejora: Detecci√≥n avanzada con fuzzy matching para nombres y emails similares.
        """
        ctx = get_current_context()
        params = ctx["params"]
        conn_id = str(params["postgres_conn_id"])
        dry_run = bool(params["dry_run"])
        
        span = None
        if tracer:
            span = tracer.start_span("check_duplicate_lead")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            hook = PostgresHook(postgres_conn_id=conn_id)
            email = lead_data.get("email", "").lower()
            phone = lead_data.get("phone")
            first_name = lead_data.get("first_name", "").lower()
            last_name = lead_data.get("last_name", "").lower()
            company = lead_data.get("company", "").lower() if lead_data.get("company") else None
            
            is_duplicate = False
            duplicate_reasons = []
            existing_lead_id = None
            
            with hook.get_conn() as conn:
                with conn.cursor() as cur:
                    # 1. Verificar duplicado exacto por email
                    cur.execute("""
                        SELECT ext_id, first_name, last_name, phone, company, score, created_at
                        FROM leads
                        WHERE LOWER(email) = %s
                        ORDER BY created_at DESC
                        LIMIT 1
                    """, (email,))
                    
                    exact_match = cur.fetchone()
                    if exact_match:
                        is_duplicate = True
                        duplicate_reasons.append("exact_email_match")
                        existing_lead_id = exact_match[0]
                        logger.info(
                            "duplicate_found_exact_email",
                            email=email,
                            existing_lead_id=existing_lead_id
                        )
                    
                    # 2. Verificar duplicado por tel√©fono (si est√° disponible)
                    if not is_duplicate and phone:
                        # Normalizar tel√©fono (remover espacios, guiones, etc.)
                        normalized_phone = re.sub(r'[^\d+]', '', phone)
                        if len(normalized_phone) >= 10:  # Solo si tiene al menos 10 d√≠gitos
                            cur.execute("""
                                SELECT ext_id, email, first_name, last_name, score, created_at
                                FROM leads
                                WHERE REGEXP_REPLACE(phone, '[^0-9+]', '', 'g') = %s
                                AND email != %s
                                ORDER BY created_at DESC
                                LIMIT 1
                            """, (normalized_phone, email))
                            
                            phone_match = cur.fetchone()
                            if phone_match:
                                is_duplicate = True
                                duplicate_reasons.append("phone_match")
                                existing_lead_id = phone_match[0]
                                logger.info(
                                    "duplicate_found_phone",
                                    email=email,
                                    phone=phone,
                                    existing_lead_id=existing_lead_id
                                )
                    
                    # 3. Verificar duplicado por nombre + empresa (fuzzy matching b√°sico)
                    if not is_duplicate and first_name and last_name and company:
                        # Buscar nombres similares con misma empresa
                        cur.execute("""
                            SELECT ext_id, email, first_name, last_name, company, score, created_at
                            FROM leads
                            WHERE LOWER(company) = %s
                            AND (
                                (LOWER(first_name) = %s AND LOWER(last_name) = %s)
                                OR (LOWER(first_name) LIKE %s AND LOWER(last_name) LIKE %s)
                            )
                            AND LOWER(email) != %s
                            ORDER BY created_at DESC
                            LIMIT 1
                        """, (
                            company,
                            first_name, last_name,
                            f"{first_name}%", f"{last_name}%",
                            email
                        ))
                        
                        name_company_match = cur.fetchone()
                        if name_company_match:
                            is_duplicate = True
                            duplicate_reasons.append("name_company_match")
                            existing_lead_id = name_company_match[0]
                            logger.info(
                                "duplicate_found_name_company",
                                email=email,
                                name=f"{first_name} {last_name}",
                                company=company,
                                existing_lead_id=existing_lead_id
                            )
                    
                    # 4. Verificar duplicado por dominio de email (mismo dominio, nombre similar)
                    if not is_duplicate and email and "@" in email:
                        domain = email.split("@")[0].lower()  # Local part antes del @
                        email_domain = email.split("@")[1].lower()
                        
                        # Buscar otros emails del mismo dominio con nombre similar
                        if first_name or last_name:
                            cur.execute("""
                                SELECT ext_id, email, first_name, last_name, score, created_at
                                FROM leads
                                WHERE LOWER(SPLIT_PART(email, '@', 2)) = %s
                                AND LOWER(email) != %s
                                AND (
                                    (first_name IS NOT NULL AND LOWER(first_name) = %s)
                                    OR (last_name IS NOT NULL AND LOWER(last_name) = %s)
                                )
                                ORDER BY created_at DESC
                                LIMIT 1
                            """, (email_domain, email, first_name or "", last_name or ""))
                            
                            domain_match = cur.fetchone()
                            if domain_match:
                                # Verificar similitud adicional antes de marcar como duplicado
                                existing_email = domain_match[1].lower()
                                existing_local = existing_email.split("@")[0]
                                
                                # Si los local parts son muy similares, es probable duplicado
                                if domain and existing_local and (
                                    domain in existing_local or existing_local in domain or
                                    abs(len(domain) - len(existing_local)) <= 2
                                ):
                                    is_duplicate = True
                                    duplicate_reasons.append("email_domain_similarity")
                                    existing_lead_id = domain_match[0]
                                    logger.info(
                                        "duplicate_found_domain_similarity",
                                        email=email,
                                        existing_email=existing_email,
                                        existing_lead_id=existing_lead_id
                                    )
            
            lead_data["is_duplicate"] = is_duplicate
            lead_data["duplicate_reasons"] = duplicate_reasons
            lead_data["existing_lead_id"] = existing_lead_id
            
            if is_duplicate:
                logger.warning(
                    "duplicate_lead_detected",
                    email=email,
                    reasons=duplicate_reasons,
                    existing_lead_id=existing_lead_id
                )
                
                # Si es duplicado, actualizar lead existente en lugar de crear nuevo
                if not dry_run and existing_lead_id:
                    # Opcional: actualizar score del lead existente si este es mejor
                    current_score = lead_data.get("score", 0)
                    # La l√≥gica de actualizaci√≥n se puede hacer en save_lead_to_db
                    lead_data["should_update_existing"] = True
            
            if span:
                span.set_attribute("duplicate.is_duplicate", is_duplicate)
                span.set_attribute("duplicate.reasons_count", len(duplicate_reasons))
                if existing_lead_id:
                    span.set_attribute("duplicate.existing_lead_id", existing_lead_id)
                span.end()
            
            return lead_data
            
        except Exception as e:
            logger.error("duplicate_check_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            # Si falla, continuar sin marca de duplicado (fail open)
            return {**lead_data, "is_duplicate": False, "duplicate_reasons": []}
    
    @task(task_id="validate_company")
    def validate_company(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """Valida y verifica informaci√≥n de empresa usando APIs externas."""
        company = lead_data.get("company")
        if not company:
            return lead_data
        
        span = None
        if tracer:
            span = tracer.start_span("validate_company")
            span.set_attribute("company", company)
        
        try:
            company_validation = {
                "name": company,
                "validated": False,
                "domain": None,
                "exists": None,
            }
            
            # Intentar encontrar dominio de la empresa
            try:
                # Normalizar nombre de empresa para buscar dominio
                domain_candidates = [
                    company.lower().replace(" ", "").replace(".", "").replace(",", "") + ".com",
                    company.lower().replace(" ", "-") + ".com",
                    company.lower().replace(" ", "") + ".com",
                ]
                
                # Verificar si alg√∫n dominio existe
                for domain in domain_candidates:
                    try:
                        socket.gethostbyname(domain)
                        company_validation["domain"] = domain
                        company_validation["exists"] = True
                        break
                    except socket.gaierror:
                        continue
                
                if not company_validation.get("domain"):
                    company_validation["exists"] = False
                    
            except Exception as e:
                logger.debug("company_domain_validation_error", error=str(e))
            
            # Verificar con Clearbit si est√° disponible
            clearbit_api_key = os.getenv("CLEARBIT_API_KEY")
            if clearbit_api_key:
                try:
                    with httpx.Client(timeout=2.0) as client:
                        response = client.get(
                            f"https://company.clearbit.com/v2/companies/find?domain={company_validation.get('domain', company)}",
                            auth=(clearbit_api_key, ''),
                        )
                        if response.status_code == 200:
                            clearbit_company = response.json()
                            company_validation["clearbit"] = {
                                "name": clearbit_company.get("name"),
                                "domain": clearbit_company.get("domain"),
                                "employees": clearbit_company.get("metrics", {}).get("employees"),
                                "industry": clearbit_company.get("category", {}).get("industry"),
                            }
                            company_validation["validated"] = True
                            
                            # Actualizar dominio si se encontr√≥
                            if clearbit_company.get("domain"):
                                company_validation["domain"] = clearbit_company["domain"]
                except Exception as e:
                    logger.debug("clearbit_company_validation_failed", error=str(e))
            
            lead_data["company_validation"] = company_validation
            
            if span:
                span.set_attribute("company.validated", company_validation.get("validated", False))
                span.set_attribute("company.domain", company_validation.get("domain"))
                span.end()
            
        except Exception as e:
            logger.error("company_validation_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
        
        return lead_data
    
    @task(task_id="enrich_lead_data")
    @enrichment_circuit_breaker
    def enrich_lead_data(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Enriquece datos del lead con APIs externas (Clearbit, FullContact, etc.).
        Mejora: Circuit breaker agregado para resiliencia.
        """
        span = None
        if tracer:
            span = tracer.start_span("enrich_lead_data")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            email = lead_data.get("email")
            company = lead_data.get("company")
            
            enriched_data = {}
            
            # Verificar cache primero
            cache_key = f"enrich:{email}"
            if enrichment_cache and cache_key in enrichment_cache:
                cached_data = enrichment_cache[cache_key]
                logger.debug("enrichment_cache_hit", email=email)
                if span:
                    span.set_attribute("enrichment.cached", True)
                    span.end()
                return {
                    **lead_data,
                    "enriched_data": cached_data,
                    "is_enriched": bool(cached_data)
                }
            
            # Enriquecer con Clearbit si est√° configurado (con timeout configurable)
            clearbit_api_key = os.getenv("CLEARBIT_API_KEY")
            if clearbit_api_key and email:
                try:
                    clearbit_start = time.time() if ENABLE_PERFORMANCE_PROFILING else None
                    with httpx.Client(timeout=ENRICHMENT_TIMEOUT, follow_redirects=True) as client:
                        response = client.get(
                            f"https://person.clearbit.com/v2/combined/find?email={email}",
                            auth=(clearbit_api_key, ''),
                            headers={"User-Agent": "LeadCapture/1.0"}
                        )
                        if response.status_code == 200:
                            clearbit_data = response.json()
                            enriched_data["clearbit"] = {
                                "person": clearbit_data.get("person", {}),
                                "company": clearbit_data.get("company", {}),
                            }
                            
                            # Actualizar datos si no existen
                            if not lead_data.get("company") and clearbit_data.get("company", {}).get("name"):
                                lead_data["company"] = clearbit_data["company"]["name"]
                            
                            if not lead_data.get("first_name") and clearbit_data.get("person", {}).get("name", {}).get("givenName"):
                                lead_data["first_name"] = clearbit_data["person"]["name"]["givenName"]
                            
                            logger.info(
                                "lead_enriched",
                                email=email,
                                source="clearbit",
                                has_company=bool(clearbit_data.get("company"))
                            )
                            if PROMETHEUS_AVAILABLE:
                                lead_metrics["enriched"].labels(source="clearbit").inc()
                            
                            if clearbit_start and ENABLE_PERFORMANCE_PROFILING:
                                duration = (time.time() - clearbit_start) * 1000
                                logger.debug("clearbit_enrichment_performance", duration_ms=duration, email=email)
                except Exception as e:
                    logger.debug("clearbit_enrichment_failed", email=email, error=str(e))
            
            # Enriquecer con Hunter.io si est√° configurado (verificar email)
            hunter_api_key = os.getenv("HUNTER_API_KEY")
            if hunter_api_key and email and not enriched_data.get("clearbit"):
                try:
                    with httpx.Client(timeout=2.0) as client:
                        response = client.get(
                            f"https://api.hunter.io/v2/email-verifier?email={email}&api_key={hunter_api_key}"
                        )
                        if response.status_code == 200:
                            hunter_data = response.json().get("data", {})
                            enriched_data["hunter"] = {
                                "score": hunter_data.get("score"),
                                "result": hunter_data.get("result"),  # deliverable, undeliverable, etc.
                                "sources": hunter_data.get("sources", []),
                            }
                            if PROMETHEUS_AVAILABLE:
                                lead_metrics["enriched"].labels(source="hunter").inc()
                except Exception as e:
                    logger.debug("hunter_enrichment_failed", email=email, error=str(e))
            
            # Enriquecer dominio de empresa
            if company and not enriched_data.get("clearbit"):
                try:
                    domain = company.lower().replace(" ", "").replace(".", "") + ".com"
                    enriched_data["inferred_domain"] = domain
                except Exception:
                    pass
            
            # Guardar en cache
            if enrichment_cache and enriched_data:
                enrichment_cache[cache_key] = enriched_data
            
            # Performance profiling
            if start_time and ENABLE_PERFORMANCE_PROFILING:
                duration = (time.time() - start_time) * 1000
                logger.debug(
                    "enrichment_performance",
                    duration_ms=duration,
                    email=email,
                    sources_count=len(enriched_data),
                    cached=False
                )
                if span:
                    span.set_attribute("enrichment.duration_ms", duration)
            
            if span:
                span.set_attribute("enrichment.has_data", bool(enriched_data))
                span.set_attribute("enrichment.sources", list(enriched_data.keys()))
                span.set_attribute("enrichment.cached", False)
                span.end()
            
            return {
                **lead_data,
                "enriched_data": enriched_data,
                "is_enriched": bool(enriched_data),
                "enrichment_cached": False
            }
            
        except Exception as e:
            logger.error("lead_enrichment_error", error=str(e), exc_info=True)
            if span:
                span.set_attribute("error", True)
                span.record_exception(e)
                span.end()
            return {**lead_data, "enriched_data": {}, "is_enriched": False}
    
    @task(task_id="calculate_lead_score")
    def calculate_lead_score(lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Calcula score y prioridad del lead con integraci√≥n ML opcional.
        Mejora: Logging estructurado, tracing y ML scoring agregado.
        """
        ctx = get_current_context()
        params = ctx["params"]
        conn_id = str(params["postgres_conn_id"])
        
        # Tracing
        span = None
        if tracer:
            span = tracer.start_span("calculate_lead_score")
            span.set_attribute("lead.email", lead_data.get("email"))
        
        try:
            hook = PostgresHook(postgres_conn_id=conn_id)
            
            score = 0
            factors = {}
            ml_score = None
            ml_confidence = None
        
            # Factores b√°sicos (max 40 puntos)
            if lead_data.get("first_name") and lead_data.get("last_name"):
                score += 10
                factors["has_full_name"] = True
            if lead_data.get("phone"):
                score += 10
                factors["has_phone"] = True
            if lead_data.get("company"):
                score += 10
                factors["has_company"] = True
            if lead_data.get("message"):
                score += 10
                factors["has_message"] = True
            
            # UTM tracking (max 20 puntos)
            if lead_data.get("utm_campaign"):
                score += 10
                factors["has_campaign"] = True
            if lead_data.get("utm_source"):
                score += 5
                factors["has_source"] = True
            if lead_data.get("utm_medium"):
                score += 5
                factors["has_medium"] = True
            
            # Source quality (max 20 puntos)
            source = lead_data.get("source", "").lower()
            if source in ["organic", "direct", "referral"]:
                score += 10
                factors["high_quality_source"] = True
            elif source in ["paid", "social"]:
                score += 5
                factors["medium_quality_source"] = True
            
            # Metadata signals (max 20 puntos)
            metadata = lead_data.get("metadata", {})
            if metadata.get("landing_page"):
                score += 5
                factors["has_landing_page"] = True
            
            # Email domain quality (max 15 puntos) - usando validaci√≥n DNS
            email = lead_data.get("email", "")
            email_validation = lead_data.get("email_validation", {})
            
            if email:
                domain = email.split("@")[1] if "@" in email else ""
                
                # Usar validaci√≥n DNS si est√° disponible
                if email_validation.get("valid_mx") is True:
                    score += 10
                    factors["valid_mx_record"] = True
                    
                    # Bonus por dominio corporativo (no personal)
                    corporate_domains = ["gmail.com", "outlook.com", "yahoo.com", "hotmail.com", "icloud.com"]
                    if domain not in corporate_domains:
                        score += 5
                        factors["corporate_email"] = True
                    else:
                        score += 2
                        factors["personal_email"] = True
                    
                    # Penalizar dominios desechables
                    if email_validation.get("is_disposable"):
                        score -= 20
                        factors["disposable_email"] = True
                    
                    # Bonus por m√∫ltiples registros MX (mejor redundancia)
                    if email_validation.get("mx_priority") is not None:
                        factors["mx_priority"] = email_validation.get("mx_priority")
                elif email_validation.get("valid_mx") is False:
                    score -= 10
                    factors["invalid_mx_record"] = True
                
                # Usar domain_score de validaci√≥n si est√° disponible
                domain_score = email_validation.get("domain_score", 0)
                if domain_score > 0:
                    score += min(domain_score // 5, 5)  # Max 5 puntos adicionales
                    factors["high_domain_quality"] = True
                elif domain_score < 0:
                    score += max(domain_score // 3, -10)  # Max -10 puntos de penalizaci√≥n
            
            # Company name quality (max 15 puntos)
            company = lead_data.get("company", "")
            if company:
                if len(company) > 3 and not company.lower() in ["test", "demo", "example"]:
                    score += 10
                    factors["valid_company"] = True
                    
                    # Bonus si la empresa est√° validada
                    company_validation = lead_data.get("company_validation", {})
                    if company_validation.get("validated"):
                        score += 5
                        factors["company_validated"] = True
                    if company_validation.get("exists"):
                        score += 3
                        factors["company_domain_exists"] = True
            
            # Penalizar duplicados
            if lead_data.get("is_duplicate"):
                score -= 20
                factors["is_duplicate"] = True
            
            # Bonus por datos enriquecidos
            if lead_data.get("is_enriched"):
                enriched_data = lead_data.get("enriched_data", {})
                if enriched_data.get("clearbit", {}).get("company"):
                    score += 15
                    factors["has_company_data"] = True
                if enriched_data.get("clearbit", {}).get("person"):
                    score += 10
                    factors["has_person_data"] = True
            
            # Penalizar spam score
            spam_score = lead_data.get("spam_score", 0)
            if spam_score > 0:
                score -= min(spam_score // 2, 30)  # M√°ximo -30 puntos
                factors["spam_penalty"] = spam_score
            
            # Bonus por carrito abandonado (integraci√≥n con sistema de recuperaci√≥n)
            try:
                with hook.get_conn() as conn:
                    with conn.cursor() as cur:
                        cur.execute("""
                            SELECT COUNT(*), COALESCE(SUM(total_amount), 0), MAX(created_at)
                            FROM abandoned_carts
                            WHERE customer_email = %s
                                AND recovered_at IS NULL
                                AND created_at > NOW() - INTERVAL '30 days'
                        """, (email,))
                        cart_result = cur.fetchone()
                        
                        if cart_result and cart_result[0] > 0:
                            cart_count = cart_result[0]
                            cart_total = float(cart_result[1] or 0)
                            
                            # Bonus por tener carrito abandonado (indica inter√©s real)
                            score += min(cart_count * 5, 15)  # Max 15 puntos
                            factors["has_abandoned_cart"] = True
                            factors["abandoned_cart_count"] = cart_count
                            
                            # Bonus adicional por valor del carrito
                            if cart_total >= 100:
                                score += 10
                                factors["high_value_cart"] = True
                            elif cart_total >= 50:
                                score += 5
                                factors["medium_value_cart"] = True
                            
                            logger.info(
                                "abandoned_cart_found",
                                email=email,
                                cart_count=cart_count,
                                cart_total=cart_total,
                                score_boost=min(cart_count * 5, 15)
                            )
            except Exception as cart_error:
                # No fallar el scoring si hay error al verificar carritos
                logger.warning(
                    "abandoned_cart_check_error",
                    email=email,
                    error=str(cart_error)
                )
            
            # An√°lisis de sentimiento e intenci√≥n en el mensaje (si existe)
            message = lead_data.get("message", "")
            if message and len(message) > 10:
                message_lower = message.lower()
                
                # Palabras clave de alta intenci√≥n
                high_intent_keywords = [
                    "comprar", "precio", "costo", "presupuesto", "cotizaci√≥n",
                    "demo", "trial", "prueba", "interesado", "urgente",
                    "necesito", "quiero", "busco", "necesitamos", "implementar"
                ]
                
                # Palabras clave positivas
                positive_keywords = [
                    "excelente", "genial", "perfecto", "interesante", "me gusta",
                    "bueno", "buena", "recomendaci√≥n", "recomendado"
                ]
                
                # Palabras clave de urgencia
                urgency_keywords = [
                    "urgente", "r√°pido", "inmediato", "ya", "ahora", "pronto",
                    "deadline", "fecha l√≠mite", "necesito ya"
                ]
                
                intent_score = 0
                sentiment_score = 0
                urgency_score = 0
                
                # Calcular intenci√≥n
                for keyword in high_intent_keywords:
                    if keyword in message_lower:
                        intent_score += 2
                
                # Calcular sentimiento
                for keyword in positive_keywords:
                    if keyword in message_lower:
                        sentiment_score += 1
                
                # Calcular urgencia
                for keyword in urgency_keywords:
                    if keyword in message_lower:
                        urgency_score += 3
                
                # Aplicar bonificaciones
                if intent_score > 0:
                    score += min(intent_score, 10)  # Max 10 puntos
                    factors["high_intent_message"] = True
                    factors["intent_score"] = intent_score
                
                if sentiment_score > 0:
                    score += min(sentiment_score, 5)  # Max 5 puntos
                    factors["positive_sentiment"] = True
                
                if urgency_score > 0:
                    score += min(urgency_score, 8)  # Max 8 puntos
                    factors["urgency_detected"] = True
                    priority = "high" if score >= 50 else priority  # Forzar alta prioridad si hay urgencia
                
                logger.info(
                    "message_analysis",
                    email=email,
                    intent_score=intent_score,
                    sentiment_score=sentiment_score,
                    urgency_score=urgency_score
                )
            
            # Bonus por comportamiento web (si est√° disponible en metadata)
            web_behavior = metadata.get("web_behavior", {})
            if web_behavior:
                # Tiempo en p√°gina
                time_on_page = web_behavior.get("time_on_page", 0)
                if time_on_page > 300:  # M√°s de 5 minutos
                    score += 8
                    factors["high_engagement"] = True
                elif time_on_page > 120:  # M√°s de 2 minutos
                    score += 4
                    factors["medium_engagement"] = True
                
                # P√°ginas visitadas
                pages_visited = web_behavior.get("pages_visited", 0)
                if pages_visited >= 5:
                    score += 7
                    factors["high_interest"] = True
                elif pages_visited >= 3:
                    score += 3
                    factors["medium_interest"] = True
                
                # Retornos al sitio
                return_visits = web_behavior.get("return_visits", 0)
                if return_visits >= 3:
                    score += 10
                    factors["high_retention"] = True
                elif return_visits >= 1:
                    score += 5
                    factors["returning_visitor"] = True
                
                # Scroll depth (profundidad de scroll)
                scroll_depth = web_behavior.get("scroll_depth", 0)
                if scroll_depth >= 80:  # Scroll m√°s del 80%
                    score += 5
                    factors["deep_engagement"] = True
                
                # Clicks en CTAs
                cta_clicks = web_behavior.get("cta_clicks", 0)
                if cta_clicks > 0:
                    score += cta_clicks * 3  # 3 puntos por cada click en CTA
                    factors["cta_engagement"] = True
                    factors["cta_clicks"] = cta_clicks
            
            # Bonus por historial de interacciones previas
            try:
                with hook.get_conn() as conn:
                    with conn.cursor() as cur:
                        # Verificar si es un lead recurrente
                        cur.execute("""
                            SELECT COUNT(*), MAX(created_at)
                            FROM leads
                            WHERE email = %s
                                AND created_at < NOW() - INTERVAL '1 day'
                        """, (email,))
                        prev_leads = cur.fetchone()
                        
                        if prev_leads and prev_leads[0] > 0:
                            # Lead recurrente indica inter√©s persistente
                            score += 8
                            factors["returning_lead"] = True
                            factors["previous_submissions"] = prev_leads[0]
                            
                            logger.info(
                                "returning_lead_detected",
                                email=email,
                                previous_count=prev_leads[0]
                            )
            except Exception as prev_error:
                logger.warning(
                    "previous_leads_check_error",
                    email=email,
                    error=str(prev_error)
                )
            
            # Cap at 100
            score = max(0, min(score, 100))
            
            # Determinar prioridad con umbrales mejorados
            if score >= 75:
                priority = "high"
                factors["qualified"] = True
            elif score >= 50:
                priority = "medium"
            elif score >= 30:
                priority = "low"
            else:
                priority = "very_low"
            
            # Marcar como calificado si supera el umbral
            is_qualified = score >= int(params.get("lead_score_threshold", 60))
        
        result = {
            **lead_data,
            "score": score,
            "priority": priority,
                "is_qualified": is_qualified,
                "scoring_factors": factors,
                "ml_score": ml_score,
                "ml_confidence": ml_confidence,
            }
            
            logger.info(
                "lead_score_calculated",
                email=lead_data.get("email"),
                score=score,
                priority=priority,
                factors_count=len(factors),
                is_qualified=is_qualified
            )
            
            # An√°lisis avanzado: Detecci√≥n de competidores mencionados
            competitors_mentioned = []
            competitor_keywords = [
                "competidor1", "competidor2", "alternativa", "vs", "comparar",
                "mejor que", "en lugar de", "actualmente uso", "estoy usando"
            ]
            
            if message:
                message_lower = message.lower()
                for keyword in competitor_keywords:
                    if keyword in message_lower:
                        # Extraer contexto alrededor de la palabra clave
                        idx = message_lower.find(keyword)
                        context = message[max(0, idx-50):min(len(message), idx+50)]
                        competitors_mentioned.append({
                            "keyword": keyword,
                            "context": context
                        })
                        score += 5  # Bonus por mencionar competidores (indica investigaci√≥n activa)
                        factors["competitor_research"] = True
                        break
            
            # Detecci√≥n de se√±ales de presupuesto
            budget_signals = []
            budget_keywords = [
                "presupuesto", "budget", "costo", "precio", "inversi√≥n",
                "cu√°nto cuesta", "tarifa", "plan", "pago", "facturaci√≥n"
            ]
            
            if message:
                for keyword in budget_keywords:
                    if keyword in message_lower:
                        budget_signals.append(keyword)
                        score += 3  # Bonus por mencionar presupuesto
                        factors["budget_discussion"] = True
                        break
            
            # An√°lisis de fit de producto (basado en mensaje y empresa)
            product_fit_score = 0
            product_keywords = {
                "enterprise": ["empresa grande", "corporativo", "multi-usuario", "equipo grande"],
                "sme": ["peque√±a empresa", "startup", "pyme", "equipo peque√±o"],
                "technical": ["api", "integraci√≥n", "desarrollador", "t√©cnico", "c√≥digo"],
                "sales": ["ventas", "crm", "pipeline", "prospectos"],
                "marketing": ["marketing", "campa√±as", "email marketing", "automation"]
            }
            
            company_size = metadata.get("company_size", "").lower()
            industry = metadata.get("industry", "").lower()
            
            # Ajustar fit seg√∫n tama√±o de empresa
            if "enterprise" in company_size or "large" in company_size:
                product_fit_score += 5
                factors["enterprise_fit"] = True
            elif "small" in company_size or "startup" in company_size:
                product_fit_score += 3
                factors["sme_fit"] = True
            
            # Ajustar fit seg√∫n industria
            high_value_industries = ["technology", "finance", "healthcare", "saas"]
            if any(ind in industry for ind in high_value_industries):
                product_fit_score += 5
                factors["high_value_industry"] = True
            
            # Ajustar fit seg√∫n palabras clave en mensaje
            if message:
                for category, keywords in product_keywords.items():
                    if any(kw in message_lower for kw in keywords):
                        product_fit_score += 3
                        factors[f"{category}_fit"] = True
            
            score += min(product_fit_score, 10)  # Max 10 puntos por fit
            
            # Predicci√≥n de conversi√≥n basada en factores
            conversion_probability = 0.0
            
            # Base probability basada en score
            conversion_probability = min(score / 100.0, 0.95)
            
            # Ajustes basados en se√±ales espec√≠ficas
            if factors.get("high_intent_message"):
                conversion_probability += 0.15
            if factors.get("urgency_detected"):
                conversion_probability += 0.10
            if factors.get("budget_discussion"):
                conversion_probability += 0.08
            if factors.get("has_abandoned_cart"):
                conversion_probability += 0.12
            if factors.get("returning_lead"):
                conversion_probability += 0.10
            if factors.get("high_engagement"):
                conversion_probability += 0.05
            
            conversion_probability = min(conversion_probability, 0.95)  # Cap at 95%
            
            # Estimaci√≥n de LTV (Lifetime Value) basada en score y factores
            base_ltv = 1000  # LTV base en USD
            ltv_multiplier = 1.0
            
            if score >= 75:
                ltv_multiplier = 3.0
            elif score >= 50:
                ltv_multiplier = 2.0
            elif score >= 30:
                ltv_multiplier = 1.5
            
            # Ajustes por factores
            if factors.get("enterprise_fit"):
                ltv_multiplier *= 2.0
            if factors.get("high_value_industry"):
                ltv_multiplier *= 1.5
            if factors.get("high_value_cart"):
                ltv_multiplier *= 1.3
            
            estimated_ltv = base_ltv * ltv_multiplier
            
            # Recomendaciones autom√°ticas basadas en an√°lisis
            recommendations = []
            
            if factors.get("urgency_detected"):
                recommendations.append({
                    "action": "contact_immediately",
                    "priority": "high",
                    "reason": "Urgencia detectada en mensaje",
                    "suggested_channel": "phone"
                })
            
            if factors.get("budget_discussion"):
                recommendations.append({
                    "action": "send_pricing",
                    "priority": "high",
                    "reason": "Mencion√≥ presupuesto",
                    "suggested_channel": "email"
                })
            
            if factors.get("has_abandoned_cart"):
                recommendations.append({
                    "action": "recover_cart",
                    "priority": "high",
                    "reason": "Tiene carrito abandonado",
                    "suggested_channel": "email"
                })
            
            if factors.get("competitor_research"):
                recommendations.append({
                    "action": "competitive_analysis",
                    "priority": "medium",
                    "reason": "Mencion√≥ competidores",
                    "suggested_channel": "email"
                })
            
            if not recommendations:
                recommendations.append({
                    "action": "standard_followup",
                    "priority": "medium",
                    "reason": "Seguimiento est√°ndar",
                    "suggested_channel": "email"
                })
            
            # Mejor momento para contactar (basado en comportamiento web si disponible)
            best_contact_time = None
            if web_behavior:
                # Si hay datos de comportamiento, sugerir contacto en horario similar
                last_visit_hour = web_behavior.get("last_visit_hour")
                if last_visit_hour:
                    best_contact_time = {
                        "hour": last_visit_hour,
                        "confidence": "medium",
                        "reason": "Basado en √∫ltimo acceso"
                    }
            
            # Si no hay datos, usar horario est√°ndar (10-11 AM, 2-3 PM)
            if not best_contact_time:
                best_contact_time = {
                    "hour": 10,
                    "confidence": "low",
                    "reason": "Horario est√°ndar recomendado"
                }
            
            # An√°lisis de redes sociales (LinkedIn, Twitter, etc.)
            social_analysis = {}
            enriched_data = lead_data.get("enriched_data", {})
            
            # LinkedIn analysis
            linkedin_data = enriched_data.get("linkedin", {})
            if linkedin_data:
                linkedin_score = 0
                
                # Bonus por tener perfil de LinkedIn
                if linkedin_data.get("profile_url"):
                    linkedin_score += 5
                    social_analysis["has_linkedin"] = True
                
                # Bonus por conexiones
                connections = linkedin_data.get("connections", 0)
                if connections >= 500:
                    linkedin_score += 10
                    social_analysis["high_network"] = True
                elif connections >= 100:
                    linkedin_score += 5
                    social_analysis["medium_network"] = True
                
                # Bonus por seniority
                seniority = linkedin_data.get("seniority", "").lower()
                if any(title in seniority for title in ["director", "vp", "cfo", "ceo", "cto", "president"]):
                    linkedin_score += 15
                    social_analysis["executive_level"] = True
                    score += 15  # Bonus directo al score
                    factors["executive_lead"] = True
                elif any(title in seniority for title in ["manager", "head", "lead", "senior"]):
                    linkedin_score += 8
                    social_analysis["manager_level"] = True
                    score += 8
                    factors["manager_lead"] = True
                
                # Bonus por industria relevante
                linkedin_industry = linkedin_data.get("industry", "").lower()
                if any(ind in linkedin_industry for ind in high_value_industries):
                    linkedin_score += 5
                    social_analysis["relevant_industry"] = True
                
                social_analysis["linkedin_score"] = linkedin_score
                score += min(linkedin_score, 20)  # Max 20 puntos de LinkedIn
            
            # An√°lisis de similaridad con leads convertidos
            similarity_analysis = {}
            try:
                with hook.get_conn() as conn:
                    with conn.cursor() as cur:
                        # Buscar leads convertidos similares
                        cur.execute("""
                            SELECT 
                                COUNT(*) as similar_count,
                                AVG(score) as avg_score,
                                AVG(estimated_value) as avg_value,
                                AVG(EXTRACT(EPOCH FROM (closed_at - created_at))/86400) as avg_days_to_close
                            FROM leads
                            WHERE stage = 'closed_won'
                                AND (
                                    (company IS NOT NULL AND company = %s)
                                    OR (email_domain = %s)
                                    OR (industry = %s AND industry IS NOT NULL)
                                )
                                AND created_at > NOW() - INTERVAL '90 days'
                        """, (
                            company or "",
                            domain if email else "",
                            industry or ""
                        ))
                        
                        similar_result = cur.fetchone()
                        
                        if similar_result and similar_result[0] > 0:
                            similar_count = similar_result[0] or 0
                            avg_score = float(similar_result[1] or 0)
                            avg_value = float(similar_result[2] or 0)
                            avg_days = float(similar_result[3] or 0)
                            
                            similarity_analysis = {
                                "similar_converted_leads": int(similar_count),
                                "average_score": round(avg_score, 2),
                                "average_value": round(avg_value, 2),
                                "average_days_to_close": round(avg_days, 1),
                                "similarity_confidence": "high" if similar_count >= 3 else "medium"
                            }
                            
                            # Bonus por tener leads similares convertidos
                            if similar_count >= 3:
                                score += 12
                                factors["high_similarity_to_won"] = True
                                conversion_probability += 0.10
                            elif similar_count >= 1:
                                score += 6
                                factors["similarity_to_won"] = True
                                conversion_probability += 0.05
                            
                            logger.info(
                                "similarity_analysis",
                                email=email,
                                similar_count=similar_count,
                                avg_score=avg_score
                            )
            except Exception as sim_error:
                logger.warning(
                    "similarity_analysis_error",
                    email=email,
                    error=str(sim_error)
                )
            
            # Predicci√≥n de tiempo hasta conversi√≥n (basado en leads similares)
            predicted_time_to_conversion = None
            if similarity_analysis.get("average_days_to_close"):
                predicted_time_to_conversion = {
                    "days": int(similarity_analysis["average_days_to_close"]),
                    "confidence": similarity_analysis.get("similarity_confidence", "low"),
                    "method": "historical_similarity"
                }
            else:
                # Predicci√≥n basada en score y factores
                base_days = 45  # Base de 45 d√≠as
                
                if score >= 75:
                    predicted_days = base_days * 0.6  # 27 d√≠as
                elif score >= 50:
                    predicted_days = base_days * 0.8  # 36 d√≠as
                else:
                    predicted_days = base_days * 1.2  # 54 d√≠as
                
                # Ajustes por factores
                if factors.get("urgency_detected"):
                    predicted_days *= 0.7  # 30% m√°s r√°pido
                if factors.get("budget_discussion"):
                    predicted_days *= 0.8  # 20% m√°s r√°pido
                if factors.get("has_abandoned_cart"):
                    predicted_days *= 0.75  # 25% m√°s r√°pido
                
                predicted_time_to_conversion = {
                    "days": int(predicted_days),
                    "confidence": "medium",
                    "method": "score_based"
                }
            
            # Predicci√≥n de riesgo de churn (para leads en pipeline)
            churn_risk_score = 0.0
            churn_factors = []
            
            # Factores que aumentan riesgo de churn
            days_since_contact = metadata.get("days_since_last_contact", 0)
            if days_since_contact > 30:
                churn_risk_score += 0.3
                churn_factors.append("no_contact_30_days")
            elif days_since_contact > 14:
                churn_risk_score += 0.15
                churn_factors.append("no_contact_14_days")
            
            if score < 30:
                churn_risk_score += 0.2
                churn_factors.append("low_score")
            
            if not factors.get("high_engagement"):
                churn_risk_score += 0.1
                churn_factors.append("low_engagement")
            
            # Factores que reducen riesgo
            if factors.get("returning_lead"):
                churn_risk_score -= 0.15
            if factors.get("has_abandoned_cart"):
                churn_risk_score -= 0.10
            if factors.get("budget_discussion"):
                churn_risk_score -= 0.10
            
            churn_risk_score = max(0.0, min(churn_risk_score, 1.0))
            
            # An√°lisis de cohorte (agrupar por fecha de captura)
            cohort_analysis = {}
            try:
                with hook.get_conn() as conn:
                    with conn.cursor() as cur:
                        # Obtener cohorte del lead (semana de captura)
                        cur.execute("""
                            SELECT 
                                DATE_TRUNC('week', created_at) as cohort_week,
                                COUNT(*) as cohort_size,
                                COUNT(*) FILTER (WHERE stage = 'closed_won') as cohort_won,
                                AVG(score) as cohort_avg_score
                            FROM leads
                            WHERE DATE_TRUNC('week', created_at) = (
                                SELECT DATE_TRUNC('week', created_at)
                                FROM leads
                                WHERE email = %s
                                LIMIT 1
                            )
                            GROUP BY DATE_TRUNC('week', created_at)
                        """, (email,))
                        
                        cohort_result = cur.fetchone()
                        
                        if cohort_result:
                            cohort_week = cohort_result[0]
                            cohort_size = cohort_result[1] or 0
                            cohort_won = cohort_result[2] or 0
                            cohort_avg_score = float(cohort_result[3] or 0)
                            
                            cohort_conversion_rate = (cohort_won / cohort_size * 100) if cohort_size > 0 else 0
                            
                            cohort_analysis = {
                                "cohort_week": cohort_week.isoformat() if cohort_week else None,
                                "cohort_size": int(cohort_size),
                                "cohort_conversions": int(cohort_won),
                                "cohort_conversion_rate": round(cohort_conversion_rate, 2),
                                "cohort_avg_score": round(cohort_avg_score, 2),
                                "lead_performance_vs_cohort": "above" if score > cohort_avg_score else "below"
                            }
            except Exception as cohort_error:
                logger.warning(
                    "cohort_analysis_error",
                    email=email,
                    error=str(cohort_error)
                )
            
            # An√°lisis de atribuci√≥n multi-touch
            attribution_analysis = {}
            utm_source = lead_data.get("utm_source", "")
            utm_medium = lead_data.get("utm_medium", "")
            utm_campaign = lead_data.get("utm_campaign", "")
            
            if utm_source or utm_medium or utm_campaign:
                attribution_analysis = {
                    "first_touch": {
                        "source": utm_source or "direct",
                        "medium": utm_medium or "none",
                        "campaign": utm_campaign or "none"
                    },
                    "last_touch": {
                        "source": utm_source or "direct",
                        "medium": utm_medium or "none",
                        "campaign": utm_campaign or "none"
                    },
                    "attribution_model": "last_touch"  # Por ahora, se puede expandir
                }
                
                # Bonus por campa√±as espec√≠ficas
                if utm_campaign:
                    high_value_campaigns = ["enterprise", "demo", "trial", "enterprise_trial"]
                    if any(camp in utm_campaign.lower() for camp in high_value_campaigns):
                        score += 8
                        factors["high_value_campaign"] = True
            
            # An√°lisis de tendencias (comparar con per√≠odo anterior)
            trend_analysis = {}
            try:
                with hook.get_conn() as conn:
                    with conn.cursor() as cur:
                        # Comparar con per√≠odo anterior (√∫ltimos 7 d√≠as vs anteriores 7 d√≠as)
                        cur.execute("""
                            SELECT 
                                COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '7 days') as last_7_days,
                                COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '14 days' 
                                    AND created_at < NOW() - INTERVAL '7 days') as previous_7_days,
                                AVG(score) FILTER (WHERE created_at >= NOW() - INTERVAL '7 days') as avg_score_recent,
                                AVG(score) FILTER (WHERE created_at >= NOW() - INTERVAL '14 days' 
                                    AND created_at < NOW() - INTERVAL '7 days') as avg_score_previous
                            FROM leads
                            WHERE source = %s OR utm_source = %s
                        """, (source, utm_source or ""))
                        
                        trend_result = cur.fetchone()
                        
                        if trend_result:
                            recent_count = trend_result[0] or 0
                            previous_count = trend_result[1] or 0
                            recent_avg = float(trend_result[2] or 0)
                            previous_avg = float(trend_result[3] or 0)
                            
                            volume_change = ((recent_count - previous_count) / previous_count * 100) if previous_count > 0 else 0
                            score_change = recent_avg - previous_avg
                            
                            trend_analysis = {
                                "volume_trend": "increasing" if volume_change > 10 else "decreasing" if volume_change < -10 else "stable",
                                "volume_change_pct": round(volume_change, 2),
                                "score_trend": "improving" if score_change > 5 else "declining" if score_change < -5 else "stable",
                                "score_change": round(score_change, 2),
                                "recent_volume": int(recent_count),
                                "previous_volume": int(previous_count)
                            }
            except Exception as trend_error:
                logger.warning(
                    "trend_analysis_error",
                    email=email,
                    error=str(trend_error)
                )
            
            # Alertas inteligentes
            intelligent_alerts = []
            
            if conversion_probability > 0.7:
                intelligent_alerts.append({
                    "type": "high_conversion_probability",
                    "severity": "high",
                    "message": f"Alta probabilidad de conversi√≥n ({conversion_probability:.1%})",
                    "action": "prioritize_contact"
                })
            
            if churn_risk_score > 0.5:
                intelligent_alerts.append({
                    "type": "high_churn_risk",
                    "severity": "medium",
                    "message": f"Riesgo de churn alto ({churn_risk_score:.1%})",
                    "action": "re_engage_immediately"
                })
            
            if factors.get("urgency_detected"):
                intelligent_alerts.append({
                    "type": "urgency_detected",
                    "severity": "critical",
                    "message": "Urgencia detectada en mensaje",
                    "action": "contact_immediately"
                })
            
            if estimated_ltv > 5000:
                intelligent_alerts.append({
                    "type": "high_ltv_lead",
                    "severity": "high",
                    "message": f"Lead de alto valor (LTV estimado: ${estimated_ltv:,.2f})",
                    "action": "assign_to_best_sales_rep"
                })
            
            # An√°lisis de Buyer Persona
            buyer_persona = {}
            persona_score = {}
            
            # Detectar persona basada en m√∫ltiples factores
            if factors.get("executive_lead") or social_analysis.get("executive_level"):
                buyer_persona["type"] = "decision_maker"
                buyer_persona["confidence"] = "high"
                persona_score["decision_maker"] = 90
            elif factors.get("manager_lead") or social_analysis.get("manager_level"):
                buyer_persona["type"] = "influencer"
                buyer_persona["confidence"] = "medium"
                persona_score["influencer"] = 70
            else:
                buyer_persona["type"] = "end_user"
                buyer_persona["confidence"] = "low"
                persona_score["end_user"] = 50
            
            # Tama√±o de empresa
            if "enterprise" in company_size or "large" in company_size:
                buyer_persona["company_size"] = "enterprise"
                persona_score["enterprise"] = 85
            elif "small" in company_size or "startup" in company_size:
                buyer_persona["company_size"] = "sme"
                persona_score["sme"] = 60
            else:
                buyer_persona["company_size"] = "mid_market"
                persona_score["mid_market"] = 75
            
            # Industria
            buyer_persona["industry"] = industry or "unknown"
            if any(ind in industry for ind in high_value_industries):
                persona_score["high_value_industry"] = 80
            
            # Calcular score de persona
            persona_final_score = sum(persona_score.values()) / len(persona_score) if persona_score else 50
            buyer_persona["persona_score"] = round(persona_final_score, 2)
            
            # Scoring de Readiness to Buy (0-100)
            readiness_score = 0
            
            # Se√±ales de alta readiness
            if factors.get("budget_discussion"):
                readiness_score += 30
            if factors.get("urgency_detected"):
                readiness_score += 25
            if factors.get("high_intent_message"):
                readiness_score += 20
            if factors.get("has_abandoned_cart"):
                readiness_score += 15
            if factors.get("competitor_research"):
                readiness_score += 10
            
            # Se√±ales de medium readiness
            if factors.get("returning_lead"):
                readiness_score += 10
            if factors.get("high_engagement"):
                readiness_score += 8
            if factors.get("returning_visitor"):
                readiness_score += 7
            
            # Base score basado en score general
            readiness_score += min(score // 2, 30)  # Max 30 puntos del score base
            
            readiness_score = min(readiness_score, 100)
            readiness_level = (
                "high" if readiness_score >= 70 else
                "medium" if readiness_score >= 40 else
                "low"
            )
            
            # An√°lisis de Engagement Velocity (qu√© tan r√°pido est√° avanzando)
            engagement_velocity = {}
            
            if web_behavior:
                # Calcular velocidad basada en interacciones recientes
                first_visit = web_behavior.get("first_visit_date")
                last_visit = web_behavior.get("last_visit_date")
                total_visits = web_behavior.get("return_visits", 0) + 1
                
                if first_visit and last_visit:
                    try:
                        from dateutil import parser
                        first = parser.parse(first_visit)
                        last = parser.parse(last_visit)
                        days_between = (last - first).days
                        
                        if days_between > 0:
                            visits_per_day = total_visits / days_between
                            engagement_velocity = {
                                "visits_per_day": round(visits_per_day, 2),
                                "velocity_score": min(visits_per_day * 10, 100),
                                "velocity_level": (
                                    "high" if visits_per_day > 0.5 else
                                    "medium" if visits_per_day > 0.2 else
                                    "low"
                                ),
                                "days_active": days_between,
                                "total_visits": total_visits
                            }
                            
                            # Bonus por alta velocidad
                            if visits_per_day > 0.5:
                                score += 5
                                factors["high_engagement_velocity"] = True
                    except Exception:
                        pass
            
            # Predicci√≥n de Deal Size (tama√±o del deal)
            predicted_deal_size = {}
            
            # Base en LTV estimado
            base_deal_size = estimated_ltv * 0.3  # 30% del LTV como deal inicial
            
            # Ajustes por factores
            if factors.get("enterprise_fit"):
                base_deal_size *= 2.0
            if factors.get("high_value_industry"):
                base_deal_size *= 1.5
            if factors.get("executive_lead"):
                base_deal_size *= 1.8
            
            # Usar datos hist√≥ricos si hay leads similares
            if similarity_analysis.get("average_value"):
                historical_avg = similarity_analysis["average_value"]
                # Promedio ponderado: 60% hist√≥rico, 40% estimado
                predicted_deal_size_value = historical_avg * 0.6 + base_deal_size * 0.4
            else:
                predicted_deal_size_value = base_deal_size
            
            predicted_deal_size = {
                "estimated_value": round(predicted_deal_size_value, 2),
                "confidence": "high" if similarity_analysis.get("similar_converted_leads", 0) >= 3 else "medium",
                "method": "historical_similarity" if similarity_analysis.get("average_value") else "ltv_based",
                "currency": currency
            }
            
            # An√°lisis de ROI por fuente (si hay datos hist√≥ricos)
            roi_analysis = {}
            try:
                with hook.get_conn() as conn:
                    with conn.cursor() as cur:
                        # Calcular ROI de la fuente
                        cur.execute("""
                            SELECT 
                                COUNT(*) as total_leads,
                                COUNT(*) FILTER (WHERE stage = 'closed_won') as won_leads,
                                AVG(estimated_value) FILTER (WHERE stage = 'closed_won') as avg_deal_value,
                                SUM(estimated_value) FILTER (WHERE stage = 'closed_won') as total_revenue
                            FROM leads
                            WHERE (source = %s OR utm_source = %s)
                                AND created_at > NOW() - INTERVAL '90 days'
                        """, (source, utm_source or ""))
                        
                        roi_result = cur.fetchone()
                        
                        if roi_result and roi_result[0] > 0:
                            total_leads_source = roi_result[0] or 0
                            won_leads_source = roi_result[1] or 0
                            avg_deal = float(roi_result[2] or 0)
                            total_revenue = float(roi_result[3] or 0)
                            
                            # Asumir costo por lead (puede venir de metadata)
                            cost_per_lead = metadata.get("cost_per_lead", 10.0)  # Default $10
                            total_cost = total_leads_source * cost_per_lead
                            
                            roi_pct = ((total_revenue - total_cost) / total_cost * 100) if total_cost > 0 else 0
                            conversion_rate = (won_leads_source / total_leads_source * 100) if total_leads_source > 0 else 0
                            
                            roi_analysis = {
                                "source": source or utm_source or "unknown",
                                "total_leads": int(total_leads_source),
                                "conversions": int(won_leads_source),
                                "conversion_rate": round(conversion_rate, 2),
                                "avg_deal_value": round(avg_deal, 2),
                                "total_revenue": round(total_revenue, 2),
                                "total_cost": round(total_cost, 2),
                                "roi_percentage": round(roi_pct, 2),
                                "roi_category": (
                                    "exceptional" if roi_pct > 500 else
                                    "excellent" if roi_pct > 200 else
                                    "good" if roi_pct > 100 else
                                    "positive" if roi_pct > 0 else
                                    "negative"
                                )
                            }
                            
                            # Bonus por fuente con buen ROI
                            if roi_pct > 200:
                                score += 5
                                factors["high_roi_source"] = True
            except Exception as roi_error:
                logger.warning(
                    "roi_analysis_error",
                    email=email,
                    error=str(roi_error)
                )
            
            # An√°lisis de calidad de datos mejorado
            data_quality_analysis = {}
            data_quality_score = 0
            data_quality_issues = []
            
            # Completitud (40 puntos)
            completeness = 0
            if email:
                completeness += 10
            if lead_data.get("first_name") and lead_data.get("last_name"):
                completeness += 10
            if lead_data.get("phone"):
                completeness += 10
            if company:
                completeness += 10
            else:
                data_quality_issues.append("missing_company")
            
            data_quality_score += completeness
            
            # Validez (30 puntos)
            validity = 0
            if email_validation.get("valid_mx"):
                validity += 15
            else:
                data_quality_issues.append("invalid_email")
            
            if company_validation.get("validated"):
                validity += 15
            else:
                if company:
                    data_quality_issues.append("unvalidated_company")
            
            data_quality_score += validity
            
            # Enriquecimiento (20 puntos)
            enrichment_quality = 0
            if lead_data.get("is_enriched"):
                enrichment_quality += 10
                if enriched_data.get("clearbit"):
                    enrichment_quality += 10
            else:
                data_quality_issues.append("not_enriched")
            
            data_quality_score += enrichment_quality
            
            # Consistencia (10 puntos)
            consistency = 10
            # Verificar consistencia entre email domain y company domain
            if email and company:
                email_domain = email.split("@")[1] if "@" in email else ""
                company_domain = company_validation.get("domain", "").lower()
                if company_domain and email_domain:
                    if company_domain not in email_domain and email_domain not in company_domain:
                        consistency -= 5
                        data_quality_issues.append("domain_mismatch")
            
            data_quality_score += consistency
            data_quality_score = max(0, min(data_quality_score, 100))
            
            data_quality_analysis = {
                "data_quality_score": data_quality_score,
                "data_quality_grade": (
                    "A" if data_quality_score >= 80 else
                    "B" if data_quality_score >= 60 else
                    "C" if data_quality_score >= 40 else
                    "D" if data_quality_score >= 20 else "F"
                ),
                "completeness": completeness,
                "validity": validity,
                "enrichment_quality": enrichment_quality,
                "consistency": consistency,
                "issues": data_quality_issues,
                "issues_count": len(data_quality_issues)
            }
            
            # Penalizar score si calidad de datos es baja
            if data_quality_score < 50:
                score -= 5
                factors["low_data_quality"] = True
            
            # Predicci√≥n de mejor producto/servicio
            product_recommendation = {}
            
            # Basado en palabras clave en mensaje y metadata
            if message:
                message_lower = message.lower()
                
                # Detectar necesidades espec√≠ficas
                if any(kw in message_lower for kw in ["api", "integraci√≥n", "desarrollador", "t√©cnico"]):
                    product_recommendation["recommended_product"] = "api_integration"
                    product_recommendation["confidence"] = "high"
                elif any(kw in message_lower for kw in ["equipo", "multi-usuario", "empresa grande"]):
                    product_recommendation["recommended_product"] = "enterprise_plan"
                    product_recommendation["confidence"] = "high"
                elif any(kw in message_lower for kw in ["peque√±a", "startup", "individual"]):
                    product_recommendation["recommended_product"] = "starter_plan"
                    product_recommendation["confidence"] = "medium"
                elif any(kw in message_lower for kw in ["marketing", "campa√±as", "email"]):
                    product_recommendation["recommended_product"] = "marketing_suite"
                    product_recommendation["confidence"] = "medium"
                else:
                    product_recommendation["recommended_product"] = "standard_plan"
                    product_recommendation["confidence"] = "low"
            else:
                # Basado en empresa y score
                if factors.get("enterprise_fit"):
                    product_recommendation["recommended_product"] = "enterprise_plan"
                elif score >= 60:
                    product_recommendation["recommended_product"] = "professional_plan"
                else:
                    product_recommendation["recommended_product"] = "starter_plan"
                product_recommendation["confidence"] = "low"
            
            # An√°lisis de sentimiento avanzado y detecci√≥n de emociones
            sentiment_analysis = {}
            emotion_detection = {}
            
            if message and len(message) > 10:
                message_lower = message.lower()
                
                # Detecci√≥n de emociones
                emotions = {
                    "excited": ["emocionado", "entusiasmado", "genial", "excelente", "perfecto", "incre√≠ble"],
                    "interested": ["interesado", "me gusta", "me interesa", "quiero saber", "informaci√≥n"],
                    "urgent": ["urgente", "r√°pido", "inmediato", "ya", "ahora", "pronto", "necesito ya"],
                    "frustrated": ["problema", "dif√≠cil", "complicado", "no funciona", "error"],
                    "curious": ["c√≥mo", "qu√©", "cu√°ndo", "d√≥nde", "por qu√©", "pregunta"],
                    "positive": ["bueno", "excelente", "genial", "perfecto", "me encanta", "recomendado"],
                    "negative": ["malo", "no me gusta", "problema", "error", "dif√≠cil"]
                }
                
                detected_emotions = []
                emotion_scores = {}
                
                for emotion, keywords in emotions.items():
                    count = sum(1 for kw in keywords if kw in message_lower)
                    if count > 0:
                        detected_emotions.append(emotion)
                        emotion_scores[emotion] = min(count * 10, 50)  # Max 50 por emoci√≥n
                
                # Calcular sentimiento general
                positive_score = emotion_scores.get("excited", 0) + emotion_scores.get("interested", 0) + emotion_scores.get("positive", 0)
                negative_score = emotion_scores.get("frustrated", 0) + emotion_scores.get("negative", 0)
                
                overall_sentiment = "positive" if positive_score > negative_score else "negative" if negative_score > positive_score else "neutral"
                sentiment_score = positive_score - negative_score  # -50 a +50, normalizado a -1 a 1
                
                sentiment_analysis = {
                    "overall_sentiment": overall_sentiment,
                    "sentiment_score": round(sentiment_score / 50.0, 2) if sentiment_score != 0 else 0.0,  # Normalizado -1 a 1
                    "positive_score": positive_score,
                    "negative_score": negative_score,
                    "detected_emotions": detected_emotions,
                    "emotion_scores": emotion_scores
                }
                
                emotion_detection = {
                    "primary_emotion": detected_emotions[0] if detected_emotions else "neutral",
                    "emotions": detected_emotions,
                    "emotion_intensity": max(emotion_scores.values()) if emotion_scores else 0
                }
                
                # Bonus por sentimiento positivo
                if overall_sentiment == "positive":
                    score += 3
                    factors["positive_sentiment_detected"] = True
                
                # Penalizar sentimiento negativo fuerte
                if overall_sentiment == "negative" and negative_score > 20:
                    score -= 5
                    factors["negative_sentiment_detected"] = True
            
            # An√°lisis de keywords y temas
            keyword_analysis = {}
            topic_extraction = {}
            
            if message:
                message_lower = message.lower()
                
                # Categor√≠as de keywords
                keyword_categories = {
                    "technical": ["api", "integraci√≥n", "desarrollador", "c√≥digo", "t√©cnico", "sdk", "webhook"],
                    "business": ["empresa", "negocio", "equipo", "organizaci√≥n", "corporativo"],
                    "pricing": ["precio", "costo", "tarifa", "plan", "facturaci√≥n", "pago", "presupuesto"],
                    "features": ["funcionalidad", "caracter√≠stica", "feature", "funci√≥n", "capacidad"],
                    "support": ["soporte", "ayuda", "asistencia", "documentaci√≥n", "tutorial"],
                    "integration": ["conectar", "sincronizar", "integrar", "compatible", "conexi√≥n"],
                    "security": ["seguridad", "privacidad", "encriptaci√≥n", "protecci√≥n", "compliance"],
                    "scalability": ["escalar", "crecer", "volumen", "tr√°fico", "usuarios", "escalabilidad"]
                }
                
                detected_keywords = {}
                topic_scores = {}
                
                for category, keywords in keyword_categories.items():
                    matches = [kw for kw in keywords if kw in message_lower]
                    if matches:
                        detected_keywords[category] = matches
                        topic_scores[category] = len(matches) * 10
                
                # Identificar tema principal
                primary_topic = max(topic_scores.items(), key=lambda x: x[1])[0] if topic_scores else "general"
                
                keyword_analysis = {
                    "detected_keywords": detected_keywords,
                    "keyword_count": sum(len(kws) for kws in detected_keywords.values()),
                    "categories_found": list(detected_keywords.keys())
                }
                
                topic_extraction = {
                    "primary_topic": primary_topic,
                    "topic_scores": topic_scores,
                    "topics": list(topic_scores.keys())
                }
                
                # Bonus por keywords relevantes
                if len(detected_keywords) >= 3:
                    score += 5
                    factors["multi_topic_interest"] = True
            
            # An√°lisis de tecnolog√≠as usadas (si est√° disponible en metadata)
            technology_analysis = {}
            tech_stack = metadata.get("technologies", []) or metadata.get("tech_stack", [])
            
            if tech_stack:
                # Tecnolog√≠as de alto valor
                high_value_techs = ["aws", "azure", "gcp", "kubernetes", "docker", "terraform", "salesforce", "hubspot"]
                detected_high_value = [tech for tech in tech_stack if any(hvt in tech.lower() for hvt in high_value_techs)]
                
                technology_analysis = {
                    "technologies": tech_stack,
                    "tech_count": len(tech_stack),
                    "high_value_technologies": detected_high_value,
                    "tech_maturity": "high" if len(detected_high_value) >= 3 else "medium" if len(detected_high_value) >= 1 else "low"
                }
                
                # Bonus por stack tecnol√≥gico avanzado
                if len(detected_high_value) >= 3:
                    score += 8
                    factors["advanced_tech_stack"] = True
                elif len(detected_high_value) >= 1:
                    score += 4
                    factors["modern_tech_stack"] = True
            
            # Predicci√≥n de ciclo de venta (sales cycle)
            sales_cycle_prediction = {}
            
            # Basado en factores y datos hist√≥ricos
            base_cycle_days = 60  # Ciclo base de 60 d√≠as
            
            # Ajustes por factores
            if factors.get("urgency_detected"):
                base_cycle_days *= 0.5  # 50% m√°s r√°pido (30 d√≠as)
            if factors.get("budget_discussion"):
                base_cycle_days *= 0.7  # 30% m√°s r√°pido (42 d√≠as)
            if factors.get("executive_lead"):
                base_cycle_days *= 0.8  # 20% m√°s r√°pido (48 d√≠as)
            if factors.get("has_abandoned_cart"):
                base_cycle_days *= 0.6  # 40% m√°s r√°pido (36 d√≠as)
            
            # Usar datos hist√≥ricos si hay leads similares
            if similarity_analysis.get("average_days_to_close"):
                historical_avg = similarity_analysis["average_days_to_close"]
                # Promedio ponderado: 70% hist√≥rico, 30% estimado
                predicted_cycle = historical_avg * 0.7 + base_cycle_days * 0.3
            else:
                predicted_cycle = base_cycle_days
            
            sales_cycle_prediction = {
                "predicted_cycle_days": int(predicted_cycle),
                "cycle_category": (
                    "fast" if predicted_cycle < 30 else
                    "medium" if predicted_cycle < 60 else
                    "long"
                ),
                "confidence": "high" if similarity_analysis.get("similar_converted_leads", 0) >= 3 else "medium",
                "method": "historical_similarity" if similarity_analysis.get("average_days_to_close") else "factor_based"
            }
            
            # An√°lisis de fit cultural (basado en empresa y mensaje)
            cultural_fit_analysis = {}
            
            # Indicadores de fit cultural
            cultural_indicators = {
                "innovation": ["innovaci√≥n", "nuevo", "moderno", "tecnolog√≠a", "avanzado"],
                "growth": ["crecer", "expandir", "escalar", "crecimiento", "desarrollo"],
                "efficiency": ["eficiencia", "optimizar", "automatizar", "mejorar", "productividad"],
                "collaboration": ["equipo", "colaboraci√≥n", "trabajar juntos", "comunicaci√≥n"]
            }
            
            cultural_scores = {}
            if message:
                message_lower = message.lower()
                for indicator, keywords in cultural_indicators.items():
                    matches = sum(1 for kw in keywords if kw in message_lower)
                    if matches > 0:
                        cultural_scores[indicator] = min(matches * 15, 50)
            
            # Fit cultural basado en industria tambi√©n
            if industry:
                if "technology" in industry or "saas" in industry:
                    cultural_scores["innovation"] = cultural_scores.get("innovation", 0) + 20
                if "finance" in industry:
                    cultural_scores["efficiency"] = cultural_scores.get("efficiency", 0) + 20
            
            cultural_fit_score = sum(cultural_scores.values()) / len(cultural_scores) if cultural_scores else 50
            
            cultural_fit_analysis = {
                "cultural_fit_score": round(cultural_fit_score, 2),
                "cultural_indicators": cultural_scores,
                "primary_cultural_value": max(cultural_scores.items(), key=lambda x: x[1])[0] if cultural_scores else "unknown",
                "fit_level": (
                    "high" if cultural_fit_score >= 60 else
                    "medium" if cultural_fit_score >= 40 else
                    "low"
                )
            }
            
            # An√°lisis de competencia m√°s detallado
            competitive_analysis = {}
            
            if competitors_mentioned:
                # Competidores comunes (puede venir de metadata o detectarse)
                known_competitors = metadata.get("known_competitors", [])
                
                competitive_analysis = {
                    "competitors_detected": len(competitors_mentioned),
                    "competitor_mentions": competitors_mentioned,
                    "known_competitors": known_competitors,
                    "competitive_research_level": (
                        "high" if len(competitors_mentioned) >= 2 else
                        "medium" if len(competitors_mentioned) >= 1 else
                        "low"
                    ),
                    "recommended_action": "competitive_comparison" if len(competitors_mentioned) > 0 else "standard_pitch"
                }
                
                # Bonus adicional por investigaci√≥n competitiva profunda
                if len(competitors_mentioned) >= 2:
                    score += 3
                    factors["deep_competitive_research"] = True
            
            # An√°lisis de comportamiento predictivo
            behavioral_prediction = {}
            
            # Basado en web behavior y engagement hist√≥rico
            web_behavior = metadata.get("web_behavior", {})
            engagement_history = metadata.get("engagement_history", [])
            
            # Patrones de comportamiento
            behavior_patterns = {
                "high_intent": False,
                "research_phase": False,
                "comparison_phase": False,
                "decision_phase": False,
                "urgency_detected": False
            }
            
            # Detectar fase del buyer journey
            if web_behavior.get("pages_visited", 0) >= 5:
                behavior_patterns["research_phase"] = True
            if web_behavior.get("return_visits", 0) >= 3:
                behavior_patterns["comparison_phase"] = True
            if web_behavior.get("cta_clicks", 0) >= 2:
                behavior_patterns["decision_phase"] = True
            if factors.get("urgency_detected"):
                behavior_patterns["urgency_detected"] = True
                behavior_patterns["high_intent"] = True
            
            # Calcular probabilidad de acci√≥n pr√≥xima
            action_probability = 0.3  # Base
            
            if behavior_patterns["decision_phase"]:
                action_probability += 0.4
            if behavior_patterns["comparison_phase"]:
                action_probability += 0.2
            if behavior_patterns["research_phase"]:
                action_probability += 0.1
            if behavior_patterns["urgency_detected"]:
                action_probability += 0.3
            
            action_probability = min(action_probability, 1.0)
            
            # Predicci√≥n de pr√≥xima acci√≥n esperada
            next_action_prediction = "continue_research"
            if action_probability >= 0.7:
                next_action_prediction = "likely_to_convert"
            elif action_probability >= 0.5:
                next_action_prediction = "request_demo"
            elif action_probability >= 0.3:
                next_action_prediction = "request_pricing"
            
            behavioral_prediction = {
                "behavior_patterns": behavior_patterns,
                "action_probability": round(action_probability, 3),
                "next_action_prediction": next_action_prediction,
                "buyer_journey_stage": (
                    "decision" if behavior_patterns["decision_phase"] else
                    "comparison" if behavior_patterns["comparison_phase"] else
                    "research" if behavior_patterns["research_phase"] else
                    "awareness"
                ),
                "days_to_next_action": int(30 / action_probability) if action_probability > 0 else 90
            }
            
            # Bonus por alta probabilidad de acci√≥n
            if action_probability >= 0.7:
                score += 8
                factors["high_action_probability"] = True
            
            # Scoring de intenci√≥n de compra avanzado
            purchase_intent_scoring = {}
            
            intent_score = 0
            intent_factors = {}
            
            # Factor 1: Se√±ales de urgencia (30%)
            if factors.get("urgency_detected"):
                intent_score += 30
                intent_factors["urgency"] = 30
            elif sentiment_analysis.get("overall_sentiment") == "positive":
                intent_score += 15
                intent_factors["positive_sentiment"] = 15
            
            # Factor 2: Discusi√≥n de presupuesto (25%)
            if factors.get("budget_discussion"):
                intent_score += 25
                intent_factors["budget_discussion"] = 25
            elif len(budget_signals) > 0:
                intent_score += 15
                intent_factors["budget_signals"] = 15
            
            # Factor 3: Comportamiento web (20%)
            if behavior_patterns["decision_phase"]:
                intent_score += 20
                intent_factors["decision_phase"] = 20
            elif behavior_patterns["comparison_phase"]:
                intent_score += 12
                intent_factors["comparison_phase"] = 12
            
            # Factor 4: Carrito abandonado (15%)
            if factors.get("has_abandoned_cart"):
                intent_score += 15
                intent_factors["abandoned_cart"] = 15
            
            # Factor 5: Lead recurrente (10%)
            if factors.get("is_recurrent_lead"):
                intent_score += 10
                intent_factors["recurrent_lead"] = 10
            
            intent_score = min(intent_score, 100)
            
            purchase_intent_scoring = {
                "intent_score": intent_score,
                "intent_level": (
                    "very_high" if intent_score >= 80 else
                    "high" if intent_score >= 60 else
                    "medium" if intent_score >= 40 else
                    "low"
                ),
                "intent_factors": intent_factors,
                "confidence": "high" if sum(intent_factors.values()) >= 60 else "medium"
            }
            
            # Bonus por alta intenci√≥n
            if intent_score >= 70:
                score += 10
                factors["high_purchase_intent"] = True
            
            # Recomendaciones de estrategia de venta personalizadas
            sales_strategy_recommendations = []
            
            # Estrategia basada en buyer persona
            if buyer_persona.get("type") == "decision-maker":
                sales_strategy_recommendations.append({
                    "strategy": "direct_approach",
                    "priority": "high",
                    "description": "Contactar directamente con propuesta de valor clara",
                    "recommended_channels": ["email", "phone"],
                    "timing": "immediate"
                })
            elif buyer_persona.get("type") == "influencer":
                sales_strategy_recommendations.append({
                    "strategy": "nurture_relationship",
                    "priority": "medium",
                    "description": "Construir relaci√≥n y proporcionar contenido de valor",
                    "recommended_channels": ["email", "linkedin"],
                    "timing": "within_48h"
                })
            
            # Estrategia basada en fase del buyer journey
            if behavior_patterns["decision_phase"]:
                sales_strategy_recommendations.append({
                    "strategy": "close_deal",
                    "priority": "high",
                    "description": "Enfocarse en cerrar con demo o propuesta personalizada",
                    "recommended_channels": ["phone", "meeting"],
                    "timing": "immediate"
                })
            elif behavior_patterns["comparison_phase"]:
                sales_strategy_recommendations.append({
                    "strategy": "competitive_positioning",
                    "priority": "high",
                    "description": "Proporcionar comparativa y casos de √©xito",
                    "recommended_channels": ["email", "webinar"],
                    "timing": "within_24h"
                })
            
            # Estrategia basada en competencia detectada
            if competitive_analysis.get("competitors_detected", 0) > 0:
                sales_strategy_recommendations.append({
                    "strategy": "competitive_differentiation",
                    "priority": "high",
                    "description": "Enfatizar diferenciadores clave vs competidores",
                    "recommended_channels": ["email", "demo"],
                    "timing": "immediate"
                })
            
            # Estrategia basada en sentimiento
            if sentiment_analysis.get("overall_sentiment") == "negative":
                sales_strategy_recommendations.append({
                    "strategy": "address_concerns",
                    "priority": "high",
                    "description": "Abordar preocupaciones y proporcionar soporte",
                    "recommended_channels": ["phone", "email"],
                    "timing": "immediate"
                })
            
            # Estrategia basada en carrito abandonado
            if factors.get("has_abandoned_cart"):
                sales_strategy_recommendations.append({
                    "strategy": "cart_recovery",
                    "priority": "high",
                    "description": "Recuperar carrito con oferta personalizada",
                    "recommended_channels": ["email"],
                    "timing": "immediate"
                })
            
            # Predicci√≥n de mejor canal de contacto
            channel_prediction = {}
            
            channel_scores = {
                "email": 50,  # Base
                "phone": 30,
                "linkedin": 20,
                "meeting": 10,
                "chat": 15
            }
            
            # Ajustes seg√∫n datos disponibles
            if phone:
                channel_scores["phone"] += 20
            if social_analysis.get("linkedin_score", 0) > 0:
                channel_scores["linkedin"] += 25
            
            # Ajustes seg√∫n buyer persona
            if buyer_persona.get("type") == "decision-maker":
                channel_scores["phone"] += 15
                channel_scores["meeting"] += 20
            elif buyer_persona.get("type") == "influencer":
                channel_scores["linkedin"] += 15
                channel_scores["email"] += 10
            
            # Ajustes seg√∫n urgencia
            if factors.get("urgency_detected"):
                channel_scores["phone"] += 25
                channel_scores["meeting"] += 15
            
            # Canal recomendado
            best_channel = max(channel_scores.items(), key=lambda x: x[1])[0]
            
            channel_prediction = {
                "recommended_channel": best_channel,
                "channel_scores": channel_scores,
                "confidence": "high" if channel_scores[best_channel] >= 60 else "medium",
                "alternative_channels": sorted(
                    [(ch, sc) for ch, sc in channel_scores.items() if ch != best_channel],
                    key=lambda x: x[1],
                    reverse=True
                )[:2]
            }
            
            # An√°lisis de patrones de comunicaci√≥n
            communication_pattern_analysis = {}
            
            # Analizar mensaje para patrones
            if message:
                message_lower = message.lower()
                
                # Detectar estilo de comunicaci√≥n
                communication_style = "formal"
                if any(word in message_lower for word in ["hola", "saludos", "gracias", "por favor"]):
                    communication_style = "friendly"
                if any(word in message_lower for word in ["urgente", "inmediato", "r√°pido"]):
                    communication_style = "direct"
                
                # Detectar nivel de detalle
                detail_level = "brief"
                if len(message) > 200:
                    detail_level = "detailed"
                elif len(message) > 100:
                    detail_level = "moderate"
                
                # Detectar preguntas
                question_count = message.count("?")
                has_questions = question_count > 0
                
                communication_pattern_analysis = {
                    "communication_style": communication_style,
                    "detail_level": detail_level,
                    "has_questions": has_questions,
                    "question_count": question_count,
                    "message_length": len(message),
                    "recommended_response_style": (
                        "detailed" if detail_level == "detailed" else
                        "concise" if detail_level == "brief" else
                        "balanced"
                    )
                }
            
            # An√°lisis de fit de producto m√°s detallado
            detailed_product_fit = {}
            
            # Calcular fit por producto espec√≠fico
            product_fits = {}
            
            # Producto 1: Enterprise Plan
            enterprise_fit = 0
            if factors.get("enterprise_fit"):
                enterprise_fit += 40
            if employees and employees > 500:
                enterprise_fit += 30
            if industry in ["technology", "finance", "healthcare"]:
                enterprise_fit += 20
            if technology_analysis.get("tech_maturity") == "high":
                enterprise_fit += 10
            product_fits["enterprise_plan"] = min(enterprise_fit, 100)
            
            # Producto 2: Professional Plan
            professional_fit = 0
            if employees and 50 <= employees <= 500:
                professional_fit += 40
            if score >= 50:
                professional_fit += 30
            if product_fit_score >= 60:
                professional_fit += 20
            if cultural_fit_analysis.get("cultural_fit_score", 0) >= 50:
                professional_fit += 10
            product_fits["professional_plan"] = min(professional_fit, 100)
            
            # Producto 3: Starter Plan
            starter_fit = 0
            if employees and employees < 50:
                starter_fit += 40
            if score < 50:
                starter_fit += 30
            if not factors.get("enterprise_fit"):
                starter_fit += 20
            if estimated_ltv < 10000:
                starter_fit += 10
            product_fits["starter_plan"] = min(starter_fit, 100)
            
            # Producto recomendado basado en fit
            best_product_fit = max(product_fits.items(), key=lambda x: x[1])
            
            detailed_product_fit = {
                "product_fits": product_fits,
                "recommended_product": best_product_fit[0],
                "recommended_product_fit_score": best_product_fit[1],
                "fit_confidence": (
                    "high" if best_product_fit[1] >= 70 else
                    "medium" if best_product_fit[1] >= 50 else
                    "low"
                )
            }
            
            # Predicci√≥n de mejor momento del d√≠a para contactar
            optimal_contact_timing = {}
            
            # Basado en web behavior si est√° disponible
            visit_times = web_behavior.get("visit_times", [])
            
            if visit_times:
                # Analizar horas m√°s frecuentes
                hour_counts = {}
                for visit_time in visit_times:
                    try:
                        hour = int(visit_time.split(":")[0])
                        hour_counts[hour] = hour_counts.get(hour, 0) + 1
                    except:
                        pass
                
                if hour_counts:
                    best_hour = max(hour_counts.items(), key=lambda x: x[1])[0]
                    optimal_contact_timing = {
                        "best_hour": best_hour,
                        "best_time_window": f"{best_hour}:00-{best_hour+1}:00",
                        "confidence": "high",
                        "method": "behavioral_data"
                    }
                else:
                    optimal_contact_timing = {
                        "best_hour": 10,  # Default: 10 AM
                        "best_time_window": "10:00-11:00",
                        "confidence": "low",
                        "method": "default"
                    }
            else:
                # Default basado en buyer persona
                if buyer_persona.get("type") == "decision-maker":
                    optimal_contact_timing = {
                        "best_hour": 9,
                        "best_time_window": "09:00-10:00",
                        "confidence": "medium",
                        "method": "persona_based"
                    }
                else:
                    optimal_contact_timing = {
                        "best_hour": 14,
                        "best_time_window": "14:00-15:00",
                        "confidence": "medium",
                        "method": "persona_based"
                    }
            
            # An√°lisis de influencia del lead
            influence_analysis = {}
            
            influence_score = 0
            influence_factors = {}
            
            # Factor 1: T√≠tulo/Posici√≥n (40%)
            if title:
                title_lower = title.lower()
                if any(role in title_lower for role in ["ceo", "cto", "cfo", "president", "director", "vp"]):
                    influence_score += 40
                    influence_factors["executive_title"] = 40
                elif any(role in title_lower for role in ["manager", "head", "lead"]):
                    influence_score += 25
                    influence_factors["manager_title"] = 25
                else:
                    influence_score += 10
                    influence_factors["standard_title"] = 10
            
            # Factor 2: LinkedIn (30%)
            if social_analysis.get("linkedin_score", 0) > 50:
                influence_score += 30
                influence_factors["high_linkedin_score"] = 30
            elif social_analysis.get("linkedin_score", 0) > 0:
                influence_score += 15
                influence_factors["linkedin_presence"] = 15
            
            # Factor 3: Tama√±o de empresa (20%)
            if employees and employees > 1000:
                influence_score += 20
                influence_factors["large_company"] = 20
            elif employees and employees > 100:
                influence_score += 10
                influence_factors["medium_company"] = 10
            
            # Factor 4: Conexiones (10%)
            if social_analysis.get("linkedin_connections", 0) > 500:
                influence_score += 10
                influence_factors["high_connections"] = 10
            
            influence_score = min(influence_score, 100)
            
            influence_analysis = {
                "influence_score": influence_score,
                "influence_level": (
                    "very_high" if influence_score >= 80 else
                    "high" if influence_score >= 60 else
                    "medium" if influence_score >= 40 else
                    "low"
                ),
                "influence_factors": influence_factors,
                "is_decision_maker": influence_score >= 60
            }
            
            # Bonus por alta influencia
            if influence_score >= 70:
                score += 12
                factors["high_influence_lead"] = True
            
            # An√°lisis de red de contactos y referencias
            network_analysis = {}
            
            # Buscar conexiones en la base de datos
            try:
                # Buscar otros leads de la misma empresa
                same_company_query = """
                    SELECT COUNT(*) as count, 
                           MAX(score) as max_score,
                           MAX(CASE WHEN status = 'converted' THEN 1 ELSE 0 END) as has_converted
                    FROM leads
                    WHERE company = %s AND email != %s
                    AND created_at > NOW() - INTERVAL '1 year'
                """
                same_company_result = hook.get_first(same_company_query, (company, email))
                
                company_connections = same_company_result[0] if same_company_result else 0
                max_company_score = same_company_result[1] if same_company_result else 0
                has_converted_company = same_company_result[2] if same_company_result else 0
                
                # Buscar leads del mismo dominio de email
                email_domain = email.split("@")[1] if "@" in email else None
                domain_connections = 0
                if email_domain:
                    domain_query = """
                        SELECT COUNT(*) as count
                        FROM leads
                        WHERE email LIKE %s AND email != %s
                        AND created_at > NOW() - INTERVAL '1 year'
                    """
                    domain_result = hook.get_first(domain_query, (f"%@{email_domain}", email))
                    domain_connections = domain_result[0] if domain_result else 0
                
                network_analysis = {
                    "company_connections": company_connections,
                    "domain_connections": domain_connections,
                    "max_company_score": max_company_score,
                    "has_converted_company": bool(has_converted_company),
                    "network_size": company_connections + domain_connections,
                    "network_strength": (
                        "strong" if (company_connections + domain_connections) >= 5 else
                        "medium" if (company_connections + domain_connections) >= 2 else
                        "weak"
                    )
                }
                
                # Bonus por red fuerte
                if network_analysis["network_size"] >= 5:
                    score += 8
                    factors["strong_network"] = True
                elif network_analysis["network_size"] >= 2:
                    score += 4
                    factors["medium_network"] = True
                
                # Bonus si hay conversiones en la empresa
                if has_converted_company:
                    score += 10
                    factors["company_has_converted"] = True
                    
            except Exception as e:
                logger.warning(f"Error en network analysis: {e}")
                network_analysis = {
                    "error": str(e),
                    "network_size": 0
                }
            
            # Predicci√≥n de mejor contenido para enviar
            content_recommendation = {}
            
            # Basado en buyer journey stage y topics detectados
            content_types = {
                "case_study": 0,
                "whitepaper": 0,
                "demo_video": 0,
                "pricing_guide": 0,
                "product_comparison": 0,
                "webinar": 0,
                "trial_offer": 0
            }
            
            # Ajustes seg√∫n buyer journey
            if behavioral_prediction.get("buyer_journey_stage") == "awareness":
                content_types["whitepaper"] += 30
                content_types["webinar"] += 25
            elif behavioral_prediction.get("buyer_journey_stage") == "research":
                content_types["case_study"] += 30
                content_types["whitepaper"] += 20
            elif behavioral_prediction.get("buyer_journey_stage") == "comparison":
                content_types["product_comparison"] += 35
                content_types["case_study"] += 25
            elif behavioral_prediction.get("buyer_journey_stage") == "decision":
                content_types["demo_video"] += 40
                content_types["trial_offer"] += 35
                content_types["pricing_guide"] += 30
            
            # Ajustes seg√∫n topics detectados
            if topic_extraction.get("primary_topic") == "pricing":
                content_types["pricing_guide"] += 30
            elif topic_extraction.get("primary_topic") == "features":
                content_types["product_comparison"] += 25
            elif topic_extraction.get("primary_topic") == "integration":
                content_types["case_study"] += 20
            
            # Ajustes seg√∫n competencia detectada
            if competitive_analysis.get("competitors_detected", 0) > 0:
                content_types["product_comparison"] += 25
            
            # Contenido recomendado
            best_content = max(content_types.items(), key=lambda x: x[1])
            
            content_recommendation = {
                "recommended_content": best_content[0],
                "content_scores": content_types,
                "confidence": "high" if best_content[1] >= 50 else "medium",
                "alternative_content": sorted(
                    [(ct, sc) for ct, sc in content_types.items() if ct != best_content[0]],
                    key=lambda x: x[1],
                    reverse=True
                )[:2]
            }
            
            # An√°lisis de riesgo competitivo
            competitive_risk_analysis = {}
            
            risk_score = 0
            risk_factors = []
            
            # Factor 1: M√∫ltiples competidores mencionados (alto riesgo)
            if competitive_analysis.get("competitors_detected", 0) >= 3:
                risk_score += 40
                risk_factors.append("multiple_competitors")
            elif competitive_analysis.get("competitors_detected", 0) >= 1:
                risk_score += 20
                risk_factors.append("competitor_mentioned")
            
            # Factor 2: Sentimiento negativo (riesgo de p√©rdida)
            if sentiment_analysis.get("overall_sentiment") == "negative":
                risk_score += 30
                risk_factors.append("negative_sentiment")
            
            # Factor 3: Bajo engagement (riesgo de abandono)
            if engagement_velocity.get("velocity", "slow") == "slow":
                risk_score += 20
                risk_factors.append("low_engagement")
            
            # Factor 4: Alto churn risk
            if churn_risk_score >= 0.7:
                risk_score += 25
                risk_factors.append("high_churn_risk")
            
            # Factor 5: Tiempo sin contacto
            days_since_contact = metadata.get("days_since_last_contact", 0)
            if days_since_contact > 30:
                risk_score += 15
                risk_factors.append("no_recent_contact")
            
            risk_score = min(risk_score, 100)
            
            competitive_risk_analysis = {
                "risk_score": risk_score,
                "risk_level": (
                    "critical" if risk_score >= 70 else
                    "high" if risk_score >= 50 else
                    "medium" if risk_score >= 30 else
                    "low"
                ),
                "risk_factors": risk_factors,
                "mitigation_strategies": [
                    "immediate_contact" if risk_score >= 50 else None,
                    "competitive_differentiation" if "multiple_competitors" in risk_factors else None,
                    "address_concerns" if "negative_sentiment" in risk_factors else None,
                    "re_engagement_campaign" if "low_engagement" in risk_factors else None
                ]
            }
            competitive_risk_analysis["mitigation_strategies"] = [s for s in competitive_risk_analysis["mitigation_strategies"] if s]
            
            # Scoring de engagement multi-canal
            multi_channel_engagement = {}
            
            # Agregar engagement de diferentes canales
            channel_engagement = {
                "email": metadata.get("email_opens", 0) + metadata.get("email_clicks", 0) * 2,
                "web": web_behavior.get("pages_visited", 0) + web_behavior.get("return_visits", 0) * 3,
                "social": social_analysis.get("linkedin_score", 0) / 10,
                "phone": 1 if metadata.get("phone_contacted", False) else 0,
                "chat": metadata.get("chat_interactions", 0)
            }
            
            total_engagement = sum(channel_engagement.values())
            
            # Canal m√°s activo
            most_active_channel = max(channel_engagement.items(), key=lambda x: x[1])[0] if channel_engagement else "none"
            
            # Diversidad de canales (m√°s canales = mejor)
            active_channels = sum(1 for v in channel_engagement.values() if v > 0)
            channel_diversity = active_channels / len(channel_engagement) if channel_engagement else 0
            
            multi_channel_engagement = {
                "channel_engagement": channel_engagement,
                "total_engagement": total_engagement,
                "most_active_channel": most_active_channel,
                "active_channels_count": active_channels,
                "channel_diversity": round(channel_diversity, 2),
                "engagement_level": (
                    "very_high" if total_engagement >= 20 else
                    "high" if total_engagement >= 10 else
                    "medium" if total_engagement >= 5 else
                    "low"
                )
            }
            
            # Bonus por engagement multi-canal alto
            if total_engagement >= 15:
                score += 7
                factors["high_multi_channel_engagement"] = True
            
            # An√°lisis de se√±ales de compra B2B
            b2b_purchase_signals = {}
            
            purchase_signals = []
            signal_strength = 0
            
            # Se√±al 1: Tama√±o de empresa (empresas grandes = m√°s probable compra)
            if employees and employees > 500:
                purchase_signals.append("large_company")
                signal_strength += 20
            
            # Se√±al 2: Presupuesto discutido
            if factors.get("budget_discussion"):
                purchase_signals.append("budget_available")
                signal_strength += 25
            
            # Se√±al 3: Urgencia detectada
            if factors.get("urgency_detected"):
                purchase_signals.append("urgent_need")
                signal_strength += 20
            
            # Se√±al 4: Decision maker identificado
            if influence_analysis.get("is_decision_maker"):
                purchase_signals.append("decision_maker")
                signal_strength += 15
            
            # Se√±al 5: Carrito abandonado
            if factors.get("has_abandoned_cart"):
                purchase_signals.append("abandoned_cart")
                signal_strength += 15
            
            # Se√±al 6: Comparaci√≥n de productos
            if behavior_patterns.get("comparison_phase"):
                purchase_signals.append("comparing_options")
                signal_strength += 10
            
            # Se√±al 7: Lead recurrente
            if factors.get("is_recurrent_lead"):
                purchase_signals.append("returning_lead")
                signal_strength += 10
            
            signal_strength = min(signal_strength, 100)
            
            b2b_purchase_signals = {
                "purchase_signals": purchase_signals,
                "signal_strength": signal_strength,
                "signal_count": len(purchase_signals),
                "purchase_likelihood": (
                    "very_high" if signal_strength >= 70 else
                    "high" if signal_strength >= 50 else
                    "medium" if signal_strength >= 30 else
                    "low"
                )
            }
            
            # Bonus por m√∫ltiples se√±ales de compra
            if len(purchase_signals) >= 4:
                score += 10
                factors["strong_purchase_signals"] = True
            
            # Predicci√≥n de mejor oferta/promoci√≥n
            offer_recommendation = {}
            
            offer_types = {
                "discount": 0,
                "trial": 0,
                "demo": 0,
                "consultation": 0,
                "case_study": 0,
                "custom_pricing": 0
            }
            
            # Ajustes seg√∫n buyer journey
            if behavioral_prediction.get("buyer_journey_stage") == "awareness":
                offer_types["case_study"] += 30
                offer_types["consultation"] += 20
            elif behavioral_prediction.get("buyer_journey_stage") == "research":
                offer_types["trial"] += 25
                offer_types["demo"] += 20
            elif behavioral_prediction.get("buyer_journey_stage") == "comparison":
                offer_types["custom_pricing"] += 35
                offer_types["discount"] += 25
            elif behavioral_prediction.get("buyer_journey_stage") == "decision":
                offer_types["discount"] += 40
                offer_types["trial"] += 30
            
            # Ajustes seg√∫n carrito abandonado
            if factors.get("has_abandoned_cart"):
                offer_types["discount"] += 35
                offer_types["trial"] += 20
            
            # Ajustes seg√∫n presupuesto
            if factors.get("budget_discussion"):
                offer_types["custom_pricing"] += 30
                offer_types["discount"] += 20
            
            # Oferta recomendada
            best_offer = max(offer_types.items(), key=lambda x: x[1])
            
            offer_recommendation = {
                "recommended_offer": best_offer[0],
                "offer_scores": offer_types,
                "confidence": "high" if best_offer[1] >= 50 else "medium",
                "offer_value": (
                    "high" if best_offer[0] in ["custom_pricing", "discount"] else
                    "medium" if best_offer[0] in ["trial", "demo"] else
                    "low"
                )
            }
            
            # An√°lisis de fit de timing (mejor momento del a√±o/mes)
            timing_fit_analysis = {}
            
            # Mes actual y estacionalidad
            current_month = datetime.utcnow().month
            current_quarter = (current_month - 1) // 3 + 1
            
            # Patrones estacionales B2B (Q4 y Q1 suelen ser mejores)
            seasonal_score = 50  # Base
            if current_quarter == 4:  # Q4 - fin de a√±o, presupuestos
                seasonal_score += 20
            elif current_quarter == 1:  # Q1 - nuevo a√±o, nuevos presupuestos
                seasonal_score += 15
            elif current_quarter == 3:  # Q3 - mitad de a√±o
                seasonal_score += 10
            
            # D√≠a de la semana (lunes-jueves mejor que viernes)
            current_weekday = datetime.utcnow().weekday()
            weekday_score = 50
            if current_weekday < 4:  # Lunes a Jueves
                weekday_score += 15
            else:  # Viernes
                weekday_score -= 10
            
            timing_fit_analysis = {
                "current_month": current_month,
                "current_quarter": current_quarter,
                "seasonal_score": seasonal_score,
                "weekday_score": weekday_score,
                "overall_timing_score": (seasonal_score + weekday_score) / 2,
                "timing_quality": (
                    "excellent" if (seasonal_score + weekday_score) / 2 >= 70 else
                    "good" if (seasonal_score + weekday_score) / 2 >= 60 else
                    "fair" if (seasonal_score + weekday_score) / 2 >= 50 else
                    "poor"
                )
            }
            
            # Bonus si el timing es excelente
            if timing_fit_analysis["overall_timing_score"] >= 70:
                score += 5
                factors["optimal_timing"] = True
            
            # Predicci√≥n de mejor vendedor para asignar
            sales_rep_recommendation = {}
            
            # Basado en especializaci√≥n, carga y performance
            try:
                # Buscar vendedores disponibles
                sales_reps_query = """
                    SELECT 
                        id, name, email,
                        specialization, current_load, 
                        avg_conversion_rate, avg_deal_size,
                        industry_expertise, product_expertise
                    FROM sales_reps
                    WHERE active = true
                    ORDER BY current_load ASC, avg_conversion_rate DESC
                    LIMIT 10
                """
                sales_reps = hook.get_records(sales_reps_query)
                
                if sales_reps:
                    rep_scores = []
                    for rep in sales_reps:
                        rep_score = 50  # Base
                        
                        # Ajuste por especializaci√≥n
                        if industry and rep[6] and industry.lower() in str(rep[6]).lower():
                            rep_score += 20
                        if rep[7] and detailed_product_fit.get("recommended_product") in str(rep[7]):
                            rep_score += 15
                        
                        # Ajuste por carga (menos carga = mejor)
                        if rep[3] and rep[3] < 10:  # Menos de 10 leads
                            rep_score += 15
                        elif rep[3] and rep[3] < 20:
                            rep_score += 10
                        
                        # Ajuste por performance
                        if rep[4] and rep[4] > 0.3:  # >30% conversi√≥n
                            rep_score += 15
                        elif rep[4] and rep[4] > 0.2:
                            rep_score += 10
                        
                        # Ajuste por tama√±o de deal
                        if rep[5] and predicted_deal_size.get("estimated_value", 0) > 0:
                            if rep[5] >= predicted_deal_size.get("estimated_value", 0) * 0.8:
                                rep_score += 10
                        
                        rep_scores.append({
                            "rep_id": rep[0],
                            "name": rep[1],
                            "email": rep[2],
                            "score": rep_score,
                            "specialization": rep[6],
                            "current_load": rep[3],
                            "conversion_rate": rep[4]
                        })
                    
                    # Mejor vendedor
                    best_rep = max(rep_scores, key=lambda x: x["score"])
                    
                    sales_rep_recommendation = {
                        "recommended_rep_id": best_rep["rep_id"],
                        "recommended_rep_name": best_rep["name"],
                        "recommended_rep_email": best_rep["email"],
                        "rep_scores": sorted(rep_scores, key=lambda x: x["score"], reverse=True)[:3],
                        "confidence": "high" if best_rep["score"] >= 70 else "medium",
                        "match_reasons": [
                            "industry_match" if industry and best_rep.get("specialization") else None,
                            "product_match" if detailed_product_fit.get("recommended_product") else None,
                            "low_load" if best_rep.get("current_load", 100) < 15 else None,
                            "high_performance" if best_rep.get("conversion_rate", 0) > 0.25 else None
                        ]
                    }
                    sales_rep_recommendation["match_reasons"] = [r for r in sales_rep_recommendation["match_reasons"] if r]
                else:
                    sales_rep_recommendation = {
                        "error": "no_sales_reps_available"
                    }
            except Exception as e:
                logger.warning(f"Error en sales rep recommendation: {e}")
                sales_rep_recommendation = {
                    "error": str(e)
                }
            
            # An√°lisis de patrones de comportamiento hist√≥rico
            historical_pattern_analysis = {}
            
            try:
                # Buscar patrones hist√≥ricos de leads similares
                historical_query = """
                    SELECT 
                        AVG(score) as avg_score,
                        AVG(EXTRACT(EPOCH FROM (updated_at - created_at))/86400) as avg_days_to_convert,
                        COUNT(*) FILTER (WHERE status = 'converted') as converted_count,
                        COUNT(*) as total_count,
                        AVG(CASE WHEN metadata->>'estimated_ltv' IS NOT NULL 
                            THEN (metadata->>'estimated_ltv')::numeric ELSE NULL END) as avg_ltv
                    FROM leads
                    WHERE company = %s OR (industry = %s AND employees BETWEEN %s AND %s)
                    AND created_at > NOW() - INTERVAL '2 years'
                    AND email != %s
                """
                employee_range_min = max(0, (employees or 0) - 100) if employees else 0
                employee_range_max = (employees or 0) + 100 if employees else 1000
                
                historical_result = hook.get_first(
                    historical_query, 
                    (company, industry, employee_range_min, employee_range_max, email)
                )
                
                if historical_result and historical_result[3] and historical_result[3] > 0:
                    conversion_rate = (historical_result[2] or 0) / historical_result[3] if historical_result[3] > 0 else 0
                    
                    historical_pattern_analysis = {
                        "similar_leads_count": historical_result[3],
                        "converted_count": historical_result[2] or 0,
                        "conversion_rate": round(conversion_rate, 3),
                        "avg_score": round(historical_result[0] or 0, 2),
                        "avg_days_to_convert": round(historical_result[1] or 0, 1) if historical_result[1] else None,
                        "avg_ltv": round(historical_result[4] or 0, 2) if historical_result[4] else None,
                        "pattern_confidence": (
                            "high" if historical_result[3] >= 10 else
                            "medium" if historical_result[3] >= 5 else
                            "low"
                        )
                    }
                    
                    # Bonus si hay buen historial de conversi√≥n
                    if conversion_rate >= 0.3:
                        score += 8
                        factors["high_historical_conversion"] = True
                else:
                    historical_pattern_analysis = {
                        "similar_leads_count": 0,
                        "pattern_confidence": "none"
                    }
            except Exception as e:
                logger.warning(f"Error en historical pattern analysis: {e}")
                historical_pattern_analysis = {
                    "error": str(e)
                }
            
            # Predicci√≥n de mejor mensaje/pitch personalizado
            personalized_message_recommendation = {}
            
            message_templates = {
                "value_proposition": 0,
                "problem_solution": 0,
                "social_proof": 0,
                "urgency_scarcity": 0,
                "educational": 0,
                "consultative": 0
            }
            
            # Ajustes seg√∫n buyer journey
            if behavioral_prediction.get("buyer_journey_stage") == "awareness":
                message_templates["educational"] += 30
                message_templates["value_proposition"] += 25
            elif behavioral_prediction.get("buyer_journey_stage") == "research":
                message_templates["problem_solution"] += 30
                message_templates["educational"] += 20
            elif behavioral_prediction.get("buyer_journey_stage") == "comparison":
                message_templates["social_proof"] += 35
                message_templates["value_proposition"] += 25
            elif behavioral_prediction.get("buyer_journey_stage") == "decision":
                message_templates["urgency_scarcity"] += 40
                message_templates["consultative"] += 30
            
            # Ajustes seg√∫n sentimiento
            if sentiment_analysis.get("overall_sentiment") == "negative":
                message_templates["problem_solution"] += 30
                message_templates["consultative"] += 25
            elif sentiment_analysis.get("overall_sentiment") == "positive":
                message_templates["value_proposition"] += 25
                message_templates["social_proof"] += 20
            
            # Ajustes seg√∫n competencia
            if competitive_analysis.get("competitors_detected", 0) > 0:
                message_templates["value_proposition"] += 30
                message_templates["social_proof"] += 25
            
            # Ajustes seg√∫n urgencia
            if factors.get("urgency_detected"):
                message_templates["urgency_scarcity"] += 35
            
            # Mejor template
            best_template = max(message_templates.items(), key=lambda x: x[1])
            
            personalized_message_recommendation = {
                "recommended_template": best_template[0],
                "template_scores": message_templates,
                "confidence": "high" if best_template[1] >= 50 else "medium",
                "key_points": {
                    "value_proposition": ["ROI", "cost_savings", "efficiency"],
                    "problem_solution": ["pain_points", "solutions", "benefits"],
                    "social_proof": ["case_studies", "testimonials", "customer_count"],
                    "urgency_scarcity": ["limited_time", "exclusive_offer", "deadline"],
                    "educational": ["best_practices", "industry_insights", "guides"],
                    "consultative": ["needs_assessment", "custom_solution", "partnership"]
                }.get(best_template[0], [])
            }
            
            # An√°lisis de fit de precio
            pricing_fit_analysis = {}
            
            # Basado en industria, tama√±o de empresa y se√±ales de presupuesto
            pricing_tiers = {
                "enterprise": 0,
                "professional": 0,
                "starter": 0,
                "custom": 0
            }
            
            # Ajustes por tama√±o de empresa
            if employees and employees > 1000:
                pricing_tiers["enterprise"] += 40
                pricing_tiers["custom"] += 30
            elif employees and employees > 100:
                pricing_tiers["professional"] += 40
                pricing_tiers["enterprise"] += 20
            else:
                pricing_tiers["starter"] += 40
                pricing_tiers["professional"] += 20
            
            # Ajustes por industria
            high_value_industries = ["finance", "healthcare", "technology", "consulting"]
            if industry and any(hvi in industry.lower() for hvi in high_value_industries):
                pricing_tiers["enterprise"] += 20
                pricing_tiers["professional"] += 15
            
            # Ajustes por se√±ales de presupuesto
            if factors.get("budget_discussion"):
                if any(kw in str(budget_signals).lower() for kw in ["enterprise", "large", "unlimited"]):
                    pricing_tiers["enterprise"] += 25
                elif any(kw in str(budget_signals).lower() for kw in ["budget", "affordable", "reasonable"]):
                    pricing_tiers["professional"] += 20
                else:
                    pricing_tiers["starter"] += 15
            
            # Ajustes por product fit
            if detailed_product_fit.get("recommended_product") == "enterprise_plan":
                pricing_tiers["enterprise"] += 30
            elif detailed_product_fit.get("recommended_product") == "professional_plan":
                pricing_tiers["professional"] += 30
            else:
                pricing_tiers["starter"] += 30
            
            # Tier recomendado
            best_tier = max(pricing_tiers.items(), key=lambda x: x[1])
            
            pricing_fit_analysis = {
                "recommended_tier": best_tier[0],
                "tier_scores": pricing_tiers,
                "confidence": "high" if best_tier[1] >= 60 else "medium",
                "estimated_price_range": {
                    "enterprise": "high",
                    "professional": "medium",
                    "starter": "low",
                    "custom": "variable"
                }.get(best_tier[0], "medium")
            }
            
            # Scoring de calidad de lead avanzado
            advanced_quality_scoring = {}
            
            quality_score = 0
            quality_factors = {}
            
            # Factor 1: Completitud de datos (25%)
            completeness = 0
            if email:
                completeness += 20
            if first_name and last_name:
                completeness += 20
            if phone:
                completeness += 20
            if company:
                completeness += 20
            if industry:
                completeness += 10
            if title:
                completeness += 10
            completeness = min(completeness, 100)
            quality_score += completeness * 0.25
            quality_factors["data_completeness"] = completeness
            
            # Factor 2: Validaci√≥n de datos (20%)
            validation_score = 0
            if lead_data.get("email_validation", {}).get("valid_mx"):
                validation_score += 50
            if lead_data.get("company_validation", {}).get("validated"):
                validation_score += 30
            if lead_data.get("phone_validation", {}).get("valid"):
                validation_score += 20
            quality_score += validation_score * 0.20
            quality_factors["data_validation"] = validation_score
            
            # Factor 3: Enriquecimiento (15%)
            enrichment_score = 0
            if enriched_data:
                enrichment_score += 30
            if social_analysis.get("linkedin_score", 0) > 0:
                enrichment_score += 25
            if technology_analysis.get("tech_count", 0) > 0:
                enrichment_score += 20
            if network_analysis.get("network_size", 0) > 0:
                enrichment_score += 25
            enrichment_score = min(enrichment_score, 100)
            quality_score += enrichment_score * 0.15
            quality_factors["data_enrichment"] = enrichment_score
            
            # Factor 4: Engagement (20%)
            engagement_quality = 0
            if multi_channel_engagement.get("total_engagement", 0) >= 15:
                engagement_quality += 40
            elif multi_channel_engagement.get("total_engagement", 0) >= 5:
                engagement_quality += 25
            if multi_channel_engagement.get("channel_diversity", 0) >= 0.6:
                engagement_quality += 30
            if engagement_velocity.get("velocity") == "fast":
                engagement_quality += 30
            engagement_quality = min(engagement_quality, 100)
            quality_score += engagement_quality * 0.20
            quality_factors["engagement_quality"] = engagement_quality
            
            # Factor 5: Se√±ales de compra (20%)
            purchase_signal_quality = 0
            if b2b_purchase_signals.get("signal_strength", 0) >= 70:
                purchase_signal_quality += 50
            elif b2b_purchase_signals.get("signal_strength", 0) >= 50:
                purchase_signal_quality += 35
            if purchase_intent_scoring.get("intent_score", 0) >= 70:
                purchase_signal_quality += 30
            if influence_analysis.get("is_decision_maker"):
                purchase_signal_quality += 20
            purchase_signal_quality = min(purchase_signal_quality, 100)
            quality_score += purchase_signal_quality * 0.20
            quality_factors["purchase_signal_quality"] = purchase_signal_quality
            
            quality_score = min(quality_score, 100)
            
            advanced_quality_scoring = {
                "quality_score": round(quality_score, 2),
                "quality_grade": (
                    "A+" if quality_score >= 90 else
                    "A" if quality_score >= 80 else
                    "B" if quality_score >= 70 else
                    "C" if quality_score >= 60 else
                    "D" if quality_score >= 50 else
                    "F"
                ),
                "quality_factors": quality_factors,
                "quality_level": (
                    "excellent" if quality_score >= 85 else
                    "good" if quality_score >= 70 else
                    "fair" if quality_score >= 60 else
                    "poor"
                )
            }
            
            # An√°lisis de se√±ales de abandono temprano
            early_abandonment_signals = {}
            
            abandonment_score = 0
            abandonment_factors = []
            
            # Se√±al 1: Bajo engagement inicial
            if multi_channel_engagement.get("total_engagement", 0) < 3:
                abandonment_score += 30
                abandonment_factors.append("low_initial_engagement")
            
            # Se√±al 2: Sin respuesta a contactos
            contact_attempts = metadata.get("contact_attempts", 0)
            contact_responses = metadata.get("contact_responses", 0)
            if contact_attempts > 2 and contact_responses == 0:
                abandonment_score += 35
                abandonment_factors.append("no_response_to_contacts")
            
            # Se√±al 3: Tiempo sin actividad
            days_since_activity = metadata.get("days_since_last_activity", 0)
            if days_since_activity > 14:
                abandonment_score += 25
                abandonment_factors.append("inactive_for_weeks")
            
            # Se√±al 4: Sentimiento negativo
            if sentiment_analysis.get("overall_sentiment") == "negative":
                abandonment_score += 20
                abandonment_factors.append("negative_sentiment")
            
            # Se√±al 5: Bajo score general
            if score < 30:
                abandonment_score += 15
                abandonment_factors.append("low_score")
            
            abandonment_score = min(abandonment_score, 100)
            
            early_abandonment_signals = {
                "abandonment_score": abandonment_score,
                "abandonment_risk": (
                    "critical" if abandonment_score >= 70 else
                    "high" if abandonment_score >= 50 else
                    "medium" if abandonment_score >= 30 else
                    "low"
                ),
                "abandonment_factors": abandonment_factors,
                "intervention_recommended": abandonment_score >= 50,
                "intervention_strategies": [
                    "re_engagement_campaign" if "low_initial_engagement" in abandonment_factors else None,
                    "different_approach" if "no_response_to_contacts" in abandonment_factors else None,
                    "incentive_offer" if "inactive_for_weeks" in abandonment_factors else None,
                    "address_concerns" if "negative_sentiment" in abandonment_factors else None,
                    "nurture_sequence" if "low_score" in abandonment_factors else None
                ]
            }
            early_abandonment_signals["intervention_strategies"] = [
                s for s in early_abandonment_signals["intervention_strategies"] if s
            ]
            
            # Predicci√≥n de mejor secuencia de nurturing
            nurturing_sequence_recommendation = {}
            
            sequence_types = {
                "educational": 0,
                "product_focused": 0,
                "social_proof": 0,
                "promotional": 0,
                "consultative": 0
            }
            
            # Ajustes seg√∫n buyer journey
            if behavioral_prediction.get("buyer_journey_stage") == "awareness":
                sequence_types["educational"] += 40
                sequence_types["social_proof"] += 25
            elif behavioral_prediction.get("buyer_journey_stage") == "research":
                sequence_types["product_focused"] += 35
                sequence_types["educational"] += 25
            elif behavioral_prediction.get("buyer_journey_stage") == "comparison":
                sequence_types["social_proof"] += 40
                sequence_types["product_focused"] += 30
            elif behavioral_prediction.get("buyer_journey_stage") == "decision":
                sequence_types["promotional"] += 45
                sequence_types["consultative"] += 30
            
            # Ajustes seg√∫n engagement
            if multi_channel_engagement.get("total_engagement", 0) < 5:
                sequence_types["educational"] += 30
                sequence_types["social_proof"] += 20
            
            # Ajustes seg√∫n score
            if score < 40:
                sequence_types["educational"] += 25
                sequence_types["consultative"] += 20
            
            # Secuencia recomendada
            best_sequence = max(sequence_types.items(), key=lambda x: x[1])
            
            nurturing_sequence_recommendation = {
                "recommended_sequence": best_sequence[0],
                "sequence_scores": sequence_types,
                "confidence": "high" if best_sequence[1] >= 50 else "medium",
                "recommended_frequency": (
                    "daily" if purchase_intent_scoring.get("intent_score", 0) >= 70 else
                    "every_2_days" if purchase_intent_scoring.get("intent_score", 0) >= 50 else
                    "weekly"
                ),
                "recommended_duration": (
                    "2_weeks" if behavioral_prediction.get("buyer_journey_stage") == "decision" else
                    "4_weeks" if behavioral_prediction.get("buyer_journey_stage") == "comparison" else
                    "6_weeks"
                )
            }
            
            # An√°lisis de fit de integraci√≥n t√©cnica
            technical_integration_fit = {}
            
            integration_score = 0
            integration_factors = {}
            
            # Factor 1: Stack tecnol√≥gico
            if technology_analysis.get("tech_maturity") == "high":
                integration_score += 30
                integration_factors["advanced_tech_stack"] = 30
            elif technology_analysis.get("tech_maturity") == "medium":
                integration_score += 20
                integration_factors["modern_tech_stack"] = 20
            
            # Factor 2: Keywords t√©cnicos
            if keyword_analysis.get("categories_found", []):
                if "technical" in keyword_analysis.get("categories_found", []):
                    integration_score += 25
                    integration_factors["technical_keywords"] = 25
                if "integration" in keyword_analysis.get("categories_found", []):
                    integration_score += 30
                    integration_factors["integration_keywords"] = 30
            
            # Factor 3: Tama√±o de empresa (empresas grandes = m√°s integraci√≥n)
            if employees and employees > 500:
                integration_score += 20
                integration_factors["large_company"] = 20
            
            # Factor 4: Product fit
            if detailed_product_fit.get("recommended_product") in ["enterprise_plan", "api_integration"]:
                integration_score += 25
                integration_factors["integration_product"] = 25
            
            integration_score = min(integration_score, 100)
            
            technical_integration_fit = {
                "integration_score": integration_score,
                "integration_readiness": (
                    "high" if integration_score >= 70 else
                    "medium" if integration_score >= 50 else
                    "low"
                ),
                "integration_factors": integration_factors,
                "recommended_integration_type": (
                    "api" if integration_score >= 70 else
                    "webhook" if integration_score >= 50 else
                    "manual"
                )
            }
            
            # Predicci√≥n de mejor momento para cerrar
            closing_timing_prediction = {}
            
            # Basado en m√∫ltiples factores
            closing_readiness = 0
            
            # Factor 1: Buyer journey stage
            if behavioral_prediction.get("buyer_journey_stage") == "decision":
                closing_readiness += 40
            elif behavioral_prediction.get("buyer_journey_stage") == "comparison":
                closing_readiness += 25
            
            # Factor 2: Purchase intent
            if purchase_intent_scoring.get("intent_score", 0) >= 70:
                closing_readiness += 30
            elif purchase_intent_scoring.get("intent_score", 0) >= 50:
                closing_readiness += 15
            
            # Factor 3: Urgencia
            if factors.get("urgency_detected"):
                closing_readiness += 20
            
            # Factor 4: Budget
            if factors.get("budget_discussion"):
                closing_readiness += 15
            
            # Factor 5: Decision maker
            if influence_analysis.get("is_decision_maker"):
                closing_readiness += 10
            
            closing_readiness = min(closing_readiness, 100)
            
            # D√≠as estimados hasta cierre
            days_to_close = 90  # Default
            if closing_readiness >= 80:
                days_to_close = 7
            elif closing_readiness >= 60:
                days_to_close = 14
            elif closing_readiness >= 40:
                days_to_close = 30
            elif closing_readiness >= 20:
                days_to_close = 60
            
            closing_timing_prediction = {
                "closing_readiness": closing_readiness,
                "readiness_level": (
                    "ready" if closing_readiness >= 80 else
                    "almost_ready" if closing_readiness >= 60 else
                    "getting_close" if closing_readiness >= 40 else
                    "needs_nurturing"
                ),
                "estimated_days_to_close": days_to_close,
                "recommended_action": (
                    "close_now" if closing_readiness >= 80 else
                    "present_proposal" if closing_readiness >= 60 else
                    "schedule_demo" if closing_readiness >= 40 else
                    "continue_nurturing"
                ),
                "confidence": (
                    "high" if closing_readiness >= 70 else
                    "medium" if closing_readiness >= 50 else
                    "low"
                )
            }
            
            # An√°lisis de fit de mercado (market fit)
            market_fit_analysis = {}
            
            market_fit_score = 0
            market_fit_factors = {}
            
            # Factor 1: Tama√±o de mercado (30%)
            if employees and employees > 1000:
                market_fit_score += 30
                market_fit_factors["large_market"] = 30
            elif employees and employees > 100:
                market_fit_score += 20
                market_fit_factors["medium_market"] = 20
            else:
                market_fit_score += 10
                market_fit_factors["small_market"] = 10
            
            # Factor 2: Industria de alto valor (25%)
            high_value_industries = ["finance", "healthcare", "technology", "consulting", "legal", "pharmaceutical"]
            if industry and any(hvi in industry.lower() for hvi in high_value_industries):
                market_fit_score += 25
                market_fit_factors["high_value_industry"] = 25
            
            # Factor 3: Crecimiento de la empresa (20%)
            # Basado en se√±ales de crecimiento (puede venir de enriched_data)
            growth_signals = enriched_data.get("growth_signals", []) if enriched_data else []
            if len(growth_signals) >= 3:
                market_fit_score += 20
                market_fit_factors["high_growth"] = 20
            elif len(growth_signals) >= 1:
                market_fit_score += 10
                market_fit_factors["moderate_growth"] = 10
            
            # Factor 4: Madurez tecnol√≥gica (15%)
            if technology_analysis.get("tech_maturity") == "high":
                market_fit_score += 15
                market_fit_factors["tech_maturity"] = 15
            elif technology_analysis.get("tech_maturity") == "medium":
                market_fit_score += 8
                market_fit_factors["tech_maturity"] = 8
            
            # Factor 5: Presencia geogr√°fica (10%)
            # Empresas con m√∫ltiples ubicaciones = mejor fit
            locations = enriched_data.get("locations", []) if enriched_data else []
            if len(locations) > 1:
                market_fit_score += 10
                market_fit_factors["multi_location"] = 10
            
            market_fit_score = min(market_fit_score, 100)
            
            market_fit_analysis = {
                "market_fit_score": market_fit_score,
                "market_fit_level": (
                    "excellent" if market_fit_score >= 80 else
                    "good" if market_fit_score >= 60 else
                    "fair" if market_fit_score >= 40 else
                    "poor"
                ),
                "market_fit_factors": market_fit_factors,
                "target_market_segment": (
                    "enterprise" if market_fit_score >= 70 else
                    "mid_market" if market_fit_score >= 50 else
                    "smb"
                )
            }
            
            # Predicci√≥n de mejor momento del a√±o para contactar
            seasonal_contact_optimization = {}
            
            current_month = datetime.utcnow().month
            current_quarter = (current_month - 1) // 3 + 1
            
            # Patrones estacionales por industria
            seasonal_scores = {}
            
            # Q1 (Ene-Mar): Mejor para tech, finance (nuevos presupuestos)
            if current_quarter == 1:
                if industry and any(ind in industry.lower() for ind in ["technology", "finance", "consulting"]):
                    seasonal_scores["q1"] = 85
                else:
                    seasonal_scores["q1"] = 70
            
            # Q2 (Abr-Jun): Bueno para mayor√≠a de industrias
            elif current_quarter == 2:
                seasonal_scores["q2"] = 75
            
            # Q3 (Jul-Sep): Mejor para retail, hospitality (preparaci√≥n para temporada alta)
            elif current_quarter == 3:
                if industry and any(ind in industry.lower() for ind in ["retail", "hospitality", "tourism"]):
                    seasonal_scores["q3"] = 80
                else:
                    seasonal_scores["q3"] = 65
            
            # Q4 (Oct-Dic): Mejor para mayor√≠a (uso de presupuesto restante)
            elif current_quarter == 4:
                seasonal_scores["q4"] = 90
            
            current_seasonal_score = seasonal_scores.get(f"q{current_quarter}", 70)
            
            seasonal_contact_optimization = {
                "current_quarter": current_quarter,
                "current_seasonal_score": current_seasonal_score,
                "seasonal_quality": (
                    "excellent" if current_seasonal_score >= 85 else
                    "good" if current_seasonal_score >= 75 else
                    "fair" if current_seasonal_score >= 65 else
                    "poor"
                ),
                "best_quarters": sorted(
                    [(q, sc) for q, sc in seasonal_scores.items()],
                    key=lambda x: x[1],
                    reverse=True
                )[:2],
                "recommendation": (
                    "contact_now" if current_seasonal_score >= 85 else
                    "good_timing" if current_seasonal_score >= 75 else
                    "consider_waiting" if current_seasonal_score < 65 else
                    "proceed"
                )
            }
            
            # An√°lisis de se√±ales de compra por industria
            industry_purchase_signals = {}
            
            # Se√±ales espec√≠ficas por industria
            industry_signals = {}
            
            if industry:
                industry_lower = industry.lower()
                
                # Technology: se√±ales de escalamiento, nuevas tecnolog√≠as
                if "technology" in industry_lower or "saas" in industry_lower:
                    if technology_analysis.get("tech_count", 0) >= 5:
                        industry_signals["tech_stack_expansion"] = True
                    if keyword_analysis.get("categories_found", []):
                        if "scalability" in keyword_analysis.get("categories_found", []):
                            industry_signals["scaling_need"] = True
                
                # Finance: se√±ales de compliance, seguridad
                elif "finance" in industry_lower or "banking" in industry_lower:
                    if keyword_analysis.get("categories_found", []):
                        if "security" in keyword_analysis.get("categories_found", []):
                            industry_signals["security_priority"] = True
                    if technology_analysis.get("tech_maturity") == "high":
                        industry_signals["compliance_readiness"] = True
                
                # Healthcare: se√±ales de HIPAA, integraci√≥n
                elif "healthcare" in industry_lower or "medical" in industry_lower:
                    if keyword_analysis.get("categories_found", []):
                        if "integration" in keyword_analysis.get("categories_found", []):
                            industry_signals["integration_need"] = True
                    if employees and employees > 500:
                        industry_signals["enterprise_healthcare"] = True
                
                # Retail: se√±ales de temporada, e-commerce
                elif "retail" in industry_lower or "ecommerce" in industry_lower:
                    if current_quarter == 3 or current_quarter == 4:
                        industry_signals["seasonal_preparation"] = True
                    if keyword_analysis.get("categories_found", []):
                        if "scalability" in keyword_analysis.get("categories_found", []):
                            industry_signals["scaling_for_season"] = True
            
            industry_purchase_signals = {
                "industry": industry,
                "industry_specific_signals": industry_signals,
                "signal_count": len(industry_signals),
                "industry_fit": (
                    "high" if len(industry_signals) >= 2 else
                    "medium" if len(industry_signals) >= 1 else
                    "low"
                )
            }
            
            # Scoring de probabilidad de renovaci√≥n
            renewal_probability_scoring = {}
            
            renewal_score = 0
            renewal_factors = {}
            
            # Factor 1: Engagement inicial (30%)
            if multi_channel_engagement.get("total_engagement", 0) >= 15:
                renewal_score += 30
                renewal_factors["high_initial_engagement"] = 30
            elif multi_channel_engagement.get("total_engagement", 0) >= 5:
                renewal_score += 15
                renewal_factors["moderate_engagement"] = 15
            
            # Factor 2: Fit de producto (25%)
            if detailed_product_fit.get("recommended_product_fit_score", 0) >= 70:
                renewal_score += 25
                renewal_factors["strong_product_fit"] = 25
            elif detailed_product_fit.get("recommended_product_fit_score", 0) >= 50:
                renewal_score += 12
                renewal_factors["moderate_product_fit"] = 12
            
            # Factor 3: Tama√±o de empresa (20%)
            # Empresas grandes = m√°s probable renovaci√≥n
            if employees and employees > 500:
                renewal_score += 20
                renewal_factors["large_company"] = 20
            elif employees and employees > 100:
                renewal_score += 10
                renewal_factors["medium_company"] = 10
            
            # Factor 4: Sentimiento positivo (15%)
            if sentiment_analysis.get("overall_sentiment") == "positive":
                renewal_score += 15
                renewal_factors["positive_sentiment"] = 15
            
            # Factor 5: Integraci√≥n t√©cnica (10%)
            if technical_integration_fit.get("integration_readiness") == "high":
                renewal_score += 10
                renewal_factors["technical_integration"] = 10
            
            renewal_score = min(renewal_score, 100)
            
            renewal_probability_scoring = {
                "renewal_probability": round(renewal_score / 100.0, 3),
                "renewal_score": renewal_score,
                "renewal_likelihood": (
                    "very_high" if renewal_score >= 80 else
                    "high" if renewal_score >= 60 else
                    "medium" if renewal_score >= 40 else
                    "low"
                ),
                "renewal_factors": renewal_factors
            }
            
            # An√°lisis de fit de producto-servicio granular
            granular_product_fit = {}
            
            # An√°lisis por caracter√≠sticas espec√≠ficas
            product_features = {
                "api_integration": 0,
                "multi_user": 0,
                "enterprise_security": 0,
                "customization": 0,
                "reporting_analytics": 0,
                "mobile_access": 0,
                "third_party_integrations": 0
            }
            
            # Ajustes seg√∫n keywords
            if keyword_analysis.get("categories_found", []):
                if "technical" in keyword_analysis.get("categories_found", []):
                    product_features["api_integration"] += 30
                    product_features["third_party_integrations"] += 25
                if "integration" in keyword_analysis.get("categories_found", []):
                    product_features["third_party_integrations"] += 35
                    product_features["api_integration"] += 25
                if "security" in keyword_analysis.get("categories_found", []):
                    product_features["enterprise_security"] += 40
                if "scalability" in keyword_analysis.get("categories_found", []):
                    product_features["multi_user"] += 30
                    product_features["reporting_analytics"] += 20
            
            # Ajustes seg√∫n tama√±o de empresa
            if employees and employees > 500:
                product_features["multi_user"] += 35
                product_features["enterprise_security"] += 30
                product_features["reporting_analytics"] += 25
            elif employees and employees > 100:
                product_features["multi_user"] += 25
                product_features["reporting_analytics"] += 20
            
            # Ajustes seg√∫n industria
            if industry:
                if "technology" in industry.lower():
                    product_features["api_integration"] += 30
                    product_features["third_party_integrations"] += 25
                elif "finance" in industry.lower():
                    product_features["enterprise_security"] += 35
                    product_features["reporting_analytics"] += 30
                elif "healthcare" in industry.lower():
                    product_features["enterprise_security"] += 30
                    product_features["customization"] += 25
            
            # Features m√°s relevantes
            top_features = sorted(
                product_features.items(),
                key=lambda x: x[1],
                reverse=True
            )[:3]
            
            granular_product_fit = {
                "product_features_scores": product_features,
                "top_features": [f[0] for f in top_features],
                "feature_priorities": dict(top_features),
                "customization_needed": product_features["customization"] >= 30,
                "integration_needed": (
                    product_features["api_integration"] >= 30 or
                    product_features["third_party_integrations"] >= 30
                )
            }
            
            # Predicci√≥n de mejor secuencia de email
            email_sequence_recommendation = {}
            
            sequence_stages = {
                "welcome": 0,
                "education": 0,
                "social_proof": 0,
                "product_demo": 0,
                "pricing": 0,
                "urgency": 0,
                "closing": 0
            }
            
            # Ajustes seg√∫n buyer journey
            if behavioral_prediction.get("buyer_journey_stage") == "awareness":
                sequence_stages["welcome"] += 40
                sequence_stages["education"] += 35
            elif behavioral_prediction.get("buyer_journey_stage") == "research":
                sequence_stages["education"] += 40
                sequence_stages["social_proof"] += 30
            elif behavioral_prediction.get("buyer_journey_stage") == "comparison":
                sequence_stages["social_proof"] += 40
                sequence_stages["product_demo"] += 35
                sequence_stages["pricing"] += 30
            elif behavioral_prediction.get("buyer_journey_stage") == "decision":
                sequence_stages["pricing"] += 45
                sequence_stages["urgency"] += 40
                sequence_stages["closing"] += 35
            
            # Ajustes seg√∫n engagement
            if multi_channel_engagement.get("total_engagement", 0) < 5:
                sequence_stages["welcome"] += 30
                sequence_stages["education"] += 25
            
            # Ajustes seg√∫n competencia
            if competitive_analysis.get("competitors_detected", 0) > 0:
                sequence_stages["social_proof"] += 30
                sequence_stages["product_demo"] += 25
            
            # Secuencia recomendada (orden de stages)
            recommended_sequence = sorted(
                sequence_stages.items(),
                key=lambda x: x[1],
                reverse=True
            )[:5]
            
            email_sequence_recommendation = {
                "recommended_sequence": [s[0] for s in recommended_sequence],
                "stage_scores": sequence_stages,
                "sequence_length": len(recommended_sequence),
                "recommended_timing": {
                    "welcome": "immediate",
                    "education": "day_2",
                    "social_proof": "day_5",
                    "product_demo": "day_7",
                    "pricing": "day_10",
                    "urgency": "day_12",
                    "closing": "day_14"
                }
            }
            
            # An√°lisis de fit de integraci√≥n t√©cnica detallado
            detailed_integration_analysis = {}
            
            integration_types = {
                "api_rest": 0,
                "api_graphql": 0,
                "webhook": 0,
                "sdk": 0,
                "oauth": 0,
                "sftp": 0,
                "manual": 0
            }
            
            # Ajustes seg√∫n stack tecnol√≥gico
            if technology_analysis.get("technologies"):
                techs = [t.lower() for t in technology_analysis.get("technologies", [])]
                if any("graphql" in t for t in techs):
                    integration_types["api_graphql"] += 35
                if any("rest" in t or "api" in t for t in techs):
                    integration_types["api_rest"] += 40
                if any("webhook" in t for t in techs):
                    integration_types["webhook"] += 30
                if any("sdk" in t or "library" in t for t in techs):
                    integration_types["sdk"] += 25
            
            # Ajustes seg√∫n keywords
            if keyword_analysis.get("categories_found", []):
                if "technical" in keyword_analysis.get("categories_found", []):
                    integration_types["api_rest"] += 30
                    integration_types["sdk"] += 25
                if "integration" in keyword_analysis.get("categories_found", []):
                    integration_types["webhook"] += 35
                    integration_types["api_rest"] += 30
            
            # Ajustes seg√∫n tama√±o
            if employees and employees > 500:
                integration_types["api_rest"] += 20
                integration_types["oauth"] += 25
            else:
                integration_types["webhook"] += 20
                integration_types["sdk"] += 20
            
            # Tipo de integraci√≥n recomendado
            best_integration = max(integration_types.items(), key=lambda x: x[1])
            
            detailed_integration_analysis = {
                "recommended_integration_type": best_integration[0],
                "integration_type_scores": integration_types,
                "confidence": "high" if best_integration[1] >= 50 else "medium",
                "complexity": (
                    "high" if best_integration[0] in ["api_rest", "api_graphql", "oauth"] else
                    "medium" if best_integration[0] in ["webhook", "sdk"] else
                    "low"
                ),
                "estimated_setup_time": {
                    "api_rest": "1-2_weeks",
                    "api_graphql": "1-2_weeks",
                    "webhook": "3-5_days",
                    "sdk": "1_week",
                    "oauth": "1-2_weeks",
                    "sftp": "2-3_days",
                    "manual": "ongoing"
                }.get(best_integration[0], "unknown")
            }
            
            # An√°lisis de timing granular (hora del d√≠a, d√≠a de la semana)
            granular_timing_analysis = {}
            
            # An√°lisis por hora del d√≠a
            hour_analysis = {}
            visit_times = web_behavior.get("visit_times", [])
            
            if visit_times:
                hour_distribution = {}
                for visit_time in visit_times:
                    try:
                        hour = int(visit_time.split(":")[0])
                        hour_distribution[hour] = hour_distribution.get(hour, 0) + 1
                    except:
                        pass
                
                if hour_distribution:
                    best_hour = max(hour_distribution.items(), key=lambda x: x[1])[0]
                    hour_analysis = {
                        "best_hour": best_hour,
                        "hour_distribution": hour_distribution,
                        "peak_hours": sorted(
                            hour_distribution.items(),
                            key=lambda x: x[1],
                            reverse=True
                        )[:3],
                        "confidence": "high"
                    }
            
            # An√°lisis por d√≠a de la semana
            weekday_analysis = {}
            visit_days = web_behavior.get("visit_days", [])
            
            if visit_days:
                day_distribution = {}
                for day in visit_days:
                    day_distribution[day] = day_distribution.get(day, 0) + 1
                
                if day_distribution:
                    best_day = max(day_distribution.items(), key=lambda x: x[1])[0]
                    weekday_analysis = {
                        "best_day": best_day,
                        "day_distribution": day_distribution,
                        "confidence": "high"
                    }
            
            # Recomendaci√≥n de timing √≥ptimo
            optimal_timing_window = {}
            if hour_analysis and weekday_analysis:
                optimal_timing_window = {
                    "best_day": weekday_analysis.get("best_day"),
                    "best_hour": hour_analysis.get("best_hour"),
                    "time_window": f"{weekday_analysis.get('best_day')} {hour_analysis.get('best_hour')}:00-{hour_analysis.get('best_hour')+1}:00",
                    "confidence": "high"
                }
            elif hour_analysis:
                optimal_timing_window = {
                    "best_hour": hour_analysis.get("best_hour"),
                    "time_window": f"{hour_analysis.get('best_hour')}:00-{hour_analysis.get('best_hour')+1}:00",
                    "confidence": "medium"
                }
            else:
                # Default basado en buyer persona
                if buyer_persona.get("type") == "decision-maker":
                    optimal_timing_window = {
                        "best_day": "tuesday",
                        "best_hour": 9,
                        "time_window": "Tuesday 09:00-10:00",
                        "confidence": "low",
                        "method": "persona_based"
                    }
                else:
                    optimal_timing_window = {
                        "best_day": "wednesday",
                        "best_hour": 14,
                        "time_window": "Wednesday 14:00-15:00",
                        "confidence": "low",
                        "method": "persona_based"
                    }
            
            granular_timing_analysis = {
                "hour_analysis": hour_analysis,
                "weekday_analysis": weekday_analysis,
                "optimal_timing_window": optimal_timing_window,
                "timing_insights": {
                    "morning_person": hour_analysis.get("best_hour", 12) < 12 if hour_analysis else None,
                    "afternoon_person": 12 <= hour_analysis.get("best_hour", 12) < 17 if hour_analysis else None,
                    "weekday_preference": weekday_analysis.get("best_day") if weekday_analysis else None
                }
            }
            
            # Predicci√≥n de mejor contenido por etapa del buyer journey
            content_by_stage_recommendation = {}
            
            stage_content_map = {
                "awareness": {
                    "blog_post": 30,
                    "whitepaper": 35,
                    "infographic": 25,
                    "video_intro": 20
                },
                "research": {
                    "case_study": 40,
                    "whitepaper": 30,
                    "product_overview": 25,
                    "comparison_guide": 20
                },
                "comparison": {
                    "product_comparison": 45,
                    "case_study": 35,
                    "testimonials": 30,
                    "pricing_guide": 25
                },
                "decision": {
                    "demo_video": 50,
                    "trial_offer": 45,
                    "pricing_guide": 40,
                    "case_study": 30
                }
            }
            
            current_stage = behavioral_prediction.get("buyer_journey_stage", "awareness")
            stage_content = stage_content_map.get(current_stage, {})
            
            # Ajustes seg√∫n topics
            if topic_extraction.get("primary_topic"):
                primary_topic = topic_extraction.get("primary_topic")
                if primary_topic == "pricing":
                    stage_content["pricing_guide"] = stage_content.get("pricing_guide", 0) + 30
                elif primary_topic == "features":
                    stage_content["product_overview"] = stage_content.get("product_overview", 0) + 25
                elif primary_topic == "integration":
                    stage_content["case_study"] = stage_content.get("case_study", 0) + 20
            
            # Mejor contenido para la etapa actual
            best_stage_content = max(stage_content.items(), key=lambda x: x[1]) if stage_content else None
            
            content_by_stage_recommendation = {
                "current_stage": current_stage,
                "recommended_content": best_stage_content[0] if best_stage_content else "blog_post",
                "content_scores": stage_content,
                "alternative_content": sorted(
                    [(c, s) for c, s in stage_content.items() if c != (best_stage_content[0] if best_stage_content else None)],
                    key=lambda x: x[1],
                    reverse=True
                )[:2] if best_stage_content else [],
                "content_sequence": [
                    stage_content_map.get("awareness", {}),
                    stage_content_map.get("research", {}),
                    stage_content_map.get("comparison", {}),
                    stage_content_map.get("decision", {})
                ]
            }
            
            # An√°lisis de se√±ales de compra avanzado
            advanced_purchase_signals = {}
            
            # Se√±ales de compra por categor√≠a
            purchase_signal_categories = {
                "budget_signals": [],
                "urgency_signals": [],
                "authority_signals": [],
                "need_signals": [],
                "timing_signals": []
            }
            
            # Budget signals
            if factors.get("budget_discussion"):
                purchase_signal_categories["budget_signals"].append("budget_discussed")
            if len(budget_signals) > 0:
                purchase_signal_categories["budget_signals"].extend(budget_signals)
            
            # Urgency signals
            if factors.get("urgency_detected"):
                purchase_signal_categories["urgency_signals"].append("urgency_detected")
            if emotion_detection.get("primary_emotion") == "urgent":
                purchase_signal_categories["urgency_signals"].append("urgent_emotion")
            if behavioral_prediction.get("action_probability", 0) >= 0.7:
                purchase_signal_categories["urgency_signals"].append("high_action_probability")
            
            # Authority signals
            if influence_analysis.get("is_decision_maker"):
                purchase_signal_categories["authority_signals"].append("decision_maker")
            if buyer_persona.get("type") == "decision-maker":
                purchase_signal_categories["authority_signals"].append("decision_maker_persona")
            if title and any(role in title.lower() for role in ["ceo", "cto", "cfo", "director", "vp"]):
                purchase_signal_categories["authority_signals"].append("executive_title")
            
            # Need signals
            if behavior_patterns.get("decision_phase"):
                purchase_signal_categories["need_signals"].append("decision_phase")
            if factors.get("has_abandoned_cart"):
                purchase_signal_categories["need_signals"].append("abandoned_cart")
            if factors.get("is_recurrent_lead"):
                purchase_signal_categories["need_signals"].append("returning_lead")
            
            # Timing signals
            if closing_timing_prediction.get("readiness_level") == "ready":
                purchase_signal_categories["timing_signals"].append("ready_to_close")
            if seasonal_contact_optimization.get("seasonal_quality") == "excellent":
                purchase_signal_categories["timing_signals"].append("optimal_season")
            if timing_fit_analysis.get("timing_quality") == "excellent":
                purchase_signal_categories["timing_signals"].append("optimal_timing")
            
            # Score por categor√≠a
            category_scores = {}
            for category, signals in purchase_signal_categories.items():
                category_scores[category] = len(signals) * 20  # 20 puntos por se√±al
            
            total_advanced_signals = sum(len(signals) for signals in purchase_signal_categories.values())
            
            advanced_purchase_signals = {
                "signal_categories": purchase_signal_categories,
                "category_scores": category_scores,
                "total_signal_count": total_advanced_signals,
                "signal_strength": min(total_advanced_signals * 10, 100),
                "strongest_category": max(category_scores.items(), key=lambda x: x[1])[0] if category_scores else None,
                "purchase_readiness": (
                    "very_high" if total_advanced_signals >= 8 else
                    "high" if total_advanced_signals >= 5 else
                    "medium" if total_advanced_signals >= 3 else
                    "low"
                )
            }
            
            # Scoring de calidad de lead con factores ML
            ml_enhanced_quality_scoring = {}
            
            # Factores ML adicionales
            ml_quality_factors = {}
            
            # Factor 1: Consistencia de datos (15%)
            consistency_score = 0
            if email and "@" in email:
                email_domain = email.split("@")[1]
                if company and email_domain in company.lower():
                    consistency_score += 30
                    ml_quality_factors["email_company_match"] = True
            if phone and company:
                # Validaci√≥n b√°sica de formato
                if len(phone.replace("-", "").replace(" ", "")) >= 10:
                    consistency_score += 20
                    ml_quality_factors["phone_format_valid"] = True
            ml_quality_factors["consistency_score"] = consistency_score
            
            # Factor 2: Riqueza de metadata (20%)
            metadata_richness = 0
            if enriched_data:
                metadata_richness += 25
            if social_analysis.get("linkedin_score", 0) > 0:
                metadata_richness += 20
            if technology_analysis.get("tech_count", 0) > 0:
                metadata_richness += 15
            if network_analysis.get("network_size", 0) > 0:
                metadata_richness += 15
            if keyword_analysis.get("keyword_count", 0) > 0:
                metadata_richness += 10
            if topic_extraction.get("topics"):
                metadata_richness += 10
            metadata_richness = min(metadata_richness, 100)
            ml_quality_factors["metadata_richness"] = metadata_richness
            
            # Factor 3: Se√±ales de comportamiento (25%)
            behavior_quality = 0
            if multi_channel_engagement.get("total_engagement", 0) >= 15:
                behavior_quality += 40
            elif multi_channel_engagement.get("total_engagement", 0) >= 5:
                behavior_quality += 25
            if multi_channel_engagement.get("channel_diversity", 0) >= 0.6:
                behavior_quality += 30
            if engagement_velocity.get("velocity") == "fast":
                behavior_quality += 30
            behavior_quality = min(behavior_quality, 100)
            ml_quality_factors["behavior_quality"] = behavior_quality
            
            # Factor 4: Predictibilidad (20%)
            predictability_score = 0
            if historical_pattern_analysis.get("pattern_confidence") == "high":
                predictability_score += 40
            elif historical_pattern_analysis.get("pattern_confidence") == "medium":
                predictability_score += 25
            if similarity_analysis.get("similar_converted_leads", 0) >= 3:
                predictability_score += 30
            if cohort_analysis.get("cohort_id"):
                predictability_score += 20
            predictability_score = min(predictibility_score, 100)
            ml_quality_factors["predictability_score"] = predictability_score
            
            # Factor 5: Se√±ales de compra (20%)
            purchase_signal_quality = 0
            if advanced_purchase_signals.get("total_signal_count", 0) >= 8:
                purchase_signal_quality += 50
            elif advanced_purchase_signals.get("total_signal_count", 0) >= 5:
                purchase_signal_quality += 35
            elif advanced_purchase_signals.get("total_signal_count", 0) >= 3:
                purchase_signal_quality += 20
            if purchase_intent_scoring.get("intent_score", 0) >= 70:
                purchase_signal_quality += 30
            if b2b_purchase_signals.get("signal_strength", 0) >= 70:
                purchase_signal_quality += 20
            purchase_signal_quality = min(purchase_signal_quality, 100)
            ml_quality_factors["purchase_signal_quality"] = purchase_signal_quality
            
            # Score ML total (promedio ponderado)
            ml_quality_score = (
                consistency_score * 0.15 +
                metadata_richness * 0.20 +
                behavior_quality * 0.25 +
                predictability_score * 0.20 +
                purchase_signal_quality * 0.20
            )
            
            ml_enhanced_quality_scoring = {
                "ml_quality_score": round(ml_quality_score, 2),
                "ml_quality_grade": (
                    "A+" if ml_quality_score >= 90 else
                    "A" if ml_quality_score >= 80 else
                    "B" if ml_quality_score >= 70 else
                    "C" if ml_quality_score >= 60 else
                    "D" if ml_quality_score >= 50 else
                    "F"
                ),
                "ml_quality_factors": ml_quality_factors,
                "quality_level": (
                    "excellent" if ml_quality_score >= 85 else
                    "good" if ml_quality_score >= 70 else
                    "fair" if ml_quality_score >= 60 else
                    "poor"
                ),
                "ml_confidence": (
                    "high" if predictability_score >= 70 else
                    "medium" if predictability_score >= 50 else
                    "low"
                )
            }
            
            # An√°lisis de fit de precio detallado con factores adicionales
            detailed_pricing_analysis = {}
            
            # An√°lisis por m√∫ltiples factores
            pricing_factors = {
                "company_size": 0,
                "industry_value": 0,
                "budget_signals": 0,
                "product_fit": 0,
                "market_fit": 0,
                "geographic": 0
            }
            
            # Company size factor
            if employees and employees > 1000:
                pricing_factors["company_size"] = 40
            elif employees and employees > 500:
                pricing_factors["company_size"] = 30
            elif employees and employees > 100:
                pricing_factors["company_size"] = 20
            else:
                pricing_factors["company_size"] = 10
            
            # Industry value factor
            high_value_industries = ["finance", "healthcare", "technology", "consulting", "legal", "pharmaceutical"]
            if industry and any(hvi in industry.lower() for hvi in high_value_industries):
                pricing_factors["industry_value"] = 35
            else:
                pricing_factors["industry_value"] = 15
            
            # Budget signals factor
            if factors.get("budget_discussion"):
                if any(kw in str(budget_signals).lower() for kw in ["enterprise", "large", "unlimited", "budget"]):
                    pricing_factors["budget_signals"] = 30
                else:
                    pricing_factors["budget_signals"] = 15
            
            # Product fit factor
            if detailed_product_fit.get("recommended_product_fit_score", 0) >= 70:
                pricing_factors["product_fit"] = 30
            elif detailed_product_fit.get("recommended_product_fit_score", 0) >= 50:
                pricing_factors["product_fit"] = 20
            
            # Market fit factor
            if market_fit_analysis.get("market_fit_score", 0) >= 70:
                pricing_factors["market_fit"] = 25
            elif market_fit_analysis.get("market_fit_score", 0) >= 50:
                pricing_factors["market_fit"] = 15
            
            # Geographic factor
            locations = enriched_data.get("locations", []) if enriched_data else []
            if len(locations) > 1:
                pricing_factors["geographic"] = 20
            elif len(locations) == 1:
                pricing_factors["geographic"] = 10
            
            # Score total de pricing
            total_pricing_score = sum(pricing_factors.values())
            
            # Tier recomendado basado en score total
            recommended_tier = "starter"
            if total_pricing_score >= 150:
                recommended_tier = "enterprise"
            elif total_pricing_score >= 100:
                recommended_tier = "professional"
            elif total_pricing_score >= 60:
                recommended_tier = "starter"
            
            detailed_pricing_analysis = {
                "pricing_factors": pricing_factors,
                "total_pricing_score": total_pricing_score,
                "recommended_tier": recommended_tier,
                "tier_confidence": (
                    "high" if total_pricing_score >= 120 else
                    "medium" if total_pricing_score >= 80 else
                    "low"
                ),
                "price_sensitivity": (
                    "low" if total_pricing_score >= 120 else
                    "medium" if total_pricing_score >= 80 else
                    "high"
                ),
                "estimated_price_range": {
                    "enterprise": "$10k-$50k+",
                    "professional": "$5k-$10k",
                    "starter": "$1k-$5k"
                }.get(recommended_tier, "unknown")
            }
            
            # An√°lisis de competencia profundo
            deep_competitive_analysis = {}
            
            # An√°lisis de competidores mencionados
            competitor_mentions = competitive_analysis.get("competitor_mentions", [])
            
            # Categorizar competidores por tipo
            competitor_categories = {
                "direct_competitors": [],
                "indirect_competitors": [],
                "alternatives": []
            }
            
            # Clasificar competidores (simplificado - en producci√≥n usar√≠a base de datos)
            known_direct = ["competitor1", "competitor2", "competitor3"]
            known_indirect = ["alternative1", "alternative2"]
            
            for comp in competitor_mentions:
                comp_lower = comp.lower()
                if any(dc in comp_lower for dc in known_direct):
                    competitor_categories["direct_competitors"].append(comp)
                elif any(ic in comp_lower for ic in known_indirect):
                    competitor_categories["indirect_competitors"].append(comp)
                else:
                    competitor_categories["alternatives"].append(comp)
            
            # An√°lisis de riesgo competitivo
            competitive_risk_factors = []
            competitive_risk_score = 0
            
            if len(competitor_categories["direct_competitors"]) >= 2:
                competitive_risk_score += 40
                competitive_risk_factors.append("multiple_direct_competitors")
            elif len(competitor_categories["direct_competitors"]) >= 1:
                competitive_risk_score += 25
                competitive_risk_factors.append("direct_competitor_mentioned")
            
            if len(competitor_categories["indirect_competitors"]) >= 2:
                competitive_risk_score += 20
                competitive_risk_factors.append("multiple_indirect_competitors")
            
            if competitive_analysis.get("competitive_research_level") == "high":
                competitive_risk_score += 15
                competitive_risk_factors.append("deep_competitive_research")
            
            competitive_risk_score = min(competitive_risk_score, 100)
            
            # Estrategias competitivas recomendadas
            competitive_strategies = []
            
            if len(competitor_categories["direct_competitors"]) > 0:
                competitive_strategies.append({
                    "strategy": "differentiation",
                    "priority": "high",
                    "description": "Enfatizar diferenciadores √∫nicos vs competidores directos",
                    "tactics": ["unique_features", "case_studies", "testimonials"]
                })
            
            if competitive_risk_score >= 50:
                competitive_strategies.append({
                    "strategy": "value_proposition",
                    "priority": "high",
                    "description": "Reforzar propuesta de valor y ROI",
                    "tactics": ["roi_calculator", "pricing_transparency", "trial_offer"]
                })
            
            if competitive_analysis.get("competitive_research_level") == "high":
                competitive_strategies.append({
                    "strategy": "consultative_selling",
                    "priority": "medium",
                    "description": "Enfoque consultativo para entender necesidades espec√≠ficas",
                    "tactics": ["needs_assessment", "custom_demo", "expert_consultation"]
                })
            
            deep_competitive_analysis = {
                "competitor_categories": competitor_categories,
                "competitive_risk_score": competitive_risk_score,
                "competitive_risk_level": (
                    "critical" if competitive_risk_score >= 70 else
                    "high" if competitive_risk_score >= 50 else
                    "medium" if competitive_risk_score >= 30 else
                    "low"
                ),
                "competitive_risk_factors": competitive_risk_factors,
                "competitive_strategies": competitive_strategies,
                "recommended_actions": [
                    "immediate_contact" if competitive_risk_score >= 50 else None,
                    "competitive_comparison" if len(competitor_categories["direct_competitors"]) > 0 else None,
                    "value_demonstration" if competitive_risk_score >= 40 else None
                ]
            }
            deep_competitive_analysis["recommended_actions"] = [
                a for a in deep_competitive_analysis["recommended_actions"] if a
            ]
            
            # Predicci√≥n de mejor momento para upselling
            upselling_timing_prediction = {}
            
            # Factores que indican readiness para upselling
            upselling_readiness = 0
            upselling_factors = {}
            
            # Factor 1: Uso actual del producto (si aplica)
            product_usage = metadata.get("product_usage", {})
            if product_usage:
                usage_score = product_usage.get("usage_score", 0)
                if usage_score >= 80:
                    upselling_readiness += 30
                    upselling_factors["high_usage"] = 30
                elif usage_score >= 60:
                    upselling_readiness += 20
                    upselling_factors["moderate_usage"] = 20
            
            # Factor 2: Engagement alto
            if multi_channel_engagement.get("total_engagement", 0) >= 15:
                upselling_readiness += 25
                upselling_factors["high_engagement"] = 25
            
            # Factor 3: Sentimiento positivo
            if sentiment_analysis.get("overall_sentiment") == "positive":
                upselling_readiness += 20
                upselling_factors["positive_sentiment"] = 20
            
            # Factor 4: Crecimiento de la empresa
            growth_signals = enriched_data.get("growth_signals", []) if enriched_data else []
            if len(growth_signals) >= 2:
                upselling_readiness += 15
                upselling_factors["company_growth"] = 15
            
            # Factor 5: Necesidades detectadas
            if granular_product_fit.get("integration_needed"):
                upselling_readiness += 10
                upselling_factors["integration_need"] = 10
            
            upselling_readiness = min(upselling_readiness, 100)
            
            # Timing recomendado
            upselling_timing = "not_ready"
            if upselling_readiness >= 70:
                upselling_timing = "immediate"
            elif upselling_readiness >= 50:
                upselling_timing = "within_30_days"
            elif upselling_readiness >= 30:
                upselling_timing = "within_90_days"
            
            upselling_timing_prediction = {
                "upselling_readiness": upselling_readiness,
                "readiness_level": (
                    "ready" if upselling_readiness >= 70 else
                    "almost_ready" if upselling_readiness >= 50 else
                    "getting_ready" if upselling_readiness >= 30 else
                    "not_ready"
                ),
                "recommended_timing": upselling_timing,
                "upselling_factors": upselling_factors,
                "recommended_products": (
                    ["enterprise_features", "advanced_analytics"] if upselling_readiness >= 70 else
                    ["professional_features"] if upselling_readiness >= 50 else
                    []
                )
            }
            
            # An√°lisis de engagement avanzado
            advanced_engagement_analysis = {}
            
            # An√°lisis de engagement por dimensi√≥n
            engagement_dimensions = {
                "frequency": 0,
                "recency": 0,
                "depth": 0,
                "breadth": 0,
                "consistency": 0
            }
            
            # Frequency: qu√© tan frecuente es el engagement
            total_visits = web_behavior.get("return_visits", 0) + 1
            days_active = metadata.get("days_active", 1)
            if days_active > 0:
                frequency_score = (total_visits / days_active) * 10
                engagement_dimensions["frequency"] = min(frequency_score, 100)
            
            # Recency: qu√© tan reciente es el √∫ltimo engagement
            days_since_last_activity = metadata.get("days_since_last_activity", 0)
            if days_since_last_activity == 0:
                engagement_dimensions["recency"] = 100
            elif days_since_last_activity <= 1:
                engagement_dimensions["recency"] = 80
            elif days_since_last_activity <= 7:
                engagement_dimensions["recency"] = 60
            elif days_since_last_activity <= 30:
                engagement_dimensions["recency"] = 40
            else:
                engagement_dimensions["recency"] = 20
            
            # Depth: profundidad del engagement
            depth_score = 0
            if web_behavior.get("time_on_page", 0) > 300:
                depth_score += 30
            if web_behavior.get("pages_visited", 0) >= 5:
                depth_score += 30
            if web_behavior.get("scroll_depth", 0) >= 80:
                depth_score += 20
            if web_behavior.get("cta_clicks", 0) >= 2:
                depth_score += 20
            engagement_dimensions["depth"] = depth_score
            
            # Breadth: amplitud del engagement (m√∫ltiples canales)
            breadth_score = multi_channel_engagement.get("channel_diversity", 0) * 100
            engagement_dimensions["breadth"] = breadth_score
            
            # Consistency: consistencia del engagement
            consistency_score = 0
            if total_visits >= 5:
                consistency_score += 40
            if days_active >= 30:
                consistency_score += 30
            if engagement_velocity.get("velocity") == "fast":
                consistency_score += 30
            engagement_dimensions["consistency"] = consistency_score
            
            # Score total de engagement avanzado
            advanced_engagement_score = sum(engagement_dimensions.values()) / len(engagement_dimensions)
            
            advanced_engagement_analysis = {
                "engagement_dimensions": engagement_dimensions,
                "advanced_engagement_score": round(advanced_engagement_score, 2),
                "engagement_profile": (
                    "power_user" if advanced_engagement_score >= 80 else
                    "active_user" if advanced_engagement_score >= 60 else
                    "moderate_user" if advanced_engagement_score >= 40 else
                    "light_user"
                ),
                "strongest_dimension": max(engagement_dimensions.items(), key=lambda x: x[1])[0] if engagement_dimensions else None,
                "improvement_opportunities": [
                    dim for dim, score in engagement_dimensions.items() if score < 50
                ]
            }
            
            # Scoring de riesgo sofisticado
            sophisticated_risk_scoring = {}
            
            # M√∫ltiples categor√≠as de riesgo
            risk_categories = {
                "churn_risk": 0,
                "competitive_risk": 0,
                "engagement_risk": 0,
                "quality_risk": 0,
                "timing_risk": 0
            }
            
            # Churn risk
            risk_categories["churn_risk"] = churn_risk_score * 100
            
            # Competitive risk
            risk_categories["competitive_risk"] = competitive_risk_analysis.get("risk_score", 0)
            
            # Engagement risk
            if advanced_engagement_score < 40:
                risk_categories["engagement_risk"] = 60
            elif advanced_engagement_score < 60:
                risk_categories["engagement_risk"] = 40
            elif advanced_engagement_score < 80:
                risk_categories["engagement_risk"] = 20
            
            # Quality risk
            if advanced_quality_scoring.get("quality_score", 100) < 60:
                risk_categories["quality_risk"] = 50
            elif advanced_quality_scoring.get("quality_score", 100) < 70:
                risk_categories["quality_risk"] = 30
            
            # Timing risk
            if seasonal_contact_optimization.get("seasonal_quality") == "poor":
                risk_categories["timing_risk"] = 40
            elif timing_fit_analysis.get("timing_quality") == "poor":
                risk_categories["timing_risk"] = 30
            
            # Score total de riesgo (promedio ponderado)
            total_risk_score = (
                risk_categories["churn_risk"] * 0.30 +
                risk_categories["competitive_risk"] * 0.25 +
                risk_categories["engagement_risk"] * 0.20 +
                risk_categories["quality_risk"] * 0.15 +
                risk_categories["timing_risk"] * 0.10
            )
            
            # Factores de riesgo cr√≠ticos
            critical_risk_factors = []
            if risk_categories["churn_risk"] >= 70:
                critical_risk_factors.append("high_churn_risk")
            if risk_categories["competitive_risk"] >= 70:
                critical_risk_factors.append("high_competitive_risk")
            if risk_categories["engagement_risk"] >= 60:
                critical_risk_factors.append("high_engagement_risk")
            
            sophisticated_risk_scoring = {
                "risk_categories": risk_categories,
                "total_risk_score": round(total_risk_score, 2),
                "overall_risk_level": (
                    "critical" if total_risk_score >= 70 else
                    "high" if total_risk_score >= 50 else
                    "medium" if total_risk_score >= 30 else
                    "low"
                ),
                "critical_risk_factors": critical_risk_factors,
                "risk_mitigation_priority": (
                    "immediate" if total_risk_score >= 70 else
                    "high" if total_risk_score >= 50 else
                    "medium" if total_risk_score >= 30 else
                    "low"
                ),
                "recommended_mitigation_strategies": [
                    "re_engagement_campaign" if risk_categories["engagement_risk"] >= 50 else None,
                    "competitive_differentiation" if risk_categories["competitive_risk"] >= 50 else None,
                    "churn_prevention" if risk_categories["churn_risk"] >= 50 else None,
                    "data_quality_improvement" if risk_categories["quality_risk"] >= 40 else None,
                    "timing_optimization" if risk_categories["timing_risk"] >= 30 else None
                ]
            }
            sophisticated_risk_scoring["recommended_mitigation_strategies"] = [
                s for s in sophisticated_risk_scoring["recommended_mitigation_strategies"] if s
            ]
            
            # An√°lisis de fit de producto ultra granular
            ultra_granular_product_fit = {}
            
            # An√°lisis por caracter√≠sticas espec√≠ficas con scoring detallado
            feature_fit_scores = {
                "api_integration": {
                    "score": product_features.get("api_integration", 0),
                    "importance": "high" if keyword_analysis.get("categories_found", []).count("technical") > 0 else "medium",
                    "recommended": product_features.get("api_integration", 0) >= 30
                },
                "multi_user": {
                    "score": product_features.get("multi_user", 0),
                    "importance": "high" if employees and employees > 100 else "medium",
                    "recommended": product_features.get("multi_user", 0) >= 30
                },
                "enterprise_security": {
                    "score": product_features.get("enterprise_security", 0),
                    "importance": "high" if industry and "finance" in industry.lower() else "medium",
                    "recommended": product_features.get("enterprise_security", 0) >= 30
                },
                "customization": {
                    "score": product_features.get("customization", 0),
                    "importance": "medium",
                    "recommended": product_features.get("customization", 0) >= 30
                },
                "reporting_analytics": {
                    "score": product_features.get("reporting_analytics", 0),
                    "importance": "high" if employees and employees > 500 else "medium",
                    "recommended": product_features.get("reporting_analytics", 0) >= 25
                },
                "mobile_access": {
                    "score": product_features.get("mobile_access", 0),
                    "importance": "medium",
                    "recommended": product_features.get("mobile_access", 0) >= 20
                },
                "third_party_integrations": {
                    "score": product_features.get("third_party_integrations", 0),
                    "importance": "high" if keyword_analysis.get("categories_found", []).count("integration") > 0 else "medium",
                    "recommended": product_features.get("third_party_integrations", 0) >= 30
                }
            }
            
            # Features cr√≠ticas recomendadas
            critical_features = [
                feat for feat, data in feature_fit_scores.items()
                if data["recommended"] and data["importance"] == "high"
            ]
            
            # Score de fit por feature
            feature_fit_total = sum(data["score"] for data in feature_fit_scores.values())
            feature_fit_average = feature_fit_total / len(feature_fit_scores) if feature_fit_scores else 0
            
            ultra_granular_product_fit = {
                "feature_fit_scores": feature_fit_scores,
                "critical_features": critical_features,
                "feature_fit_average": round(feature_fit_average, 2),
                "feature_fit_level": (
                    "excellent" if feature_fit_average >= 70 else
                    "good" if feature_fit_average >= 50 else
                    "fair" if feature_fit_average >= 30 else
                    "poor"
                ),
                "recommended_feature_priority": sorted(
                    [(feat, data["score"]) for feat, data in feature_fit_scores.items()],
                    key=lambda x: x[1],
                    reverse=True
                )[:5]
            }
            
            # An√°lisis de comportamiento predictivo avanzado
            predictive_behavioral_analysis = {}
            
            # Patrones de comportamiento hist√≥ricos
            behavioral_patterns = {
                "visit_pattern": "unknown",
                "engagement_trend": "unknown",
                "content_preference": "unknown",
                "interaction_style": "unknown"
            }
            
            # Analizar patr√≥n de visitas
            visit_times = metadata.get("visit_times", [])
            if visit_times:
                # Determinar si hay un patr√≥n (ma√±ana, tarde, noche)
                morning_visits = sum(1 for t in visit_times if 6 <= t < 12)
                afternoon_visits = sum(1 for t in visit_times if 12 <= t < 18)
                evening_visits = sum(1 for t in visit_times if 18 <= t < 24 or t < 6)
                
                if morning_visits > afternoon_visits and morning_visits > evening_visits:
                    behavioral_patterns["visit_pattern"] = "morning_person"
                elif afternoon_visits > evening_visits:
                    behavioral_patterns["visit_pattern"] = "afternoon_person"
                else:
                    behavioral_patterns["visit_pattern"] = "evening_person"
            
            # Analizar tendencia de engagement
            engagement_history = metadata.get("engagement_history", [])
            if len(engagement_history) >= 3:
                recent_avg = sum(engagement_history[-3:]) / 3
                older_avg = sum(engagement_history[:-3]) / max(len(engagement_history) - 3, 1)
                
                if recent_avg > older_avg * 1.2:
                    behavioral_patterns["engagement_trend"] = "increasing"
                elif recent_avg < older_avg * 0.8:
                    behavioral_patterns["engagement_trend"] = "decreasing"
                else:
                    behavioral_patterns["engagement_trend"] = "stable"
            
            # Preferencia de contenido
            content_interactions = metadata.get("content_interactions", {})
            if content_interactions:
                max_interactions = max(content_interactions.values()) if content_interactions.values() else 0
                preferred_content = [
                    k for k, v in content_interactions.items()
                    if v == max_interactions
                ]
                if preferred_content:
                    behavioral_patterns["content_preference"] = preferred_content[0]
            
            # Estilo de interacci√≥n
            interaction_types = metadata.get("interaction_types", {})
            if interaction_types:
                total_interactions = sum(interaction_types.values())
                if total_interactions > 0:
                    email_ratio = interaction_types.get("email", 0) / total_interactions
                    web_ratio = interaction_types.get("web", 0) / total_interactions
                    phone_ratio = interaction_types.get("phone", 0) / total_interactions
                    
                    if email_ratio > 0.5:
                        behavioral_patterns["interaction_style"] = "email_preferred"
                    elif web_ratio > 0.5:
                        behavioral_patterns["interaction_style"] = "web_preferred"
                    elif phone_ratio > 0.3:
                        behavioral_patterns["interaction_style"] = "phone_preferred"
                    else:
                        behavioral_patterns["interaction_style"] = "multi_channel"
            
            # Predicci√≥n de pr√≥xima acci√≥n
            next_action_probability = {
                "email_open": 0.3,
                "page_visit": 0.4,
                "form_submit": 0.2,
                "purchase": 0.1
            }
            
            # Ajustar probabilidades basado en comportamiento
            if behavioral_patterns["engagement_trend"] == "increasing":
                next_action_probability["purchase"] += 0.1
                next_action_probability["form_submit"] += 0.05
            
            if behavioral_patterns["interaction_style"] == "email_preferred":
                next_action_probability["email_open"] += 0.2
            elif behavioral_patterns["interaction_style"] == "web_preferred":
                next_action_probability["page_visit"] += 0.2
            
            # Normalizar probabilidades
            total_prob = sum(next_action_probability.values())
            next_action_probability = {
                k: round(v / total_prob, 3)
                for k, v in next_action_probability.items()
            }
            
            predicted_next_action = max(next_action_probability.items(), key=lambda x: x[1])[0]
            
            predictive_behavioral_analysis = {
                "behavioral_patterns": behavioral_patterns,
                "next_action_probability": next_action_probability,
                "predicted_next_action": predicted_next_action,
                "behavioral_confidence": (
                    "high" if len(engagement_history) >= 5 else
                    "medium" if len(engagement_history) >= 3 else
                    "low"
                ) if engagement_history else "low"
            }
            
            # Scoring de intenci√≥n de compra con ML avanzado
            ml_purchase_intent_scoring = {}
            
            # Factores ML para intenci√≥n de compra
            ml_intent_factors = {
                "behavioral_signals": 0,
                "temporal_signals": 0,
                "contextual_signals": 0,
                "social_signals": 0,
                "engagement_signals": 0
            }
            
            # Behavioral signals (40% weight)
            behavioral_score = 0
            if web_behavior.get("return_visits", 0) >= 3:
                behavioral_score += 20
            if web_behavior.get("pages_visited", 0) >= 5:
                behavioral_score += 20
            if web_behavior.get("time_on_page", 0) > 300:
                behavioral_score += 20
            if web_behavior.get("cta_clicks", 0) >= 2:
                behavioral_score += 20
            if behavioral_patterns["engagement_trend"] == "increasing":
                behavioral_score += 20
            ml_intent_factors["behavioral_signals"] = behavioral_score
            
            # Temporal signals (20% weight)
            temporal_score = 0
            days_since_first_contact = metadata.get("days_since_first_contact", 0)
            if 7 <= days_since_first_contact <= 30:
                temporal_score += 30  # Sweet spot
            elif days_since_first_contact <= 7:
                temporal_score += 20  # Early stage
            elif days_since_first_contact <= 60:
                temporal_score += 15  # Still engaged
            
            # Timing √≥ptimo
            current_hour = metadata.get("current_hour", 12)
            if 9 <= current_hour <= 17:
                temporal_score += 20
            
            # D√≠a de la semana
            current_day = metadata.get("current_day_of_week", 3)
            if 1 <= current_day <= 4:  # Martes a Viernes
                temporal_score += 20
            
            ml_intent_factors["temporal_signals"] = temporal_score
            
            # Contextual signals (20% weight)
            contextual_score = 0
            if budget_signals:
                contextual_score += 30
            if urgency_signals:
                contextual_score += 25
            if authority_signals:
                contextual_score += 20
            if need_signals:
                contextual_score += 15
            if timing_signals:
                contextual_score += 10
            ml_intent_factors["contextual_signals"] = min(contextual_score, 100)
            
            # Social signals (10% weight)
            social_score = 0
            if influence_analysis.get("influence_score", 0) >= 70:
                social_score += 30
            if network_analysis.get("network_size", 0) >= 10:
                social_score += 20
            if social_analysis.get("linkedin_score", 0) >= 50:
                social_score += 25
            if buyer_persona.get("type") == "decision-maker":
                social_score += 25
            ml_intent_factors["social_signals"] = min(social_score, 100)
            
            # Engagement signals (10% weight)
            engagement_score = 0
            if advanced_engagement_score >= 80:
                engagement_score += 40
            elif advanced_engagement_score >= 60:
                engagement_score += 30
            elif advanced_engagement_score >= 40:
                engagement_score += 20
            
            if multi_channel_engagement.get("total_engagement", 0) >= 15:
                engagement_score += 30
            
            if engagement_velocity.get("velocity") == "fast":
                engagement_score += 30
            ml_intent_factors["engagement_signals"] = min(engagement_score, 100)
            
            # Calcular score ML de intenci√≥n de compra (ponderado)
            ml_purchase_intent_score = (
                ml_intent_factors["behavioral_signals"] * 0.40 +
                ml_intent_factors["temporal_signals"] * 0.20 +
                ml_intent_factors["contextual_signals"] * 0.20 +
                ml_intent_factors["social_signals"] * 0.10 +
                ml_intent_factors["engagement_signals"] * 0.10
            )
            
            ml_purchase_intent_level = (
                "very_high" if ml_purchase_intent_score >= 80 else
                "high" if ml_purchase_intent_score >= 60 else
                "medium" if ml_purchase_intent_score >= 40 else
                "low"
            )
            
            ml_purchase_intent_scoring = {
                "ml_intent_score": round(ml_purchase_intent_score, 2),
                "ml_intent_level": ml_purchase_intent_level,
                "ml_intent_factors": ml_intent_factors,
                "predicted_days_to_purchase": (
                    1 if ml_purchase_intent_score >= 80 else
                    3 if ml_purchase_intent_score >= 60 else
                    7 if ml_purchase_intent_score >= 40 else
                    14
                ),
                "confidence_level": (
                    "high" if all(v >= 50 for v in ml_intent_factors.values()) else
                    "medium" if sum(1 for v in ml_intent_factors.values() if v >= 50) >= 3 else
                    "low"
                )
            }
            
            # Recomendaciones de estrategia de venta personalizadas
            personalized_sales_strategy = {}
            
            # Determinar tipo de estrategia basado en perfil del lead
            strategy_type = "consultative"
            strategy_priority = "medium"
            strategy_tactics = []
            
            # An√°lisis de perfil para estrategia
            if buyer_persona.get("type") == "decision-maker":
                strategy_type = "executive"
                strategy_priority = "high"
                strategy_tactics.extend([
                    "high_level_value_proposition",
                    "roi_focused_discussion",
                    "executive_summary"
                ])
            elif buyer_persona.get("type") == "influencer":
                strategy_type = "relationship"
                strategy_tactics.extend([
                    "build_trust",
                    "provide_resources",
                    "peer_references"
                ])
            
            # Ajustar seg√∫n engagement
            if advanced_engagement_score >= 80:
                strategy_type = "accelerated"
                strategy_priority = "high"
                strategy_tactics.extend([
                    "fast_track_demo",
                    "immediate_follow_up",
                    "special_offer"
                ])
            elif advanced_engagement_score < 40:
                strategy_type = "nurturing"
                strategy_priority = "low"
                strategy_tactics.extend([
                    "educational_content",
                    "gradual_engagement",
                    "value_first_approach"
                ])
            
            # Ajustar seg√∫n riesgo competitivo
            if deep_competitive_analysis.get("competitive_risk_score", 0) >= 50:
                strategy_priority = "high"
                strategy_tactics.extend([
                    "competitive_differentiation",
                    "unique_value_proposition",
                    "case_studies"
                ])
            
            # Ajustar seg√∫n intenci√≥n de compra ML
            if ml_purchase_intent_score >= 70:
                strategy_type = "closing"
                strategy_priority = "critical"
                strategy_tactics.extend([
                    "proposal_preparation",
                    "decision_maker_meeting",
                    "trial_offer"
                ])
            
            # Mensaje personalizado recomendado
            recommended_message = {
                "tone": "professional",
                "focus": "value",
                "length": "medium",
                "cta": "schedule_demo"
            }
            
            if sentiment_analysis.get("overall_sentiment") == "positive":
                recommended_message["tone"] = "enthusiastic"
            elif sentiment_analysis.get("overall_sentiment") == "negative":
                recommended_message["tone"] = "empathetic"
            
            if ml_purchase_intent_score >= 70:
                recommended_message["focus"] = "closing"
                recommended_message["cta"] = "request_proposal"
            elif ml_purchase_intent_score < 40:
                recommended_message["focus"] = "education"
                recommended_message["cta"] = "download_resource"
            
            personalized_sales_strategy = {
                "strategy_type": strategy_type,
                "strategy_priority": strategy_priority,
                "strategy_tactics": list(set(strategy_tactics)),  # Remove duplicates
                "recommended_message": recommended_message,
                "optimal_contact_method": (
                    "phone" if behavioral_patterns["interaction_style"] == "phone_preferred" else
                    "email" if behavioral_patterns["interaction_style"] == "email_preferred" else
                    "web_chat" if behavioral_patterns["interaction_style"] == "web_preferred" else
                    "multi_channel"
                ),
                "expected_response_time": (
                    "immediate" if ml_purchase_intent_score >= 70 else
                    "within_1_hour" if ml_purchase_intent_score >= 50 else
                    "within_24_hours" if ml_purchase_intent_score >= 30 else
                    "within_48_hours"
                ),
                "recommended_follow_up_sequence": (
                    "aggressive" if ml_purchase_intent_score >= 70 else
                    "standard" if ml_purchase_intent_score >= 40 else
                    "gentle"
                )
            }
            
            # An√°lisis de ciclo de venta optimizado
            optimized_sales_cycle_analysis = {}
            
            # Determinar etapa actual del ciclo
            current_stage = "awareness"
            if ml_purchase_intent_score >= 70:
                current_stage = "closing"
            elif ml_purchase_intent_score >= 50:
                current_stage = "evaluation"
            elif ml_purchase_intent_score >= 30:
                current_stage = "consideration"
            elif advanced_engagement_score >= 40:
                current_stage = "interest"
            
            # Predicci√≥n de duraci√≥n del ciclo
            predicted_cycle_duration = {
                "min_days": 7,
                "max_days": 30,
                "expected_days": 14
            }
            
            # Ajustar seg√∫n factores
            if buyer_persona.get("type") == "decision-maker":
                predicted_cycle_duration["expected_days"] = 10
                predicted_cycle_duration["min_days"] = 5
            elif employees and employees > 1000:
                predicted_cycle_duration["expected_days"] = 21
                predicted_cycle_duration["max_days"] = 45
            
            if ml_purchase_intent_score >= 70:
                predicted_cycle_duration["expected_days"] = 5
                predicted_cycle_duration["min_days"] = 1
                predicted_cycle_duration["max_days"] = 10
            
            # Probabilidad de cierre por etapa
            stage_closing_probability = {
                "awareness": 0.05,
                "interest": 0.15,
                "consideration": 0.30,
                "evaluation": 0.60,
                "closing": 0.85
            }
            
            current_closing_probability = stage_closing_probability.get(current_stage, 0.10)
            
            # Acciones recomendadas por etapa
            stage_actions = {
                "awareness": [
                    "educational_content",
                    "brand_awareness",
                    "thought_leadership"
                ],
                "interest": [
                    "product_overview",
                    "benefit_focused_content",
                    "light_touch_engagement"
                ],
                "consideration": [
                    "detailed_demo",
                    "case_studies",
                    "pricing_information"
                ],
                "evaluation": [
                    "trial_offer",
                    "technical_documentation",
                    "reference_calls"
                ],
                "closing": [
                    "proposal",
                    "contract_negotiation",
                    "decision_maker_meeting"
                ]
            }
            
            recommended_actions = stage_actions.get(current_stage, [])
            
            # Optimizaci√≥n del ciclo
            cycle_optimization_opportunities = []
            
            if current_stage == "awareness" and advanced_engagement_score >= 60:
                cycle_optimization_opportunities.append("accelerate_to_interest")
            
            if current_stage == "consideration" and ml_purchase_intent_score >= 50:
                cycle_optimization_opportunities.append("accelerate_to_evaluation")
            
            if current_stage == "evaluation" and ml_purchase_intent_score >= 70:
                cycle_optimization_opportunities.append("accelerate_to_closing")
            
            if deep_competitive_analysis.get("competitive_risk_score", 0) >= 50:
                cycle_optimization_opportunities.append("competitive_acceleration")
            
            optimized_sales_cycle_analysis = {
                "current_stage": current_stage,
                "predicted_cycle_duration": predicted_cycle_duration,
                "current_closing_probability": round(current_closing_probability, 3),
                "stage_closing_probability": stage_closing_probability,
                "recommended_actions": recommended_actions,
                "cycle_optimization_opportunities": cycle_optimization_opportunities,
                "stage_progress_percentage": (
                    100 if current_stage == "closing" else
                    80 if current_stage == "evaluation" else
                    60 if current_stage == "consideration" else
                    40 if current_stage == "interest" else
                    20
                ),
                "next_stage": (
                    "interest" if current_stage == "awareness" else
                    "consideration" if current_stage == "interest" else
                    "evaluation" if current_stage == "consideration" else
                    "closing" if current_stage == "evaluation" else
                    "closed"
                )
            }
            
            # ============================================
            # NUEVAS FUNCIONALIDADES ULTRA AVANZADAS
            # ============================================
            
            # 1. AN√ÅLISIS DE ROI PREDICTIVO
            predictive_roi_analysis = {}
            try:
                # Calcular ROI esperado basado en m√∫ltiples factores
                base_ltv = estimated_ltv
                acquisition_cost = 100  # Costo de adquisici√≥n estimado
                
                # Factores que afectan el ROI
                roi_factors = {
                    "ltv_multiplier": 1.0,
                    "conversion_probability_multiplier": conversion_probability / 100,
                    "engagement_multiplier": min(advanced_engagement_analysis.get("advanced_engagement_score", 0) / 100, 1.5) if advanced_engagement_analysis else 1.0,
                    "product_fit_multiplier": product_fit_score / 100,
                    "competitive_risk_penalty": max(0, 1 - (deep_competitive_analysis.get("competitive_risk_score", 0) / 200)) if deep_competitive_analysis else 1.0,
                    "timing_multiplier": 1.2 if granular_timing_analysis and granular_timing_analysis.get("optimal_timing_window", {}).get("quality") == "optimal" else 1.0
                }
                
                adjusted_ltv = base_ltv * roi_factors["ltv_multiplier"] * roi_factors["conversion_probability_multiplier"] * \
                              roi_factors["engagement_multiplier"] * roi_factors["product_fit_multiplier"] * \
                              roi_factors["competitive_risk_penalty"] * roi_factors["timing_multiplier"]
                
                predicted_roi = ((adjusted_ltv - acquisition_cost) / acquisition_cost) * 100 if acquisition_cost > 0 else 0
                roi_payback_period = acquisition_cost / (adjusted_ltv / 12) if adjusted_ltv > 0 else 999  # Meses
                
                # Clasificar ROI
                roi_category = (
                    "exceptional" if predicted_roi >= 500 else
                    "excellent" if predicted_roi >= 300 else
                    "very_good" if predicted_roi >= 200 else
                    "good" if predicted_roi >= 100 else
                    "moderate" if predicted_roi >= 50 else
                    "low" if predicted_roi >= 0 else
                    "negative"
                )
                
                predictive_roi_analysis = {
                    "predicted_roi_percentage": round(predicted_roi, 2),
                    "adjusted_ltv": round(adjusted_ltv, 2),
                    "acquisition_cost": acquisition_cost,
                    "roi_payback_period_months": round(roi_payback_period, 1),
                    "roi_category": roi_category,
                    "roi_factors": roi_factors,
                    "roi_confidence": (
                        "high" if conversion_probability >= 70 and product_fit_score >= 70 else
                        "medium" if conversion_probability >= 50 and product_fit_score >= 50 else
                        "low"
                    ),
                    "roi_optimization_opportunities": [
                        opp for opp in [
                            "improve_engagement" if roi_factors["engagement_multiplier"] < 1.0 else None,
                            "improve_product_fit" if roi_factors["product_fit_multiplier"] < 0.8 else None,
                            "reduce_competitive_risk" if roi_factors["competitive_risk_penalty"] < 0.9 else None,
                            "optimize_timing" if roi_factors["timing_multiplier"] < 1.1 else None,
                            "increase_conversion_probability" if roi_factors["conversion_probability_multiplier"] < 0.7 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "predictive_roi_analysis_completed",
                    email=email,
                    predicted_roi=predicted_roi,
                    roi_category=roi_category,
                    adjusted_ltv=adjusted_ltv,
                    payback_period=roi_payback_period
                )
            except Exception as e:
                logger.warning("predictive_roi_analysis_error", email=email, error=str(e))
            
            # 2. AN√ÅLISIS DE FIT CULTURAL Y ORGANIZACIONAL AVANZADO
            advanced_cultural_fit_analysis = {}
            try:
                company_name = lead_data.get("company", "")
                industry = lead_data.get("industry", "")
                company_size = lead_data.get("company_size", "")
                message = lead_data.get("message", "").lower()
                
                # Factores de fit cultural
                cultural_factors = {
                    "innovation_alignment": 0,
                    "growth_mindset": 0,
                    "technology_adoption": 0,
                    "collaboration_culture": 0,
                    "data_driven": 0,
                    "customer_focus": 0
                }
                
                # Detectar se√±ales de innovaci√≥n
                innovation_keywords = ["innovaci√≥n", "innovaci√≥n", "disruptivo", "transformaci√≥n", "digital", "automatizaci√≥n", "ia", "ai", "machine learning"]
                if any(kw in message for kw in innovation_keywords):
                    cultural_factors["innovation_alignment"] = 80
                
                # Detectar crecimiento
                growth_keywords = ["crecimiento", "expansi√≥n", "escalar", "crecer", "nuevo mercado", "internacional"]
                if any(kw in message for kw in growth_keywords):
                    cultural_factors["growth_mindset"] = 75
                
                # Detectar adopci√≥n tecnol√≥gica
                tech_keywords = ["api", "integraci√≥n", "sistema", "plataforma", "software", "tecnolog√≠a"]
                if any(kw in message for kw in tech_keywords):
                    cultural_factors["technology_adoption"] = 70
                
                # Detectar colaboraci√≥n
                collaboration_keywords = ["equipo", "colaboraci√≥n", "trabajo conjunto", "partnership"]
                if any(kw in message for kw in collaboration_keywords):
                    cultural_factors["collaboration_culture"] = 65
                
                # Detectar enfoque en datos
                data_keywords = ["datos", "analytics", "m√©tricas", "kpi", "reportes", "insights"]
                if any(kw in message for kw in data_keywords):
                    cultural_factors["data_driven"] = 70
                
                # Detectar enfoque en cliente
                customer_keywords = ["cliente", "usuario", "experiencia", "satisfacci√≥n", "servicio"]
                if any(kw in message for kw in customer_keywords):
                    cultural_factors["customer_focus"] = 75
                
                # Ajustar por tama√±o de empresa
                if company_size in ["enterprise", "large"]:
                    for factor in cultural_factors:
                        cultural_factors[factor] = min(cultural_factors[factor] + 10, 100)
                
                # Calcular score total de fit cultural
                cultural_fit_score = sum(cultural_factors.values()) / len(cultural_factors)
                
                # An√°lisis organizacional
                organizational_fit = {
                    "decision_making_speed": (
                        "fast" if company_size in ["startup", "small"] else
                        "medium" if company_size == "medium" else
                        "slow"
                    ),
                    "budget_flexibility": (
                        "high" if company_size in ["startup", "small"] else
                        "medium" if company_size == "medium" else
                        "low"
                    ),
                    "risk_tolerance": (
                        "high" if company_size in ["startup", "small"] else
                        "medium" if company_size == "medium" else
                        "low"
                    )
                }
                
                advanced_cultural_fit_analysis = {
                    "cultural_fit_score": round(cultural_fit_score, 2),
                    "cultural_fit_level": (
                        "excellent" if cultural_fit_score >= 80 else
                        "very_good" if cultural_fit_score >= 70 else
                        "good" if cultural_fit_score >= 60 else
                        "moderate" if cultural_fit_score >= 50 else
                        "low"
                    ),
                    "cultural_factors": cultural_factors,
                    "organizational_fit": organizational_fit,
                    "cultural_alignment_strengths": [
                        k for k, v in cultural_factors.items() if v >= 70
                    ],
                    "cultural_alignment_weaknesses": [
                        k for k, v in cultural_factors.items() if v < 50
                    ],
                    "recommended_approach": (
                        "aggressive" if cultural_fit_score >= 80 and organizational_fit["decision_making_speed"] == "fast" else
                        "standard" if cultural_fit_score >= 60 else
                        "conservative"
                    )
                }
                
                logger.info(
                    "advanced_cultural_fit_analysis_completed",
                    email=email,
                    cultural_fit_score=cultural_fit_score,
                    cultural_fit_level=advanced_cultural_fit_analysis.get("cultural_fit_level")
                )
            except Exception as e:
                logger.warning("advanced_cultural_fit_analysis_error", email=email, error=str(e))
            
            # 3. PREDICCI√ìN GRANULAR DEL MEJOR MOMENTO PARA CERRAR
            granular_closing_timing_prediction = {}
            try:
                # Factores que afectan el timing de cierre
                closing_factors = {
                    "urgency_signals": len([s for s in budget_signals if "urgent" in s.lower()]),
                    "ml_intent_score": ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0,
                    "engagement_momentum": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "competitive_pressure": deep_competitive_analysis.get("competitive_risk_score", 0) if deep_competitive_analysis else 0,
                    "stage_readiness": optimized_sales_cycle_analysis.get("stage_progress_percentage", 0),
                    "seasonal_factor": seasonal_contact_optimization.get("current_seasonal_score", 50) if seasonal_contact_optimization else 50
                }
                
                # Calcular d√≠as √≥ptimos para cerrar
                base_days = ml_purchase_intent_scoring.get("predicted_days_to_purchase", 90) if ml_purchase_intent_scoring else 90
                
                # Ajustes basados en factores
                urgency_adjustment = -15 if closing_factors["urgency_signals"] >= 2 else -5 if closing_factors["urgency_signals"] >= 1 else 0
                intent_adjustment = -20 if closing_factors["ml_intent_score"] >= 80 else -10 if closing_factors["ml_intent_score"] >= 60 else 0
                engagement_adjustment = -10 if closing_factors["engagement_momentum"] >= 80 else 0
                competitive_adjustment = -15 if closing_factors["competitive_pressure"] >= 60 else 0
                stage_adjustment = -30 if closing_factors["stage_readiness"] >= 80 else -15 if closing_factors["stage_readiness"] >= 60 else 0
                seasonal_adjustment = -10 if closing_factors["seasonal_factor"] >= 70 else 0
                
                optimal_days_to_close = max(7, base_days + urgency_adjustment + intent_adjustment + engagement_adjustment + 
                                          competitive_adjustment + stage_adjustment + seasonal_adjustment)
                
                # Calcular fecha √≥ptima
                from datetime import datetime, timedelta
                optimal_closing_date = (datetime.utcnow() + timedelta(days=int(optimal_days_to_close))).isoformat()
                
                # Calcular ventana de cierre (d√≠as antes y despu√©s de la fecha √≥ptima)
                closing_window_start = max(7, optimal_days_to_close - 7)
                closing_window_end = optimal_days_to_close + 14
                
                # Calcular probabilidad de cierre en ventana √≥ptima
                closing_probability_in_window = min(95, 
                    conversion_probability + 
                    (10 if closing_factors["ml_intent_score"] >= 70 else 0) +
                    (10 if closing_factors["urgency_signals"] >= 2 else 0) +
                    (5 if closing_factors["engagement_momentum"] >= 80 else 0)
                )
                
                granular_closing_timing_prediction = {
                    "optimal_days_to_close": int(optimal_days_to_close),
                    "optimal_closing_date": optimal_closing_date,
                    "closing_window": {
                        "start_days": int(closing_window_start),
                        "end_days": int(closing_window_end),
                        "window_duration_days": int(closing_window_end - closing_window_start)
                    },
                    "closing_probability_in_window": round(closing_probability_in_window, 2),
                    "closing_readiness_score": round(
                        (closing_factors["ml_intent_score"] * 0.3 +
                         closing_factors["engagement_momentum"] * 0.2 +
                         closing_factors["stage_readiness"] * 0.2 +
                         (100 - closing_factors["competitive_pressure"]) * 0.15 +
                         closing_factors["seasonal_factor"] * 0.15), 2
                    ),
                    "closing_factors": closing_factors,
                    "recommended_closing_strategy": (
                        "aggressive" if optimal_days_to_close <= 14 and closing_probability_in_window >= 70 else
                        "standard" if optimal_days_to_close <= 30 and closing_probability_in_window >= 60 else
                        "patient"
                    ),
                    "closing_risks": [
                        risk for risk in [
                            "competitive_pressure" if closing_factors["competitive_pressure"] >= 60 else None,
                            "low_engagement" if closing_factors["engagement_momentum"] < 50 else None,
                            "early_stage" if closing_factors["stage_readiness"] < 40 else None,
                            "seasonal_timing" if closing_factors["seasonal_factor"] < 40 else None
                        ] if risk
                    ]
                }
                
                logger.info(
                    "granular_closing_timing_prediction_completed",
                    email=email,
                    optimal_days_to_close=optimal_days_to_close,
                    closing_probability=closing_probability_in_window,
                    recommended_strategy=granular_closing_timing_prediction.get("recommended_closing_strategy")
                )
            except Exception as e:
                logger.warning("granular_closing_timing_prediction_error", email=email, error=str(e))
            
            # 4. AN√ÅLISIS DE CONTENIDO AVANZADO
            advanced_content_analysis = {}
            try:
                message = lead_data.get("message", "")
                message_lower = message.lower()
                
                # An√°lisis de longitud y estructura
                content_metrics = {
                    "message_length": len(message),
                    "word_count": len(message.split()),
                    "sentence_count": len([s for s in message.split('.') if s.strip()]),
                    "question_count": message.count('?'),
                    "exclamation_count": message.count('!'),
                    "has_specific_questions": any(q in message_lower for q in ["cu√°nto", "c√≥mo", "qu√©", "cu√°ndo", "d√≥nde", "por qu√©"]),
                    "has_numbers": any(char.isdigit() for char in message),
                    "has_urls": "http" in message_lower or "www." in message_lower,
                    "has_emails": "@" in message
                }
                
                # An√°lisis de intenci√≥n en contenido
                content_intent_signals = {
                    "immediate_interest": any(kw in message_lower for kw in ["ahora", "inmediato", "urgente", "r√°pido", "ya"]),
                    "research_phase": any(kw in message_lower for kw in ["informaci√≥n", "investigar", "comparar", "evaluar", "opciones"]),
                    "decision_phase": any(kw in message_lower for kw in ["decidir", "elegir", "seleccionar", "contratar", "comprar"]),
                    "budget_discussion": any(kw in message_lower for kw in ["precio", "costo", "presupuesto", "tarifa", "pago"]),
                    "technical_discussion": any(kw in message_lower for kw in ["t√©cnico", "implementaci√≥n", "integraci√≥n", "api", "sistema"]),
                    "partnership_interest": any(kw in message_lower for kw in ["partnership", "colaboraci√≥n", "alianza", "socio"])
                }
                
                # An√°lisis de complejidad del mensaje
                content_complexity = (
                    "high" if content_metrics["word_count"] >= 100 and content_metrics["sentence_count"] >= 5 else
                    "medium" if content_metrics["word_count"] >= 50 else
                    "low"
                )
                
                # An√°lisis de profesionalismo
                professionalism_score = 0
                if content_metrics["word_count"] >= 20:
                    professionalism_score += 20
                if content_metrics["sentence_count"] >= 2:
                    professionalism_score += 20
                if content_metrics["has_specific_questions"]:
                    professionalism_score += 30
                if not any(word in message_lower for word in ["hola", "hi", "hello"]) or content_metrics["word_count"] > 10:
                    professionalism_score += 30
                
                # Detectar tipo de contenido
                content_type = (
                    "technical_inquiry" if content_intent_signals["technical_discussion"] else
                    "budget_inquiry" if content_intent_signals["budget_discussion"] else
                    "partnership_inquiry" if content_intent_signals["partnership_interest"] else
                    "research_inquiry" if content_intent_signals["research_phase"] else
                    "decision_inquiry" if content_intent_signals["decision_phase"] else
                    "general_inquiry"
                )
                
                advanced_content_analysis = {
                    "content_metrics": content_metrics,
                    "content_intent_signals": content_intent_signals,
                    "content_complexity": content_complexity,
                    "professionalism_score": professionalism_score,
                    "professionalism_level": (
                        "high" if professionalism_score >= 70 else
                        "medium" if professionalism_score >= 50 else
                        "low"
                    ),
                    "content_type": content_type,
                    "content_quality_score": round(
                        (professionalism_score * 0.4 +
                         (100 if content_metrics["has_specific_questions"] else 0) * 0.3 +
                         (min(content_metrics["word_count"] / 2, 30) if content_metrics["word_count"] > 0 else 0) * 0.3), 2
                    ),
                    "recommended_response_style": (
                        "detailed_technical" if content_type == "technical_inquiry" else
                        "pricing_focused" if content_type == "budget_inquiry" else
                        "partnership_focused" if content_type == "partnership_inquiry" else
                        "educational" if content_type == "research_inquiry" else
                        "closing_focused" if content_type == "decision_inquiry" else
                        "friendly_professional"
                    ),
                    "content_insights": [
                        insight for insight in [
                            "high_engagement_expected" if content_metrics["has_specific_questions"] else None,
                            "budget_conscious" if content_intent_signals["budget_discussion"] else None,
                            "technical_lead" if content_intent_signals["technical_discussion"] else None,
                            "decision_maker" if content_intent_signals["decision_phase"] else None,
                            "research_phase" if content_intent_signals["research_phase"] else None
                        ] if insight
                    ]
                }
                
                logger.info(
                    "advanced_content_analysis_completed",
                    email=email,
                    content_type=content_type,
                    professionalism_score=professionalism_score,
                    content_quality_score=advanced_content_analysis.get("content_quality_score")
                )
            except Exception as e:
                logger.warning("advanced_content_analysis_error", email=email, error=str(e))
            
            # 5. AN√ÅLISIS DE INFLUENCIA EN REDES SOCIALES AVANZADO
            advanced_social_influence_analysis = {}
            try:
                # An√°lisis de influencia basado en datos disponibles
                social_data = social_analysis if social_analysis else {}
                
                # Factores de influencia
                influence_factors = {
                    "linkedin_presence": social_data.get("linkedin_score", 0),
                    "company_size_factor": (
                        30 if company_size in ["enterprise", "large"] else
                        20 if company_size == "medium" else
                        10
                    ),
                    "industry_influence": (
                        25 if industry in ["technology", "finance", "consulting"] else
                        15 if industry in ["healthcare", "education", "government"] else
                        10
                    ),
                    "engagement_factor": min(advanced_engagement_analysis.get("advanced_engagement_score", 0) / 2, 25) if advanced_engagement_analysis else 0,
                    "network_size_factor": min(network_analysis.get("network_size", 0) / 10, 20) if network_analysis else 0
                }
                
                # Calcular score de influencia total
                total_influence_score = sum(influence_factors.values())
                
                # Clasificar nivel de influencia
                influence_level = (
                    "very_high" if total_influence_score >= 80 else
                    "high" if total_influence_score >= 60 else
                    "medium" if total_influence_score >= 40 else
                    "low"
                )
                
                # Calcular reach potencial (personas que podr√≠an ser influenciadas)
                estimated_reach = (
                    10000 if influence_level == "very_high" else
                    5000 if influence_level == "high" else
                    2000 if influence_level == "medium" else
                    500
                )
                
                advanced_social_influence_analysis = {
                    "total_influence_score": round(total_influence_score, 2),
                    "influence_level": influence_level,
                    "influence_factors": influence_factors,
                    "estimated_reach": estimated_reach,
                    "influence_strengths": [
                        k for k, v in influence_factors.items() if v >= 20
                    ],
                    "influence_opportunities": [
                        opp for opp in [
                            "build_linkedin_presence" if influence_factors["linkedin_presence"] < 20 else None,
                            "increase_engagement" if influence_factors["engagement_factor"] < 15 else None,
                            "expand_network" if influence_factors["network_size_factor"] < 15 else None
                        ] if opp
                    ],
                    "recommended_influence_strategy": (
                        "high_touch" if influence_level in ["very_high", "high"] else
                        "standard" if influence_level == "medium" else
                        "automated"
                    ),
                    "referral_potential": (
                        "very_high" if influence_level in ["very_high", "high"] and total_influence_score >= 70 else
                        "high" if influence_level == "medium" and total_influence_score >= 50 else
                        "medium" if influence_level == "medium" else
                        "low"
                    )
                }
                
                logger.info(
                    "advanced_social_influence_analysis_completed",
                    email=email,
                    total_influence_score=total_influence_score,
                    influence_level=influence_level,
                    estimated_reach=estimated_reach
                )
            except Exception as e:
                logger.warning("advanced_social_influence_analysis_error", email=email, error=str(e))
            
            # 6. AN√ÅLISIS DE PATRONES DE COMUNICACI√ìN AVANZADO
            advanced_communication_pattern_analysis = {}
            try:
                message = lead_data.get("message", "")
                message_lower = message.lower()
                
                # An√°lisis de estilo de comunicaci√≥n
                communication_style_indicators = {
                    "formal": any(kw in message_lower for kw in ["estimado", "atentamente", "saludos cordiales", "le escribo"]),
                    "casual": any(kw in message_lower for kw in ["hola", "hi", "hey", "gracias", "thanks"]),
                    "technical": any(kw in message_lower for kw in ["api", "integraci√≥n", "sistema", "t√©cnico", "implementaci√≥n"]),
                    "business": any(kw in message_lower for kw in ["negocio", "empresa", "organizaci√≥n", "corporativo"]),
                    "urgent": any(kw in message_lower for kw in ["urgente", "inmediato", "r√°pido", "ya", "ahora"]),
                    "questioning": message.count('?') >= 2,
                    "detailed": len(message.split()) >= 50,
                    "brief": len(message.split()) <= 10
                }
                
                # Determinar estilo dominante
                style_scores = {
                    "formal": 30 if communication_style_indicators["formal"] else 0,
                    "casual": 20 if communication_style_indicators["casual"] else 0,
                    "technical": 25 if communication_style_indicators["technical"] else 0,
                    "business": 25 if communication_style_indicators["business"] else 0
                }
                
                dominant_style = max(style_scores.items(), key=lambda x: x[1])[0] if max(style_scores.values()) > 0 else "neutral"
                
                # An√°lisis de frecuencia esperada de comunicaci√≥n
                expected_communication_frequency = (
                    "daily" if communication_style_indicators["urgent"] and ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 70 else
                    "every_2_days" if ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 60 else
                    "weekly" if ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 40 else
                    "bi_weekly" if ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 20 else
                    "monthly"
                ) if ml_purchase_intent_scoring else "weekly"
                
                # An√°lisis de preferencia de canal
                channel_preferences = {
                    "email": 50,  # Base
                    "phone": 0,
                    "video_call": 0,
                    "chat": 0,
                    "in_person": 0
                }
                
                if communication_style_indicators["urgent"]:
                    channel_preferences["phone"] += 30
                if communication_style_indicators["technical"]:
                    channel_preferences["video_call"] += 25
                if communication_style_indicators["detailed"]:
                    channel_preferences["email"] += 20
                if not communication_style_indicators["brief"]:
                    channel_preferences["chat"] += 15
                
                preferred_channel = max(channel_preferences.items(), key=lambda x: x[1])[0]
                
                # An√°lisis de tiempo de respuesta esperado
                expected_response_time = (
                    "immediate" if communication_style_indicators["urgent"] else
                    "within_1_hour" if ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 70 else
                    "within_4_hours" if ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 50 else
                    "within_24_hours" if ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 30 else
                    "within_48_hours"
                ) if ml_purchase_intent_scoring else "within_24_hours"
                
                advanced_communication_pattern_analysis = {
                    "communication_style_indicators": communication_style_indicators,
                    "dominant_style": dominant_style,
                    "style_scores": style_scores,
                    "expected_communication_frequency": expected_communication_frequency,
                    "channel_preferences": channel_preferences,
                    "preferred_channel": preferred_channel,
                    "expected_response_time": expected_response_time,
                    "communication_complexity": (
                        "high" if communication_style_indicators["technical"] and communication_style_indicators["detailed"] else
                        "medium" if communication_style_indicators["detailed"] or communication_style_indicators["technical"] else
                        "low"
                    ),
                    "recommended_communication_approach": (
                        "formal_detailed" if dominant_style == "formal" and communication_style_indicators["detailed"] else
                        "casual_friendly" if dominant_style == "casual" else
                        "technical_detailed" if dominant_style == "technical" else
                        "business_professional" if dominant_style == "business" else
                        "adaptive"
                    ),
                    "communication_insights": [
                        insight for insight in [
                            "prefers_detailed_communication" if communication_style_indicators["detailed"] else None,
                            "technical_lead" if communication_style_indicators["technical"] else None,
                            "time_sensitive" if communication_style_indicators["urgent"] else None,
                            "question_focused" if communication_style_indicators["questioning"] else None,
                            "brief_communicator" if communication_style_indicators["brief"] else None
                        ] if insight
                    ]
                }
                
                logger.info(
                    "advanced_communication_pattern_analysis_completed",
                    email=email,
                    dominant_style=dominant_style,
                    preferred_channel=preferred_channel,
                    expected_frequency=expected_communication_frequency
                )
            except Exception as e:
                logger.warning("advanced_communication_pattern_analysis_error", email=email, error=str(e))
            
            # 7. PREDICCI√ìN DE MEJOR CANAL DE COMUNICACI√ìN
            optimal_channel_prediction = {}
            try:
                # Factores que influyen en la selecci√≥n de canal
                channel_factors = {
                    "email": {
                        "base_score": 40,
                        "engagement_bonus": min(advanced_engagement_analysis.get("advanced_engagement_score", 0) / 5, 20) if advanced_engagement_analysis else 0,
                        "content_complexity_bonus": 15 if advanced_content_analysis.get("content_complexity") == "high" else 0,
                        "professionalism_bonus": 10 if advanced_content_analysis.get("professionalism_level") == "high" else 0,
                        "timezone_bonus": 5 if granular_timing_analysis else 0
                    },
                    "phone": {
                        "base_score": 30,
                        "urgency_bonus": 25 if any(s in str(budget_signals).lower() for s in ["urgent", "inmediato", "r√°pido"]) else 0,
                        "ml_intent_bonus": min(ml_purchase_intent_scoring.get("ml_intent_score", 0) / 4, 20) if ml_purchase_intent_scoring else 0,
                        "engagement_bonus": 10 if advanced_engagement_analysis.get("advanced_engagement_score", 0) >= 70 else 0,
                        "cultural_fit_bonus": 10 if advanced_cultural_fit_analysis.get("cultural_fit_score", 0) >= 70 else 0
                    },
                    "video_call": {
                        "base_score": 20,
                        "technical_bonus": 20 if advanced_content_analysis.get("content_type") == "technical_inquiry" else 0,
                        "complexity_bonus": 15 if advanced_content_analysis.get("content_complexity") == "high" else 0,
                        "enterprise_bonus": 15 if company_size in ["enterprise", "large"] else 0,
                        "product_fit_bonus": 10 if product_fit_score >= 70 else 0
                    },
                    "chat": {
                        "base_score": 25,
                        "immediacy_bonus": 20 if ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 60 else 0,
                        "engagement_bonus": 15 if advanced_engagement_analysis.get("advanced_engagement_score", 0) >= 60 else 0,
                        "brief_communication_bonus": 10 if advanced_content_analysis.get("content_metrics", {}).get("word_count", 0) < 30 else 0
                    },
                    "in_person": {
                        "base_score": 10,
                        "enterprise_bonus": 20 if company_size in ["enterprise", "large"] else 0,
                        "high_value_bonus": 15 if estimated_ltv >= 50000 else 0,
                        "cultural_fit_bonus": 15 if advanced_cultural_fit_analysis.get("cultural_fit_score", 0) >= 80 else 0,
                        "influence_bonus": 10 if advanced_social_influence_analysis.get("influence_level") in ["very_high", "high"] else 0
                    }
                }
                
                # Calcular scores totales por canal
                channel_scores = {
                    channel: sum(factors.values())
                    for channel, factors in channel_factors.items()
                }
                
                # Determinar canal √≥ptimo
                optimal_channel = max(channel_scores.items(), key=lambda x: x[1])[0]
                optimal_channel_score = channel_scores[optimal_channel]
                
                # Calcular confianza en la predicci√≥n
                confidence = (
                    "high" if optimal_channel_score >= 70 and (optimal_channel_score - max(v for k, v in channel_scores.items() if k != optimal_channel)) >= 15 else
                    "medium" if optimal_channel_score >= 50 and (optimal_channel_score - max(v for k, v in channel_scores.items() if k != optimal_channel)) >= 10 else
                    "low"
                )
                
                # Recomendaci√≥n de canal secundario
                sorted_channels = sorted(channel_scores.items(), key=lambda x: x[1], reverse=True)
                secondary_channel = sorted_channels[1][0] if len(sorted_channels) > 1 else None
                
                optimal_channel_prediction = {
                    "optimal_channel": optimal_channel,
                    "optimal_channel_score": round(optimal_channel_score, 2),
                    "channel_scores": {k: round(v, 2) for k, v in channel_scores.items()},
                    "channel_factors": channel_factors,
                    "confidence": confidence,
                    "secondary_channel": secondary_channel,
                    "recommended_sequence": [
                        optimal_channel,
                        secondary_channel
                    ] if secondary_channel else [optimal_channel],
                    "channel_insights": [
                        insight for insight in [
                            "email_best_for_detailed" if optimal_channel == "email" and advanced_content_analysis.get("content_complexity") == "high" else None,
                            "phone_best_for_urgency" if optimal_channel == "phone" and any("urgent" in str(s).lower() for s in budget_signals) else None,
                            "video_best_for_technical" if optimal_channel == "video_call" and advanced_content_analysis.get("content_type") == "technical_inquiry" else None,
                            "chat_best_for_immediate" if optimal_channel == "chat" and ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 60 else None,
                            "in_person_best_for_enterprise" if optimal_channel == "in_person" and company_size in ["enterprise", "large"] else None
                        ] if insight
                    ]
                }
                
                logger.info(
                    "optimal_channel_prediction_completed",
                    email=email,
                    optimal_channel=optimal_channel,
                    optimal_channel_score=optimal_channel_score,
                    confidence=confidence
                )
            except Exception as e:
                logger.warning("optimal_channel_prediction_error", email=email, error=str(e))
            
            # 8. AN√ÅLISIS DE RIESGO DE ABANDONO TEMPRANO AVANZADO
            advanced_early_abandonment_analysis = {}
            try:
                # Factores de riesgo de abandono temprano
                abandonment_risk_factors = {
                    "low_engagement": max(0, 50 - (advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0)),
                    "low_intent": max(0, 50 - (ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0)),
                    "competitive_risk": deep_competitive_analysis.get("competitive_risk_score", 0) if deep_competitive_analysis else 0,
                    "poor_product_fit": max(0, 50 - product_fit_score),
                    "negative_sentiment": max(0, -sentiment_analysis.get("sentiment_score", 0)) if sentiment_analysis and sentiment_analysis.get("sentiment_score", 0) < 0 else 0,
                    "low_quality_data": max(0, 50 - (ml_enhanced_quality_scoring.get("ml_quality_score", 0) if ml_enhanced_quality_scoring else 0)),
                    "timing_mismatch": max(0, 50 - (seasonal_contact_optimization.get("current_seasonal_score", 50) if seasonal_contact_optimization else 50)),
                    "cultural_mismatch": max(0, 50 - (advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0))
                }
                
                # Calcular score total de riesgo de abandono
                total_abandonment_risk = sum(abandonment_risk_factors.values()) / len(abandonment_risk_factors)
                
                # Clasificar nivel de riesgo
                abandonment_risk_level = (
                    "critical" if total_abandonment_risk >= 70 else
                    "high" if total_abandonment_risk >= 50 else
                    "medium" if total_abandonment_risk >= 30 else
                    "low" if total_abandonment_risk >= 15 else
                    "minimal"
                )
                
                # Factores cr√≠ticos (los que m√°s contribuyen al riesgo)
                critical_risk_factors = [
                    factor for factor, score in abandonment_risk_factors.items()
                    if score >= 40
                ]
                
                # Estrategias de intervenci√≥n recomendadas
                intervention_strategies = []
                if abandonment_risk_factors["low_engagement"] >= 30:
                    intervention_strategies.append("increase_engagement_immediately")
                if abandonment_risk_factors["low_intent"] >= 30:
                    intervention_strategies.append("nurture_with_educational_content")
                if abandonment_risk_factors["competitive_risk"] >= 40:
                    intervention_strategies.append("competitive_differentiation")
                if abandonment_risk_factors["poor_product_fit"] >= 30:
                    intervention_strategies.append("reassess_product_fit")
                if abandonment_risk_factors["negative_sentiment"] >= 20:
                    intervention_strategies.append("address_concerns_immediately")
                if abandonment_risk_factors["cultural_mismatch"] >= 30:
                    intervention_strategies.append("adjust_communication_style")
                
                # Calcular probabilidad de abandono
                abandonment_probability = min(95, total_abandonment_risk * 1.2)
                
                # Tiempo estimado hasta abandono (d√≠as)
                estimated_days_to_abandonment = (
                    7 if abandonment_risk_level == "critical" else
                    14 if abandonment_risk_level == "high" else
                    30 if abandonment_risk_level == "medium" else
                    60 if abandonment_risk_level == "low" else
                    90
                )
                
                advanced_early_abandonment_analysis = {
                    "total_abandonment_risk": round(total_abandonment_risk, 2),
                    "abandonment_risk_level": abandonment_risk_level,
                    "abandonment_risk_factors": {k: round(v, 2) for k, v in abandonment_risk_factors.items()},
                    "critical_risk_factors": critical_risk_factors,
                    "abandonment_probability": round(abandonment_probability, 2),
                    "estimated_days_to_abandonment": estimated_days_to_abandonment,
                    "intervention_strategies": intervention_strategies,
                    "intervention_urgency": (
                        "immediate" if abandonment_risk_level in ["critical", "high"] else
                        "within_24_hours" if abandonment_risk_level == "medium" else
                        "within_week" if abandonment_risk_level == "low" else
                        "monitor"
                    ),
                    "recommended_interventions": {
                        "immediate_actions": [
                            action for action in [
                                "personalized_outreach" if abandonment_risk_factors["low_engagement"] >= 30 else None,
                                "address_concerns" if abandonment_risk_factors["negative_sentiment"] >= 20 else None,
                                "competitive_response" if abandonment_risk_factors["competitive_risk"] >= 40 else None
                            ] if action
                        ],
                        "short_term_actions": [
                            action for action in [
                                "nurturing_sequence" if abandonment_risk_factors["low_intent"] >= 30 else None,
                                "product_demo" if abandonment_risk_factors["poor_product_fit"] >= 30 else None,
                                "cultural_alignment_content" if abandonment_risk_factors["cultural_mismatch"] >= 30 else None
                            ] if action
                        ]
                    }
                }
                
                logger.info(
                    "advanced_early_abandonment_analysis_completed",
                    email=email,
                    total_abandonment_risk=total_abandonment_risk,
                    abandonment_risk_level=abandonment_risk_level,
                    estimated_days_to_abandonment=estimated_days_to_abandonment
                )
            except Exception as e:
                logger.warning("advanced_early_abandonment_analysis_error", email=email, error=str(e))
            
            # 9. RECOMENDACIONES DE NURTURING SEQUENCE M√ÅS PERSONALIZADAS
            ultra_personalized_nurturing_recommendation = {}
            try:
                # Determinar etapa del buyer journey
                buyer_journey_stage = optimized_sales_cycle_analysis.get("current_stage", "awareness") if optimized_sales_cycle_analysis else "awareness"
                
                # Factores para personalizaci√≥n
                personalization_factors = {
                    "buyer_journey_stage": buyer_journey_stage,
                    "ml_intent_score": ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "content_type_preference": advanced_content_analysis.get("content_type", "general_inquiry") if advanced_content_analysis else "general_inquiry",
                    "communication_style": advanced_communication_pattern_analysis.get("dominant_style", "neutral") if advanced_communication_pattern_analysis else "neutral",
                    "competitive_context": deep_competitive_analysis.get("competitive_risk_level", "low") if deep_competitive_analysis else "low",
                    "product_fit": product_fit_score,
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0
                }
                
                # Generar secuencia de nurturing personalizada
                nurturing_sequence = []
                
                # Email 1: Basado en etapa y contenido
                if buyer_journey_stage == "awareness":
                    nurturing_sequence.append({
                        "step": 1,
                        "type": "educational",
                        "subject": "Gu√≠a completa: [Tema relevante]",
                        "timing_days": 0,
                        "content_focus": "education_and_awareness",
                        "cta": "learn_more"
                    })
                elif buyer_journey_stage == "interest":
                    nurturing_sequence.append({
                        "step": 1,
                        "type": "value_proposition",
                        "subject": "C√≥mo [Producto] puede ayudar a [Industria]",
                        "timing_days": 0,
                        "content_focus": "benefits_and_value",
                        "cta": "see_demo"
                    })
                elif buyer_journey_stage == "consideration":
                    nurturing_sequence.append({
                        "step": 1,
                        "type": "social_proof",
                        "subject": "Casos de √©xito: [Industria similar]",
                        "timing_days": 0,
                        "content_focus": "case_studies_and_testimonials",
                        "cta": "request_demo"
                    })
                
                # Email 2: Basado en engagement
                if personalization_factors["engagement_level"] >= 60:
                    nurturing_sequence.append({
                        "step": 2,
                        "type": "demo_invitation",
                        "subject": "Reserva una demo personalizada",
                        "timing_days": 3,
                        "content_focus": "product_demo",
                        "cta": "schedule_demo"
                    })
                else:
                    nurturing_sequence.append({
                        "step": 2,
                        "type": "educational",
                        "subject": "Recursos adicionales para [Tema]",
                        "timing_days": 5,
                        "content_focus": "additional_resources",
                        "cta": "download_resource"
                    })
                
                # Email 3: Basado en competitive context
                if personalization_factors["competitive_context"] in ["medium", "high"]:
                    nurturing_sequence.append({
                        "step": 3,
                        "type": "competitive_differentiation",
                        "subject": "Por qu√© elegir [Producto] vs [Competidor]",
                        "timing_days": 7,
                        "content_focus": "competitive_advantages",
                        "cta": "compare_features"
                    })
                else:
                    nurturing_sequence.append({
                        "step": 3,
                        "type": "social_proof",
                        "subject": "Lo que dicen nuestros clientes",
                        "timing_days": 10,
                        "content_focus": "testimonials",
                        "cta": "read_reviews"
                    })
                
                # Email 4: Basado en product fit
                if personalization_factors["product_fit"] >= 70:
                    nurturing_sequence.append({
                        "step": 4,
                        "type": "pricing",
                        "subject": "Planes y precios para [Tipo de empresa]",
                        "timing_days": 14,
                        "content_focus": "pricing_information",
                        "cta": "view_pricing"
                    })
                else:
                    nurturing_sequence.append({
                        "step": 4,
                        "type": "use_cases",
                        "subject": "Casos de uso para [Industria]",
                        "timing_days": 14,
                        "content_focus": "use_cases",
                        "cta": "explore_use_cases"
                    })
                
                # Calcular duraci√≥n total de la secuencia
                sequence_duration_days = max(step.get("timing_days", 0) for step in nurturing_sequence) if nurturing_sequence else 0
                
                ultra_personalized_nurturing_recommendation = {
                    "nurturing_sequence": nurturing_sequence,
                    "sequence_length": len(nurturing_sequence),
                    "sequence_duration_days": sequence_duration_days,
                    "personalization_factors": personalization_factors,
                    "recommended_frequency": (
                        "daily" if personalization_factors["ml_intent_score"] >= 70 else
                        "every_2_days" if personalization_factors["ml_intent_score"] >= 50 else
                        "weekly" if personalization_factors["ml_intent_score"] >= 30 else
                        "bi_weekly"
                    ),
                    "sequence_goals": [
                        goal for goal in [
                            "increase_awareness" if buyer_journey_stage == "awareness" else None,
                            "build_interest" if buyer_journey_stage == "interest" else None,
                            "facilitate_consideration" if buyer_journey_stage == "consideration" else None,
                            "accelerate_decision" if buyer_journey_stage in ["evaluation", "closing"] else None,
                            "address_competitive_concerns" if personalization_factors["competitive_context"] in ["medium", "high"] else None,
                            "improve_product_fit_understanding" if personalization_factors["product_fit"] < 70 else None
                        ] if goal
                    ],
                    "expected_outcomes": {
                        "engagement_increase": round(personalization_factors["engagement_level"] * 1.2, 2) if personalization_factors["engagement_level"] < 80 else 100,
                        "intent_increase": round(personalization_factors["ml_intent_score"] * 1.15, 2) if personalization_factors["ml_intent_score"] < 80 else 100,
                        "conversion_probability": round(conversion_probability * 1.1, 2) if conversion_probability < 80 else 100
                    }
                }
                
                logger.info(
                    "ultra_personalized_nurturing_recommendation_completed",
                    email=email,
                    sequence_length=len(nurturing_sequence),
                    sequence_duration_days=sequence_duration_days,
                    buyer_journey_stage=buyer_journey_stage
                )
            except Exception as e:
                logger.warning("ultra_personalized_nurturing_recommendation_error", email=email, error=str(e))
            
            # 10. AN√ÅLISIS DE SATISFACCI√ìN DEL CLIENTE PREDICTIVO
            predictive_customer_satisfaction_analysis = {}
            try:
                # Factores que influyen en la satisfacci√≥n
                satisfaction_factors = {
                    "sentiment_score": max(0, sentiment_analysis.get("sentiment_score", 0)) if sentiment_analysis else 50,
                    "engagement_quality": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 50,
                    "product_fit": product_fit_score,
                    "response_quality": advanced_content_analysis.get("professionalism_score", 0) if advanced_content_analysis else 50,
                    "communication_quality": 70 if advanced_communication_pattern_analysis.get("communication_complexity") == "high" else 50 if advanced_communication_pattern_analysis else 50,
                    "cultural_alignment": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 50
                }
                
                # Calcular score de satisfacci√≥n
                satisfaction_score = sum(satisfaction_factors.values()) / len(satisfaction_factors)
                
                # Clasificar nivel de satisfacci√≥n
                satisfaction_level = (
                    "very_satisfied" if satisfaction_score >= 85 else
                    "satisfied" if satisfaction_score >= 70 else
                    "neutral" if satisfaction_score >= 50 else
                    "dissatisfied" if satisfaction_score >= 30 else
                    "very_dissatisfied"
                )
                
                # Factores que m√°s contribuyen a la satisfacci√≥n
                satisfaction_strengths = [
                    factor for factor, score in satisfaction_factors.items()
                    if score >= 70
                ]
                
                # Factores que m√°s afectan negativamente
                satisfaction_weaknesses = [
                    factor for factor, score in satisfaction_factors.items()
                    if score < 50
                ]
                
                # Recomendaciones para mejorar satisfacci√≥n
                satisfaction_improvement_recommendations = []
                if satisfaction_factors["sentiment_score"] < 60:
                    satisfaction_improvement_recommendations.append("address_negative_sentiment_immediately")
                if satisfaction_factors["engagement_quality"] < 60:
                    satisfaction_improvement_recommendations.append("increase_engagement_quality")
                if satisfaction_factors["product_fit"] < 60:
                    satisfaction_improvement_recommendations.append("improve_product_fit_communication")
                if satisfaction_factors["response_quality"] < 60:
                    satisfaction_improvement_recommendations.append("enhance_response_professionalism")
                if satisfaction_factors["cultural_alignment"] < 60:
                    satisfaction_improvement_recommendations.append("align_communication_with_culture")
                
                # Predicci√≥n de satisfacci√≥n futura
                predicted_future_satisfaction = min(100, satisfaction_score * 1.1) if satisfaction_score >= 70 else satisfaction_score * 0.9
                
                predictive_customer_satisfaction_analysis = {
                    "satisfaction_score": round(satisfaction_score, 2),
                    "satisfaction_level": satisfaction_level,
                    "satisfaction_factors": {k: round(v, 2) for k, v in satisfaction_factors.items()},
                    "satisfaction_strengths": satisfaction_strengths,
                    "satisfaction_weaknesses": satisfaction_weaknesses,
                    "satisfaction_improvement_recommendations": satisfaction_improvement_recommendations,
                    "predicted_future_satisfaction": round(predicted_future_satisfaction, 2),
                    "satisfaction_trend": (
                        "improving" if predicted_future_satisfaction > satisfaction_score else
                        "stable" if abs(predicted_future_satisfaction - satisfaction_score) < 5 else
                        "declining"
                    ),
                    "retention_probability": (
                        "very_high" if satisfaction_score >= 85 else
                        "high" if satisfaction_score >= 70 else
                        "medium" if satisfaction_score >= 50 else
                        "low" if satisfaction_score >= 30 else
                        "very_low"
                    )
                }
                
                logger.info(
                    "predictive_customer_satisfaction_analysis_completed",
                    email=email,
                    satisfaction_score=satisfaction_score,
                    satisfaction_level=satisfaction_level,
                    predicted_future_satisfaction=predicted_future_satisfaction
                )
            except Exception as e:
                logger.warning("predictive_customer_satisfaction_analysis_error", email=email, error=str(e))
            
            # 11. AN√ÅLISIS DE CUSTOMER JOURNEY COMPLETO
            comprehensive_customer_journey_analysis = {}
            try:
                # Determinar etapa actual del journey
                current_journey_stage = optimized_sales_cycle_analysis.get("current_stage", "awareness") if optimized_sales_cycle_analysis else "awareness"
                
                # Mapear etapas del journey
                journey_stages = {
                    "awareness": {
                        "stage_name": "Awareness",
                        "description": "Lead conoce el problema pero no la soluci√≥n",
                        "typical_duration_days": 7,
                        "key_activities": ["content_consumption", "research", "problem_identification"],
                        "success_indicators": ["content_engagement", "time_on_site", "page_views"]
                    },
                    "interest": {
                        "stage_name": "Interest",
                        "description": "Lead muestra inter√©s en la soluci√≥n",
                        "typical_duration_days": 14,
                        "key_activities": ["demo_requests", "pricing_inquiries", "feature_research"],
                        "success_indicators": ["demo_scheduled", "pricing_page_views", "feature_comparisons"]
                    },
                    "consideration": {
                        "stage_name": "Consideration",
                        "description": "Lead eval√∫a opciones y compara soluciones",
                        "typical_duration_days": 21,
                        "key_activities": ["competitive_research", "case_study_reviews", "trial_signups"],
                        "success_indicators": ["trial_activation", "case_study_engagement", "comparison_activity"]
                    },
                    "evaluation": {
                        "stage_name": "Evaluation",
                        "description": "Lead est√° en proceso de decisi√≥n final",
                        "typical_duration_days": 14,
                        "key_activities": ["final_demos", "contract_reviews", "stakeholder_meetings"],
                        "success_indicators": ["contract_views", "stakeholder_engagement", "final_questions"]
                    },
                    "closing": {
                        "stage_name": "Closing",
                        "description": "Lead est√° listo para cerrar el trato",
                        "typical_duration_days": 7,
                        "key_activities": ["contract_signing", "onboarding_prep", "payment_processing"],
                        "success_indicators": ["contract_signed", "payment_initiated", "onboarding_started"]
                    }
                }
                
                # Calcular progreso en el journey
                stage_index = list(journey_stages.keys()).index(current_journey_stage) if current_journey_stage in journey_stages else 0
                journey_progress = ((stage_index + 1) / len(journey_stages)) * 100
                
                # Calcular tiempo en journey
                days_in_journey = sum(
                    journey_stages[stage]["typical_duration_days"]
                    for stage in list(journey_stages.keys())[:stage_index]
                )
                
                # Predecir pr√≥xima etapa
                next_stage = list(journey_stages.keys())[stage_index + 1] if stage_index < len(journey_stages) - 1 else "closed"
                
                # Calcular velocidad del journey
                journey_velocity = (
                    "fast" if days_in_journey < sum(s["typical_duration_days"] for s in journey_stages.values()) / 2 else
                    "normal" if days_in_journey <= sum(s["typical_duration_days"] for s in journey_stages.values()) else
                    "slow"
                )
                
                # Identificar touchpoints por etapa
                touchpoints_by_stage = {
                    "awareness": ["website_visit", "content_download", "blog_read"],
                    "interest": ["demo_request", "pricing_view", "feature_exploration"],
                    "consideration": ["trial_signup", "case_study_view", "comparison_research"],
                    "evaluation": ["final_demo", "contract_review", "stakeholder_meeting"],
                    "closing": ["contract_signing", "payment", "onboarding"]
                }
                
                comprehensive_customer_journey_analysis = {
                    "current_stage": current_journey_stage,
                    "current_stage_details": journey_stages.get(current_journey_stage, {}),
                    "journey_progress_percentage": round(journey_progress, 2),
                    "days_in_journey": days_in_journey,
                    "next_stage": next_stage,
                    "next_stage_details": journey_stages.get(next_stage, {}),
                    "journey_velocity": journey_velocity,
                    "touchpoints_by_stage": touchpoints_by_stage,
                    "all_stages": list(journey_stages.keys()),
                    "journey_completion_estimate_days": sum(
                        journey_stages[stage]["typical_duration_days"]
                        for stage in list(journey_stages.keys())[stage_index:]
                    ),
                    "journey_health_score": round(
                        (journey_progress * 0.3 +
                         (100 if journey_velocity == "fast" else 70 if journey_velocity == "normal" else 40) * 0.3 +
                         min(ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0, 100) * 0.4), 2
                    ),
                    "journey_optimization_opportunities": [
                        opp for opp in [
                            "accelerate_to_next_stage" if journey_velocity == "slow" and ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 60 else None,
                            "increase_engagement" if advanced_engagement_analysis.get("advanced_engagement_score", 0) < 50 else None,
                            "address_competitive_concerns" if deep_competitive_analysis.get("competitive_risk_score", 0) >= 50 else None,
                            "improve_product_fit_understanding" if product_fit_score < 60 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "comprehensive_customer_journey_analysis_completed",
                    email=email,
                    current_stage=current_journey_stage,
                    journey_progress=journey_progress,
                    journey_velocity=journey_velocity
                )
            except Exception as e:
                logger.warning("comprehensive_customer_journey_analysis_error", email=email, error=str(e))
            
            # 12. OPTIMIZACI√ìN DE CONVERSI√ìN CON A/B TESTING
            conversion_optimization_ab_testing = {}
            try:
                # Determinar variantes de A/B test basadas en perfil del lead
                ab_test_variants = {
                    "welcome_message": {
                        "variant_a": {
                            "name": "Personalized Welcome",
                            "content": f"Hola {lead_data.get('first_name', '')}, gracias por tu inter√©s en [Producto]",
                            "target_score_range": (70, 100),
                            "expected_conversion_lift": 15
                        },
                        "variant_b": {
                            "name": "Value-Focused Welcome",
                            "content": "Descubre c√≥mo [Producto] puede transformar tu [Industria]",
                            "target_score_range": (40, 70),
                            "expected_conversion_lift": 10
                        },
                        "variant_c": {
                            "name": "Problem-Agitation Welcome",
                            "content": "¬øLuchando con [Problema com√∫n]? [Producto] tiene la soluci√≥n",
                            "target_score_range": (0, 40),
                            "expected_conversion_lift": 8
                        }
                    },
                    "contact_timing": {
                        "variant_a": {
                            "name": "Immediate Contact",
                            "timing": "within_1_hour",
                            "target_intent_range": (70, 100),
                            "expected_conversion_lift": 20
                        },
                        "variant_b": {
                            "name": "Same Day Contact",
                            "timing": "within_4_hours",
                            "target_intent_range": (40, 70),
                            "expected_conversion_lift": 12
                        },
                        "variant_c": {
                            "name": "Next Day Contact",
                            "timing": "within_24_hours",
                            "target_intent_range": (0, 40),
                            "expected_conversion_lift": 5
                        }
                    },
                    "channel_selection": {
                        "variant_a": {
                            "name": "Optimal Channel",
                            "channel": optimal_channel_prediction.get("optimal_channel", "email") if optimal_channel_prediction else "email",
                            "target_fit_range": (70, 100),
                            "expected_conversion_lift": 18
                        },
                        "variant_b": {
                            "name": "Multi-Channel",
                            "channel": "multi_channel",
                            "target_fit_range": (40, 70),
                            "expected_conversion_lift": 10
                        },
                        "variant_c": {
                            "name": "Standard Channel",
                            "channel": "email",
                            "target_fit_range": (0, 40),
                            "expected_conversion_lift": 3
                        }
                    }
                }
                
                # Asignar variantes basadas en perfil
                ml_intent_score = ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0
                
                welcome_variant = (
                    "variant_a" if score >= 70 else
                    "variant_b" if score >= 40 else
                    "variant_c"
                )
                
                timing_variant = (
                    "variant_a" if ml_intent_score >= 70 else
                    "variant_b" if ml_intent_score >= 40 else
                    "variant_c"
                )
                
                channel_variant = (
                    "variant_a" if product_fit_score >= 70 else
                    "variant_b" if product_fit_score >= 40 else
                    "variant_c"
                )
                
                # Calcular conversi√≥n esperada por variante
                base_conversion = conversion_probability
                
                welcome_lift = ab_test_variants["welcome_message"][welcome_variant]["expected_conversion_lift"]
                timing_lift = ab_test_variants["contact_timing"][timing_variant]["expected_conversion_lift"]
                channel_lift = ab_test_variants["channel_selection"][channel_variant]["expected_conversion_lift"]
                
                optimized_conversion_probability = min(95, base_conversion + (welcome_lift + timing_lift + channel_lift) / 3)
                
                conversion_optimization_ab_testing = {
                    "assigned_variants": {
                        "welcome_message": welcome_variant,
                        "contact_timing": timing_variant,
                        "channel_selection": channel_variant
                    },
                    "variant_details": {
                        "welcome_message": ab_test_variants["welcome_message"][welcome_variant],
                        "contact_timing": ab_test_variants["contact_timing"][timing_variant],
                        "channel_selection": ab_test_variants["channel_selection"][channel_variant]
                    },
                    "base_conversion_probability": round(base_conversion, 2),
                    "optimized_conversion_probability": round(optimized_conversion_probability, 2),
                    "expected_conversion_lift": round(optimized_conversion_probability - base_conversion, 2),
                    "total_expected_lift_percentage": round(((optimized_conversion_probability - base_conversion) / base_conversion * 100) if base_conversion > 0 else 0, 2),
                    "ab_test_recommendations": [
                        rec for rec in [
                            "test_welcome_variants" if score >= 50 else None,
                            "test_timing_variants" if ml_intent_score >= 50 else None,
                            "test_channel_variants" if product_fit_score >= 50 else None,
                            "run_full_ab_test_suite" if score >= 70 and ml_intent_score >= 60 and product_fit_score >= 70 else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "conversion_optimization_ab_testing_completed",
                    email=email,
                    welcome_variant=welcome_variant,
                    timing_variant=timing_variant,
                    channel_variant=channel_variant,
                    optimized_conversion=optimized_conversion_probability
                )
            except Exception as e:
                logger.warning("conversion_optimization_ab_testing_error", email=email, error=str(e))
            
            # 13. AN√ÅLISIS DE RETENCI√ìN Y CHURN AVANZADO
            advanced_retention_churn_analysis = {}
            try:
                # Factores de retenci√≥n
                retention_factors = {
                    "satisfaction_score": predictive_customer_satisfaction_analysis.get("satisfaction_score", 0) if predictive_customer_satisfaction_analysis else 50,
                    "product_fit": product_fit_score,
                    "engagement_quality": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 50,
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 50,
                    "competitive_risk": 100 - (deep_competitive_analysis.get("competitive_risk_score", 0) if deep_competitive_analysis else 0),
                    "support_quality": 70 if advanced_content_analysis.get("professionalism_level") == "high" else 50 if advanced_content_analysis else 50
                }
                
                # Calcular score de retenci√≥n
                retention_score = sum(retention_factors.values()) / len(retention_factors)
                
                # Calcular probabilidad de churn
                churn_probability = max(0, 100 - retention_score)
                
                # Clasificar riesgo de churn
                churn_risk_level = (
                    "critical" if churn_probability >= 70 else
                    "high" if churn_probability >= 50 else
                    "medium" if churn_probability >= 30 else
                    "low" if churn_probability >= 15 else
                    "minimal"
                )
                
                # Factores de riesgo de churn
                churn_risk_factors = {
                    "low_satisfaction": max(0, 50 - retention_factors["satisfaction_score"]),
                    "poor_product_fit": max(0, 50 - retention_factors["product_fit"]),
                    "low_engagement": max(0, 50 - retention_factors["engagement_quality"]),
                    "cultural_mismatch": max(0, 50 - retention_factors["cultural_fit"]),
                    "competitive_pressure": max(0, retention_factors["competitive_risk"] - 50),
                    "poor_support": max(0, 50 - retention_factors["support_quality"])
                }
                
                # Estrategias de retenci√≥n recomendadas
                retention_strategies = []
                if churn_risk_factors["low_satisfaction"] >= 30:
                    retention_strategies.append("improve_satisfaction_immediately")
                if churn_risk_factors["poor_product_fit"] >= 30:
                    retention_strategies.append("reassess_and_improve_product_fit")
                if churn_risk_factors["low_engagement"] >= 30:
                    retention_strategies.append("increase_engagement_programs")
                if churn_risk_factors["cultural_mismatch"] >= 30:
                    retention_strategies.append("align_communication_and_culture")
                if churn_risk_factors["competitive_pressure"] >= 30:
                    retention_strategies.append("competitive_differentiation_and_value")
                if churn_risk_factors["poor_support"] >= 30:
                    retention_strategies.append("enhance_support_quality")
                
                # Calcular valor de retenci√≥n
                retention_value = estimated_ltv * (retention_score / 100)
                churn_cost = estimated_ltv - retention_value
                
                advanced_retention_churn_analysis = {
                    "retention_score": round(retention_score, 2),
                    "retention_level": (
                        "very_high" if retention_score >= 85 else
                        "high" if retention_score >= 70 else
                        "medium" if retention_score >= 50 else
                        "low" if retention_score >= 30 else
                        "very_low"
                    ),
                    "churn_probability": round(churn_probability, 2),
                    "churn_risk_level": churn_risk_level,
                    "retention_factors": {k: round(v, 2) for k, v in retention_factors.items()},
                    "churn_risk_factors": {k: round(v, 2) for k, v in churn_risk_factors.items()},
                    "retention_strategies": retention_strategies,
                    "retention_value": round(retention_value, 2),
                    "churn_cost": round(churn_cost, 2),
                    "retention_priority": (
                        "critical" if churn_risk_level in ["critical", "high"] else
                        "high" if churn_risk_level == "medium" else
                        "medium" if churn_risk_level == "low" else
                        "low"
                    ),
                    "recommended_retention_actions": {
                        "immediate": [
                            action for action in [
                                "personalized_outreach" if churn_risk_factors["low_satisfaction"] >= 30 else None,
                                "product_fit_reassessment" if churn_risk_factors["poor_product_fit"] >= 30 else None,
                                "competitive_response" if churn_risk_factors["competitive_pressure"] >= 30 else None
                            ] if action
                        ],
                        "short_term": [
                            action for action in [
                                "engagement_campaign" if churn_risk_factors["low_engagement"] >= 30 else None,
                                "cultural_alignment_program" if churn_risk_factors["cultural_mismatch"] >= 30 else None,
                                "support_enhancement" if churn_risk_factors["poor_support"] >= 30 else None
                            ] if action
                        ]
                    }
                }
                
                logger.info(
                    "advanced_retention_churn_analysis_completed",
                    email=email,
                    retention_score=retention_score,
                    churn_probability=churn_probability,
                    churn_risk_level=churn_risk_level
                )
            except Exception as e:
                logger.warning("advanced_retention_churn_analysis_error", email=email, error=str(e))
            
            # 14. SISTEMA DE ALERTAS PROACTIVAS INTELIGENTES
            intelligent_proactive_alerts = {}
            try:
                alerts = []
                alert_priority = {}
                
                # Alerta 1: Alta probabilidad de conversi√≥n
                if conversion_probability >= 80:
                    alerts.append({
                        "type": "high_conversion_probability",
                        "priority": "high",
                        "message": f"Lead con {conversion_probability:.1f}% de probabilidad de conversi√≥n - Acci√≥n inmediata recomendada",
                        "recommended_action": "immediate_outreach",
                        "urgency": "high"
                    })
                    alert_priority["high_conversion_probability"] = "high"
                
                # Alerta 2: Urgencia detectada
                urgency_signals_count = len([s for s in budget_signals if "urgent" in str(s).lower()])
                if urgency_signals_count >= 2 or ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 80:
                    alerts.append({
                        "type": "urgency_detected",
                        "priority": "critical",
                        "message": f"Urgencia alta detectada - {urgency_signals_count} se√±ales de urgencia identificadas",
                        "recommended_action": "immediate_contact",
                        "urgency": "critical"
                    })
                    alert_priority["urgency_detected"] = "critical"
                
                # Alerta 3: Alto riesgo de churn
                if advanced_early_abandonment_analysis.get("abandonment_risk_level") in ["critical", "high"]:
                    alerts.append({
                        "type": "high_churn_risk",
                        "priority": "high",
                        "message": f"Riesgo de abandono {advanced_early_abandonment_analysis.get('abandonment_risk_level')} - Intervenci√≥n requerida",
                        "recommended_action": "intervention_required",
                        "urgency": "high"
                    })
                    alert_priority["high_churn_risk"] = "high"
                
                # Alerta 4: Lead de alto valor
                if estimated_ltv >= 50000:
                    alerts.append({
                        "type": "high_value_lead",
                        "priority": "high",
                        "message": f"Lead de alto valor - LTV estimado: ${estimated_ltv:,.0f}",
                        "recommended_action": "priority_handling",
                        "urgency": "medium"
                    })
                    alert_priority["high_value_lead"] = "high"
                
                # Alerta 5: Competencia detectada
                if deep_competitive_analysis.get("competitive_risk_score", 0) >= 60:
                    alerts.append({
                        "type": "competition_detected",
                        "priority": "medium",
                        "message": f"Competencia detectada - Riesgo competitivo: {deep_competitive_analysis.get('competitive_risk_score', 0):.0f}%",
                        "recommended_action": "competitive_response",
                        "urgency": "medium"
                    })
                    alert_priority["competition_detected"] = "medium"
                
                # Alerta 6: Sentimiento negativo
                if sentiment_analysis and sentiment_analysis.get("sentiment_score", 0) < -20:
                    alerts.append({
                        "type": "negative_sentiment",
                        "priority": "high",
                        "message": f"Sentimiento negativo detectado - Score: {sentiment_analysis.get('sentiment_score', 0):.0f}",
                        "recommended_action": "address_concerns_immediately",
                        "urgency": "high"
                    })
                    alert_priority["negative_sentiment"] = "high"
                
                # Alerta 7: Momento √≥ptimo para cerrar
                if granular_closing_timing_prediction.get("closing_readiness_score", 0) >= 80:
                    alerts.append({
                        "type": "optimal_closing_window",
                        "priority": "high",
                        "message": f"Ventana √≥ptima para cerrar - Readiness: {granular_closing_timing_prediction.get('closing_readiness_score', 0):.0f}%",
                        "recommended_action": "initiate_closing_process",
                        "urgency": "high"
                    })
                    alert_priority["optimal_closing_window"] = "high"
                
                # Priorizar alertas
                priority_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
                alerts.sort(key=lambda x: priority_order.get(x["priority"], 99))
                
                intelligent_proactive_alerts = {
                    "alerts": alerts,
                    "total_alerts": len(alerts),
                    "critical_alerts": len([a for a in alerts if a["priority"] == "critical"]),
                    "high_priority_alerts": len([a for a in alerts if a["priority"] == "high"]),
                    "alert_priority_map": alert_priority,
                    "requires_immediate_action": any(a["priority"] == "critical" for a in alerts),
                    "recommended_next_steps": [
                        a["recommended_action"] for a in alerts[:3]  # Top 3 acciones
                    ]
                }
                
                logger.info(
                    "intelligent_proactive_alerts_completed",
                    email=email,
                    total_alerts=len(alerts),
                    critical_alerts=intelligent_proactive_alerts.get("critical_alerts", 0),
                    requires_immediate_action=intelligent_proactive_alerts.get("requires_immediate_action", False)
                )
            except Exception as e:
                logger.warning("intelligent_proactive_alerts_error", email=email, error=str(e))

            # 15. PREDICCI√ìN DE UPSELLING/CROSS-SELLING AVANZADA
            advanced_upselling_crossselling_prediction = {}
            try:
                upselling_readiness_factors = {
                    "product_usage": min(advanced_engagement_analysis.get("advanced_engagement_score", 0) / 2, 30) if advanced_engagement_analysis else 0,
                    "satisfaction": predictive_customer_satisfaction_analysis.get("satisfaction_score", 0) if predictive_customer_satisfaction_analysis else 0,
                    "company_growth": 20 if company_size in ["startup", "small"] else 15 if company_size == "medium" else 10,
                    "budget_signals": 15 if len(budget_signals) >= 2 else 10 if len(budget_signals) >= 1 else 5,
                    "ml_intent": min(ml_purchase_intent_scoring.get("ml_intent_score", 0) / 3, 25) if ml_purchase_intent_scoring else 0
                }
                upselling_readiness_score = sum(upselling_readiness_factors.values())
                crossselling_readiness_factors = {
                    "product_fit": min(product_fit_score / 2, 30),
                    "engagement": min(advanced_engagement_analysis.get("advanced_engagement_score", 0) / 3, 25) if advanced_engagement_analysis else 0,
                    "satisfaction": min(predictive_customer_satisfaction_analysis.get("satisfaction_score", 0) / 4, 20) if predictive_customer_satisfaction_analysis else 0,
                    "cultural_fit": min(advanced_cultural_fit_analysis.get("cultural_fit_score", 0) / 4, 15) if advanced_cultural_fit_analysis else 0,
                    "needs_detection": 10 if advanced_content_analysis.get("content_type") in ["technical_inquiry", "partnership_inquiry"] else 0
                }
                crossselling_readiness_score = sum(crossselling_readiness_factors.values())
                upselling_timing = (
                    "immediate" if upselling_readiness_score >= 80 else
                    "within_30_days" if upselling_readiness_score >= 60 else
                    "within_90_days" if upselling_readiness_score >= 40 else
                    "not_ready"
                )
                crossselling_timing = (
                    "immediate" if crossselling_readiness_score >= 70 else
                    "within_30_days" if crossselling_readiness_score >= 50 else
                    "within_90_days" if crossselling_readiness_score >= 30 else
                    "not_ready"
                )
                upselling_products = []
                if upselling_readiness_score >= 60:
                    if company_size in ["enterprise", "large"]:
                        upselling_products.append("enterprise_plan")
                    if advanced_content_analysis.get("content_type") == "technical_inquiry":
                        upselling_products.append("api_access")
                    if advanced_engagement_analysis.get("advanced_engagement_score", 0) >= 70:
                        upselling_products.append("premium_support")
                crossselling_products = []
                if crossselling_readiness_score >= 50:
                    if advanced_content_analysis.get("content_type") == "technical_inquiry":
                        crossselling_products.append("integration_tools")
                    if product_fit_score >= 70:
                        crossselling_products.append("training_programs")
                    if advanced_cultural_fit_analysis.get("cultural_fit_score", 0) >= 70:
                        crossselling_products.append("consulting_services")
                advanced_upselling_crossselling_prediction = {
                    "upselling_readiness_score": round(upselling_readiness_score, 2),
                    "upselling_readiness_level": (
                        "very_high" if upselling_readiness_score >= 80 else
                        "high" if upselling_readiness_score >= 60 else
                        "medium" if upselling_readiness_score >= 40 else
                        "low"
                    ),
                    "upselling_timing": upselling_timing,
                    "upselling_readiness_factors": {k: round(v, 2) for k, v in upselling_readiness_factors.items()},
                    "recommended_upselling_products": upselling_products,
                    "predicted_upselling_value": round(estimated_ltv * 0.3, 2) if upselling_readiness_score >= 60 else 0,
                    "crossselling_readiness_score": round(crossselling_readiness_score, 2),
                    "crossselling_readiness_level": (
                        "very_high" if crossselling_readiness_score >= 70 else
                        "high" if crossselling_readiness_score >= 50 else
                        "medium" if crossselling_readiness_score >= 30 else
                        "low"
                    ),
                    "crossselling_timing": crossselling_timing,
                    "crossselling_readiness_factors": {k: round(v, 2) for k, v in crossselling_readiness_factors.items()},
                    "recommended_crossselling_products": crossselling_products,
                    "predicted_crossselling_value": round(estimated_ltv * 0.2, 2) if crossselling_readiness_score >= 50 else 0,
                    "total_expansion_opportunity": round(
                        (estimated_ltv * 0.3 if upselling_readiness_score >= 60 else 0) +
                        (estimated_ltv * 0.2 if crossselling_readiness_score >= 50 else 0), 2
                    )
                }
                logger.info(
                    "advanced_upselling_crossselling_prediction_completed",
                    email=email,
                    upselling_readiness=upselling_readiness_score,
                    crossselling_readiness=crossselling_readiness_score
                )
            except Exception as e:
                logger.warning("advanced_upselling_crossselling_prediction_error", email=email, error=str(e))
            
            # 16. AN√ÅLISIS DE COHORTES AVANZADO
            advanced_cohort_analysis = {}
            try:
                from datetime import datetime
                lead_created_date = lead_data.get("created_at")
                if isinstance(lead_created_date, str):
                    try:
                        lead_created_date = datetime.fromisoformat(lead_created_date.replace('Z', '+00:00'))
                    except:
                        lead_created_date = datetime.utcnow()
                elif not lead_created_date:
                    lead_created_date = datetime.utcnow()
                
                cohort_month = lead_created_date.strftime("%Y-%m")
                days_since_cohort_start = (datetime.utcnow() - lead_created_date.replace(day=1)).days
                
                cohort_type = (
                    "new" if days_since_cohort_start <= 30 else
                    "recent" if days_since_cohort_start <= 90 else
                    "established" if days_since_cohort_start <= 180 else
                    "mature"
                )
                
                cohort_score = (
                    100 if cohort_type == "new" and score >= 70 else
                    80 if cohort_type == "new" and score >= 50 else
                    70 if cohort_type == "recent" and score >= 60 else
                    60 if cohort_type == "recent" and score >= 40 else
                    50 if cohort_type == "established" else
                    40
                )
                
                cohort_behavior_prediction = {
                    "expected_conversion_rate": (
                        0.25 if cohort_type == "new" and score >= 70 else
                        0.15 if cohort_type == "new" and score >= 50 else
                        0.10 if cohort_type == "recent" and score >= 60 else
                        0.05 if cohort_type == "recent" else
                        0.02
                    ),
                    "expected_ltv": estimated_ltv,
                    "expected_retention_rate": (
                        0.90 if cohort_score >= 80 else
                        0.75 if cohort_score >= 60 else
                        0.60 if cohort_score >= 40 else
                        0.40
                    )
                }
                
                advanced_cohort_analysis = {
                    "cohort_month": cohort_month,
                    "cohort_type": cohort_type,
                    "days_since_cohort_start": days_since_cohort_start,
                    "cohort_score": cohort_score,
                    "cohort_behavior_prediction": cohort_behavior_prediction,
                    "cohort_health": (
                        "excellent" if cohort_score >= 80 else
                        "good" if cohort_score >= 60 else
                        "fair" if cohort_score >= 40 else
                        "poor"
                    ),
                    "cohort_optimization_opportunities": [
                        opp for opp in [
                            "accelerate_new_cohorts" if cohort_type == "new" and score < 50 else None,
                            "re_engage_recent_cohorts" if cohort_type == "recent" and score < 40 else None,
                            "retain_established_cohorts" if cohort_type == "established" and score < 50 else None,
                            "reactivate_mature_cohorts" if cohort_type == "mature" and score < 40 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "advanced_cohort_analysis_completed",
                    email=email,
                    cohort_month=cohort_month,
                    cohort_type=cohort_type,
                    cohort_score=cohort_score
                )
            except Exception as e:
                logger.warning("advanced_cohort_analysis_error", email=email, error=str(e))
            
            # 17. AN√ÅLISIS DE ATRIBUCI√ìN MULTI-TOUCH
            multi_touch_attribution_analysis = {}
            try:
                from datetime import datetime
                touchpoints = []
                
                if lead_data.get("source"):
                    touchpoints.append({
                        "type": "source",
                        "channel": lead_data.get("source"),
                        "timestamp": lead_data.get("created_at"),
                        "weight": 0.3
                    })
                
                if lead_data.get("utm_campaign"):
                    touchpoints.append({
                        "type": "campaign",
                        "channel": lead_data.get("utm_campaign"),
                        "timestamp": lead_data.get("created_at"),
                        "weight": 0.25
                    })
                
                if lead_data.get("referrer"):
                    touchpoints.append({
                        "type": "referrer",
                        "channel": lead_data.get("referrer"),
                        "timestamp": lead_data.get("created_at"),
                        "weight": 0.2
                    })
                
                if advanced_engagement_analysis:
                    touchpoints.append({
                        "type": "engagement",
                        "channel": "email",
                        "timestamp": datetime.utcnow().isoformat(),
                        "weight": 0.15
                    })
                
                if social_analysis and social_analysis.get("linkedin_score", 0) > 0:
                    touchpoints.append({
                        "type": "social",
                        "channel": "linkedin",
                        "timestamp": datetime.utcnow().isoformat(),
                        "weight": 0.1
                    })
                
                attribution_model = "multi_touch_linear"
                total_weight = sum(tp["weight"] for tp in touchpoints)
                attribution_scores = {
                    tp["channel"]: round((tp["weight"] / total_weight) * 100, 2) if total_weight > 0 else 0
                    for tp in touchpoints
                }
                
                primary_channel = max(attribution_scores.items(), key=lambda x: x[1])[0] if attribution_scores else "unknown"
                
                multi_touch_attribution_analysis = {
                    "touchpoints": touchpoints,
                    "total_touchpoints": len(touchpoints),
                    "attribution_model": attribution_model,
                    "attribution_scores": attribution_scores,
                    "primary_channel": primary_channel,
                    "channel_diversity": (
                        "high" if len(touchpoints) >= 4 else
                        "medium" if len(touchpoints) >= 2 else
                        "low"
                    ),
                    "attribution_insights": [
                        insight for insight in [
                            "multi_channel_journey" if len(touchpoints) >= 3 else None,
                            "strong_source_attribution" if attribution_scores.get(lead_data.get("source", ""), 0) >= 40 else None,
                            "campaign_driven" if attribution_scores.get(lead_data.get("utm_campaign", ""), 0) >= 30 else None,
                            "social_influenced" if any(tp["type"] == "social" for tp in touchpoints) else None
                        ] if insight
                    ]
                }
                
                logger.info(
                    "multi_touch_attribution_analysis_completed",
                    email=email,
                    total_touchpoints=len(touchpoints),
                    primary_channel=primary_channel
                )
            except Exception as e:
                logger.warning("multi_touch_attribution_analysis_error", email=email, error=str(e))
            
            # 18. AN√ÅLISIS DE TENDENCIAS Y PATRONES TEMPORALES
            temporal_trends_pattern_analysis = {}
            try:
                from datetime import datetime
                current_hour = datetime.utcnow().hour
                current_day = datetime.utcnow().weekday()
                current_month = datetime.utcnow().month
                
                activity_patterns = {
                    "hour_of_day": current_hour,
                    "day_of_week": current_day,
                    "month": current_month,
                    "is_business_hours": 9 <= current_hour <= 17,
                    "is_weekend": current_day >= 5,
                    "is_peak_season": current_month in [10, 11, 12, 1]
                }
                
                timing_score = 0
                if activity_patterns["is_business_hours"]:
                    timing_score += 30
                if not activity_patterns["is_weekend"]:
                    timing_score += 20
                if activity_patterns["is_peak_season"]:
                    timing_score += 25
                if 10 <= current_hour <= 14:
                    timing_score += 25
                
                engagement_trend = (
                    "increasing" if advanced_engagement_analysis and advanced_engagement_analysis.get("advanced_engagement_score", 0) >= 70 else
                    "stable" if advanced_engagement_analysis and 40 <= advanced_engagement_analysis.get("advanced_engagement_score", 0) < 70 else
                    "declining"
                ) if advanced_engagement_analysis else "unknown"
                
                intent_trend = (
                    "increasing" if ml_purchase_intent_scoring and ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 70 else
                    "stable" if ml_purchase_intent_scoring and 40 <= ml_purchase_intent_scoring.get("ml_intent_score", 0) < 70 else
                    "declining"
                ) if ml_purchase_intent_scoring else "unknown"
                
                temporal_trends_pattern_analysis = {
                    "activity_patterns": activity_patterns,
                    "timing_score": timing_score,
                    "optimal_timing": (
                        "optimal" if timing_score >= 70 else
                        "good" if timing_score >= 50 else
                        "fair" if timing_score >= 30 else
                        "poor"
                    ),
                    "engagement_trend": engagement_trend,
                    "intent_trend": intent_trend,
                    "temporal_insights": [
                        insight for insight in [
                            "peak_season_opportunity" if activity_patterns["is_peak_season"] else None,
                            "business_hours_contact" if activity_patterns["is_business_hours"] else None,
                            "weekend_lead" if activity_patterns["is_weekend"] else None,
                            "increasing_engagement" if engagement_trend == "increasing" else None,
                            "increasing_intent" if intent_trend == "increasing" else None
                        ] if insight
                    ],
                    "recommended_timing_adjustments": [
                        adj for adj in [
                            "contact_during_business_hours" if not activity_patterns["is_business_hours"] else None,
                            "avoid_weekends" if activity_patterns["is_weekend"] else None,
                            "leverage_peak_season" if activity_patterns["is_peak_season"] else None
                        ] if adj
                    ]
                }
                
                logger.info(
                    "temporal_trends_pattern_analysis_completed",
                    email=email,
                    timing_score=timing_score,
                    engagement_trend=engagement_trend
                )
            except Exception as e:
                logger.warning("temporal_trends_pattern_analysis_error", email=email, error=str(e))
            

            # 19. RECOMENDACIONES DE CONTENIDO DIN√ÅMICO INTELIGENTE
            intelligent_content_recommendations = {}
            try:
                # Determinar tipo de contenido basado en m√∫ltiples factores
                buyer_stage = optimized_sales_cycle_analysis.get("current_stage", "awareness") if optimized_sales_cycle_analysis else "awareness"
                content_type_pref = advanced_content_analysis.get("content_type", "general_inquiry") if advanced_content_analysis else "general_inquiry"
                ml_intent = ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0
                
                # Mapeo de contenido por etapa
                content_by_stage = {
                    "awareness": [
                        {"type": "blog_post", "topic": "industry_trends", "priority": 1},
                        {"type": "ebook", "topic": "problem_awareness", "priority": 2},
                        {"type": "infographic", "topic": "statistics", "priority": 3}
                    ],
                    "interest": [
                        {"type": "case_study", "topic": "success_stories", "priority": 1},
                        {"type": "webinar", "topic": "solution_overview", "priority": 2},
                        {"type": "demo_video", "topic": "product_tour", "priority": 3}
                    ],
                    "consideration": [
                        {"type": "comparison_guide", "topic": "vs_competitors", "priority": 1},
                        {"type": "pricing_page", "topic": "plans_features", "priority": 2},
                        {"type": "testimonial", "topic": "customer_reviews", "priority": 3}
                    ],
                    "evaluation": [
                        {"type": "trial_offer", "topic": "free_trial", "priority": 1},
                        {"type": "roi_calculator", "topic": "value_calculation", "priority": 2},
                        {"type": "implementation_guide", "topic": "getting_started", "priority": 3}
                    ],
                    "closing": [
                        {"type": "contract_template", "topic": "next_steps", "priority": 1},
                        {"type": "onboarding_guide", "topic": "setup_process", "priority": 2},
                        {"type": "success_kit", "topic": "resources", "priority": 3}
                    ]
                }
                
                # Recomendaciones personalizadas
                recommended_content = content_by_stage.get(buyer_stage, [])
                
                # Ajustar por tipo de contenido preferido
                if content_type_pref == "technical_inquiry":
                    recommended_content.insert(0, {"type": "technical_documentation", "topic": "api_integration", "priority": 0})
                elif content_type_pref == "budget_inquiry":
                    recommended_content.insert(0, {"type": "pricing_calculator", "topic": "cost_analysis", "priority": 0})
                
                # Ajustar por intenci√≥n ML
                if ml_intent >= 70:
                    recommended_content.insert(0, {"type": "demo_request", "topic": "personalized_demo", "priority": 0})
                elif ml_intent >= 50:
                    recommended_content.insert(0, {"type": "consultation_call", "topic": "expert_advice", "priority": 0})
                
                intelligent_content_recommendations = {
                    "recommended_content": recommended_content[:5],  # Top 5
                    "content_count": len(recommended_content),
                    "primary_content_type": recommended_content[0]["type"] if recommended_content else "unknown",
                    "content_strategy": (
                        "accelerate" if ml_intent >= 70 else
                        "nurture" if ml_intent >= 40 else
                        "educate"
                    ),
                    "content_personalization_factors": {
                        "buyer_stage": buyer_stage,
                        "content_type_preference": content_type_pref,
                        "ml_intent_score": ml_intent,
                        "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0
                    }
                }
                
                logger.info(
                    "intelligent_content_recommendations_completed",
                    email=email,
                    primary_content_type=intelligent_content_recommendations.get("primary_content_type"),
                    content_strategy=intelligent_content_recommendations.get("content_strategy")
                )
            except Exception as e:
                logger.warning("intelligent_content_recommendations_error", email=email, error=str(e))
            
            # 20. AN√ÅLISIS DE FIT DE PRECIO ULTRA SOFISTICADO
            ultra_sophisticated_pricing_fit_analysis = {}
            try:
                # Factores de pricing fit
                pricing_factors = {
                    "company_size_factor": (
                        30 if company_size in ["enterprise", "large"] else
                        20 if company_size == "medium" else
                        10
                    ),
                    "industry_value_factor": (
                        25 if industry in ["technology", "finance", "healthcare"] else
                        15 if industry in ["retail", "manufacturing"] else
                        10
                    ),
                    "budget_signals_factor": (
                        20 if len(budget_signals) >= 3 else
                        15 if len(budget_signals) >= 2 else
                        10 if len(budget_signals) >= 1 else
                        5
                    ),
                    "product_fit_factor": min(product_fit_score / 3, 15),
                    "market_fit_factor": min(market_fit_analysis.get("market_fit_score", 0) / 4, 10) if market_fit_analysis else 5,
                    "geographic_factor": 5  # Base
                }
                
                total_pricing_fit_score = sum(pricing_factors.values())
                
                # Determinar tier recomendado
                recommended_tier = (
                    "enterprise" if total_pricing_fit_score >= 90 else
                    "professional" if total_pricing_fit_score >= 70 else
                    "business" if total_pricing_fit_score >= 50 else
                    "starter"
                )
                
                # Estimar rango de precio
                price_ranges = {
                    "enterprise": {"min": 10000, "max": 50000, "typical": 25000},
                    "professional": {"min": 5000, "max": 10000, "typical": 7500},
                    "business": {"min": 2000, "max": 5000, "typical": 3500},
                    "starter": {"min": 500, "max": 2000, "typical": 1000}
                }
                
                price_range = price_ranges.get(recommended_tier, {"min": 0, "max": 0, "typical": 0})
                
                # Calcular sensibilidad al precio
                price_sensitivity = (
                    "low" if total_pricing_fit_score >= 80 else
                    "medium" if total_pricing_fit_score >= 60 else
                    "high" if total_pricing_fit_score >= 40 else
                    "very_high"
                )
                
                ultra_sophisticated_pricing_fit_analysis = {
                    "total_pricing_fit_score": round(total_pricing_fit_score, 2),
                    "pricing_factors": {k: round(v, 2) for k, v in pricing_factors.items()},
                    "recommended_tier": recommended_tier,
                    "price_range": price_range,
                    "estimated_price": price_range["typical"],
                    "price_sensitivity": price_sensitivity,
                    "pricing_confidence": (
                        "high" if total_pricing_fit_score >= 80 else
                        "medium" if total_pricing_fit_score >= 60 else
                        "low"
                    ),
                    "pricing_recommendations": [
                        rec for rec in [
                            "offer_enterprise_tier" if recommended_tier == "enterprise" and total_pricing_fit_score >= 90 else None,
                            "highlight_value_proposition" if price_sensitivity == "high" else None,
                            "offer_flexible_pricing" if price_sensitivity in ["high", "very_high"] else None,
                            "emphasize_roi" if total_pricing_fit_score >= 70 else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "ultra_sophisticated_pricing_fit_analysis_completed",
                    email=email,
                    recommended_tier=recommended_tier,
                    estimated_price=price_range["typical"],
                    price_sensitivity=price_sensitivity
                )
            except Exception as e:
                logger.warning("ultra_sophisticated_pricing_fit_analysis_error", email=email, error=str(e))
            
            # 21. SISTEMA DE SCORING DE CALIDAD DE DATOS MEJORADO
            enhanced_data_quality_scoring = {}
            try:
                # Factores de calidad de datos
                quality_factors = {
                    "completeness": 0,
                    "validity": 0,
                    "consistency": 0,
                    "accuracy": 0,
                    "timeliness": 0
                }
                
                # Completitud
                completeness_score = 0
                if lead_data.get("email"): completeness_score += 20
                if lead_data.get("first_name") and lead_data.get("last_name"): completeness_score += 20
                if lead_data.get("phone"): completeness_score += 15
                if lead_data.get("company"): completeness_score += 15
                if lead_data.get("message"): completeness_score += 10
                if lead_data.get("source"): completeness_score += 10
                if lead_data.get("utm_campaign"): completeness_score += 10
                quality_factors["completeness"] = completeness_score
                
                # Validez
                validity_score = 0
                email = lead_data.get("email", "")
                if "@" in email and "." in email.split("@")[1]:
                    validity_score += 30
                if lead_data.get("phone") and len(str(lead_data.get("phone"))) >= 10:
                    validity_score += 25
                if lead_data.get("first_name") and len(lead_data.get("first_name")) >= 2:
                    validity_score += 25
                if lead_data.get("company") and len(lead_data.get("company")) >= 2:
                    validity_score += 20
                quality_factors["validity"] = validity_score
                
                # Consistencia
                consistency_score = 70  # Base
                if lead_data.get("email") and lead_data.get("company"):
                    email_domain = email.split("@")[1] if "@" in email else ""
                    company_lower = lead_data.get("company", "").lower()
                    if email_domain and company_lower and email_domain.replace(".", "") in company_lower.replace(" ", ""):
                        consistency_score += 20
                if lead_data.get("first_name") and lead_data.get("last_name"):
                    consistency_score += 10
                quality_factors["consistency"] = min(consistency_score, 100)
                
                # Precisi√≥n (basado en ML quality scoring si existe)
                accuracy_score = ml_enhanced_quality_scoring.get("ml_quality_score", 50) if ml_enhanced_quality_scoring else 50
                quality_factors["accuracy"] = accuracy_score
                
                # Actualidad (timeliness)
                timeliness_score = 100  # Asumimos que los datos son actuales
                quality_factors["timeliness"] = timeliness_score
                
                # Score total de calidad
                total_quality_score = sum(quality_factors.values()) / len(quality_factors)
                
                enhanced_data_quality_scoring = {
                    "total_quality_score": round(total_quality_score, 2),
                    "quality_factors": {k: round(v, 2) for k, v in quality_factors.items()},
                    "quality_grade": (
                        "A" if total_quality_score >= 90 else
                        "B" if total_quality_score >= 80 else
                        "C" if total_quality_score >= 70 else
                        "D" if total_quality_score >= 60 else
                        "F"
                    ),
                    "quality_level": (
                        "excellent" if total_quality_score >= 90 else
                        "very_good" if total_quality_score >= 80 else
                        "good" if total_quality_score >= 70 else
                        "fair" if total_quality_score >= 60 else
                        "poor"
                    ),
                    "quality_improvement_opportunities": [
                        opp for opp in [
                            "improve_completeness" if quality_factors["completeness"] < 70 else None,
                            "enhance_validity" if quality_factors["validity"] < 70 else None,
                            "increase_consistency" if quality_factors["consistency"] < 80 else None,
                            "improve_accuracy" if quality_factors["accuracy"] < 70 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "enhanced_data_quality_scoring_completed",
                    email=email,
                    total_quality_score=total_quality_score,
                    quality_grade=enhanced_data_quality_scoring.get("quality_grade")
                )
            except Exception as e:
                logger.warning("enhanced_data_quality_scoring_error", email=email, error=str(e))
            

            # 22. PREDICCI√ìN DE LIFETIME VALUE MEJORADA
            enhanced_ltv_prediction = {}
            try:
                # Factores base para LTV
                base_ltv = estimated_ltv
                
                # Factores de ajuste
                ltv_adjustment_factors = {
                    "engagement_multiplier": min(advanced_engagement_analysis.get("advanced_engagement_score", 0) / 100, 1.5) if advanced_engagement_analysis else 1.0,
                    "satisfaction_multiplier": min(predictive_customer_satisfaction_analysis.get("satisfaction_score", 0) / 100, 1.3) if predictive_customer_satisfaction_analysis else 1.0,
                    "product_fit_multiplier": min(product_fit_score / 100, 1.2),
                    "cultural_fit_multiplier": min(advanced_cultural_fit_analysis.get("cultural_fit_score", 0) / 100, 1.2) if advanced_cultural_fit_analysis else 1.0,
                    "retention_multiplier": min(advanced_retention_churn_analysis.get("retention_score", 0) / 100, 1.4) if advanced_retention_churn_analysis else 1.0,
                    "expansion_multiplier": 1.0 + (advanced_upselling_crossselling_prediction.get("total_expansion_opportunity", 0) / estimated_ltv) if estimated_ltv > 0 and advanced_upselling_crossselling_prediction else 1.0
                }
                
                # Calcular LTV ajustado
                adjusted_ltv = base_ltv
                for factor, multiplier in ltv_adjustment_factors.items():
                    adjusted_ltv *= multiplier
                
                # Predicci√≥n de LTV a 1, 2, 3 a√±os
                ltv_1_year = adjusted_ltv * 0.4
                ltv_2_years = adjusted_ltv * 0.7
                ltv_3_years = adjusted_ltv
                
                # Clasificar LTV
                ltv_category = (
                    "very_high" if adjusted_ltv >= 100000 else
                    "high" if adjusted_ltv >= 50000 else
                    "medium" if adjusted_ltv >= 20000 else
                    "low" if adjusted_ltv >= 5000 else
                    "very_low"
                )
                
                enhanced_ltv_prediction = {
                    "base_ltv": round(base_ltv, 2),
                    "adjusted_ltv": round(adjusted_ltv, 2),
                    "ltv_adjustment_factors": {k: round(v, 3) for k, v in ltv_adjustment_factors.items()},
                    "ltv_projection": {
                        "1_year": round(ltv_1_year, 2),
                        "2_years": round(ltv_2_years, 2),
                        "3_years": round(ltv_3_years, 2)
                    },
                    "ltv_category": ltv_category,
                    "ltv_confidence": (
                        "high" if all(m >= 0.9 for m in ltv_adjustment_factors.values() if isinstance(m, (int, float))) else
                        "medium" if all(m >= 0.7 for m in ltv_adjustment_factors.values() if isinstance(m, (int, float))) else
                        "low"
                    ),
                    "ltv_optimization_opportunities": [
                        opp for opp in [
                            "increase_engagement" if ltv_adjustment_factors["engagement_multiplier"] < 1.1 else None,
                            "improve_satisfaction" if ltv_adjustment_factors["satisfaction_multiplier"] < 1.1 else None,
                            "enhance_product_fit" if ltv_adjustment_factors["product_fit_multiplier"] < 1.1 else None,
                            "improve_retention" if ltv_adjustment_factors["retention_multiplier"] < 1.2 else None,
                            "focus_on_expansion" if ltv_adjustment_factors["expansion_multiplier"] < 1.1 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "enhanced_ltv_prediction_completed",
                    email=email,
                    base_ltv=base_ltv,
                    adjusted_ltv=adjusted_ltv,
                    ltv_category=ltv_category
                )
            except Exception as e:
                logger.warning("enhanced_ltv_prediction_error", email=email, error=str(e))
            
            # 23. RECOMENDACIONES DE PRODUCTOS/SERVICIOS ULTRA INTELIGENTES
            ultra_intelligent_product_recommendations = {}
            try:
                # An√°lisis de necesidades
                detected_needs = []
                message_lower = lead_data.get("message", "").lower()
                
                if any(kw in message_lower for kw in ["api", "integraci√≥n", "integraci√≥n", "sistema"]):
                    detected_needs.append("integration")
                if any(kw in message_lower for kw in ["seguridad", "security", "compliance", "gdpr"]):
                    detected_needs.append("security")
                if any(kw in message_lower for kw in ["escalar", "crecer", "growth", "expansi√≥n"]):
                    detected_needs.append("scalability")
                if any(kw in message_lower for kw in ["analytics", "reportes", "datos", "m√©tricas"]):
                    detected_needs.append("analytics")
                if any(kw in message_lower for kw in ["soporte", "support", "ayuda", "help"]):
                    detected_needs.append("support")
                
                # Productos recomendados basados en necesidades
                product_recommendations = []
                
                if "integration" in detected_needs:
                    product_recommendations.append({
                        "product": "API Access",
                        "priority": 1,
                        "reason": "Integration needs detected",
                        "fit_score": 90
                    })
                
                if "security" in detected_needs or company_size in ["enterprise", "large"]:
                    product_recommendations.append({
                        "product": "Enterprise Security",
                        "priority": 1,
                        "reason": "Security requirements identified",
                        "fit_score": 85
                    })
                
                if "scalability" in detected_needs:
                    product_recommendations.append({
                        "product": "Scalable Infrastructure",
                        "priority": 2,
                        "reason": "Growth and scaling needs",
                        "fit_score": 80
                    })
                
                if "analytics" in detected_needs:
                    product_recommendations.append({
                        "product": "Advanced Analytics",
                        "priority": 2,
                        "reason": "Data and reporting needs",
                        "fit_score": 75
                    })
                
                if "support" in detected_needs or advanced_engagement_analysis.get("advanced_engagement_score", 0) >= 70:
                    product_recommendations.append({
                        "product": "Premium Support",
                        "priority": 3,
                        "reason": "High engagement and support needs",
                        "fit_score": 70
                    })
                
                # Ordenar por prioridad y fit score
                product_recommendations.sort(key=lambda x: (x["priority"], -x["fit_score"]))
                
                ultra_intelligent_product_recommendations = {
                    "detected_needs": detected_needs,
                    "product_recommendations": product_recommendations[:5],  # Top 5
                    "recommendation_count": len(product_recommendations),
                    "primary_recommendation": product_recommendations[0] if product_recommendations else None,
                    "recommendation_confidence": (
                        "high" if product_recommendations and product_recommendations[0]["fit_score"] >= 85 else
                        "medium" if product_recommendations and product_recommendations[0]["fit_score"] >= 70 else
                        "low"
                    ),
                    "recommendation_factors": {
                        "detected_needs_count": len(detected_needs),
                        "product_fit_score": product_fit_score,
                        "company_size": company_size,
                        "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0
                    }
                }
                
                logger.info(
                    "ultra_intelligent_product_recommendations_completed",
                    email=email,
                    detected_needs_count=len(detected_needs),
                    recommendation_count=len(product_recommendations)
                )
            except Exception as e:
                logger.warning("ultra_intelligent_product_recommendations_error", email=email, error=str(e))
            
            # 24. AN√ÅLISIS DE FIT DE MERCADO ULTRA MEJORADO
            ultra_enhanced_market_fit_analysis = {}
            try:
                # Factores de market fit
                market_fit_factors = {
                    "industry_alignment": (
                        30 if industry in ["technology", "finance", "healthcare"] else
                        20 if industry in ["retail", "manufacturing", "education"] else
                        10
                    ),
                    "company_size_fit": (
                        25 if company_size in ["enterprise", "large"] else
                        20 if company_size == "medium" else
                        15 if company_size == "small" else
                        10
                    ),
                    "geographic_presence": 15,  # Base
                    "growth_potential": (
                        20 if company_size in ["startup", "small"] else
                        15 if company_size == "medium" else
                        10
                    ),
                    "technology_adoption": (
                        15 if advanced_content_analysis.get("content_type") == "technical_inquiry" else
                        10 if advanced_cultural_fit_analysis.get("cultural_factors", {}).get("technology_adoption", 0) >= 70 else
                        5
                    ) if advanced_content_analysis else 5,
                    "budget_availability": (
                        15 if len(budget_signals) >= 2 else
                        10 if len(budget_signals) >= 1 else
                        5
                    )
                }
                
                total_market_fit_score = sum(market_fit_factors.values())
                
                # Clasificar market fit
                market_fit_level = (
                    "excellent" if total_market_fit_score >= 90 else
                    "very_good" if total_market_fit_score >= 80 else
                    "good" if total_market_fit_score >= 70 else
                    "fair" if total_market_fit_score >= 60 else
                    "poor"
                )
                
                # Oportunidades de mercado
                market_opportunities = []
                if market_fit_factors["industry_alignment"] >= 25:
                    market_opportunities.append("strong_industry_fit")
                if market_fit_factors["company_size_fit"] >= 20:
                    market_opportunities.append("ideal_company_size")
                if market_fit_factors["growth_potential"] >= 15:
                    market_opportunities.append("high_growth_potential")
                if market_fit_factors["technology_adoption"] >= 12:
                    market_opportunities.append("tech_forward_company")
                
                ultra_enhanced_market_fit_analysis = {
                    "total_market_fit_score": round(total_market_fit_score, 2),
                    "market_fit_level": market_fit_level,
                    "market_fit_factors": {k: round(v, 2) for k, v in market_fit_factors.items()},
                    "market_opportunities": market_opportunities,
                    "market_fit_strengths": [
                        k for k, v in market_fit_factors.items() if v >= 20
                    ],
                    "market_fit_weaknesses": [
                        k for k, v in market_fit_factors.items() if v < 10
                    ],
                    "market_fit_confidence": (
                        "high" if total_market_fit_score >= 80 else
                        "medium" if total_market_fit_score >= 60 else
                        "low"
                    ),
                    "market_optimization_recommendations": [
                        rec for rec in [
                            "target_similar_industries" if market_fit_factors["industry_alignment"] >= 25 else None,
                            "focus_on_company_size_segment" if market_fit_factors["company_size_fit"] >= 20 else None,
                            "emphasize_growth_benefits" if market_fit_factors["growth_potential"] >= 15 else None,
                            "highlight_technology_advantages" if market_fit_factors["technology_adoption"] >= 12 else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "ultra_enhanced_market_fit_analysis_completed",
                    email=email,
                    total_market_fit_score=total_market_fit_score,
                    market_fit_level=market_fit_level
                )
            except Exception as e:
                logger.warning("ultra_enhanced_market_fit_analysis_error", email=email, error=str(e))
            
            # 25. AN√ÅLISIS DE PATRONES DE COMPORTAMIENTO PREDICTIVO AVANZADO
            advanced_predictive_behavior_patterns = {}
            try:
                # Analizar patrones hist√≥ricos de comportamiento
                behavior_patterns = {
                    "visit_frequency": (
                        "high" if advanced_engagement_analysis.get("advanced_engagement_score", 0) >= 70 else
                        "medium" if advanced_engagement_analysis.get("advanced_engagement_score", 0) >= 40 else
                        "low"
                    ) if advanced_engagement_analysis else "unknown",
                    "engagement_consistency": (
                        "consistent" if advanced_engagement_analysis.get("engagement_profile") == "power_user" else
                        "moderate" if advanced_engagement_analysis.get("engagement_profile") in ["regular_user", "moderate_user"] else
                        "irregular"
                    ) if advanced_engagement_analysis else "unknown",
                    "content_preference": advanced_content_analysis.get("content_type", "general_inquiry") if advanced_content_analysis else "unknown",
                    "communication_style": advanced_communication_pattern_analysis.get("dominant_style", "neutral") if advanced_communication_pattern_analysis else "unknown",
                    "response_time": (
                        "fast" if ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 70 else
                        "medium" if ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 40 else
                        "slow"
                    ) if ml_purchase_intent_scoring else "unknown"
                }
                
                # Predecir pr√≥ximo comportamiento
                predicted_next_behavior = {
                    "likely_action": (
                        "purchase" if ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 70 else
                        "demo_request" if ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 50 else
                        "content_consumption" if advanced_engagement_analysis.get("advanced_engagement_score", 0) >= 50 else
                        "research"
                    ) if ml_purchase_intent_scoring else "research",
                    "predicted_timing": (
                        "within_7_days" if ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 70 else
                        "within_30_days" if ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 50 else
                        "within_90_days" if ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 30 else
                        "beyond_90_days"
                    ) if ml_purchase_intent_scoring else "beyond_90_days",
                    "confidence": (
                        "high" if ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 70 and advanced_engagement_analysis.get("advanced_engagement_score", 0) >= 60 else
                        "medium" if ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 50 or advanced_engagement_analysis.get("advanced_engagement_score", 0) >= 50 else
                        "low"
                    ) if ml_purchase_intent_scoring and advanced_engagement_analysis else "low"
                }
                
                # Identificar patrones an√≥malos
                anomalous_patterns = []
                if behavior_patterns["visit_frequency"] == "high" and behavior_patterns["engagement_consistency"] == "irregular":
                    anomalous_patterns.append("high_frequency_irregular_engagement")
                if behavior_patterns["response_time"] == "fast" and ml_purchase_intent_scoring.get("ml_intent_score", 0) < 30:
                    anomalous_patterns.append("fast_response_low_intent")
                
                advanced_predictive_behavior_patterns = {
                    "behavior_patterns": behavior_patterns,
                    "predicted_next_behavior": predicted_next_behavior,
                    "anomalous_patterns": anomalous_patterns,
                    "behavior_consistency_score": (
                        90 if behavior_patterns["visit_frequency"] == "high" and behavior_patterns["engagement_consistency"] == "consistent" else
                        70 if behavior_patterns["visit_frequency"] == "medium" and behavior_patterns["engagement_consistency"] == "moderate" else
                        50 if behavior_patterns["visit_frequency"] == "low" else
                        30
                    ),
                    "behavior_insights": [
                        insight for insight in [
                            "highly_engaged_lead" if behavior_patterns["visit_frequency"] == "high" else None,
                            "consistent_behavior" if behavior_patterns["engagement_consistency"] == "consistent" else None,
                            "technical_lead" if behavior_patterns["content_preference"] == "technical_inquiry" else None,
                            "fast_responder" if behavior_patterns["response_time"] == "fast" else None,
                            "anomalous_behavior_detected" if anomalous_patterns else None
                        ] if insight
                    ]
                }
                
                logger.info(
                    "advanced_predictive_behavior_patterns_completed",
                    email=email,
                    likely_action=predicted_next_behavior.get("likely_action"),
                    predicted_timing=predicted_next_behavior.get("predicted_timing")
                )
            except Exception as e:
                logger.warning("advanced_predictive_behavior_patterns_error", email=email, error=str(e))
            

            # 26. RECOMENDACIONES DE MENSAJES PERSONALIZADOS INTELIGENTES
            intelligent_personalized_message_recommendations = {}
            try:
                # Determinar tono del mensaje
                message_tone = (
                    "professional_formal" if advanced_communication_pattern_analysis.get("dominant_style") == "formal" else
                    "friendly_casual" if advanced_communication_pattern_analysis.get("dominant_style") == "casual" else
                    "technical_detailed" if advanced_content_analysis.get("content_type") == "technical_inquiry" else
                    "value_focused" if advanced_content_analysis.get("content_type") == "budget_inquiry" else
                    "relationship_building"
                ) if advanced_communication_pattern_analysis else "professional"
                
                # Determinar enfoque del mensaje
                message_focus = (
                    "problem_solution" if buyer_persona.get("type") == "problem_solver" else
                    "value_proposition" if ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 60 else
                    "education" if buyer_persona.get("type") == "researcher" else
                    "social_proof" if deep_competitive_analysis.get("competitive_risk_score", 0) >= 50 else
                    "relationship"
                ) if buyer_persona else "value_proposition"
                
                # Generar recomendaciones de mensaje
                message_recommendations = []
                
                # Recomendaci√≥n 1: Saludo personalizado
                greeting = f"Hola {lead_data.get('first_name', '')}" if lead_data.get('first_name') else "Hola"
                message_recommendations.append({
                    "section": "greeting",
                    "content": greeting,
                    "tone": message_tone,
                    "priority": 1
                })
                
                # Recomendaci√≥n 2: Valor principal
                if message_focus == "value_proposition":
                    value_message = f"Gracias por tu inter√©s en [Producto]. Basado en tu perfil de {company_size} en {industry}, podemos ayudarte a [Beneficio principal]."
                elif message_focus == "problem_solution":
                    value_message = f"Entiendo que est√°s buscando una soluci√≥n para [Problema com√∫n en {industry}]. [Producto] est√° dise√±ado espec√≠ficamente para resolver esto."
                else:
                    value_message = f"[Producto] puede transformar la forma en que {lead_data.get('company', 'tu empresa')} opera en {industry}."
                
                message_recommendations.append({
                    "section": "value_proposition",
                    "content": value_message,
                    "tone": message_tone,
                    "priority": 2
                })
                
                # Recomendaci√≥n 3: CTA personalizado
                if ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 70:
                    cta = "¬øTe gustar√≠a agendar una demo personalizada esta semana?"
                elif ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 50:
                    cta = "¬øTe gustar√≠a conocer m√°s sobre c√≥mo [Producto] puede ayudarte?"
                else:
                    cta = "¬øTe gustar√≠a recibir m√°s informaci√≥n sobre [Producto]?"
                
                message_recommendations.append({
                    "section": "cta",
                    "content": cta,
                    "tone": message_tone,
                    "priority": 3
                })
                
                intelligent_personalized_message_recommendations = {
                    "message_tone": message_tone,
                    "message_focus": message_focus,
                    "message_recommendations": message_recommendations,
                    "personalization_factors": {
                        "buyer_persona": buyer_persona.get("type") if buyer_persona else "unknown",
                        "communication_style": advanced_communication_pattern_analysis.get("dominant_style", "neutral") if advanced_communication_pattern_analysis else "neutral",
                        "ml_intent_score": ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0,
                        "competitive_context": deep_competitive_analysis.get("competitive_risk_level", "low") if deep_competitive_analysis else "low"
                    },
                    "message_effectiveness_score": round(
                        (ml_purchase_intent_scoring.get("ml_intent_score", 0) * 0.4 +
                         advanced_engagement_analysis.get("advanced_engagement_score", 0) * 0.3 +
                         product_fit_score * 0.3) if ml_purchase_intent_scoring and advanced_engagement_analysis else 50, 2
                    )
                }
                
                logger.info(
                    "intelligent_personalized_message_recommendations_completed",
                    email=email,
                    message_tone=message_tone,
                    message_focus=message_focus
                )
            except Exception as e:
                logger.warning("intelligent_personalized_message_recommendations_error", email=email, error=str(e))
            
            # 27. AN√ÅLISIS DE FIT DE INTEGRACI√ìN T√âCNICA AVANZADO
            advanced_technical_integration_fit_analysis = {}
            try:
                # Factores de fit t√©cnico
                technical_fit_factors = {
                    "technical_readiness": (
                        30 if advanced_content_analysis.get("content_type") == "technical_inquiry" else
                        20 if advanced_content_analysis.get("content_complexity") == "high" else
                        10
                    ) if advanced_content_analysis else 10,
                    "api_capability": (
                        25 if any(kw in lead_data.get("message", "").lower() for kw in ["api", "rest", "graphql", "webhook"]) else
                        15 if advanced_content_analysis.get("content_type") == "technical_inquiry" else
                        5
                    ),
                    "integration_experience": (
                        20 if company_size in ["enterprise", "large"] else
                        15 if company_size == "medium" else
                        10
                    ),
                    "technical_resources": (
                        15 if company_size in ["enterprise", "large"] else
                        10 if company_size == "medium" else
                        5
                    ),
                    "urgency_factor": (
                        10 if any("urgent" in str(s).lower() for s in budget_signals) else
                        5
                    )
                }
                
                total_technical_fit_score = sum(technical_fit_factors.values())
                
                # Determinar tipo de integraci√≥n recomendada
                recommended_integration_type = (
                    "api_rest" if technical_fit_factors["api_capability"] >= 20 else
                    "sdk" if technical_fit_factors["technical_readiness"] >= 25 else
                    "webhook" if technical_fit_factors["api_capability"] >= 15 else
                    "manual" if technical_fit_factors["technical_readiness"] < 15 else
                    "guided"
                )
                
                # Estimar complejidad y tiempo
                integration_complexity = (
                    "low" if total_technical_fit_score >= 70 else
                    "medium" if total_technical_fit_score >= 50 else
                    "high"
                )
                
                estimated_setup_time = (
                    "1-2_days" if integration_complexity == "low" else
                    "3-5_days" if integration_complexity == "medium" else
                    "1-2_weeks"
                )
                
                advanced_technical_integration_fit_analysis = {
                    "total_technical_fit_score": round(total_technical_fit_score, 2),
                    "technical_fit_factors": {k: round(v, 2) for k, v in technical_fit_factors.items()},
                    "recommended_integration_type": recommended_integration_type,
                    "integration_complexity": integration_complexity,
                    "estimated_setup_time": estimated_setup_time,
                    "technical_readiness_level": (
                        "high" if total_technical_fit_score >= 70 else
                        "medium" if total_technical_fit_score >= 50 else
                        "low"
                    ),
                    "integration_support_needed": (
                        "minimal" if total_technical_fit_score >= 70 else
                        "moderate" if total_technical_fit_score >= 50 else
                        "extensive"
                    ),
                    "integration_recommendations": [
                        rec for rec in [
                            "provide_api_documentation" if technical_fit_factors["api_capability"] >= 20 else None,
                            "offer_technical_support" if technical_fit_factors["technical_readiness"] < 25 else None,
                            "schedule_integration_call" if integration_complexity == "high" else None,
                            "provide_sdk_access" if recommended_integration_type == "sdk" else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "advanced_technical_integration_fit_analysis_completed",
                    email=email,
                    total_technical_fit_score=total_technical_fit_score,
                    recommended_integration_type=recommended_integration_type
                )
            except Exception as e:
                logger.warning("advanced_technical_integration_fit_analysis_error", email=email, error=str(e))
            
            # 28. SISTEMA DE SCORING DE PRIORIDAD ULTRA MEJORADO
            ultra_enhanced_priority_scoring = {}
            try:
                # Factores de prioridad
                priority_factors = {
                    "conversion_probability": conversion_probability * 0.25,
                    "ml_intent_score": ml_purchase_intent_scoring.get("ml_intent_score", 0) * 0.20 if ml_purchase_intent_scoring else 0,
                    "estimated_ltv": min(estimated_ltv / 1000, 30),  # Cap at 30 points
                    "urgency_signals": len(budget_signals) * 5 if budget_signals else 0,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) * 0.15 if advanced_engagement_analysis else 0,
                    "competitive_risk": deep_competitive_analysis.get("competitive_risk_score", 0) * 0.10 if deep_competitive_analysis else 0
                }
                
                total_priority_score = sum(priority_factors.values())
                
                # Clasificar prioridad
                priority_level = (
                    "critical" if total_priority_score >= 80 else
                    "high" if total_priority_score >= 60 else
                    "medium" if total_priority_score >= 40 else
                    "low" if total_priority_score >= 20 else
                    "very_low"
                )
                
                # Calcular score de acci√≥n requerida
                action_required_score = (
                    100 if priority_level == "critical" else
                    80 if priority_level == "high" else
                    60 if priority_level == "medium" else
                    40 if priority_level == "low" else
                    20
                )
                
                ultra_enhanced_priority_scoring = {
                    "total_priority_score": round(total_priority_score, 2),
                    "priority_level": priority_level,
                    "priority_factors": {k: round(v, 2) for k, v in priority_factors.items()},
                    "action_required_score": action_required_score,
                    "action_urgency": (
                        "immediate" if priority_level == "critical" else
                        "high" if priority_level == "high" else
                        "medium" if priority_level == "medium" else
                        "low"
                    ),
                    "priority_ranking": (
                        "top_10_percent" if total_priority_score >= 80 else
                        "top_25_percent" if total_priority_score >= 60 else
                        "top_50_percent" if total_priority_score >= 40 else
                        "bottom_50_percent"
                    ),
                    "priority_insights": [
                        insight for insight in [
                            "high_conversion_probability" if priority_factors["conversion_probability"] >= 20 else None,
                            "high_intent" if priority_factors["ml_intent_score"] >= 15 else None,
                            "high_value_lead" if priority_factors["estimated_ltv"] >= 20 else None,
                            "urgent_lead" if priority_factors["urgency_signals"] >= 15 else None,
                            "highly_engaged" if priority_factors["engagement_level"] >= 12 else None,
                            "competitive_risk" if priority_factors["competitive_risk"] >= 8 else None
                        ] if insight
                    ]
                }
                
                logger.info(
                    "ultra_enhanced_priority_scoring_completed",
                    email=email,
                    total_priority_score=total_priority_score,
                    priority_level=priority_level
                )
            except Exception as e:
                logger.warning("ultra_enhanced_priority_scoring_error", email=email, error=str(e))
            
            # 29. AN√ÅLISIS DE OPORTUNIDADES DE EXPANSI√ìN AVANZADO
            advanced_expansion_opportunities_analysis = {}
            try:
                # Oportunidades de expansi√≥n
                expansion_opportunities = []
                
                # Oportunidad 1: Upselling
                if advanced_upselling_crossselling_prediction.get("upselling_readiness_score", 0) >= 60:
                    expansion_opportunities.append({
                        "type": "upselling",
                        "readiness_score": advanced_upselling_crossselling_prediction.get("upselling_readiness_score", 0),
                        "recommended_products": advanced_upselling_crossselling_prediction.get("recommended_upselling_products", []),
                        "estimated_value": advanced_upselling_crossselling_prediction.get("predicted_upselling_value", 0),
                        "timing": advanced_upselling_crossselling_prediction.get("upselling_timing", "not_ready"),
                        "priority": 1
                    })
                
                # Oportunidad 2: Cross-selling
                if advanced_upselling_crossselling_prediction.get("crossselling_readiness_score", 0) >= 50:
                    expansion_opportunities.append({
                        "type": "cross_selling",
                        "readiness_score": advanced_upselling_crossselling_prediction.get("crossselling_readiness_score", 0),
                        "recommended_products": advanced_upselling_crossselling_prediction.get("recommended_crossselling_products", []),
                        "estimated_value": advanced_upselling_crossselling_prediction.get("predicted_crossselling_value", 0),
                        "timing": advanced_upselling_crossselling_prediction.get("crossselling_timing", "not_ready"),
                        "priority": 2
                    })
                
                # Oportunidad 3: Productos adicionales
                if ultra_intelligent_product_recommendations.get("recommendation_count", 0) >= 2:
                    expansion_opportunities.append({
                        "type": "additional_products",
                        "readiness_score": 60,
                        "recommended_products": [p["product"] for p in ultra_intelligent_product_recommendations.get("product_recommendations", [])[:3]],
                        "estimated_value": estimated_ltv * 0.15,
                        "timing": "within_90_days",
                        "priority": 3
                    })
                
                # Calcular valor total de expansi√≥n
                total_expansion_value = sum(opp.get("estimated_value", 0) for opp in expansion_opportunities)
                
                # Ordenar por prioridad y readiness
                expansion_opportunities.sort(key=lambda x: (x["priority"], -x["readiness_score"]))
                
                advanced_expansion_opportunities_analysis = {
                    "expansion_opportunities": expansion_opportunities,
                    "total_opportunities": len(expansion_opportunities),
                    "total_expansion_value": round(total_expansion_value, 2),
                    "expansion_potential": (
                        "very_high" if total_expansion_value >= estimated_ltv * 0.5 else
                        "high" if total_expansion_value >= estimated_ltv * 0.3 else
                        "medium" if total_expansion_value >= estimated_ltv * 0.15 else
                        "low"
                    ),
                    "primary_opportunity": expansion_opportunities[0] if expansion_opportunities else None,
                    "expansion_timeline": (
                        "immediate" if any(opp["timing"] == "immediate" for opp in expansion_opportunities) else
                        "short_term" if any("30_days" in str(opp.get("timing", "")) for opp in expansion_opportunities) else
                        "medium_term" if any("90_days" in str(opp.get("timing", "")) for opp in expansion_opportunities) else
                        "long_term"
                    ),
                    "expansion_recommendations": [
                        rec for rec in [
                            "focus_on_upselling" if any(opp["type"] == "upselling" for opp in expansion_opportunities) else None,
                            "explore_cross_selling" if any(opp["type"] == "cross_selling" for opp in expansion_opportunities) else None,
                            "introduce_additional_products" if any(opp["type"] == "additional_products" for opp in expansion_opportunities) else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "advanced_expansion_opportunities_analysis_completed",
                    email=email,
                    total_opportunities=len(expansion_opportunities),
                    total_expansion_value=total_expansion_value
                )
            except Exception as e:
                logger.warning("advanced_expansion_opportunities_analysis_error", email=email, error=str(e))
            

            # 30. AN√ÅLISIS DE TIMING √ìPTIMO ULTRA MEJORADO
            ultra_enhanced_optimal_timing_analysis = {}
            try:
                from datetime import datetime, timedelta
                
                # Factores de timing
                timing_factors = {
                    "hour_of_day": datetime.utcnow().hour,
                    "day_of_week": datetime.utcnow().weekday(),
                    "seasonal_factor": seasonal_contact_optimization.get("current_seasonal_score", 50) if seasonal_contact_optimization else 50,
                    "engagement_pattern": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 50,
                    "ml_intent": ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0,
                    "urgency_level": len(budget_signals) * 10 if budget_signals else 0
                }
                
                # Calcular score de timing √≥ptimo
                timing_score = 0
                
                # Factor hora del d√≠a (9-17 es mejor)
                if 9 <= timing_factors["hour_of_day"] <= 17:
                    timing_score += 25
                elif 8 <= timing_factors["hour_of_day"] <= 18:
                    timing_score += 15
                
                # Factor d√≠a de la semana (lunes-jueves es mejor)
                if timing_factors["day_of_week"] < 5:  # Lunes a viernes
                    timing_score += 20
                
                # Factor estacional
                timing_score += timing_factors["seasonal_factor"] * 0.2
                
                # Factor engagement
                timing_score += timing_factors["engagement_pattern"] * 0.15
                
                # Factor intenci√≥n ML
                timing_score += timing_factors["ml_intent"] * 0.15
                
                # Factor urgencia
                timing_score += min(timing_factors["urgency_level"], 25)
                
                timing_score = min(timing_score, 100)
                
                # Determinar ventana √≥ptima
                optimal_timing_window = {
                    "best_hours": [10, 11, 14, 15] if timing_score >= 70 else [9, 10, 15, 16],
                    "best_days": [0, 1, 2, 3] if timing_score >= 70 else [0, 1, 2, 3, 4],  # Lunes a jueves o lunes a viernes
                    "avoid_hours": [12, 13, 18, 19, 20] if timing_score >= 70 else [],
                    "avoid_days": [5, 6]  # S√°bado y domingo
                }
                
                # Calcular pr√≥xima ventana √≥ptima
                now = datetime.utcnow()
                next_optimal_time = None
                if 9 <= now.hour <= 17 and now.weekday() < 5:
                    # Si estamos en horario √≥ptimo, recomendar ahora
                    next_optimal_time = now.isoformat()
                else:
                    # Calcular pr√≥xima ventana
                    days_ahead = (0 if now.weekday() < 5 else (5 - now.weekday())) % 7
                    if days_ahead == 0 and now.hour >= 17:
                        days_ahead = 1
                    next_optimal = now + timedelta(days=days_ahead)
                    next_optimal = next_optimal.replace(hour=10, minute=0, second=0, microsecond=0)
                    next_optimal_time = next_optimal.isoformat()
                
                ultra_enhanced_optimal_timing_analysis = {
                    "timing_score": round(timing_score, 2),
                    "timing_factors": {k: round(v, 2) if isinstance(v, (int, float)) else v for k, v in timing_factors.items()},
                    "optimal_timing_window": optimal_timing_window,
                    "next_optimal_time": next_optimal_time,
                    "timing_quality": (
                        "optimal" if timing_score >= 80 else
                        "very_good" if timing_score >= 70 else
                        "good" if timing_score >= 60 else
                        "fair" if timing_score >= 50 else
                        "poor"
                    ),
                    "timing_recommendations": [
                        rec for rec in [
                            "contact_immediately" if timing_score >= 80 else None,
                            "contact_today" if timing_score >= 70 and timing_factors["day_of_week"] < 5 else None,
                            "wait_for_optimal_window" if timing_score < 60 else None,
                            "avoid_weekend" if timing_factors["day_of_week"] >= 5 else None,
                            "leverage_seasonal_timing" if timing_factors["seasonal_factor"] >= 70 else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "ultra_enhanced_optimal_timing_analysis_completed",
                    email=email,
                    timing_score=timing_score,
                    timing_quality=ultra_enhanced_optimal_timing_analysis.get("timing_quality")
                )
            except Exception as e:
                logger.warning("ultra_enhanced_optimal_timing_analysis_error", email=email, error=str(e))
            
            # 31. SISTEMA DE SCORING DE FIT DE EQUIPO DE VENTAS
            sales_team_fit_scoring = {}
            try:
                # Factores de fit con equipo de ventas
                sales_team_fit_factors = {
                    "industry_expertise": (
                        25 if industry in ["technology", "finance", "healthcare"] else
                        20 if industry in ["retail", "manufacturing"] else
                        15
                    ),
                    "company_size_match": (
                        20 if company_size in ["enterprise", "large"] else
                        18 if company_size == "medium" else
                        15
                    ),
                    "deal_size_match": (
                        20 if estimated_ltv >= 50000 else
                        18 if estimated_ltv >= 20000 else
                        15 if estimated_ltv >= 5000 else
                        10
                    ),
                    "geographic_alignment": 15,  # Base
                    "communication_style_match": (
                        20 if advanced_communication_pattern_analysis.get("dominant_style") in ["formal", "business"] else
                        15 if advanced_communication_pattern_analysis.get("dominant_style") == "technical" else
                        10
                    ) if advanced_communication_pattern_analysis else 10,
                    "urgency_match": (
                        15 if len(budget_signals) >= 2 else
                        10 if len(budget_signals) >= 1 else
                        5
                    )
                }
                
                total_sales_team_fit_score = sum(sales_team_fit_factors.values())
                
                # Determinar tipo de rep recomendado
                recommended_rep_type = (
                    "enterprise_specialist" if company_size in ["enterprise", "large"] and estimated_ltv >= 50000 else
                    "mid_market_specialist" if company_size == "medium" and estimated_ltv >= 20000 else
                    "smb_specialist" if company_size in ["small", "startup"] else
                    "technical_specialist" if advanced_content_analysis.get("content_type") == "technical_inquiry" else
                    "generalist"
                )
                
                sales_team_fit_scoring = {
                    "total_sales_team_fit_score": round(total_sales_team_fit_score, 2),
                    "sales_team_fit_factors": {k: round(v, 2) for k, v in sales_team_fit_factors.items()},
                    "recommended_rep_type": recommended_rep_type,
                    "sales_team_fit_level": (
                        "excellent" if total_sales_team_fit_score >= 90 else
                        "very_good" if total_sales_team_fit_score >= 80 else
                        "good" if total_sales_team_fit_score >= 70 else
                        "fair" if total_sales_team_fit_score >= 60 else
                        "poor"
                    ),
                    "sales_team_fit_strengths": [
                        k for k, v in sales_team_fit_factors.items() if v >= 18
                    ],
                    "sales_team_fit_weaknesses": [
                        k for k, v in sales_team_fit_factors.items() if v < 12
                    ],
                    "sales_team_recommendations": [
                        rec for rec in [
                            "assign_enterprise_rep" if recommended_rep_type == "enterprise_specialist" else None,
                            "assign_technical_rep" if recommended_rep_type == "technical_specialist" else None,
                            "assign_industry_specialist" if sales_team_fit_factors["industry_expertise"] >= 25 else None,
                            "provide_sales_support" if total_sales_team_fit_score < 70 else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "sales_team_fit_scoring_completed",
                    email=email,
                    total_sales_team_fit_score=total_sales_team_fit_score,
                    recommended_rep_type=recommended_rep_type
                )
            except Exception as e:
                logger.warning("sales_team_fit_scoring_error", email=email, error=str(e))
            
            # 32. AN√ÅLISIS DE PREDICCI√ìN DE CICLO DE VENTA ULTRA MEJORADO
            ultra_enhanced_sales_cycle_prediction = {}
            try:
                # Factores que afectan la duraci√≥n del ciclo
                cycle_duration_factors = {
                    "ml_intent_score": ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "product_fit": product_fit_score,
                    "competitive_risk": deep_competitive_analysis.get("competitive_risk_score", 0) if deep_competitive_analysis else 0,
                    "company_size_factor": (
                        -10 if company_size in ["startup", "small"] else
                        0 if company_size == "medium" else
                        15
                    ),
                    "urgency_factor": (
                        -20 if len(budget_signals) >= 3 else
                        -15 if len(budget_signals) >= 2 else
                        -10 if len(budget_signals) >= 1 else
                        0
                    )
                }
                
                # Duraci√≥n base del ciclo (d√≠as)
                base_cycle_duration = 90
                
                # Ajustes basados en factores
                intent_adjustment = -20 if cycle_duration_factors["ml_intent_score"] >= 70 else -10 if cycle_duration_factors["ml_intent_score"] >= 50 else 0
                engagement_adjustment = -15 if cycle_duration_factors["engagement_level"] >= 70 else -5 if cycle_duration_factors["engagement_level"] >= 50 else 0
                product_fit_adjustment = -10 if cycle_duration_factors["product_fit"] >= 70 else 0
                competitive_adjustment = 10 if cycle_duration_factors["competitive_risk"] >= 50 else 0
                company_size_adjustment = cycle_duration_factors["company_size_factor"]
                urgency_adjustment = cycle_duration_factors["urgency_factor"]
                
                predicted_cycle_duration = max(14, base_cycle_duration + 
                                             intent_adjustment + engagement_adjustment + 
                                             product_fit_adjustment + competitive_adjustment + 
                                             company_size_adjustment + urgency_adjustment)
                
                # Calcular probabilidad de cierre por etapa
                current_stage = optimized_sales_cycle_analysis.get("current_stage", "awareness") if optimized_sales_cycle_analysis else "awareness"
                stage_closing_probabilities = {
                    "awareness": 0.05,
                    "interest": 0.15,
                    "consideration": 0.35,
                    "evaluation": 0.65,
                    "closing": 0.90
                }
                
                current_closing_probability = stage_closing_probabilities.get(current_stage, 0.05)
                
                # Ajustar probabilidad basada en factores
                probability_adjustment = (
                    (cycle_duration_factors["ml_intent_score"] / 100) * 0.2 +
                    (cycle_duration_factors["engagement_level"] / 100) * 0.15 +
                    (cycle_duration_factors["product_fit"] / 100) * 0.1 -
                    (cycle_duration_factors["competitive_risk"] / 100) * 0.15
                )
                
                adjusted_closing_probability = min(0.95, max(0.05, current_closing_probability + probability_adjustment))
                
                ultra_enhanced_sales_cycle_prediction = {
                    "predicted_cycle_duration_days": int(predicted_cycle_duration),
                    "cycle_duration_factors": {k: round(v, 2) for k, v in cycle_duration_factors.items()},
                    "current_stage": current_stage,
                    "current_closing_probability": round(adjusted_closing_probability, 3),
                    "stage_closing_probabilities": stage_closing_probabilities,
                    "cycle_velocity": (
                        "fast" if predicted_cycle_duration <= 45 else
                        "normal" if predicted_cycle_duration <= 90 else
                        "slow"
                    ),
                    "cycle_optimization_opportunities": [
                        opp for opp in [
                            "accelerate_with_high_intent" if cycle_duration_factors["ml_intent_score"] >= 70 else None,
                            "increase_engagement" if cycle_duration_factors["engagement_level"] < 50 else None,
                            "improve_product_fit" if cycle_duration_factors["product_fit"] < 60 else None,
                            "address_competitive_risk" if cycle_duration_factors["competitive_risk"] >= 50 else None,
                            "leverage_urgency" if cycle_duration_factors["urgency_factor"] < 0 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "ultra_enhanced_sales_cycle_prediction_completed",
                    email=email,
                    predicted_cycle_duration=predicted_cycle_duration,
                    current_closing_probability=adjusted_closing_probability
                )
            except Exception as e:
                logger.warning("ultra_enhanced_sales_cycle_prediction_error", email=email, error=str(e))
            
            # 33. SISTEMA DE RECOMENDACIONES DE CONTENIDO POR ETAPA MEJORADO
            enhanced_stage_based_content_recommendations = {}
            try:
                current_stage = optimized_sales_cycle_analysis.get("current_stage", "awareness") if optimized_sales_cycle_analysis else "awareness"
                
                # Contenido detallado por etapa
                stage_content_map = {
                    "awareness": {
                        "primary_content": [
                            {"type": "blog_post", "topic": "industry_challenges", "priority": 1},
                            {"type": "infographic", "topic": "problem_statistics", "priority": 2},
                            {"type": "ebook", "topic": "problem_awareness", "priority": 3}
                        ],
                        "secondary_content": [
                            {"type": "video", "topic": "industry_overview", "priority": 4},
                            {"type": "webinar", "topic": "trends_analysis", "priority": 5}
                        ],
                        "content_goals": ["educate", "create_awareness", "identify_problems"]
                    },
                    "interest": {
                        "primary_content": [
                            {"type": "case_study", "topic": "success_stories", "priority": 1},
                            {"type": "demo_video", "topic": "product_overview", "priority": 2},
                            {"type": "webinar", "topic": "solution_introduction", "priority": 3}
                        ],
                        "secondary_content": [
                            {"type": "whitepaper", "topic": "solution_benefits", "priority": 4},
                            {"type": "comparison", "topic": "vs_alternatives", "priority": 5}
                        ],
                        "content_goals": ["build_interest", "show_value", "differentiate"]
                    },
                    "consideration": {
                        "primary_content": [
                            {"type": "comparison_guide", "topic": "vs_competitors", "priority": 1},
                            {"type": "pricing_page", "topic": "plans_features", "priority": 2},
                            {"type": "testimonial", "topic": "customer_reviews", "priority": 3}
                        ],
                        "secondary_content": [
                            {"type": "roi_calculator", "topic": "value_calculation", "priority": 4},
                            {"type": "faq", "topic": "common_questions", "priority": 5}
                        ],
                        "content_goals": ["facilitate_decision", "address_concerns", "show_roi"]
                    },
                    "evaluation": {
                        "primary_content": [
                            {"type": "trial_offer", "topic": "free_trial", "priority": 1},
                            {"type": "implementation_guide", "topic": "getting_started", "priority": 2},
                            {"type": "security_doc", "topic": "compliance_security", "priority": 3}
                        ],
                        "secondary_content": [
                            {"type": "integration_guide", "topic": "technical_setup", "priority": 4},
                            {"type": "support_resources", "topic": "help_center", "priority": 5}
                        ],
                        "content_goals": ["enable_trial", "reduce_friction", "build_confidence"]
                    },
                    "closing": {
                        "primary_content": [
                            {"type": "contract_template", "topic": "next_steps", "priority": 1},
                            {"type": "onboarding_guide", "topic": "setup_process", "priority": 2},
                            {"type": "success_kit", "topic": "resources", "priority": 3}
                        ],
                        "secondary_content": [
                            {"type": "payment_info", "topic": "billing_setup", "priority": 4},
                            {"type": "welcome_package", "topic": "getting_started", "priority": 5}
                        ],
                        "content_goals": ["finalize_deal", "smooth_onboarding", "set_expectations"]
                    }
                }
                
                stage_content = stage_content_map.get(current_stage, stage_content_map["awareness"])
                
                # Personalizar contenido basado en perfil
                personalized_content = stage_content["primary_content"].copy()
                
                # Ajustar por tipo de contenido preferido
                if advanced_content_analysis.get("content_type") == "technical_inquiry":
                    personalized_content.insert(0, {"type": "technical_documentation", "topic": "api_integration", "priority": 0})
                
                if advanced_content_analysis.get("content_type") == "budget_inquiry":
                    personalized_content.insert(0, {"type": "pricing_calculator", "topic": "cost_analysis", "priority": 0})
                
                enhanced_stage_based_content_recommendations = {
                    "current_stage": current_stage,
                    "recommended_content": personalized_content[:5],  # Top 5
                    "secondary_content": stage_content["secondary_content"][:3],  # Top 3
                    "content_goals": stage_content["content_goals"],
                    "content_count": len(personalized_content),
                    "content_personalization": {
                        "based_on_stage": True,
                        "based_on_content_preference": advanced_content_analysis.get("content_type") != "general_inquiry" if advanced_content_analysis else False,
                        "based_on_engagement": advanced_engagement_analysis.get("advanced_engagement_score", 0) >= 50 if advanced_engagement_analysis else False
                    },
                    "content_effectiveness_score": round(
                        (ml_purchase_intent_scoring.get("ml_intent_score", 0) * 0.4 +
                         advanced_engagement_analysis.get("advanced_engagement_score", 0) * 0.3 +
                         product_fit_score * 0.3) if ml_purchase_intent_scoring and advanced_engagement_analysis else 50, 2
                    )
                }
                
                logger.info(
                    "enhanced_stage_based_content_recommendations_completed",
                    email=email,
                    current_stage=current_stage,
                    content_count=len(personalized_content)
                )
            except Exception as e:
                logger.warning("enhanced_stage_based_content_recommendations_error", email=email, error=str(e))
            

            # 34. AN√ÅLISIS DE PREDICCI√ìN DE ABANDONO TEMPRANO ULTRA MEJORADO
            ultra_enhanced_early_abandonment_prediction = {}
            try:
                # Factores de riesgo de abandono
                abandonment_risk_factors = {
                    "low_engagement": max(0, 60 - (advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0)),
                    "low_intent": max(0, 60 - (ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0)),
                    "competitive_risk": deep_competitive_analysis.get("competitive_risk_score", 0) if deep_competitive_analysis else 0,
                    "poor_product_fit": max(0, 60 - product_fit_score),
                    "negative_sentiment": max(0, -sentiment_analysis.get("sentiment_score", 0)) if sentiment_analysis and sentiment_analysis.get("sentiment_score", 0) < 0 else 0,
                    "low_quality_data": max(0, 60 - (enhanced_data_quality_scoring.get("total_quality_score", 0) if enhanced_data_quality_scoring else 0)),
                    "timing_mismatch": max(0, 50 - (ultra_enhanced_optimal_timing_analysis.get("timing_score", 50) if ultra_enhanced_optimal_timing_analysis else 50)),
                    "cultural_mismatch": max(0, 60 - (advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0)),
                    "poor_communication_fit": max(0, 50 - (advanced_communication_pattern_analysis.get("communication_complexity", "low") == "high" and 30 or 20) if advanced_communication_pattern_analysis else 0)
                }
                
                total_abandonment_risk = sum(abandonment_risk_factors.values()) / len(abandonment_risk_factors)
                
                abandonment_risk_level = (
                    "critical" if total_abandonment_risk >= 70 else
                    "high" if total_abandonment_risk >= 50 else
                    "medium" if total_abandonment_risk >= 30 else
                    "low" if total_abandonment_risk >= 15 else
                    "minimal"
                )
                
                # Calcular probabilidad de abandono
                abandonment_probability = min(95, total_abandonment_risk * 1.2)
                
                # Tiempo estimado hasta abandono
                estimated_days_to_abandonment = (
                    5 if abandonment_risk_level == "critical" else
                    10 if abandonment_risk_level == "high" else
                    20 if abandonment_risk_level == "medium" else
                    45 if abandonment_risk_level == "low" else
                    90
                )
                
                # Estrategias de intervenci√≥n
                intervention_strategies = []
                if abandonment_risk_factors["low_engagement"] >= 30:
                    intervention_strategies.append({
                        "strategy": "increase_engagement_immediately",
                        "priority": "high",
                        "actions": ["personalized_outreach", "relevant_content", "quick_response"]
                    })
                if abandonment_risk_factors["low_intent"] >= 30:
                    intervention_strategies.append({
                        "strategy": "nurture_with_educational_content",
                        "priority": "medium",
                        "actions": ["educational_emails", "case_studies", "webinars"]
                    })
                if abandonment_risk_factors["competitive_risk"] >= 40:
                    intervention_strategies.append({
                        "strategy": "competitive_differentiation",
                        "priority": "high",
                        "actions": ["competitive_comparison", "unique_value_prop", "testimonial_focus"]
                    })
                
                ultra_enhanced_early_abandonment_prediction = {
                    "total_abandonment_risk": round(total_abandonment_risk, 2),
                    "abandonment_risk_level": abandonment_risk_level,
                    "abandonment_risk_factors": {k: round(v, 2) for k, v in abandonment_risk_factors.items()},
                    "abandonment_probability": round(abandonment_probability, 2),
                    "estimated_days_to_abandonment": estimated_days_to_abandonment,
                    "intervention_strategies": intervention_strategies,
                    "intervention_urgency": (
                        "immediate" if abandonment_risk_level in ["critical", "high"] else
                        "within_24_hours" if abandonment_risk_level == "medium" else
                        "within_week" if abandonment_risk_level == "low" else
                        "monitor"
                    ),
                    "retention_probability": round(100 - abandonment_probability, 2),
                    "critical_intervention_actions": [
                        action for strategy in intervention_strategies 
                        if strategy["priority"] == "high"
                        for action in strategy["actions"]
                    ]
                }
                
                logger.info(
                    "ultra_enhanced_early_abandonment_prediction_completed",
                    email=email,
                    total_abandonment_risk=total_abandonment_risk,
                    abandonment_risk_level=abandonment_risk_level,
                    estimated_days_to_abandonment=estimated_days_to_abandonment
                )
            except Exception as e:
                logger.warning("ultra_enhanced_early_abandonment_prediction_error", email=email, error=str(e))
            
            # 35. SISTEMA DE SCORING DE FIT DE PRODUCTO ULTRA MEJORADO
            ultra_enhanced_product_fit_scoring = {}
            try:
                # Factores de fit de producto
                product_fit_factors = {
                    "industry_alignment": (
                        25 if industry in ["technology", "finance", "healthcare"] else
                        20 if industry in ["retail", "manufacturing"] else
                        15
                    ),
                    "company_size_fit": (
                        25 if company_size in ["enterprise", "large"] else
                        20 if company_size == "medium" else
                        15
                    ),
                    "feature_requirements": min(product_fit_score / 2, 25),
                    "use_case_match": (
                        20 if advanced_content_analysis.get("content_type") in ["technical_inquiry", "partnership_inquiry"] else
                        15 if advanced_content_analysis.get("content_type") == "budget_inquiry" else
                        10
                    ) if advanced_content_analysis else 10,
                    "technical_capability": (
                        15 if advanced_technical_integration_fit_analysis.get("total_technical_fit_score", 0) >= 60 else
                        10 if advanced_technical_integration_fit_analysis.get("total_technical_fit_score", 0) >= 40 else
                        5
                    ) if advanced_technical_integration_fit_analysis else 5,
                    "budget_alignment": (
                        15 if len(budget_signals) >= 2 else
                        10 if len(budget_signals) >= 1 else
                        5
                    )
                }
                
                total_product_fit_score = sum(product_fit_factors.values())
                
                # Clasificar fit de producto
                product_fit_level = (
                    "excellent" if total_product_fit_score >= 90 else
                    "very_good" if total_product_fit_score >= 80 else
                    "good" if total_product_fit_score >= 70 else
                    "fair" if total_product_fit_score >= 60 else
                    "poor"
                )
                
                # Identificar gaps de fit
                fit_gaps = [
                    gap for gap in [
                        "industry_mismatch" if product_fit_factors["industry_alignment"] < 15 else None,
                        "size_mismatch" if product_fit_factors["company_size_fit"] < 15 else None,
                        "feature_gaps" if product_fit_factors["feature_requirements"] < 15 else None,
                        "use_case_mismatch" if product_fit_factors["use_case_match"] < 10 else None,
                        "technical_limitations" if product_fit_factors["technical_capability"] < 10 else None,
                        "budget_constraints" if product_fit_factors["budget_alignment"] < 10 else None
                    ] if gap
                ]
                
                ultra_enhanced_product_fit_scoring = {
                    "total_product_fit_score": round(total_product_fit_score, 2),
                    "product_fit_level": product_fit_level,
                    "product_fit_factors": {k: round(v, 2) for k, v in product_fit_factors.items()},
                    "fit_gaps": fit_gaps,
                    "product_fit_strengths": [
                        k for k, v in product_fit_factors.items() if v >= 20
                    ],
                    "product_fit_weaknesses": [
                        k for k, v in product_fit_factors.items() if v < 12
                    ],
                    "product_fit_confidence": (
                        "high" if total_product_fit_score >= 80 else
                        "medium" if total_product_fit_score >= 60 else
                        "low"
                    ),
                    "product_fit_recommendations": [
                        rec for rec in [
                            "highlight_industry_specific_features" if product_fit_factors["industry_alignment"] >= 20 else None,
                            "emphasize_size_appropriate_solution" if product_fit_factors["company_size_fit"] >= 20 else None,
                            "address_feature_gaps" if product_fit_factors["feature_requirements"] < 15 else None,
                            "showcase_relevant_use_cases" if product_fit_factors["use_case_match"] < 15 else None,
                            "provide_technical_support" if product_fit_factors["technical_capability"] < 12 else None,
                            "offer_flexible_pricing" if product_fit_factors["budget_alignment"] < 12 else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "ultra_enhanced_product_fit_scoring_completed",
                    email=email,
                    total_product_fit_score=total_product_fit_score,
                    product_fit_level=product_fit_level
                )
            except Exception as e:
                logger.warning("ultra_enhanced_product_fit_scoring_error", email=email, error=str(e))
            
            # 36. AN√ÅLISIS DE RECOMENDACIONES DE OFERTAS PERSONALIZADAS
            personalized_offer_recommendations = {}
            try:
                # Determinar tipo de oferta basado en perfil
                offer_factors = {
                    "ml_intent_score": ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "product_fit": product_fit_score,
                    "budget_signals": len(budget_signals),
                    "competitive_risk": deep_competitive_analysis.get("competitive_risk_score", 0) if deep_competitive_analysis else 0,
                    "urgency_level": len([s for s in budget_signals if "urgent" in str(s).lower()]) if budget_signals else 0
                }
                
                # Generar recomendaciones de ofertas
                offer_recommendations = []
                
                # Oferta 1: Basada en intenci√≥n
                if offer_factors["ml_intent_score"] >= 70:
                    offer_recommendations.append({
                        "type": "high_intent_discount",
                        "discount_percentage": 15,
                        "description": "Descuento por alta intenci√≥n de compra",
                        "priority": 1,
                        "validity_days": 7
                    })
                elif offer_factors["ml_intent_score"] >= 50:
                    offer_recommendations.append({
                        "type": "standard_discount",
                        "discount_percentage": 10,
                        "description": "Descuento est√°ndar",
                        "priority": 2,
                        "validity_days": 14
                    })
                
                # Oferta 2: Basada en urgencia
                if offer_factors["urgency_level"] >= 2:
                    offer_recommendations.append({
                        "type": "urgent_offer",
                        "discount_percentage": 20,
                        "description": "Oferta especial por urgencia",
                        "priority": 1,
                        "validity_days": 3
                    })
                
                # Oferta 3: Basada en competitive risk
                if offer_factors["competitive_risk"] >= 60:
                    offer_recommendations.append({
                        "type": "competitive_match",
                        "discount_percentage": 25,
                        "description": "Oferta competitiva",
                        "priority": 1,
                        "validity_days": 5
                    })
                
                # Oferta 4: Basada en engagement
                if offer_factors["engagement_level"] >= 70:
                    offer_recommendations.append({
                        "type": "loyalty_reward",
                        "discount_percentage": 12,
                        "description": "Recompensa por engagement",
                        "priority": 2,
                        "validity_days": 10
                    })
                
                # Ordenar por prioridad
                offer_recommendations.sort(key=lambda x: x["priority"])
                
                # Oferta principal
                primary_offer = offer_recommendations[0] if offer_recommendations else None
                
                personalized_offer_recommendations = {
                    "offer_recommendations": offer_recommendations[:3],  # Top 3
                    "primary_offer": primary_offer,
                    "total_offers": len(offer_recommendations),
                    "offer_factors": {k: round(v, 2) for k, v in offer_factors.items()},
                    "recommended_offer_strategy": (
                        "aggressive" if primary_offer and primary_offer["discount_percentage"] >= 20 else
                        "competitive" if primary_offer and primary_offer["discount_percentage"] >= 15 else
                        "standard" if primary_offer and primary_offer["discount_percentage"] >= 10 else
                        "conservative"
                    ),
                    "offer_effectiveness_score": round(
                        (offer_factors["ml_intent_score"] * 0.3 +
                         offer_factors["engagement_level"] * 0.25 +
                         offer_factors["product_fit"] * 0.2 +
                         (offer_factors["budget_signals"] * 10) * 0.15 +
                         (100 - offer_factors["competitive_risk"]) * 0.1), 2
                    ) if offer_factors["ml_intent_score"] > 0 else 0
                }
                
                logger.info(
                    "personalized_offer_recommendations_completed",
                    email=email,
                    primary_offer_type=primary_offer.get("type") if primary_offer else "none",
                    total_offers=len(offer_recommendations)
                )
            except Exception as e:
                logger.warning("personalized_offer_recommendations_error", email=email, error=str(e))
            
            # 37. SISTEMA DE SCORING DE CALIDAD DE LEAD ULTRA MEJORADO
            ultra_enhanced_lead_quality_scoring = {}
            try:
                # Factores de calidad de lead
                lead_quality_factors = {
                    "data_completeness": enhanced_data_quality_scoring.get("quality_factors", {}).get("completeness", 0) if enhanced_data_quality_scoring else 50,
                    "data_validity": enhanced_data_quality_scoring.get("quality_factors", {}).get("validity", 0) if enhanced_data_quality_scoring else 50,
                    "engagement_quality": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "intent_quality": ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0,
                    "fit_quality": (product_fit_score + (advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0)) / 2,
                    "behavioral_quality": advanced_predictive_behavior_patterns.get("behavior_consistency_score", 0) if advanced_predictive_behavior_patterns else 50
                }
                
                # Calcular score total de calidad
                total_lead_quality_score = sum(lead_quality_factors.values()) / len(lead_quality_factors)
                
                # Clasificar calidad de lead
                lead_quality_level = (
                    "excellent" if total_lead_quality_score >= 85 else
                    "very_good" if total_lead_quality_score >= 75 else
                    "good" if total_lead_quality_score >= 65 else
                    "fair" if total_lead_quality_score >= 50 else
                    "poor"
                )
                
                # Calcular score de calificaci√≥n (MQL/SQL)
                qualification_score = (
                    total_lead_quality_score * 0.4 +
                    conversion_probability * 0.3 +
                    (ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0) * 0.3
                )
                
                is_sql = qualification_score >= 70
                is_mql = qualification_score >= 50 and not is_sql
                
                ultra_enhanced_lead_quality_scoring = {
                    "total_lead_quality_score": round(total_lead_quality_score, 2),
                    "lead_quality_level": lead_quality_level,
                    "lead_quality_factors": {k: round(v, 2) for k, v in lead_quality_factors.items()},
                    "qualification_score": round(qualification_score, 2),
                    "is_sql": is_sql,
                    "is_mql": is_mql,
                    "qualification_status": (
                        "sql" if is_sql else
                        "mql" if is_mql else
                        "unqualified"
                    ),
                    "quality_strengths": [
                        k for k, v in lead_quality_factors.items() if v >= 70
                    ],
                    "quality_weaknesses": [
                        k for k, v in lead_quality_factors.items() if v < 50
                    ],
                    "quality_improvement_opportunities": [
                        opp for opp in [
                            "improve_data_completeness" if lead_quality_factors["data_completeness"] < 70 else None,
                            "enhance_engagement" if lead_quality_factors["engagement_quality"] < 60 else None,
                            "increase_intent" if lead_quality_factors["intent_quality"] < 60 else None,
                            "improve_fit" if lead_quality_factors["fit_quality"] < 60 else None,
                            "enhance_behavioral_consistency" if lead_quality_factors["behavioral_quality"] < 60 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "ultra_enhanced_lead_quality_scoring_completed",
                    email=email,
                    total_lead_quality_score=total_lead_quality_score,
                    qualification_status=ultra_enhanced_lead_quality_scoring.get("qualification_status")
                )
            except Exception as e:
                logger.warning("ultra_enhanced_lead_quality_scoring_error", email=email, error=str(e))
            

            # 38. AN√ÅLISIS DE PREDICCI√ìN DE VALOR DE CUENTA MEJORADO
            enhanced_account_value_prediction = {}
            try:
                # Factores que afectan el valor de cuenta
                value_factors = {
                    "base_ltv": estimated_ltv,
                    "expansion_potential": advanced_expansion_opportunities_analysis.get("total_expansion_value", 0) if advanced_expansion_opportunities_analysis else 0,
                    "retention_likelihood": advanced_retention_churn_analysis.get("retention_score", 0) if advanced_retention_churn_analysis else 50,
                    "upselling_potential": advanced_upselling_crossselling_prediction.get("predicted_upselling_value", 0) if advanced_upselling_crossselling_prediction else 0,
                    "crossselling_potential": advanced_upselling_crossselling_prediction.get("predicted_crossselling_value", 0) if advanced_upselling_crossselling_prediction else 0,
                    "referral_potential": advanced_social_influence_analysis.get("estimated_reach", 0) * 0.1 if advanced_social_influence_analysis else 0
                }
                
                # Calcular valor total de cuenta
                total_account_value = (
                    value_factors["base_ltv"] +
                    value_factors["expansion_potential"] +
                    (value_factors["base_ltv"] * (value_factors["retention_likelihood"] / 100)) +
                    value_factors["upselling_potential"] +
                    value_factors["crossselling_potential"] +
                    value_factors["referral_potential"]
                )
                
                # Proyecci√≥n de valor a diferentes horizontes
                account_value_projection = {
                    "1_year": round(total_account_value * 0.4, 2),
                    "2_years": round(total_account_value * 0.7, 2),
                    "3_years": round(total_account_value, 2),
                    "5_years": round(total_account_value * 1.5, 2)
                }
                
                # Clasificar valor de cuenta
                account_value_category = (
                    "enterprise" if total_account_value >= 100000 else
                    "large" if total_account_value >= 50000 else
                    "medium" if total_account_value >= 20000 else
                    "small" if total_account_value >= 5000 else
                    "micro"
                )
                
                enhanced_account_value_prediction = {
                    "total_account_value": round(total_account_value, 2),
                    "account_value_category": account_value_category,
                    "value_factors": {k: round(v, 2) for k, v in value_factors.items()},
                    "account_value_projection": account_value_projection,
                    "value_growth_potential": (
                        "very_high" if value_factors["expansion_potential"] >= total_account_value * 0.5 else
                        "high" if value_factors["expansion_potential"] >= total_account_value * 0.3 else
                        "medium" if value_factors["expansion_potential"] >= total_account_value * 0.15 else
                        "low"
                    ),
                    "value_confidence": (
                        "high" if value_factors["retention_likelihood"] >= 70 and value_factors["base_ltv"] >= 20000 else
                        "medium" if value_factors["retention_likelihood"] >= 50 and value_factors["base_ltv"] >= 10000 else
                        "low"
                    ),
                    "value_optimization_opportunities": [
                        opp for opp in [
                            "focus_on_expansion" if value_factors["expansion_potential"] >= total_account_value * 0.3 else None,
                            "improve_retention" if value_factors["retention_likelihood"] < 70 else None,
                            "pursue_upselling" if value_factors["upselling_potential"] >= total_account_value * 0.2 else None,
                            "explore_crossselling" if value_factors["crossselling_potential"] >= total_account_value * 0.15 else None,
                            "leverage_referrals" if value_factors["referral_potential"] >= total_account_value * 0.1 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "enhanced_account_value_prediction_completed",
                    email=email,
                    total_account_value=total_account_value,
                    account_value_category=account_value_category
                )
            except Exception as e:
                logger.warning("enhanced_account_value_prediction_error", email=email, error=str(e))
            
            # 39. SISTEMA DE SCORING DE FIT DE CANAL ULTRA MEJORADO
            ultra_enhanced_channel_fit_scoring = {}
            try:
                # Scoring de fit por canal
                channel_fit_scores = {
                    "email": {
                        "base_score": 40,
                        "engagement_bonus": min(advanced_engagement_analysis.get("advanced_engagement_score", 0) / 5, 20) if advanced_engagement_analysis else 0,
                        "content_complexity_bonus": 15 if advanced_content_analysis.get("content_complexity") == "high" else 0,
                        "professionalism_bonus": 10 if advanced_content_analysis.get("professionalism_level") == "high" else 0,
                        "timezone_bonus": 5 if ultra_enhanced_optimal_timing_analysis else 0
                    },
                    "phone": {
                        "base_score": 30,
                        "urgency_bonus": 25 if any("urgent" in str(s).lower() for s in budget_signals) else 0,
                        "ml_intent_bonus": min(ml_purchase_intent_scoring.get("ml_intent_score", 0) / 4, 20) if ml_purchase_intent_scoring else 0,
                        "engagement_bonus": 10 if advanced_engagement_analysis.get("advanced_engagement_score", 0) >= 70 else 0,
                        "cultural_fit_bonus": 10 if advanced_cultural_fit_analysis.get("cultural_fit_score", 0) >= 70 else 0
                    },
                    "video_call": {
                        "base_score": 20,
                        "technical_bonus": 20 if advanced_content_analysis.get("content_type") == "technical_inquiry" else 0,
                        "complexity_bonus": 15 if advanced_content_analysis.get("content_complexity") == "high" else 0,
                        "enterprise_bonus": 15 if company_size in ["enterprise", "large"] else 0,
                        "product_fit_bonus": 10 if product_fit_score >= 70 else 0
                    },
                    "chat": {
                        "base_score": 25,
                        "immediacy_bonus": 20 if ml_purchase_intent_scoring.get("ml_intent_score", 0) >= 60 else 0,
                        "engagement_bonus": 15 if advanced_engagement_analysis.get("advanced_engagement_score", 0) >= 60 else 0,
                        "brief_communication_bonus": 10 if advanced_content_analysis.get("content_metrics", {}).get("word_count", 0) < 30 else 0
                    },
                    "in_person": {
                        "base_score": 10,
                        "enterprise_bonus": 20 if company_size in ["enterprise", "large"] else 0,
                        "high_value_bonus": 15 if estimated_ltv >= 50000 else 0,
                        "cultural_fit_bonus": 15 if advanced_cultural_fit_analysis.get("cultural_fit_score", 0) >= 80 else 0,
                        "influence_bonus": 10 if advanced_social_influence_analysis.get("influence_level") in ["very_high", "high"] else 0
                    },
                    "social_media": {
                        "base_score": 15,
                        "social_presence_bonus": 20 if advanced_social_influence_analysis.get("influence_level") in ["very_high", "high"] else 0,
                        "engagement_bonus": 15 if advanced_engagement_analysis.get("advanced_engagement_score", 0) >= 60 else 0,
                        "content_share_bonus": 10 if advanced_content_analysis.get("content_type") in ["technical_inquiry", "partnership_inquiry"] else 0
                    }
                }
                
                # Calcular scores totales
                channel_total_scores = {
                    channel: sum(factors.values())
                    for channel, factors in channel_fit_scores.items()
                }
                
                # Determinar canal √≥ptimo
                optimal_channel = max(channel_total_scores.items(), key=lambda x: x[1])[0]
                optimal_channel_score = channel_total_scores[optimal_channel]
                
                # Calcular confianza
                confidence = (
                    "high" if optimal_channel_score >= 70 and (optimal_channel_score - max(v for k, v in channel_total_scores.items() if k != optimal_channel)) >= 15 else
                    "medium" if optimal_channel_score >= 50 and (optimal_channel_score - max(v for k, v in channel_total_scores.items() if k != optimal_channel)) >= 10 else
                    "low"
                )
                
                ultra_enhanced_channel_fit_scoring = {
                    "optimal_channel": optimal_channel,
                    "optimal_channel_score": round(optimal_channel_score, 2),
                    "channel_fit_scores": {k: round(v, 2) for k, v in channel_total_scores.items()},
                    "channel_fit_factors": channel_fit_scores,
                    "confidence": confidence,
                    "channel_fit_level": (
                        "excellent" if optimal_channel_score >= 80 else
                        "very_good" if optimal_channel_score >= 70 else
                        "good" if optimal_channel_score >= 60 else
                        "fair" if optimal_channel_score >= 50 else
                        "poor"
                    ),
                    "secondary_channels": sorted(
                        [(k, v) for k, v in channel_total_scores.items() if k != optimal_channel],
                        key=lambda x: x[1],
                        reverse=True
                    )[:2],
                    "channel_recommendations": [
                        rec for rec in [
                            f"use_{optimal_channel}_as_primary" if optimal_channel_score >= 70 else None,
                            "consider_multi_channel_approach" if optimal_channel_score < 60 else None,
                            "test_alternative_channels" if confidence == "low" else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "ultra_enhanced_channel_fit_scoring_completed",
                    email=email,
                    optimal_channel=optimal_channel,
                    optimal_channel_score=optimal_channel_score,
                    confidence=confidence
                )
            except Exception as e:
                logger.warning("ultra_enhanced_channel_fit_scoring_error", email=email, error=str(e))
            
            # 40. AN√ÅLISIS DE RECOMENDACIONES DE SECUENCIA DE CONTACTO MEJORADO
            enhanced_contact_sequence_recommendations = {}
            try:
                # Determinar secuencia √≥ptima de contacto
                optimal_channel = ultra_enhanced_channel_fit_scoring.get("optimal_channel", "email") if ultra_enhanced_channel_fit_scoring else "email"
                ml_intent = ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0
                priority_level = ultra_enhanced_priority_scoring.get("priority_level", "medium") if ultra_enhanced_priority_scoring else "medium"
                
                # Generar secuencia de contacto
                contact_sequence = []
                
                # Contacto 1: Inicial
                if priority_level == "critical":
                    contact_sequence.append({
                        "step": 1,
                        "channel": "phone",
                        "timing": "immediate",
                        "message_type": "urgent_outreach",
                        "priority": 1
                    })
                elif ml_intent >= 70:
                    contact_sequence.append({
                        "step": 1,
                        "channel": optimal_channel,
                        "timing": "within_1_hour",
                        "message_type": "high_intent_outreach",
                        "priority": 1
                    })
                else:
                    contact_sequence.append({
                        "step": 1,
                        "channel": optimal_channel,
                        "timing": "within_24_hours",
                        "message_type": "initial_outreach",
                        "priority": 1
                    })
                
                # Contacto 2: Follow-up
                if ml_intent >= 60:
                    contact_sequence.append({
                        "step": 2,
                        "channel": "email" if optimal_channel != "email" else "phone",
                        "timing": "within_48_hours",
                        "message_type": "follow_up",
                        "priority": 2
                    })
                else:
                    contact_sequence.append({
                        "step": 2,
                        "channel": "email",
                        "timing": "within_3_days",
                        "message_type": "nurturing",
                        "priority": 2
                    })
                
                # Contacto 3: Nurturing o cierre
                if ml_intent >= 70:
                    contact_sequence.append({
                        "step": 3,
                        "channel": "phone" if optimal_channel != "phone" else "video_call",
                        "timing": "within_1_week",
                        "message_type": "closing_outreach",
                        "priority": 1
                    })
                else:
                    contact_sequence.append({
                        "step": 3,
                        "channel": "email",
                        "timing": "within_1_week",
                        "message_type": "value_proposition",
                        "priority": 3
                    })
                
                enhanced_contact_sequence_recommendations = {
                    "contact_sequence": contact_sequence,
                    "sequence_length": len(contact_sequence),
                    "sequence_duration_days": (
                        7 if ml_intent >= 70 else
                        14 if ml_intent >= 50 else
                        21
                    ),
                    "primary_channel": optimal_channel,
                    "sequence_strategy": (
                        "aggressive" if priority_level == "critical" or ml_intent >= 70 else
                        "standard" if ml_intent >= 50 else
                        "nurturing"
                    ),
                    "sequence_goals": [
                        goal for goal in [
                            "immediate_engagement" if priority_level == "critical" else None,
                            "accelerate_conversion" if ml_intent >= 70 else None,
                            "build_relationship" if ml_intent < 50 else None,
                            "educate_lead" if ml_intent < 40 else None
                        ] if goal
                    ],
                    "expected_outcomes": {
                        "response_rate": (
                            0.40 if priority_level == "critical" else
                            0.30 if ml_intent >= 70 else
                            0.20 if ml_intent >= 50 else
                            0.10
                        ),
                        "conversion_probability": round(conversion_probability * 1.1, 2) if ml_intent >= 70 else conversion_probability
                    }
                }
                
                logger.info(
                    "enhanced_contact_sequence_recommendations_completed",
                    email=email,
                    sequence_length=len(contact_sequence),
                    primary_channel=optimal_channel,
                    sequence_strategy=enhanced_contact_sequence_recommendations.get("sequence_strategy")
                )
            except Exception as e:
                logger.warning("enhanced_contact_sequence_recommendations_error", email=email, error=str(e))
            
            # 41. SISTEMA DE SCORING DE PROBABILIDAD DE CIERRE ULTRA MEJORADO
            ultra_enhanced_closing_probability_scoring = {}
            try:
                # Factores que afectan la probabilidad de cierre
                closing_probability_factors = {
                    "ml_intent_score": ml_purchase_intent_scoring.get("ml_intent_score", 0) * 0.25 if ml_purchase_intent_scoring else 0,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) * 0.20 if advanced_engagement_analysis else 0,
                    "product_fit": product_fit_score * 0.15,
                    "budget_availability": len(budget_signals) * 8 if budget_signals else 0,
                    "urgency_signals": len([s for s in budget_signals if "urgent" in str(s).lower()]) * 10 if budget_signals else 0,
                    "competitive_risk": (100 - deep_competitive_analysis.get("competitive_risk_score", 0)) * 0.10 if deep_competitive_analysis else 50,
                    "timing_quality": ultra_enhanced_optimal_timing_analysis.get("timing_score", 50) * 0.10 if ultra_enhanced_optimal_timing_analysis else 50,
                    "satisfaction_level": predictive_customer_satisfaction_analysis.get("satisfaction_score", 50) * 0.10 if predictive_customer_satisfaction_analysis else 50
                }
                
                # Calcular probabilidad de cierre
                total_closing_probability = sum(closing_probability_factors.values()) / 100
                total_closing_probability = min(95, max(5, total_closing_probability))
                
                # Ajustar por etapa actual
                current_stage = optimized_sales_cycle_analysis.get("current_stage", "awareness") if optimized_sales_cycle_analysis else "awareness"
                stage_multipliers = {
                    "awareness": 0.3,
                    "interest": 0.5,
                    "consideration": 0.7,
                    "evaluation": 0.85,
                    "closing": 0.95
                }
                
                stage_adjusted_probability = total_closing_probability * stage_multipliers.get(current_stage, 0.3)
                
                # Clasificar probabilidad
                closing_probability_level = (
                    "very_high" if stage_adjusted_probability >= 80 else
                    "high" if stage_adjusted_probability >= 60 else
                    "medium" if stage_adjusted_probability >= 40 else
                    "low" if stage_adjusted_probability >= 20 else
                    "very_low"
                )
                
                ultra_enhanced_closing_probability_scoring = {
                    "total_closing_probability": round(total_closing_probability, 2),
                    "stage_adjusted_probability": round(stage_adjusted_probability, 2),
                    "current_stage": current_stage,
                    "closing_probability_level": closing_probability_level,
                    "closing_probability_factors": {k: round(v, 2) for k, v in closing_probability_factors.items()},
                    "closing_confidence": (
                        "high" if stage_adjusted_probability >= 70 and len([f for f, v in closing_probability_factors.items() if v >= 15]) >= 4 else
                        "medium" if stage_adjusted_probability >= 50 and len([f for f, v in closing_probability_factors.items() if v >= 10]) >= 3 else
                        "low"
                    ),
                    "closing_strengths": [
                        k for k, v in closing_probability_factors.items() if v >= 15
                    ],
                    "closing_weaknesses": [
                        k for k, v in closing_probability_factors.items() if v < 8
                    ],
                    "closing_optimization_opportunities": [
                        opp for opp in [
                            "increase_intent" if closing_probability_factors["ml_intent_score"] < 15 else None,
                            "improve_engagement" if closing_probability_factors["engagement_level"] < 12 else None,
                            "enhance_product_fit" if closing_probability_factors["product_fit"] < 10 else None,
                            "address_budget_concerns" if closing_probability_factors["budget_availability"] < 10 else None,
                            "reduce_competitive_risk" if closing_probability_factors["competitive_risk"] < 8 else None,
                            "optimize_timing" if closing_probability_factors["timing_quality"] < 8 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "ultra_enhanced_closing_probability_scoring_completed",
                    email=email,
                    total_closing_probability=total_closing_probability,
                    stage_adjusted_probability=stage_adjusted_probability,
                    closing_probability_level=closing_probability_level
                )
            except Exception as e:
                logger.warning("ultra_enhanced_closing_probability_scoring_error", email=email, error=str(e))
            

            # 42. AN√ÅLISIS DE PREDICCI√ìN DE RIESGO DE CHURN ULTRA MEJORADO
            ultra_enhanced_churn_risk_prediction = {}
            try:
                # Factores de riesgo de churn
                churn_risk_factors = {
                    "low_satisfaction": max(0, 60 - (predictive_customer_satisfaction_analysis.get("satisfaction_score", 0) if predictive_customer_satisfaction_analysis else 0)),
                    "poor_product_fit": max(0, 60 - product_fit_score),
                    "low_engagement": max(0, 60 - (advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0)),
                    "cultural_mismatch": max(0, 60 - (advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0)),
                    "competitive_pressure": deep_competitive_analysis.get("competitive_risk_score", 0) if deep_competitive_analysis else 0,
                    "poor_support": max(0, 50 - (enhanced_data_quality_scoring.get("quality_factors", {}).get("validity", 0) if enhanced_data_quality_scoring else 0)),
                    "low_retention_score": max(0, 60 - (advanced_retention_churn_analysis.get("retention_score", 0) if advanced_retention_churn_analysis else 0)),
                    "abandonment_signals": advanced_early_abandonment_analysis.get("total_abandonment_risk", 0) if advanced_early_abandonment_analysis else 0
                }
                
                total_churn_risk = sum(churn_risk_factors.values()) / len(churn_risk_factors)
                
                churn_risk_level = (
                    "critical" if total_churn_risk >= 70 else
                    "high" if total_churn_risk >= 50 else
                    "medium" if total_churn_risk >= 30 else
                    "low" if total_churn_risk >= 15 else
                    "minimal"
                )
                
                # Calcular probabilidad de churn
                churn_probability = min(95, total_churn_risk * 1.15)
                
                # Tiempo estimado hasta churn
                estimated_days_to_churn = (
                    14 if churn_risk_level == "critical" else
                    30 if churn_risk_level == "high" else
                    60 if churn_risk_level == "medium" else
                    90 if churn_risk_level == "low" else
                    180
                )
                
                # Estrategias de retenci√≥n
                retention_strategies = []
                if churn_risk_factors["low_satisfaction"] >= 30:
                    retention_strategies.append({
                        "strategy": "improve_satisfaction",
                        "priority": "critical",
                        "actions": ["personalized_outreach", "address_concerns", "value_reminder"]
                    })
                if churn_risk_factors["poor_product_fit"] >= 30:
                    retention_strategies.append({
                        "strategy": "reassess_product_fit",
                        "priority": "high",
                        "actions": ["product_demo", "use_case_alignment", "feature_highlight"]
                    })
                if churn_risk_factors["competitive_pressure"] >= 40:
                    retention_strategies.append({
                        "strategy": "competitive_differentiation",
                        "priority": "high",
                        "actions": ["competitive_comparison", "unique_value", "testimonial_focus"]
                    })
                
                ultra_enhanced_churn_risk_prediction = {
                    "total_churn_risk": round(total_churn_risk, 2),
                    "churn_risk_level": churn_risk_level,
                    "churn_risk_factors": {k: round(v, 2) for k, v in churn_risk_factors.items()},
                    "churn_probability": round(churn_probability, 2),
                    "estimated_days_to_churn": estimated_days_to_churn,
                    "retention_strategies": retention_strategies,
                    "retention_urgency": (
                        "immediate" if churn_risk_level in ["critical", "high"] else
                        "within_week" if churn_risk_level == "medium" else
                        "within_month" if churn_risk_level == "low" else
                        "monitor"
                    ),
                    "retention_probability": round(100 - churn_probability, 2),
                    "retention_value": round(estimated_ltv * (100 - churn_probability) / 100, 2),
                    "churn_cost": round(estimated_ltv - (estimated_ltv * (100 - churn_probability) / 100), 2)
                }
                
                logger.info(
                    "ultra_enhanced_churn_risk_prediction_completed",
                    email=email,
                    total_churn_risk=total_churn_risk,
                    churn_risk_level=churn_risk_level,
                    estimated_days_to_churn=estimated_days_to_churn
                )
            except Exception as e:
                logger.warning("ultra_enhanced_churn_risk_prediction_error", email=email, error=str(e))
            
            # 43. AN√ÅLISIS DE RECOMENDACIONES DE ESTRATEGIA DE VENTA ULTRA PERSONALIZADA
            ultra_personalized_sales_strategy_recommendations = {}
            try:
                # Determinar estrategia basada en m√∫ltiples factores
                strategy_factors = {
                    "buyer_persona": buyer_persona.get("type", "unknown") if buyer_persona else "unknown",
                    "ml_intent": ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "competitive_risk": deep_competitive_analysis.get("competitive_risk_score", 0) if deep_competitive_analysis else 0,
                    "product_fit": product_fit_score,
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0,
                    "priority_level": ultra_enhanced_priority_scoring.get("priority_level", "medium") if ultra_enhanced_priority_scoring else "medium"
                }
                
                # Determinar estrategia principal
                if strategy_factors["priority_level"] == "critical" or strategy_factors["ml_intent"] >= 80:
                    primary_strategy = "aggressive_closing"
                elif strategy_factors["competitive_risk"] >= 60:
                    primary_strategy = "competitive_defense"
                elif strategy_factors["ml_intent"] >= 60 and strategy_factors["engagement_level"] >= 60:
                    primary_strategy = "accelerated_nurturing"
                elif strategy_factors["product_fit"] >= 70 and strategy_factors["cultural_fit"] >= 70:
                    primary_strategy = "relationship_building"
                elif strategy_factors["ml_intent"] < 40:
                    primary_strategy = "educational_nurturing"
                else:
                    primary_strategy = "standard_approach"
                
                # T√°cticas espec√≠ficas
                strategy_tactics = []
                if primary_strategy == "aggressive_closing":
                    strategy_tactics = ["immediate_contact", "personalized_demo", "competitive_pricing", "executive_sponsor"]
                elif primary_strategy == "competitive_defense":
                    strategy_tactics = ["competitive_comparison", "unique_value_prop", "testimonial_focus", "risk_reversal"]
                elif primary_strategy == "accelerated_nurturing":
                    strategy_tactics = ["frequent_touchpoints", "value_content", "social_proof", "trial_offer"]
                elif primary_strategy == "relationship_building":
                    strategy_tactics = ["personalized_outreach", "industry_insights", "case_studies", "consultative_approach"]
                elif primary_strategy == "educational_nurturing":
                    strategy_tactics = ["educational_content", "webinars", "best_practices", "gradual_engagement"]
                else:
                    strategy_tactics = ["standard_outreach", "value_proposition", "follow_up_sequence"]
                
                # Mensaje recomendado
                if primary_strategy == "aggressive_closing":
                    recommended_message = f"Basado en tu alto inter√©s en [Producto], me gustar√≠a agendar una demo personalizada esta semana para mostrarte c√≥mo podemos ayudarte a [Beneficio principal]."
                elif primary_strategy == "competitive_defense":
                    recommended_message = f"Entiendo que est√°s evaluando opciones. [Producto] se diferencia por [Diferenciador √∫nico]. ¬øTe gustar√≠a ver una comparaci√≥n detallada?"
                elif primary_strategy == "relationship_building":
                    recommended_message = f"Hola {lead_data.get('first_name', '')}, gracias por tu inter√©s. Me gustar√≠a compartirte c√≥mo empresas similares en {industry} est√°n usando [Producto] para [Resultado espec√≠fico]."
                else:
                    recommended_message = f"Gracias por tu inter√©s en [Producto]. Me gustar√≠a enviarte informaci√≥n relevante sobre c√≥mo [Producto] puede ayudar a {lead_data.get('company', 'tu empresa')}."
                
                ultra_personalized_sales_strategy_recommendations = {
                    "primary_strategy": primary_strategy,
                    "strategy_factors": strategy_factors,
                    "strategy_tactics": strategy_tactics,
                    "recommended_message": recommended_message,
                    "strategy_priority": (
                        "critical" if strategy_factors["priority_level"] == "critical" else
                        "high" if strategy_factors["ml_intent"] >= 70 else
                        "medium" if strategy_factors["ml_intent"] >= 50 else
                        "low"
                    ),
                    "expected_outcomes": {
                        "response_rate": (
                            0.50 if primary_strategy == "aggressive_closing" else
                            0.40 if primary_strategy == "competitive_defense" else
                            0.30 if primary_strategy == "accelerated_nurturing" else
                            0.25 if primary_strategy == "relationship_building" else
                            0.15
                        ),
                        "conversion_probability": round(conversion_probability * 1.2, 2) if primary_strategy == "aggressive_closing" else round(conversion_probability * 1.1, 2) if primary_strategy in ["competitive_defense", "accelerated_nurturing"] else conversion_probability
                    },
                    "strategy_confidence": (
                        "high" if len([f for f, v in strategy_factors.items() if isinstance(v, (int, float)) and v >= 60]) >= 4 else
                        "medium" if len([f for f, v in strategy_factors.items() if isinstance(v, (int, float)) and v >= 50]) >= 3 else
                        "low"
                    )
                }
                
                logger.info(
                    "ultra_personalized_sales_strategy_recommendations_completed",
                    email=email,
                    primary_strategy=primary_strategy,
                    strategy_priority=ultra_personalized_sales_strategy_recommendations.get("strategy_priority")
                )
            except Exception as e:
                logger.warning("ultra_personalized_sales_strategy_recommendations_error", email=email, error=str(e))
            
            # 44. AN√ÅLISIS DE PREDICCI√ìN DE TIEMPO DE RESPUESTA √ìPTIMO
            optimal_response_time_prediction = {}
            try:
                from datetime import datetime, timedelta
                
                # Factores que afectan el tiempo de respuesta √≥ptimo
                response_time_factors = {
                    "urgency_level": len([s for s in budget_signals if "urgent" in str(s).lower()]) if budget_signals else 0,
                    "ml_intent_score": ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0,
                    "priority_level": ultra_enhanced_priority_scoring.get("priority_level", "medium") if ultra_enhanced_priority_scoring else "medium",
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "competitive_risk": deep_competitive_analysis.get("competitive_risk_score", 0) if deep_competitive_analysis else 0
                }
                
                # Calcular tiempo de respuesta √≥ptimo
                if response_time_factors["priority_level"] == "critical" or response_time_factors["urgency_level"] >= 2:
                    optimal_response_time_minutes = 15
                    optimal_response_time = "immediate"
                elif response_time_factors["ml_intent_score"] >= 70:
                    optimal_response_time_minutes = 60
                    optimal_response_time = "within_1_hour"
                elif response_time_factors["ml_intent_score"] >= 50:
                    optimal_response_time_minutes = 240
                    optimal_response_time = "within_4_hours"
                elif response_time_factors["engagement_level"] >= 60:
                    optimal_response_time_minutes = 1440
                    optimal_response_time = "within_24_hours"
                else:
                    optimal_response_time_minutes = 2880
                    optimal_response_time = "within_48_hours"
                
                # Calcular fecha/hora √≥ptima de respuesta
                now = datetime.utcnow()
                optimal_response_datetime = now + timedelta(minutes=optimal_response_time_minutes)
                
                # Ajustar a horario de negocio si es necesario
                if optimal_response_datetime.hour < 9:
                    optimal_response_datetime = optimal_response_datetime.replace(hour=9, minute=0)
                elif optimal_response_datetime.hour >= 18:
                    optimal_response_datetime = optimal_response_datetime + timedelta(days=1)
                    optimal_response_datetime = optimal_response_datetime.replace(hour=9, minute=0)
                
                optimal_response_time_prediction = {
                    "optimal_response_time": optimal_response_time,
                    "optimal_response_time_minutes": optimal_response_time_minutes,
                    "optimal_response_datetime": optimal_response_datetime.isoformat(),
                    "response_time_factors": {k: round(v, 2) if isinstance(v, (int, float)) else v for k, v in response_time_factors.items()},
                    "response_urgency": (
                        "critical" if optimal_response_time == "immediate" else
                        "high" if optimal_response_time == "within_1_hour" else
                        "medium" if optimal_response_time == "within_4_hours" else
                        "low"
                    ),
                    "response_priority_score": round(
                        (100 if optimal_response_time == "immediate" else
                         80 if optimal_response_time == "within_1_hour" else
                         60 if optimal_response_time == "within_4_hours" else
                         40 if optimal_response_time == "within_24_hours" else
                         20), 2
                    ),
                    "response_recommendations": [
                        rec for rec in [
                            "respond_immediately" if optimal_response_time == "immediate" else None,
                            "respond_today" if optimal_response_time == "within_1_hour" else None,
                            "respond_same_day" if optimal_response_time == "within_4_hours" else None,
                            "respond_within_24h" if optimal_response_time == "within_24_hours" else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "optimal_response_time_prediction_completed",
                    email=email,
                    optimal_response_time=optimal_response_time,
                    optimal_response_time_minutes=optimal_response_time_minutes
                )
            except Exception as e:
                logger.warning("optimal_response_time_prediction_error", email=email, error=str(e))
            
            # 45. SISTEMA DE SCORING DE FIT DE PRECIO ULTRA MEJORADO
            ultra_enhanced_pricing_fit_scoring = {}
            try:
                # Factores de fit de precio
                pricing_fit_factors = {
                    "company_size_factor": (
                        30 if company_size in ["enterprise", "large"] else
                        25 if company_size == "medium" else
                        20 if company_size == "small" else
                        15
                    ),
                    "industry_value_factor": (
                        25 if industry in ["technology", "finance", "healthcare"] else
                        20 if industry in ["retail", "manufacturing"] else
                        15
                    ),
                    "budget_signals_factor": (
                        20 if len(budget_signals) >= 3 else
                        15 if len(budget_signals) >= 2 else
                        10 if len(budget_signals) >= 1 else
                        5
                    ),
                    "product_fit_factor": min(product_fit_score / 3, 15),
                    "market_fit_factor": min(ultra_enhanced_market_fit_analysis.get("total_market_fit_score", 0) / 5, 10) if ultra_enhanced_market_fit_analysis else 5,
                    "ltv_factor": min(estimated_ltv / 2000, 10)  # Cap at 10 points
                }
                
                total_pricing_fit_score = sum(pricing_fit_factors.values())
                
                # Determinar tier de precio recomendado
                recommended_pricing_tier = (
                    "enterprise" if total_pricing_fit_score >= 90 else
                    "professional" if total_pricing_fit_score >= 75 else
                    "business" if total_pricing_fit_score >= 60 else
                    "starter"
                )
                
                # Estimar rango de precio
                pricing_ranges = {
                    "enterprise": {"min": 10000, "max": 50000, "typical": 25000},
                    "professional": {"min": 5000, "max": 10000, "typical": 7500},
                    "business": {"min": 2000, "max": 5000, "typical": 3500},
                    "starter": {"min": 500, "max": 2000, "typical": 1000}
                }
                
                price_range = pricing_ranges.get(recommended_pricing_tier, {"min": 0, "max": 0, "typical": 0})
                
                # Calcular sensibilidad al precio
                price_sensitivity = (
                    "very_low" if total_pricing_fit_score >= 90 else
                    "low" if total_pricing_fit_score >= 75 else
                    "medium" if total_pricing_fit_score >= 60 else
                    "high" if total_pricing_fit_score >= 45 else
                    "very_high"
                )
                
                ultra_enhanced_pricing_fit_scoring = {
                    "total_pricing_fit_score": round(total_pricing_fit_score, 2),
                    "pricing_fit_factors": {k: round(v, 2) for k, v in pricing_fit_factors.items()},
                    "recommended_pricing_tier": recommended_pricing_tier,
                    "price_range": price_range,
                    "estimated_price": price_range["typical"],
                    "price_sensitivity": price_sensitivity,
                    "pricing_fit_level": (
                        "excellent" if total_pricing_fit_score >= 90 else
                        "very_good" if total_pricing_fit_score >= 75 else
                        "good" if total_pricing_fit_score >= 60 else
                        "fair" if total_pricing_fit_score >= 45 else
                        "poor"
                    ),
                    "pricing_confidence": (
                        "high" if total_pricing_fit_score >= 80 else
                        "medium" if total_pricing_fit_score >= 60 else
                        "low"
                    ),
                    "pricing_recommendations": [
                        rec for rec in [
                            "offer_enterprise_tier" if recommended_pricing_tier == "enterprise" else None,
                            "highlight_value_proposition" if price_sensitivity in ["high", "very_high"] else None,
                            "offer_flexible_pricing" if price_sensitivity in ["high", "very_high"] else None,
                            "emphasize_roi" if total_pricing_fit_score >= 70 else None,
                            "provide_pricing_transparency" if price_sensitivity in ["medium", "high"] else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "ultra_enhanced_pricing_fit_scoring_completed",
                    email=email,
                    recommended_pricing_tier=recommended_pricing_tier,
                    estimated_price=price_range["typical"],
                    price_sensitivity=price_sensitivity
                )
            except Exception as e:
                logger.warning("ultra_enhanced_pricing_fit_scoring_error", email=email, error=str(e))
            

            # 46. AN√ÅLISIS DE PREDICCI√ìN DE VALOR DE REFERIDO
            referral_value_prediction = {}
            try:
                # Factores que afectan el valor de referido
                referral_factors = {
                    "social_influence": advanced_social_influence_analysis.get("total_influence_score", 0) if advanced_social_influence_analysis else 0,
                    "network_size": network_analysis.get("network_size", 0) if network_analysis else 0,
                    "satisfaction_level": predictive_customer_satisfaction_analysis.get("satisfaction_score", 0) if predictive_customer_satisfaction_analysis else 0,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "company_size_factor": (
                        30 if company_size in ["enterprise", "large"] else
                        20 if company_size == "medium" else
                        10
                    ),
                    "industry_influence": (
                        25 if industry in ["technology", "finance", "consulting"] else
                        15 if industry in ["healthcare", "education"] else
                        10
                    )
                }
                
                # Calcular valor potencial de referidos
                referral_potential_score = sum(referral_factors.values()) / len(referral_factors)
                
                # Estimar n√∫mero de referidos potenciales
                estimated_referrals = (
                    10 if referral_potential_score >= 80 else
                    5 if referral_potential_score >= 60 else
                    3 if referral_potential_score >= 40 else
                    1
                )
                
                # Calcular valor de referidos
                average_referral_value = estimated_ltv * 0.8  # Referidos t√≠picamente tienen 80% del LTV del referidor
                total_referral_value = estimated_referrals * average_referral_value
                
                referral_value_prediction = {
                    "referral_potential_score": round(referral_potential_score, 2),
                    "referral_factors": {k: round(v, 2) for k, v in referral_factors.items()},
                    "estimated_referrals": estimated_referrals,
                    "average_referral_value": round(average_referral_value, 2),
                    "total_referral_value": round(total_referral_value, 2),
                    "referral_potential_level": (
                        "very_high" if referral_potential_score >= 80 else
                        "high" if referral_potential_score >= 60 else
                        "medium" if referral_potential_score >= 40 else
                        "low"
                    ),
                    "referral_strategy": (
                        "aggressive_referral_program" if referral_potential_score >= 70 else
                        "standard_referral_program" if referral_potential_score >= 50 else
                        "passive_referral_encouragement"
                    ),
                    "referral_recommendations": [
                        rec for rec in [
                            "implement_referral_program" if referral_potential_score >= 60 else None,
                            "offer_referral_incentives" if referral_potential_score >= 70 else None,
                            "leverage_social_influence" if referral_factors["social_influence"] >= 60 else None,
                            "focus_on_satisfied_customers" if referral_factors["satisfaction_level"] >= 80 else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "referral_value_prediction_completed",
                    email=email,
                    referral_potential_score=referral_potential_score,
                    estimated_referrals=estimated_referrals,
                    total_referral_value=total_referral_value
                )
            except Exception as e:
                logger.warning("referral_value_prediction_error", email=email, error=str(e))
            
            # 47. AN√ÅLISIS DE PREDICCI√ìN DE TIEMPO DE DECISI√ìN
            decision_timing_prediction = {}
            try:
                # Factores que afectan el tiempo de decisi√≥n
                decision_timing_factors = {
                    "ml_intent_score": ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0,
                    "urgency_signals": len([s for s in budget_signals if "urgent" in str(s).lower()]) if budget_signals else 0,
                    "company_size_factor": (
                        -10 if company_size in ["startup", "small"] else
                        0 if company_size == "medium" else
                        15
                    ),
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "product_fit": product_fit_score,
                    "competitive_risk": deep_competitive_analysis.get("competitive_risk_score", 0) if deep_competitive_analysis else 0
                }
                
                # Calcular d√≠as hasta decisi√≥n
                base_decision_days = 60
                
                intent_adjustment = -20 if decision_timing_factors["ml_intent_score"] >= 70 else -10 if decision_timing_factors["ml_intent_score"] >= 50 else 0
                urgency_adjustment = -15 if decision_timing_factors["urgency_signals"] >= 2 else -5 if decision_timing_factors["urgency_signals"] >= 1 else 0
                company_size_adjustment = decision_timing_factors["company_size_factor"]
                engagement_adjustment = -10 if decision_timing_factors["engagement_level"] >= 70 else 0
                product_fit_adjustment = -10 if decision_timing_factors["product_fit"] >= 70 else 0
                competitive_adjustment = 10 if decision_timing_factors["competitive_risk"] >= 50 else 0
                
                predicted_decision_days = max(7, base_decision_days + 
                                             intent_adjustment + urgency_adjustment + 
                                             company_size_adjustment + engagement_adjustment + 
                                             product_fit_adjustment + competitive_adjustment)
                
                # Calcular fecha de decisi√≥n
                from datetime import datetime, timedelta
                predicted_decision_date = (datetime.utcnow() + timedelta(days=int(predicted_decision_days))).isoformat()
                
                decision_timing_prediction = {
                    "predicted_decision_days": int(predicted_decision_days),
                    "predicted_decision_date": predicted_decision_date,
                    "decision_timing_factors": {k: round(v, 2) if isinstance(v, (int, float)) else v for k, v in decision_timing_factors.items()},
                    "decision_velocity": (
                        "fast" if predicted_decision_days <= 30 else
                        "normal" if predicted_decision_days <= 60 else
                        "slow"
                    ),
                    "decision_confidence": (
                        "high" if decision_timing_factors["ml_intent_score"] >= 70 and decision_timing_factors["urgency_signals"] >= 1 else
                        "medium" if decision_timing_factors["ml_intent_score"] >= 50 or decision_timing_factors["urgency_signals"] >= 1 else
                        "low"
                    ),
                    "decision_acceleration_opportunities": [
                        opp for opp in [
                            "increase_intent" if decision_timing_factors["ml_intent_score"] < 50 else None,
                            "create_urgency" if decision_timing_factors["urgency_signals"] == 0 else None,
                            "improve_engagement" if decision_timing_factors["engagement_level"] < 60 else None,
                            "enhance_product_fit" if decision_timing_factors["product_fit"] < 70 else None,
                            "address_competitive_concerns" if decision_timing_factors["competitive_risk"] >= 50 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "decision_timing_prediction_completed",
                    email=email,
                    predicted_decision_days=predicted_decision_days,
                    decision_velocity=decision_timing_prediction.get("decision_velocity")
                )
            except Exception as e:
                logger.warning("decision_timing_prediction_error", email=email, error=str(e))
            
            # 48. SISTEMA DE SCORING DE FIT DE INTEGRACI√ìN ULTRA MEJORADO
            ultra_enhanced_integration_fit_scoring = {}
            try:
                # Factores de fit de integraci√≥n
                integration_fit_factors = {
                    "technical_readiness": advanced_technical_integration_fit_analysis.get("total_technical_fit_score", 0) if advanced_technical_integration_fit_analysis else 0,
                    "api_capability": (
                        25 if any(kw in lead_data.get("message", "").lower() for kw in ["api", "rest", "graphql", "webhook"]) else
                        15 if advanced_content_analysis.get("content_type") == "technical_inquiry" else
                        5
                    ),
                    "integration_experience": (
                        20 if company_size in ["enterprise", "large"] else
                        15 if company_size == "medium" else
                        10
                    ),
                    "technical_resources": (
                        15 if company_size in ["enterprise", "large"] else
                        10 if company_size == "medium" else
                        5
                    ),
                    "urgency_factor": (
                        10 if any("urgent" in str(s).lower() for s in budget_signals) else
                        5
                    ),
                    "product_fit_for_integration": min(product_fit_score / 4, 15)
                }
                
                total_integration_fit_score = sum(integration_fit_factors.values())
                
                # Determinar tipo de integraci√≥n recomendada
                recommended_integration_type = (
                    "api_rest" if integration_fit_factors["api_capability"] >= 20 else
                    "sdk" if integration_fit_factors["technical_readiness"] >= 60 else
                    "webhook" if integration_fit_factors["api_capability"] >= 15 else
                    "graphql" if integration_fit_factors["api_capability"] >= 18 else
                    "manual" if integration_fit_factors["technical_readiness"] < 40 else
                    "guided"
                )
                
                # Estimar complejidad y tiempo
                integration_complexity = (
                    "low" if total_integration_fit_score >= 70 else
                    "medium" if total_integration_fit_score >= 50 else
                    "high"
                )
                
                estimated_setup_time = (
                    "1-2_days" if integration_complexity == "low" else
                    "3-5_days" if integration_complexity == "medium" else
                    "1-2_weeks"
                )
                
                ultra_enhanced_integration_fit_scoring = {
                    "total_integration_fit_score": round(total_integration_fit_score, 2),
                    "integration_fit_factors": {k: round(v, 2) for k, v in integration_fit_factors.items()},
                    "recommended_integration_type": recommended_integration_type,
                    "integration_complexity": integration_complexity,
                    "estimated_setup_time": estimated_setup_time,
                    "integration_readiness_level": (
                        "high" if total_integration_fit_score >= 70 else
                        "medium" if total_integration_fit_score >= 50 else
                        "low"
                    ),
                    "integration_support_needed": (
                        "minimal" if total_integration_fit_score >= 70 else
                        "moderate" if total_integration_fit_score >= 50 else
                        "extensive"
                    ),
                    "integration_recommendations": [
                        rec for rec in [
                            "provide_api_documentation" if integration_fit_factors["api_capability"] >= 20 else None,
                            "offer_technical_support" if integration_fit_factors["technical_readiness"] < 50 else None,
                            "schedule_integration_call" if integration_complexity == "high" else None,
                            "provide_sdk_access" if recommended_integration_type == "sdk" else None,
                            "offer_guided_setup" if integration_complexity == "medium" else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "ultra_enhanced_integration_fit_scoring_completed",
                    email=email,
                    total_integration_fit_score=total_integration_fit_score,
                    recommended_integration_type=recommended_integration_type
                )
            except Exception as e:
                logger.warning("ultra_enhanced_integration_fit_scoring_error", email=email, error=str(e))
            
            # 49. AN√ÅLISIS DE PREDICCI√ìN DE SATISFACCI√ìN FUTURA
            future_satisfaction_prediction = {}
            try:
                # Factores que predicen satisfacci√≥n futura
                satisfaction_prediction_factors = {
                    "current_satisfaction": predictive_customer_satisfaction_analysis.get("satisfaction_score", 0) if predictive_customer_satisfaction_analysis else 50,
                    "product_fit": product_fit_score,
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0,
                    "engagement_quality": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "support_quality": enhanced_data_quality_scoring.get("quality_factors", {}).get("validity", 0) if enhanced_data_quality_scoring else 50,
                    "communication_quality": 70 if advanced_communication_pattern_analysis.get("communication_complexity") == "high" else 50 if advanced_communication_pattern_analysis else 50
                }
                
                # Calcular satisfacci√≥n futura predicha
                predicted_future_satisfaction = sum(satisfaction_prediction_factors.values()) / len(satisfaction_prediction_factors)
                
                # Ajustar por tendencias
                current_satisfaction = satisfaction_prediction_factors["current_satisfaction"]
                satisfaction_trend = (
                    "improving" if predicted_future_satisfaction > current_satisfaction + 5 else
                    "stable" if abs(predicted_future_satisfaction - current_satisfaction) <= 5 else
                    "declining"
                )
                
                # Predicci√≥n a diferentes horizontes
                satisfaction_projection = {
                    "1_month": round(min(100, predicted_future_satisfaction * 1.05), 2),
                    "3_months": round(min(100, predicted_future_satisfaction * 1.1), 2),
                    "6_months": round(min(100, predicted_future_satisfaction * 1.15), 2),
                    "12_months": round(min(100, predicted_future_satisfaction * 1.2), 2)
                }
                
                future_satisfaction_prediction = {
                    "predicted_future_satisfaction": round(predicted_future_satisfaction, 2),
                    "current_satisfaction": round(current_satisfaction, 2),
                    "satisfaction_trend": satisfaction_trend,
                    "satisfaction_prediction_factors": {k: round(v, 2) for k, v in satisfaction_prediction_factors.items()},
                    "satisfaction_projection": satisfaction_projection,
                    "satisfaction_level": (
                        "very_high" if predicted_future_satisfaction >= 85 else
                        "high" if predicted_future_satisfaction >= 70 else
                        "medium" if predicted_future_satisfaction >= 50 else
                        "low" if predicted_future_satisfaction >= 30 else
                        "very_low"
                    ),
                    "satisfaction_improvement_opportunities": [
                        opp for opp in [
                            "improve_product_fit" if satisfaction_prediction_factors["product_fit"] < 70 else None,
                            "enhance_cultural_alignment" if satisfaction_prediction_factors["cultural_fit"] < 70 else None,
                            "increase_engagement" if satisfaction_prediction_factors["engagement_quality"] < 60 else None,
                            "improve_support_quality" if satisfaction_prediction_factors["support_quality"] < 60 else None,
                            "enhance_communication" if satisfaction_prediction_factors["communication_quality"] < 60 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "future_satisfaction_prediction_completed",
                    email=email,
                    predicted_future_satisfaction=predicted_future_satisfaction,
                    satisfaction_trend=satisfaction_trend
                )
            except Exception as e:
                logger.warning("future_satisfaction_prediction_error", email=email, error=str(e))
            
            # 50. SISTEMA DE SCORING DE FIT DE MERCADO ULTRA MEJORADO
            ultra_enhanced_market_fit_scoring = {}
            try:
                # Factores de market fit
                market_fit_factors = {
                    "industry_alignment": (
                        30 if industry in ["technology", "finance", "healthcare"] else
                        25 if industry in ["retail", "manufacturing", "education"] else
                        20
                    ),
                    "company_size_fit": (
                        25 if company_size in ["enterprise", "large"] else
                        22 if company_size == "medium" else
                        18 if company_size == "small" else
                        15
                    ),
                    "geographic_presence": 15,  # Base
                    "growth_potential": (
                        20 if company_size in ["startup", "small"] else
                        18 if company_size == "medium" else
                        15
                    ),
                    "technology_adoption": (
                        20 if advanced_content_analysis.get("content_type") == "technical_inquiry" else
                        15 if advanced_cultural_fit_analysis.get("cultural_factors", {}).get("technology_adoption", 0) >= 70 else
                        10
                    ) if advanced_content_analysis else 10,
                    "budget_availability": (
                        15 if len(budget_signals) >= 2 else
                        12 if len(budget_signals) >= 1 else
                        8
                    ),
                    "market_timing": seasonal_contact_optimization.get("current_seasonal_score", 50) / 5 if seasonal_contact_optimization else 10
                }
                
                total_market_fit_score = sum(market_fit_factors.values())
                
                # Clasificar market fit
                market_fit_level = (
                    "excellent" if total_market_fit_score >= 120 else
                    "very_good" if total_market_fit_score >= 100 else
                    "good" if total_market_fit_score >= 85 else
                    "fair" if total_market_fit_score >= 70 else
                    "poor"
                )
                
                # Oportunidades de mercado
                market_opportunities = []
                if market_fit_factors["industry_alignment"] >= 25:
                    market_opportunities.append("strong_industry_fit")
                if market_fit_factors["company_size_fit"] >= 22:
                    market_opportunities.append("ideal_company_size")
                if market_fit_factors["growth_potential"] >= 18:
                    market_opportunities.append("high_growth_potential")
                if market_fit_factors["technology_adoption"] >= 15:
                    market_opportunities.append("tech_forward_company")
                if market_fit_factors["budget_availability"] >= 12:
                    market_opportunities.append("budget_available")
                
                ultra_enhanced_market_fit_scoring = {
                    "total_market_fit_score": round(total_market_fit_score, 2),
                    "market_fit_level": market_fit_level,
                    "market_fit_factors": {k: round(v, 2) for k, v in market_fit_factors.items()},
                    "market_opportunities": market_opportunities,
                    "market_fit_strengths": [
                        k for k, v in market_fit_factors.items() if v >= 20
                    ],
                    "market_fit_weaknesses": [
                        k for k, v in market_fit_factors.items() if v < 12
                    ],
                    "market_fit_confidence": (
                        "high" if total_market_fit_score >= 100 else
                        "medium" if total_market_fit_score >= 85 else
                        "low"
                    ),
                    "market_optimization_recommendations": [
                        rec for rec in [
                            "target_similar_industries" if market_fit_factors["industry_alignment"] >= 25 else None,
                            "focus_on_company_size_segment" if market_fit_factors["company_size_fit"] >= 22 else None,
                            "emphasize_growth_benefits" if market_fit_factors["growth_potential"] >= 18 else None,
                            "highlight_technology_advantages" if market_fit_factors["technology_adoption"] >= 15 else None,
                            "leverage_budget_availability" if market_fit_factors["budget_availability"] >= 12 else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "ultra_enhanced_market_fit_scoring_completed",
                    email=email,
                    total_market_fit_score=total_market_fit_score,
                    market_fit_level=market_fit_level
                )
            except Exception as e:
                logger.warning("ultra_enhanced_market_fit_scoring_error", email=email, error=str(e))
            

            # 51. AN√ÅLISIS DE PREDICCI√ìN DE VALOR DE VIDA DEL CLIENTE MEJORADO
            enhanced_customer_lifetime_value_prediction = {}
            try:
                # Factores que afectan el LTV
                ltv_factors = {
                    "base_ltv": estimated_ltv,
                    "engagement_multiplier": min(advanced_engagement_analysis.get("advanced_engagement_score", 0) / 50, 1.5) if advanced_engagement_analysis else 1.0,
                    "satisfaction_multiplier": min(predictive_customer_satisfaction_analysis.get("satisfaction_score", 0) / 50, 1.3) if predictive_customer_satisfaction_analysis else 1.0,
                    "product_fit_multiplier": product_fit_score / 100,
                    "cultural_fit_multiplier": min(advanced_cultural_fit_analysis.get("cultural_fit_score", 0) / 50, 1.2) if advanced_cultural_fit_analysis else 1.0,
                    "retention_multiplier": min(advanced_retention_churn_analysis.get("retention_score", 0) / 50, 1.4) if advanced_retention_churn_analysis else 1.0,
                    "expansion_multiplier": min((advanced_upselling_crossselling_prediction.get("total_expansion_value", 0) / estimated_ltv) if estimated_ltv > 0 else 0, 0.5) + 1.0 if advanced_upselling_crossselling_prediction else 1.0,
                    "referral_multiplier": min(referral_value_prediction.get("referral_potential_score", 0) / 100, 0.3) + 1.0 if referral_value_prediction else 1.0
                }
                
                # Calcular LTV ajustado
                adjusted_ltv = estimated_ltv
                for factor, multiplier in ltv_factors.items():
                    if factor != "base_ltv":
                        adjusted_ltv *= multiplier
                
                # Proyecci√≥n de LTV a diferentes horizontes
                ltv_projection = {
                    "1_year": round(adjusted_ltv, 2),
                    "3_years": round(adjusted_ltv * 2.5, 2),
                    "5_years": round(adjusted_ltv * 4.0, 2)
                }
                
                # Clasificar LTV
                ltv_category = (
                    "very_high" if adjusted_ltv >= 100000 else
                    "high" if adjusted_ltv >= 50000 else
                    "medium" if adjusted_ltv >= 20000 else
                    "low" if adjusted_ltv >= 5000 else
                    "very_low"
                )
                
                enhanced_customer_lifetime_value_prediction = {
                    "base_ltv": round(estimated_ltv, 2),
                    "adjusted_ltv": round(adjusted_ltv, 2),
                    "ltv_factors": {k: round(v, 3) if isinstance(v, float) else v for k, v in ltv_factors.items()},
                    "ltv_projection": ltv_projection,
                    "ltv_category": ltv_category,
                    "ltv_confidence": (
                        "high" if all(m >= 0.9 for k, m in ltv_factors.items() if k != "base_ltv" and isinstance(m, float)) else
                        "medium" if all(m >= 0.7 for k, m in ltv_factors.items() if k != "base_ltv" and isinstance(m, float)) else
                        "low"
                    ),
                    "ltv_optimization_opportunities": [
                        opp for opp in [
                            "increase_engagement" if ltv_factors["engagement_multiplier"] < 1.2 else None,
                            "improve_satisfaction" if ltv_factors["satisfaction_multiplier"] < 1.1 else None,
                            "enhance_product_fit" if ltv_factors["product_fit_multiplier"] < 0.8 else None,
                            "improve_cultural_alignment" if ltv_factors["cultural_fit_multiplier"] < 1.1 else None,
                            "increase_retention" if ltv_factors["retention_multiplier"] < 1.2 else None,
                            "focus_on_expansion" if ltv_factors["expansion_multiplier"] < 1.2 else None,
                            "leverage_referrals" if ltv_factors["referral_multiplier"] < 1.2 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "enhanced_customer_lifetime_value_prediction_completed",
                    email=email,
                    base_ltv=estimated_ltv,
                    adjusted_ltv=adjusted_ltv,
                    ltv_category=ltv_category
                )
            except Exception as e:
                logger.warning("enhanced_customer_lifetime_value_prediction_error", email=email, error=str(e))
            
            # 52. AN√ÅLISIS DE PREDICCI√ìN DE CONVERSI√ìN MULTI-ETAPA
            multi_stage_conversion_prediction = {}
            try:
                # Definir etapas de conversi√≥n
                conversion_stages = {
                    "lead": {"probability": 100, "days": 0},
                    "qualified": {"probability": conversion_probability, "days": 7},
                    "demo_scheduled": {"probability": min(95, conversion_probability * 1.1), "days": 14},
                    "proposal_sent": {"probability": min(90, conversion_probability * 0.95), "days": 30},
                    "negotiation": {"probability": min(85, conversion_probability * 0.9), "days": 45},
                    "closed_won": {"probability": conversion_probability, "days": 60}
                }
                
                # Ajustar probabilidades por factores
                ml_intent = ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0
                engagement = advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0
                product_fit = product_fit_score
                
                for stage in conversion_stages:
                    if stage != "lead":
                        # Ajustar por ML intent
                        if ml_intent >= 70:
                            conversion_stages[stage]["probability"] = min(95, conversion_stages[stage]["probability"] * 1.15)
                        elif ml_intent >= 50:
                            conversion_stages[stage]["probability"] = min(90, conversion_stages[stage]["probability"] * 1.1)
                        
                        # Ajustar por engagement
                        if engagement >= 70:
                            conversion_stages[stage]["probability"] = min(95, conversion_stages[stage]["probability"] * 1.1)
                        
                        # Ajustar por product fit
                        if product_fit >= 70:
                            conversion_stages[stage]["probability"] = min(95, conversion_stages[stage]["probability"] * 1.05)
                
                # Calcular probabilidad acumulada de llegar a cada etapa
                cumulative_probability = {}
                prev_prob = 100
                for stage, data in conversion_stages.items():
                    cumulative_probability[stage] = round(prev_prob * (data["probability"] / 100), 2)
                    prev_prob = cumulative_probability[stage]
                
                # Identificar cuellos de botella
                bottlenecks = []
                prev_stage = None
                for stage, data in conversion_stages.items():
                    if prev_stage and cumulative_probability[stage] < cumulative_probability[prev_stage] * 0.8:
                        bottlenecks.append({
                            "stage": stage,
                            "drop_off": round(cumulative_probability[prev_stage] - cumulative_probability[stage], 2)
                        })
                    prev_stage = stage
                
                multi_stage_conversion_prediction = {
                    "conversion_stages": {k: {**v, "cumulative_probability": cumulative_probability[k]} for k, v in conversion_stages.items()},
                    "cumulative_probability": cumulative_probability,
                    "bottlenecks": bottlenecks,
                    "overall_conversion_probability": round(cumulative_probability.get("closed_won", 0), 2),
                    "estimated_days_to_close": conversion_stages["closed_won"]["days"],
                    "conversion_velocity": (
                        "fast" if conversion_stages["closed_won"]["days"] <= 45 else
                        "normal" if conversion_stages["closed_won"]["days"] <= 60 else
                        "slow"
                    ),
                    "optimization_recommendations": [
                        rec for rec in [
                            f"focus_on_{b['stage']}_stage" if b["drop_off"] >= 10 else None
                            for b in bottlenecks
                        ] if rec
                    ]
                }
                
                logger.info(
                    "multi_stage_conversion_prediction_completed",
                    email=email,
                    overall_conversion_probability=multi_stage_conversion_prediction.get("overall_conversion_probability"),
                    estimated_days_to_close=conversion_stages["closed_won"]["days"]
                )
            except Exception as e:
                logger.warning("multi_stage_conversion_prediction_error", email=email, error=str(e))
            
            # 53. SISTEMA DE SCORING DE FIT DE PRODUCTO POR INDUSTRIA
            industry_specific_product_fit_scoring = {}
            try:
                # Mapeo de fit por industria
                industry_product_fit = {
                    "technology": {
                        "base_score": 80,
                        "key_features": ["api_integration", "scalability", "automation", "analytics"],
                        "weight": 1.2
                    },
                    "finance": {
                        "base_score": 75,
                        "key_features": ["security", "compliance", "reporting", "audit_trail"],
                        "weight": 1.15
                    },
                    "healthcare": {
                        "base_score": 70,
                        "key_features": ["hipaa_compliance", "security", "data_privacy", "integration"],
                        "weight": 1.1
                    },
                    "retail": {
                        "base_score": 65,
                        "key_features": ["inventory_management", "pos_integration", "analytics", "customer_insights"],
                        "weight": 1.05
                    },
                    "manufacturing": {
                        "base_score": 60,
                        "key_features": ["supply_chain", "automation", "quality_control", "reporting"],
                        "weight": 1.0
                    }
                }
                
                # Obtener fit base para la industria
                industry_fit = industry_product_fit.get(industry, {
                    "base_score": 50,
                    "key_features": [],
                    "weight": 1.0
                })
                
                # Ajustar score por caracter√≠sticas detectadas
                detected_features = []
                message_lower = lead_data.get("message", "").lower()
                for feature in industry_fit["key_features"]:
                    if any(kw in message_lower for kw in feature.split("_")):
                        detected_features.append(feature)
                
                # Calcular score ajustado
                base_industry_score = industry_fit["base_score"]
                feature_bonus = len(detected_features) * 5
                adjusted_industry_score = min(100, base_industry_score + feature_bonus)
                final_industry_score = adjusted_industry_score * industry_fit["weight"]
                
                industry_specific_product_fit_scoring = {
                    "industry": industry,
                    "industry_base_score": base_industry_score,
                    "detected_features": detected_features,
                    "feature_bonus": feature_bonus,
                    "adjusted_industry_score": round(adjusted_industry_score, 2),
                    "final_industry_score": round(min(100, final_industry_score), 2),
                    "industry_fit_level": (
                        "excellent" if final_industry_score >= 90 else
                        "very_good" if final_industry_score >= 80 else
                        "good" if final_industry_score >= 70 else
                        "fair" if final_industry_score >= 60 else
                        "poor"
                    ),
                    "industry_specific_recommendations": [
                        rec for rec in [
                            f"highlight_{feature}_benefits" for feature in detected_features[:3]
                        ] if rec
                    ],
                    "missing_features": [
                        f for f in industry_fit["key_features"] if f not in detected_features
                    ]
                }
                
                logger.info(
                    "industry_specific_product_fit_scoring_completed",
                    email=email,
                    industry=industry,
                    final_industry_score=final_industry_score,
                    detected_features_count=len(detected_features)
                )
            except Exception as e:
                logger.warning("industry_specific_product_fit_scoring_error", email=email, error=str(e))
            
            # 54. AN√ÅLISIS DE PREDICCI√ìN DE ADOPCI√ìN DE PRODUCTO
            product_adoption_prediction = {}
            try:
                # Factores que afectan la adopci√≥n
                adoption_factors = {
                    "technical_readiness": ultra_enhanced_integration_fit_scoring.get("total_integration_fit_score", 0) if ultra_enhanced_integration_fit_scoring else 0,
                    "product_fit": product_fit_score,
                    "company_size_factor": (
                        25 if company_size in ["enterprise", "large"] else
                        20 if company_size == "medium" else
                        15
                    ),
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0,
                    "support_quality": enhanced_data_quality_scoring.get("quality_factors", {}).get("validity", 0) if enhanced_data_quality_scoring else 50
                }
                
                # Calcular score de adopci√≥n
                adoption_score = sum(adoption_factors.values()) / len(adoption_factors)
                
                # Predecir tiempo de adopci√≥n
                adoption_time_days = (
                    7 if adoption_score >= 80 else
                    14 if adoption_score >= 70 else
                    30 if adoption_score >= 60 else
                    60 if adoption_score >= 50 else
                    90
                )
                
                # Predecir tasa de adopci√≥n
                adoption_rate = (
                    "high" if adoption_score >= 80 else
                    "medium" if adoption_score >= 60 else
                    "low"
                )
                
                # Predecir profundidad de adopci√≥n
                adoption_depth = (
                    "full" if adoption_score >= 80 and product_fit_score >= 80 else
                    "moderate" if adoption_score >= 60 and product_fit_score >= 60 else
                    "limited"
                )
                
                product_adoption_prediction = {
                    "adoption_score": round(adoption_score, 2),
                    "adoption_factors": {k: round(v, 2) for k, v in adoption_factors.items()},
                    "predicted_adoption_time_days": adoption_time_days,
                    "predicted_adoption_rate": adoption_rate,
                    "predicted_adoption_depth": adoption_depth,
                    "adoption_readiness_level": (
                        "very_high" if adoption_score >= 80 else
                        "high" if adoption_score >= 70 else
                        "medium" if adoption_score >= 60 else
                        "low" if adoption_score >= 50 else
                        "very_low"
                    ),
                    "adoption_acceleration_opportunities": [
                        opp for opp in [
                            "improve_technical_readiness" if adoption_factors["technical_readiness"] < 60 else None,
                            "enhance_product_fit" if adoption_factors["product_fit"] < 70 else None,
                            "increase_engagement" if adoption_factors["engagement_level"] < 60 else None,
                            "improve_cultural_alignment" if adoption_factors["cultural_fit"] < 60 else None,
                            "enhance_support_quality" if adoption_factors["support_quality"] < 60 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "product_adoption_prediction_completed",
                    email=email,
                    adoption_score=adoption_score,
                    predicted_adoption_time_days=adoption_time_days,
                    adoption_rate=adoption_rate
                )
            except Exception as e:
                logger.warning("product_adoption_prediction_error", email=email, error=str(e))
            
            # 55. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO DE ONBOARDING
            onboarding_success_prediction = {}
            try:
                # Factores que afectan el √©xito del onboarding
                onboarding_factors = {
                    "product_fit": product_fit_score,
                    "technical_readiness": ultra_enhanced_integration_fit_scoring.get("total_integration_fit_score", 0) if ultra_enhanced_integration_fit_scoring else 0,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "company_size_factor": (
                        20 if company_size in ["enterprise", "large"] else
                        15 if company_size == "medium" else
                        10
                    ),
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0,
                    "communication_quality": 70 if advanced_communication_pattern_analysis.get("communication_complexity") == "high" else 50 if advanced_communication_pattern_analysis else 50
                }
                
                # Calcular score de √©xito de onboarding
                onboarding_success_score = sum(onboarding_factors.values()) / len(onboarding_factors)
                
                # Predecir tiempo de onboarding
                onboarding_time_days = (
                    7 if onboarding_success_score >= 80 else
                    14 if onboarding_success_score >= 70 else
                    21 if onboarding_success_score >= 60 else
                    30 if onboarding_success_score >= 50 else
                    45
                )
                
                # Predecir probabilidad de √©xito
                success_probability = min(95, onboarding_success_score * 1.1)
                
                # Identificar riesgos
                onboarding_risks = []
                if onboarding_factors["technical_readiness"] < 50:
                    onboarding_risks.append("technical_challenges")
                if onboarding_factors["engagement_level"] < 50:
                    onboarding_risks.append("low_engagement")
                if onboarding_factors["cultural_fit"] < 50:
                    onboarding_risks.append("cultural_mismatch")
                if onboarding_factors["communication_quality"] < 50:
                    onboarding_risks.append("communication_barriers")
                
                onboarding_success_prediction = {
                    "onboarding_success_score": round(onboarding_success_score, 2),
                    "onboarding_factors": {k: round(v, 2) for k, v in onboarding_factors.items()},
                    "predicted_onboarding_time_days": onboarding_time_days,
                    "success_probability": round(success_probability, 2),
                    "onboarding_risks": onboarding_risks,
                    "onboarding_readiness_level": (
                        "very_high" if onboarding_success_score >= 80 else
                        "high" if onboarding_success_score >= 70 else
                        "medium" if onboarding_success_score >= 60 else
                        "low" if onboarding_success_score >= 50 else
                        "very_low"
                    ),
                    "onboarding_recommendations": [
                        rec for rec in [
                            "provide_technical_support" if onboarding_factors["technical_readiness"] < 60 else None,
                            "increase_engagement_programs" if onboarding_factors["engagement_level"] < 60 else None,
                            "assign_dedicated_onboarding_manager" if onboarding_success_score >= 70 else None,
                            "create_custom_onboarding_plan" if company_size in ["enterprise", "large"] else None,
                            "provide_communication_training" if onboarding_factors["communication_quality"] < 60 else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "onboarding_success_prediction_completed",
                    email=email,
                    onboarding_success_score=onboarding_success_score,
                    success_probability=success_probability,
                    predicted_onboarding_time_days=onboarding_time_days
                )
            except Exception as e:
                logger.warning("onboarding_success_prediction_error", email=email, error=str(e))
            

            # 56. AN√ÅLISIS DE PREDICCI√ìN DE CHURN MEJORADO
            enhanced_churn_prediction = {}
            try:
                # Factores de riesgo de churn
                churn_risk_factors = {
                    "satisfaction_score": max(0, 100 - (predictive_customer_satisfaction_analysis.get("satisfaction_score", 50) if predictive_customer_satisfaction_analysis else 50)),
                    "engagement_drop": max(0, 50 - (advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0)),
                    "product_fit_issues": max(0, 50 - product_fit_score),
                    "competitive_pressure": deep_competitive_analysis.get("competitive_risk_score", 0) if deep_competitive_analysis else 0,
                    "support_quality_issues": max(0, 50 - (enhanced_data_quality_scoring.get("quality_factors", {}).get("validity", 50) if enhanced_data_quality_scoring else 50)),
                    "cultural_mismatch": max(0, 50 - (advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0)),
                    "onboarding_risks": len(onboarding_success_prediction.get("onboarding_risks", [])) * 10 if onboarding_success_prediction else 0,
                    "adoption_challenges": max(0, 50 - (product_adoption_prediction.get("adoption_score", 0) if product_adoption_prediction else 0))
                }
                
                # Calcular score total de riesgo de churn
                total_churn_risk = sum(churn_risk_factors.values()) / len(churn_risk_factors)
                
                # Clasificar nivel de riesgo
                churn_risk_level = (
                    "critical" if total_churn_risk >= 70 else
                    "high" if total_churn_risk >= 50 else
                    "medium" if total_churn_risk >= 30 else
                    "low" if total_churn_risk >= 15 else
                    "minimal"
                )
                
                # Predecir tiempo hasta churn
                predicted_days_to_churn = (
                    30 if churn_risk_level == "critical" else
                    60 if churn_risk_level == "high" else
                    90 if churn_risk_level == "medium" else
                    180 if churn_risk_level == "low" else
                    365
                )
                
                # Calcular probabilidad de churn
                churn_probability = min(95, total_churn_risk * 1.2)
                
                # Factores cr√≠ticos
                critical_churn_factors = [
                    factor for factor, score in churn_risk_factors.items()
                    if score >= 40
                ]
                
                # Estrategias de retenci√≥n
                retention_strategies = []
                if churn_risk_factors["satisfaction_score"] >= 30:
                    retention_strategies.append("improve_satisfaction_immediately")
                if churn_risk_factors["engagement_drop"] >= 30:
                    retention_strategies.append("re_engagement_campaign")
                if churn_risk_factors["product_fit_issues"] >= 30:
                    retention_strategies.append("product_fit_reassessment")
                if churn_risk_factors["competitive_pressure"] >= 40:
                    retention_strategies.append("competitive_differentiation")
                if churn_risk_factors["onboarding_risks"] >= 20:
                    retention_strategies.append("onboarding_support")
                
                enhanced_churn_prediction = {
                    "total_churn_risk": round(total_churn_risk, 2),
                    "churn_risk_level": churn_risk_level,
                    "churn_risk_factors": {k: round(v, 2) for k, v in churn_risk_factors.items()},
                    "predicted_days_to_churn": predicted_days_to_churn,
                    "churn_probability": round(churn_probability, 2),
                    "critical_churn_factors": critical_churn_factors,
                    "retention_strategies": retention_strategies,
                    "retention_urgency": (
                        "immediate" if churn_risk_level in ["critical", "high"] else
                        "within_week" if churn_risk_level == "medium" else
                        "within_month" if churn_risk_level == "low" else
                        "monitor"
                    ),
                    "retention_priority": (
                        "critical" if churn_risk_level in ["critical", "high"] else
                        "high" if churn_risk_level == "medium" else
                        "medium" if churn_risk_level == "low" else
                        "low"
                    )
                }
                
                logger.info(
                    "enhanced_churn_prediction_completed",
                    email=email,
                    total_churn_risk=total_churn_risk,
                    churn_risk_level=churn_risk_level,
                    predicted_days_to_churn=predicted_days_to_churn
                )
            except Exception as e:
                logger.warning("enhanced_churn_prediction_error", email=email, error=str(e))
            
            # 57. AN√ÅLISIS DE PREDICCI√ìN DE EXPANSI√ìN MEJORADO
            enhanced_expansion_prediction = {}
            try:
                # Factores de expansi√≥n
                expansion_factors = {
                    "current_satisfaction": predictive_customer_satisfaction_analysis.get("satisfaction_score", 0) if predictive_customer_satisfaction_analysis else 0,
                    "product_adoption": product_adoption_prediction.get("adoption_score", 0) if product_adoption_prediction else 0,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "company_growth": market_fit_analysis.get("growth_potential_score", 0) if market_fit_analysis else 0,
                    "product_fit": product_fit_score,
                    "budget_availability": len(budget_signals) * 15 if budget_signals else 0,
                    "upselling_readiness": advanced_upselling_crossselling_prediction.get("upselling_readiness_score", 0) if advanced_upselling_crossselling_prediction else 0,
                    "cross_selling_readiness": advanced_upselling_crossselling_prediction.get("cross_selling_readiness_score", 0) if advanced_upselling_crossselling_prediction else 0
                }
                
                # Calcular score de expansi√≥n
                expansion_score = sum(expansion_factors.values()) / len(expansion_factors)
                
                # Predecir valor de expansi√≥n
                base_expansion_value = estimated_ltv * 0.3
                expansion_multiplier = expansion_score / 100
                predicted_expansion_value = base_expansion_value * expansion_multiplier
                
                # Predecir tiempo hasta expansi√≥n
                expansion_time_days = (
                    30 if expansion_score >= 80 else
                    60 if expansion_score >= 70 else
                    90 if expansion_score >= 60 else
                    180 if expansion_score >= 50 else
                    365
                )
                
                # Tipo de expansi√≥n m√°s probable
                expansion_type = (
                    "upsell" if expansion_factors["upselling_readiness"] >= expansion_factors["cross_selling_readiness"] else
                    "cross_sell"
                )
                
                enhanced_expansion_prediction = {
                    "expansion_score": round(expansion_score, 2),
                    "expansion_factors": {k: round(v, 2) for k, v in expansion_factors.items()},
                    "predicted_expansion_value": round(predicted_expansion_value, 2),
                    "predicted_expansion_time_days": expansion_time_days,
                    "expansion_type": expansion_type,
                    "expansion_probability": round(min(95, expansion_score * 1.1), 2),
                    "expansion_readiness_level": (
                        "very_high" if expansion_score >= 80 else
                        "high" if expansion_score >= 70 else
                        "medium" if expansion_score >= 60 else
                        "low" if expansion_score >= 50 else
                        "very_low"
                    ),
                    "expansion_opportunities": [
                        opp for opp in [
                            "upsell_enterprise_plan" if expansion_factors["upselling_readiness"] >= 60 else None,
                            "cross_sell_addons" if expansion_factors["cross_selling_readiness"] >= 60 else None,
                            "expand_usage" if expansion_factors["product_adoption"] >= 70 else None,
                            "leverage_growth" if expansion_factors["company_growth"] >= 70 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "enhanced_expansion_prediction_completed",
                    email=email,
                    expansion_score=expansion_score,
                    predicted_expansion_value=predicted_expansion_value,
                    expansion_type=expansion_type
                )
            except Exception as e:
                logger.warning("enhanced_expansion_prediction_error", email=email, error=str(e))
            
            # 58. AN√ÅLISIS DE PREDICCI√ìN DE SATISFACCI√ìN DEL CLIENTE MEJORADO
            enhanced_customer_satisfaction_prediction = {}
            try:
                # Factores de satisfacci√≥n
                satisfaction_factors = {
                    "product_fit": product_fit_score,
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0,
                    "engagement_quality": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "communication_quality": 70 if advanced_communication_pattern_analysis.get("communication_complexity") == "high" else 50 if advanced_communication_pattern_analysis else 50,
                    "support_quality": enhanced_data_quality_scoring.get("quality_factors", {}).get("validity", 50) if enhanced_data_quality_scoring else 50,
                    "onboarding_success": onboarding_success_prediction.get("onboarding_success_score", 0) if onboarding_success_prediction else 0,
                    "adoption_success": product_adoption_prediction.get("adoption_score", 0) if product_adoption_prediction else 0,
                    "value_realization": min(100, (estimated_ltv / 10000) * 10) if estimated_ltv > 0 else 0
                }
                
                # Calcular score de satisfacci√≥n
                satisfaction_score = sum(satisfaction_factors.values()) / len(satisfaction_factors)
                
                # Predecir satisfacci√≥n futura
                future_satisfaction_1m = min(100, satisfaction_score * 1.05)
                future_satisfaction_3m = min(100, satisfaction_score * 1.1)
                future_satisfaction_6m = min(100, satisfaction_score * 1.15)
                future_satisfaction_12m = min(100, satisfaction_score * 1.2)
                
                # Tendencia de satisfacci√≥n
                satisfaction_trend = (
                    "improving" if future_satisfaction_12m > satisfaction_score + 10 else
                    "stable" if abs(future_satisfaction_12m - satisfaction_score) <= 10 else
                    "declining"
                )
                
                # Factores que m√°s contribuyen
                satisfaction_strengths = [
                    factor for factor, score in satisfaction_factors.items()
                    if score >= 70
                ]
                
                # Factores que m√°s afectan negativamente
                satisfaction_weaknesses = [
                    factor for factor, score in satisfaction_factors.items()
                    if score < 50
                ]
                
                enhanced_customer_satisfaction_prediction = {
                    "satisfaction_score": round(satisfaction_score, 2),
                    "satisfaction_factors": {k: round(v, 2) for k, v in satisfaction_factors.items()},
                    "future_satisfaction_projection": {
                        "1_month": round(future_satisfaction_1m, 2),
                        "3_months": round(future_satisfaction_3m, 2),
                        "6_months": round(future_satisfaction_6m, 2),
                        "12_months": round(future_satisfaction_12m, 2)
                    },
                    "satisfaction_trend": satisfaction_trend,
                    "satisfaction_level": (
                        "very_high" if satisfaction_score >= 85 else
                        "high" if satisfaction_score >= 70 else
                        "medium" if satisfaction_score >= 50 else
                        "low" if satisfaction_score >= 30 else
                        "very_low"
                    ),
                    "satisfaction_strengths": satisfaction_strengths,
                    "satisfaction_weaknesses": satisfaction_weaknesses,
                    "satisfaction_improvement_recommendations": [
                        rec for rec in [
                            f"improve_{weakness}" for weakness in satisfaction_weaknesses[:3]
                        ] if rec
                    ]
                }
                
                logger.info(
                    "enhanced_customer_satisfaction_prediction_completed",
                    email=email,
                    satisfaction_score=satisfaction_score,
                    satisfaction_trend=satisfaction_trend
                )
            except Exception as e:
                logger.warning("enhanced_customer_satisfaction_prediction_error", email=email, error=str(e))
            
            # 59. AN√ÅLISIS DE PREDICCI√ìN DE TIEMPO DE RESPUESTA MEJORADO
            enhanced_response_time_prediction = {}
            try:
                # Factores que afectan el tiempo de respuesta
                response_time_factors = {
                    "urgency_level": len([s for s in budget_signals if "urgent" in str(s).lower()]) * 20 if budget_signals else 0,
                    "ml_intent_score": ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0,
                    "priority_score": ultra_enhanced_priority_scoring.get("total_priority_score", 0) if ultra_enhanced_priority_scoring else 0,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "competitive_risk": deep_competitive_analysis.get("competitive_risk_score", 0) if deep_competitive_analysis else 0,
                    "ltv_factor": min(30, estimated_ltv / 1000) if estimated_ltv > 0 else 0
                }
                
                # Calcular score de urgencia de respuesta
                response_urgency_score = sum(response_time_factors.values())
                
                # Determinar tiempo de respuesta √≥ptimo
                if response_urgency_score >= 150:
                    optimal_response_time = "immediate"
                    optimal_response_minutes = 0
                elif response_urgency_score >= 100:
                    optimal_response_time = "within_1_hour"
                    optimal_response_minutes = 60
                elif response_urgency_score >= 70:
                    optimal_response_time = "within_4_hours"
                    optimal_response_minutes = 240
                elif response_urgency_score >= 40:
                    optimal_response_time = "within_24_hours"
                    optimal_response_minutes = 1440
                else:
                    optimal_response_time = "within_48_hours"
                    optimal_response_minutes = 2880
                
                # Calcular fecha/hora √≥ptima de respuesta
                from datetime import datetime, timedelta
                optimal_response_datetime = (datetime.utcnow() + timedelta(minutes=optimal_response_minutes)).isoformat()
                
                # Ajustar a horario de negocio
                current_hour = datetime.utcnow().hour
                if current_hour < 9 or current_hour >= 17:
                    business_hours_adjustment = "next_business_day"
                else:
                    business_hours_adjustment = "same_day"
                
                enhanced_response_time_prediction = {
                    "response_urgency_score": round(response_urgency_score, 2),
                    "response_time_factors": {k: round(v, 2) for k, v in response_time_factors.items()},
                    "optimal_response_time": optimal_response_time,
                    "optimal_response_minutes": optimal_response_minutes,
                    "optimal_response_datetime": optimal_response_datetime,
                    "business_hours_adjustment": business_hours_adjustment,
                    "response_urgency_level": (
                        "critical" if response_urgency_score >= 150 else
                        "high" if response_urgency_score >= 100 else
                        "medium" if response_urgency_score >= 70 else
                        "low" if response_urgency_score >= 40 else
                        "very_low"
                    ),
                    "response_priority": (
                        "critical" if response_urgency_score >= 150 else
                        "high" if response_urgency_score >= 100 else
                        "medium" if response_urgency_score >= 70 else
                        "low"
                    )
                }
                
                logger.info(
                    "enhanced_response_time_prediction_completed",
                    email=email,
                    response_urgency_score=response_urgency_score,
                    optimal_response_time=optimal_response_time
                )
            except Exception as e:
                logger.warning("enhanced_response_time_prediction_error", email=email, error=str(e))
            
            # 60. AN√ÅLISIS DE PREDICCI√ìN DE VALOR DE CUENTA MEJORADO
            enhanced_account_value_prediction = {}
            try:
                # Componentes del valor de cuenta
                account_value_components = {
                    "base_ltv": estimated_ltv,
                    "expansion_value": enhanced_expansion_prediction.get("predicted_expansion_value", 0) if enhanced_expansion_prediction else 0,
                    "referral_value": referral_value_prediction.get("total_referral_value", 0) if referral_value_prediction else 0,
                    "retention_value": estimated_ltv * (advanced_retention_churn_analysis.get("retention_score", 0) / 100) if advanced_retention_churn_analysis else estimated_ltv * 0.8
                }
                
                # Calcular valor total de cuenta
                total_account_value = sum(account_value_components.values())
                
                # Proyecci√≥n de valor a diferentes horizontes
                account_value_projection = {
                    "1_year": round(total_account_value, 2),
                    "3_years": round(total_account_value * 2.5, 2),
                    "5_years": round(total_account_value * 4.0, 2)
                }
                
                # Clasificar valor de cuenta
                account_value_category = (
                    "very_high" if total_account_value >= 200000 else
                    "high" if total_account_value >= 100000 else
                    "medium" if total_account_value >= 50000 else
                    "low" if total_account_value >= 20000 else
                    "very_low"
                )
                
                # Distribuci√≥n del valor
                value_distribution = {
                    component: round((value / total_account_value * 100), 2) if total_account_value > 0 else 0
                    for component, value in account_value_components.items()
                }
                
                enhanced_account_value_prediction = {
                    "total_account_value": round(total_account_value, 2),
                    "account_value_components": {k: round(v, 2) for k, v in account_value_components.items()},
                    "account_value_projection": account_value_projection,
                    "account_value_category": account_value_category,
                    "value_distribution": value_distribution,
                    "account_value_confidence": (
                        "high" if all(v > 0 for v in account_value_components.values()) else
                        "medium" if sum(1 for v in account_value_components.values() if v > 0) >= 2 else
                        "low"
                    ),
                    "account_value_optimization_opportunities": [
                        opp for opp in [
                            "focus_on_expansion" if account_value_components["expansion_value"] < total_account_value * 0.2 else None,
                            "leverage_referrals" if account_value_components["referral_value"] < total_account_value * 0.1 else None,
                            "improve_retention" if account_value_components["retention_value"] < total_account_value * 0.7 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "enhanced_account_value_prediction_completed",
                    email=email,
                    total_account_value=total_account_value,
                    account_value_category=account_value_category
                )
            except Exception as e:
                logger.warning("enhanced_account_value_prediction_error", email=email, error=str(e))
            

            # 61. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO DE VENTA MEJORADO
            enhanced_sales_success_prediction = {}
            try:
                # Factores de √©xito de venta
                sales_success_factors = {
                    "conversion_probability": conversion_probability,
                    "ml_intent_score": ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0,
                    "product_fit": product_fit_score,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "budget_availability": len(budget_signals) * 15 if budget_signals else 0,
                    "decision_timing": 100 - (decision_timing_prediction.get("predicted_decision_days", 90) / 90 * 100) if decision_timing_prediction else 50,
                    "competitive_advantage": 100 - (deep_competitive_analysis.get("competitive_risk_score", 0) if deep_competitive_analysis else 0),
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0
                }
                
                # Calcular score de √©xito de venta
                sales_success_score = sum(sales_success_factors.values()) / len(sales_success_factors)
                
                # Predecir probabilidad de cierre
                close_probability = min(95, sales_success_score * 1.1)
                
                # Predecir tiempo hasta cierre
                days_to_close = (
                    14 if sales_success_score >= 80 else
                    30 if sales_success_score >= 70 else
                    60 if sales_success_score >= 60 else
                    90 if sales_success_score >= 50 else
                    120
                )
                
                # Identificar factores cr√≠ticos de √©xito
                critical_success_factors = [
                    factor for factor, score in sales_success_factors.items()
                    if score >= 70
                ]
                
                # Identificar barreras
                barriers = []
                if sales_success_factors["budget_availability"] < 30:
                    barriers.append("budget_constraints")
                if sales_success_factors["competitive_advantage"] < 50:
                    barriers.append("competitive_pressure")
                if sales_success_factors["decision_timing"] < 40:
                    barriers.append("timing_issues")
                if sales_success_factors["product_fit"] < 60:
                    barriers.append("product_fit_concerns")
                
                enhanced_sales_success_prediction = {
                    "sales_success_score": round(sales_success_score, 2),
                    "sales_success_factors": {k: round(v, 2) for k, v in sales_success_factors.items()},
                    "close_probability": round(close_probability, 2),
                    "predicted_days_to_close": days_to_close,
                    "critical_success_factors": critical_success_factors,
                    "barriers": barriers,
                    "sales_success_level": (
                        "very_high" if sales_success_score >= 80 else
                        "high" if sales_success_score >= 70 else
                        "medium" if sales_success_score >= 60 else
                        "low" if sales_success_score >= 50 else
                        "very_low"
                    ),
                    "sales_acceleration_opportunities": [
                        opp for opp in [
                            "address_budget_concerns" if "budget_constraints" in barriers else None,
                            "competitive_differentiation" if "competitive_pressure" in barriers else None,
                            "optimize_timing" if "timing_issues" in barriers else None,
                            "improve_product_fit" if "product_fit_concerns" in barriers else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "enhanced_sales_success_prediction_completed",
                    email=email,
                    sales_success_score=sales_success_score,
                    close_probability=close_probability,
                    predicted_days_to_close=days_to_close
                )
            except Exception as e:
                logger.warning("enhanced_sales_success_prediction_error", email=email, error=str(e))
            
            # 62. AN√ÅLISIS DE PREDICCI√ìN DE VALOR DE REFERIDO MEJORADO
            enhanced_referral_value_prediction = {}
            try:
                # Factores de valor de referido
                referral_value_factors = {
                    "social_influence": advanced_social_influence_analysis.get("total_influence_score", 0) if advanced_social_influence_analysis else 0,
                    "satisfaction_level": predictive_customer_satisfaction_analysis.get("satisfaction_score", 0) if predictive_customer_satisfaction_analysis else 0,
                    "network_size": network_analysis.get("network_size", 0) if network_analysis else 0,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "company_size_factor": (
                        30 if company_size in ["enterprise", "large"] else
                        20 if company_size == "medium" else
                        10
                    ),
                    "industry_influence": (
                        25 if industry in ["technology", "finance", "consulting"] else
                        15 if industry in ["healthcare", "education"] else
                        10
                    ),
                    "referral_history": 0  # Asumir 0 si no hay historial
                }
                
                # Calcular score de potencial de referido
                referral_potential_score = sum(referral_value_factors.values()) / len(referral_value_factors)
                
                # Estimar n√∫mero de referidos
                estimated_referrals = (
                    15 if referral_potential_score >= 80 else
                    10 if referral_potential_score >= 70 else
                    7 if referral_potential_score >= 60 else
                    5 if referral_potential_score >= 50 else
                    3 if referral_potential_score >= 40 else
                    1
                )
                
                # Calcular valor promedio de referido
                average_referral_ltv = estimated_ltv * 0.75  # Referidos t√≠picamente tienen 75% del LTV
                total_referral_value = estimated_referrals * average_referral_ltv
                
                # Predecir tiempo hasta primer referido
                days_to_first_referral = (
                    30 if referral_potential_score >= 80 else
                    60 if referral_potential_score >= 70 else
                    90 if referral_potential_score >= 60 else
                    180 if referral_potential_score >= 50 else
                    365
                )
                
                enhanced_referral_value_prediction = {
                    "referral_potential_score": round(referral_potential_score, 2),
                    "referral_value_factors": {k: round(v, 2) for k, v in referral_value_factors.items()},
                    "estimated_referrals": estimated_referrals,
                    "average_referral_ltv": round(average_referral_ltv, 2),
                    "total_referral_value": round(total_referral_value, 2),
                    "predicted_days_to_first_referral": days_to_first_referral,
                    "referral_potential_level": (
                        "very_high" if referral_potential_score >= 80 else
                        "high" if referral_potential_score >= 70 else
                        "medium" if referral_potential_score >= 60 else
                        "low" if referral_potential_score >= 50 else
                        "very_low"
                    ),
                    "referral_program_recommendations": [
                        rec for rec in [
                            "implement_aggressive_referral_program" if referral_potential_score >= 70 else None,
                            "offer_referral_incentives" if referral_potential_score >= 60 else None,
                            "leverage_social_influence" if referral_value_factors["social_influence"] >= 60 else None,
                            "focus_on_satisfied_customers" if referral_value_factors["satisfaction_level"] >= 80 else None,
                            "create_referral_content" if referral_potential_score >= 50 else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "enhanced_referral_value_prediction_completed",
                    email=email,
                    referral_potential_score=referral_potential_score,
                    estimated_referrals=estimated_referrals,
                    total_referral_value=total_referral_value
                )
            except Exception as e:
                logger.warning("enhanced_referral_value_prediction_error", email=email, error=str(e))
            
            # 63. AN√ÅLISIS DE PREDICCI√ìN DE ADOPCI√ìN DE TECNOLOG√çA
            technology_adoption_prediction = {}
            try:
                # Factores de adopci√≥n tecnol√≥gica
                tech_adoption_factors = {
                    "technical_readiness": ultra_enhanced_integration_fit_scoring.get("total_integration_fit_score", 0) if ultra_enhanced_integration_fit_scoring else 0,
                    "tech_maturity": technology_analysis.get("tech_maturity", "medium") if technology_analysis else "medium",
                    "company_size_factor": (
                        25 if company_size in ["enterprise", "large"] else
                        20 if company_size == "medium" else
                        15
                    ),
                    "industry_tech_adoption": (
                        25 if industry in ["technology", "finance"] else
                        20 if industry in ["healthcare", "retail"] else
                        15
                    ),
                    "cultural_tech_alignment": advanced_cultural_fit_analysis.get("cultural_factors", {}).get("technology_adoption", 0) if advanced_cultural_fit_analysis else 0,
                    "integration_complexity": (
                        20 if ultra_enhanced_integration_fit_scoring.get("integration_complexity") == "low" else
                        15 if ultra_enhanced_integration_fit_scoring.get("integration_complexity") == "medium" else
                        10
                    ) if ultra_enhanced_integration_fit_scoring else 15
                }
                
                # Calcular score de adopci√≥n tecnol√≥gica
                tech_adoption_score = sum(tech_adoption_factors.values()) / len(tech_adoption_factors)
                
                # Predecir tiempo de adopci√≥n
                adoption_time_days = (
                    7 if tech_adoption_score >= 80 else
                    14 if tech_adoption_score >= 70 else
                    30 if tech_adoption_score >= 60 else
                    60 if tech_adoption_score >= 50 else
                    90
                )
                
                # Predecir profundidad de adopci√≥n
                adoption_depth = (
                    "full" if tech_adoption_score >= 80 else
                    "moderate" if tech_adoption_score >= 60 else
                    "limited"
                )
                
                technology_adoption_prediction = {
                    "tech_adoption_score": round(tech_adoption_score, 2),
                    "tech_adoption_factors": {k: round(v, 2) if isinstance(v, (int, float)) else v for k, v in tech_adoption_factors.items()},
                    "predicted_adoption_time_days": adoption_time_days,
                    "predicted_adoption_depth": adoption_depth,
                    "tech_adoption_readiness_level": (
                        "very_high" if tech_adoption_score >= 80 else
                        "high" if tech_adoption_score >= 70 else
                        "medium" if tech_adoption_score >= 60 else
                        "low" if tech_adoption_score >= 50 else
                        "very_low"
                    ),
                    "tech_adoption_acceleration_opportunities": [
                        opp for opp in [
                            "provide_technical_documentation" if tech_adoption_factors["technical_readiness"] < 60 else None,
                            "offer_integration_support" if tech_adoption_factors["integration_complexity"] < 15 else None,
                            "create_tech_onboarding_program" if tech_adoption_score < 70 else None,
                            "leverage_tech_maturity" if tech_adoption_factors["tech_maturity"] in ["high", "very_high"] else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "technology_adoption_prediction_completed",
                    email=email,
                    tech_adoption_score=tech_adoption_score,
                    predicted_adoption_time_days=adoption_time_days,
                    adoption_depth=adoption_depth
                )
            except Exception as e:
                logger.warning("technology_adoption_prediction_error", email=email, error=str(e))
            
            # 64. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO DE IMPLEMENTACI√ìN
            implementation_success_prediction = {}
            try:
                # Factores de √©xito de implementaci√≥n
                implementation_factors = {
                    "technical_readiness": ultra_enhanced_integration_fit_scoring.get("total_integration_fit_score", 0) if ultra_enhanced_integration_fit_scoring else 0,
                    "product_fit": product_fit_score,
                    "company_size_factor": (
                        20 if company_size in ["enterprise", "large"] else
                        15 if company_size == "medium" else
                        10
                    ),
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0,
                    "communication_quality": 70 if advanced_communication_pattern_analysis.get("communication_complexity") == "high" else 50 if advanced_communication_pattern_analysis else 50,
                    "support_quality": enhanced_data_quality_scoring.get("quality_factors", {}).get("validity", 50) if enhanced_data_quality_scoring else 50,
                    "onboarding_success": onboarding_success_prediction.get("onboarding_success_score", 0) if onboarding_success_prediction else 0
                }
                
                # Calcular score de √©xito de implementaci√≥n
                implementation_success_score = sum(implementation_factors.values()) / len(implementation_factors)
                
                # Predecir tiempo de implementaci√≥n
                implementation_time_days = (
                    14 if implementation_success_score >= 80 else
                    21 if implementation_success_score >= 70 else
                    30 if implementation_success_score >= 60 else
                    45 if implementation_success_score >= 50 else
                    60
                )
                
                # Predecir probabilidad de √©xito
                success_probability = min(95, implementation_success_score * 1.1)
                
                # Identificar riesgos
                implementation_risks = []
                if implementation_factors["technical_readiness"] < 50:
                    implementation_risks.append("technical_challenges")
                if implementation_factors["product_fit"] < 60:
                    implementation_risks.append("product_fit_issues")
                if implementation_factors["engagement_level"] < 50:
                    implementation_risks.append("low_engagement")
                if implementation_factors["cultural_fit"] < 50:
                    implementation_risks.append("cultural_mismatch")
                
                implementation_success_prediction = {
                    "implementation_success_score": round(implementation_success_score, 2),
                    "implementation_factors": {k: round(v, 2) for k, v in implementation_factors.items()},
                    "predicted_implementation_time_days": implementation_time_days,
                    "success_probability": round(success_probability, 2),
                    "implementation_risks": implementation_risks,
                    "implementation_readiness_level": (
                        "very_high" if implementation_success_score >= 80 else
                        "high" if implementation_success_score >= 70 else
                        "medium" if implementation_success_score >= 60 else
                        "low" if implementation_success_score >= 50 else
                        "very_low"
                    ),
                    "implementation_recommendations": [
                        rec for rec in [
                            "provide_technical_support" if implementation_factors["technical_readiness"] < 60 else None,
                            "improve_product_fit" if implementation_factors["product_fit"] < 60 else None,
                            "increase_engagement" if implementation_factors["engagement_level"] < 60 else None,
                            "assign_dedicated_implementation_manager" if implementation_success_score >= 70 else None,
                            "create_custom_implementation_plan" if company_size in ["enterprise", "large"] else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "implementation_success_prediction_completed",
                    email=email,
                    implementation_success_score=implementation_success_score,
                    success_probability=success_probability,
                    predicted_implementation_time_days=implementation_time_days
                )
            except Exception as e:
                logger.warning("implementation_success_prediction_error", email=email, error=str(e))
            
            # 65. AN√ÅLISIS DE PREDICCI√ìN DE VALOR TOTAL DEL CLIENTE
            total_customer_value_prediction = {}
            try:
                # Componentes del valor total del cliente
                total_value_components = {
                    "base_ltv": estimated_ltv,
                    "expansion_value": enhanced_expansion_prediction.get("predicted_expansion_value", 0) if enhanced_expansion_prediction else 0,
                    "referral_value": enhanced_referral_value_prediction.get("total_referral_value", 0) if enhanced_referral_value_prediction else 0,
                    "retention_value": estimated_ltv * (advanced_retention_churn_analysis.get("retention_score", 0) / 100) if advanced_retention_churn_analysis else estimated_ltv * 0.8,
                    "upselling_value": advanced_upselling_crossselling_prediction.get("estimated_upsell_value", 0) if advanced_upselling_crossselling_prediction else 0,
                    "cross_selling_value": advanced_upselling_crossselling_prediction.get("estimated_cross_sell_value", 0) if advanced_upselling_crossselling_prediction else 0
                }
                
                # Calcular valor total del cliente
                total_customer_value = sum(total_value_components.values())
                
                # Proyecci√≥n de valor a diferentes horizontes
                customer_value_projection = {
                    "1_year": round(total_customer_value, 2),
                    "3_years": round(total_customer_value * 2.8, 2),
                    "5_years": round(total_customer_value * 5.0, 2),
                    "10_years": round(total_customer_value * 8.0, 2)
                }
                
                # Clasificar valor total
                total_value_category = (
                    "very_high" if total_customer_value >= 500000 else
                    "high" if total_customer_value >= 200000 else
                    "medium" if total_customer_value >= 100000 else
                    "low" if total_customer_value >= 50000 else
                    "very_low"
                )
                
                # Distribuci√≥n del valor
                value_distribution = {
                    component: round((value / total_customer_value * 100), 2) if total_customer_value > 0 else 0
                    for component, value in total_value_components.items()
                }
                
                total_customer_value_prediction = {
                    "total_customer_value": round(total_customer_value, 2),
                    "total_value_components": {k: round(v, 2) for k, v in total_value_components.items()},
                    "customer_value_projection": customer_value_projection,
                    "total_value_category": total_value_category,
                    "value_distribution": value_distribution,
                    "value_confidence": (
                        "high" if all(v > 0 for v in total_value_components.values()) else
                        "medium" if sum(1 for v in total_value_components.values() if v > 0) >= 3 else
                        "low"
                    ),
                    "value_optimization_opportunities": [
                        opp for opp in [
                            "maximize_expansion" if total_value_components["expansion_value"] < total_customer_value * 0.15 else None,
                            "leverage_referrals" if total_value_components["referral_value"] < total_customer_value * 0.1 else None,
                            "improve_retention" if total_value_components["retention_value"] < total_customer_value * 0.6 else None,
                            "focus_on_upselling" if total_value_components["upselling_value"] < total_customer_value * 0.1 else None,
                            "increase_cross_selling" if total_value_components["cross_selling_value"] < total_customer_value * 0.05 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "total_customer_value_prediction_completed",
                    email=email,
                    total_customer_value=total_customer_value,
                    total_value_category=total_value_category
                )
            except Exception as e:
                logger.warning("total_customer_value_prediction_error", email=email, error=str(e))
            

            # 66. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO DE RENOVACI√ìN
            renewal_success_prediction = {}
            try:
                # Factores de √©xito de renovaci√≥n
                renewal_factors = {
                    "satisfaction_score": predictive_customer_satisfaction_analysis.get("satisfaction_score", 0) if predictive_customer_satisfaction_analysis else 0,
                    "product_fit": product_fit_score,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "adoption_success": product_adoption_prediction.get("adoption_score", 0) if product_adoption_prediction else 0,
                    "implementation_success": implementation_success_prediction.get("implementation_success_score", 0) if implementation_success_prediction else 0,
                    "value_realization": min(100, (estimated_ltv / 10000) * 10) if estimated_ltv > 0 else 0,
                    "retention_score": advanced_retention_churn_analysis.get("retention_score", 0) if advanced_retention_churn_analysis else 0,
                    "competitive_advantage": 100 - (deep_competitive_analysis.get("competitive_risk_score", 0) if deep_competitive_analysis else 0)
                }
                
                # Calcular score de √©xito de renovaci√≥n
                renewal_success_score = sum(renewal_factors.values()) / len(renewal_factors)
                
                # Predecir probabilidad de renovaci√≥n
                renewal_probability = min(95, renewal_success_score * 1.1)
                
                # Predecir tiempo hasta renovaci√≥n
                days_to_renewal = (
                    30 if renewal_success_score >= 80 else
                    60 if renewal_success_score >= 70 else
                    90 if renewal_success_score >= 60 else
                    120 if renewal_success_score >= 50 else
                    180
                )
                
                # Identificar factores cr√≠ticos
                critical_renewal_factors = [
                    factor for factor, score in renewal_factors.items()
                    if score >= 70
                ]
                
                # Identificar riesgos de no renovaci√≥n
                renewal_risks = []
                if renewal_factors["satisfaction_score"] < 60:
                    renewal_risks.append("low_satisfaction")
                if renewal_factors["product_fit"] < 60:
                    renewal_risks.append("poor_product_fit")
                if renewal_factors["adoption_success"] < 50:
                    renewal_risks.append("low_adoption")
                if renewal_factors["competitive_advantage"] < 50:
                    renewal_risks.append("competitive_pressure")
                
                renewal_success_prediction = {
                    "renewal_success_score": round(renewal_success_score, 2),
                    "renewal_factors": {k: round(v, 2) for k, v in renewal_factors.items()},
                    "renewal_probability": round(renewal_probability, 2),
                    "predicted_days_to_renewal": days_to_renewal,
                    "critical_renewal_factors": critical_renewal_factors,
                    "renewal_risks": renewal_risks,
                    "renewal_success_level": (
                        "very_high" if renewal_success_score >= 80 else
                        "high" if renewal_success_score >= 70 else
                        "medium" if renewal_success_score >= 60 else
                        "low" if renewal_success_score >= 50 else
                        "very_low"
                    ),
                    "renewal_strategies": [
                        strategy for strategy in [
                            "improve_satisfaction" if "low_satisfaction" in renewal_risks else None,
                            "enhance_product_fit" if "poor_product_fit" in renewal_risks else None,
                            "increase_adoption" if "low_adoption" in renewal_risks else None,
                            "competitive_differentiation" if "competitive_pressure" in renewal_risks else None,
                            "early_renewal_incentive" if renewal_success_score >= 70 else None
                        ] if strategy
                    ]
                }
                
                logger.info(
                    "renewal_success_prediction_completed",
                    email=email,
                    renewal_success_score=renewal_success_score,
                    renewal_probability=renewal_probability,
                    predicted_days_to_renewal=days_to_renewal
                )
            except Exception as e:
                logger.warning("renewal_success_prediction_error", email=email, error=str(e))
            
            # 67. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO DE UPSELLING
            upselling_success_prediction = {}
            try:
                # Factores de √©xito de upselling
                upselling_factors = {
                    "current_satisfaction": predictive_customer_satisfaction_analysis.get("satisfaction_score", 0) if predictive_customer_satisfaction_analysis else 0,
                    "product_adoption": product_adoption_prediction.get("adoption_score", 0) if product_adoption_prediction else 0,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "company_growth": market_fit_analysis.get("growth_potential_score", 0) if market_fit_analysis else 0,
                    "budget_availability": len(budget_signals) * 20 if budget_signals else 0,
                    "product_fit": product_fit_score,
                    "value_realization": min(100, (estimated_ltv / 10000) * 10) if estimated_ltv > 0 else 0,
                    "upselling_readiness": advanced_upselling_crossselling_prediction.get("upselling_readiness_score", 0) if advanced_upselling_crossselling_prediction else 0
                }
                
                # Calcular score de √©xito de upselling
                upselling_success_score = sum(upselling_factors.values()) / len(upselling_factors)
                
                # Predecir probabilidad de upselling
                upselling_probability = min(95, upselling_success_score * 1.1)
                
                # Predecir tiempo hasta upselling
                days_to_upselling = (
                    30 if upselling_success_score >= 80 else
                    60 if upselling_success_score >= 70 else
                    90 if upselling_success_score >= 60 else
                    180 if upselling_success_score >= 50 else
                    365
                )
                
                # Predecir valor de upselling
                predicted_upsell_value = estimated_ltv * 0.25 if upselling_success_score >= 60 else estimated_ltv * 0.15
                
                upselling_success_prediction = {
                    "upselling_success_score": round(upselling_success_score, 2),
                    "upselling_factors": {k: round(v, 2) for k, v in upselling_factors.items()},
                    "upselling_probability": round(upselling_probability, 2),
                    "predicted_days_to_upselling": days_to_upselling,
                    "predicted_upsell_value": round(predicted_upsell_value, 2),
                    "upselling_success_level": (
                        "very_high" if upselling_success_score >= 80 else
                        "high" if upselling_success_score >= 70 else
                        "medium" if upselling_success_score >= 60 else
                        "low" if upselling_success_score >= 50 else
                        "very_low"
                    ),
                    "upselling_opportunities": [
                        opp for opp in [
                            "upsell_enterprise_plan" if upselling_success_score >= 70 else None,
                            "upsell_premium_features" if upselling_success_score >= 60 else None,
                            "upsell_addon_services" if upselling_success_score >= 50 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "upselling_success_prediction_completed",
                    email=email,
                    upselling_success_score=upselling_success_score,
                    upselling_probability=upselling_probability,
                    predicted_upsell_value=predicted_upsell_value
                )
            except Exception as e:
                logger.warning("upselling_success_prediction_error", email=email, error=str(e))
            
            # 68. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO DE CROSS-SELLING
            cross_selling_success_prediction = {}
            try:
                # Factores de √©xito de cross-selling
                cross_selling_factors = {
                    "current_satisfaction": predictive_customer_satisfaction_analysis.get("satisfaction_score", 0) if predictive_customer_satisfaction_analysis else 0,
                    "product_fit": product_fit_score,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0,
                    "budget_availability": len(budget_signals) * 15 if budget_signals else 0,
                    "cross_selling_readiness": advanced_upselling_crossselling_prediction.get("cross_selling_readiness_score", 0) if advanced_upselling_crossselling_prediction else 0,
                    "needs_detection": len(keyword_analysis.get("needs_keywords", [])) * 10 if keyword_analysis else 0,
                    "value_realization": min(100, (estimated_ltv / 10000) * 10) if estimated_ltv > 0 else 0
                }
                
                # Calcular score de √©xito de cross-selling
                cross_selling_success_score = sum(cross_selling_factors.values()) / len(cross_selling_factors)
                
                # Predecir probabilidad de cross-selling
                cross_selling_probability = min(95, cross_selling_success_score * 1.1)
                
                # Predecir tiempo hasta cross-selling
                days_to_cross_selling = (
                    45 if cross_selling_success_score >= 80 else
                    90 if cross_selling_success_score >= 70 else
                    120 if cross_selling_success_score >= 60 else
                    180 if cross_selling_success_score >= 50 else
                    365
                )
                
                # Predecir valor de cross-selling
                predicted_cross_sell_value = estimated_ltv * 0.20 if cross_selling_success_score >= 60 else estimated_ltv * 0.10
                
                cross_selling_success_prediction = {
                    "cross_selling_success_score": round(cross_selling_success_score, 2),
                    "cross_selling_factors": {k: round(v, 2) for k, v in cross_selling_factors.items()},
                    "cross_selling_probability": round(cross_selling_probability, 2),
                    "predicted_days_to_cross_selling": days_to_cross_selling,
                    "predicted_cross_sell_value": round(predicted_cross_sell_value, 2),
                    "cross_selling_success_level": (
                        "very_high" if cross_selling_success_score >= 80 else
                        "high" if cross_selling_success_score >= 70 else
                        "medium" if cross_selling_success_score >= 60 else
                        "low" if cross_selling_success_score >= 50 else
                        "very_low"
                    ),
                    "cross_selling_opportunities": [
                        opp for opp in [
                            "cross_sell_integrations" if cross_selling_success_score >= 70 else None,
                            "cross_sell_analytics" if cross_selling_success_score >= 60 else None,
                            "cross_sell_support" if cross_selling_success_score >= 50 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "cross_selling_success_prediction_completed",
                    email=email,
                    cross_selling_success_score=cross_selling_success_score,
                    cross_selling_probability=cross_selling_probability,
                    predicted_cross_sell_value=predicted_cross_sell_value
                )
            except Exception as e:
                logger.warning("cross_selling_success_prediction_error", email=email, error=str(e))
            
            # 69. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO DE REFERIDOS
            referral_success_prediction = {}
            try:
                # Factores de √©xito de referidos
                referral_success_factors = {
                    "satisfaction_level": predictive_customer_satisfaction_analysis.get("satisfaction_score", 0) if predictive_customer_satisfaction_analysis else 0,
                    "social_influence": advanced_social_influence_analysis.get("total_influence_score", 0) if advanced_social_influence_analysis else 0,
                    "network_size": network_analysis.get("network_size", 0) if network_analysis else 0,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "company_size_factor": (
                        30 if company_size in ["enterprise", "large"] else
                        20 if company_size == "medium" else
                        10
                    ),
                    "industry_influence": (
                        25 if industry in ["technology", "finance", "consulting"] else
                        15 if industry in ["healthcare", "education"] else
                        10
                    ),
                    "value_realization": min(100, (estimated_ltv / 10000) * 10) if estimated_ltv > 0 else 0,
                    "referral_potential": enhanced_referral_value_prediction.get("referral_potential_score", 0) if enhanced_referral_value_prediction else 0
                }
                
                # Calcular score de √©xito de referidos
                referral_success_score = sum(referral_success_factors.values()) / len(referral_success_factors)
                
                # Predecir probabilidad de referidos
                referral_probability = min(95, referral_success_score * 1.1)
                
                # Predecir n√∫mero de referidos
                predicted_referrals = (
                    20 if referral_success_score >= 80 else
                    15 if referral_success_score >= 70 else
                    10 if referral_success_score >= 60 else
                    7 if referral_success_score >= 50 else
                    5 if referral_success_score >= 40 else
                    2
                )
                
                # Predecir tiempo hasta primer referido
                days_to_first_referral = (
                    30 if referral_success_score >= 80 else
                    60 if referral_success_score >= 70 else
                    90 if referral_success_score >= 60 else
                    180 if referral_success_score >= 50 else
                    365
                )
                
                referral_success_prediction = {
                    "referral_success_score": round(referral_success_score, 2),
                    "referral_success_factors": {k: round(v, 2) for k, v in referral_success_factors.items()},
                    "referral_probability": round(referral_probability, 2),
                    "predicted_referrals": predicted_referrals,
                    "predicted_days_to_first_referral": days_to_first_referral,
                    "referral_success_level": (
                        "very_high" if referral_success_score >= 80 else
                        "high" if referral_success_score >= 70 else
                        "medium" if referral_success_score >= 60 else
                        "low" if referral_success_score >= 50 else
                        "very_low"
                    ),
                    "referral_program_recommendations": [
                        rec for rec in [
                            "implement_aggressive_referral_program" if referral_success_score >= 70 else None,
                            "offer_referral_incentives" if referral_success_score >= 60 else None,
                            "create_referral_content" if referral_success_score >= 50 else None,
                            "leverage_social_influence" if referral_success_factors["social_influence"] >= 60 else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "referral_success_prediction_completed",
                    email=email,
                    referral_success_score=referral_success_score,
                    referral_probability=referral_probability,
                    predicted_referrals=predicted_referrals
                )
            except Exception as e:
                logger.warning("referral_success_prediction_error", email=email, error=str(e))
            
            # 70. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO GENERAL DEL CLIENTE
            overall_customer_success_prediction = {}
            try:
                # Factores de √©xito general
                overall_success_factors = {
                    "satisfaction": predictive_customer_satisfaction_analysis.get("satisfaction_score", 0) if predictive_customer_satisfaction_analysis else 0,
                    "adoption": product_adoption_prediction.get("adoption_score", 0) if product_adoption_prediction else 0,
                    "implementation": implementation_success_prediction.get("implementation_success_score", 0) if implementation_success_prediction else 0,
                    "renewal": renewal_success_prediction.get("renewal_success_score", 0) if renewal_success_prediction else 0,
                    "expansion": enhanced_expansion_prediction.get("expansion_score", 0) if enhanced_expansion_prediction else 0,
                    "retention": advanced_retention_churn_analysis.get("retention_score", 0) if advanced_retention_churn_analysis else 0,
                    "engagement": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "value_realization": min(100, (estimated_ltv / 10000) * 10) if estimated_ltv > 0 else 0
                }
                
                # Calcular score de √©xito general
                overall_success_score = sum(overall_success_factors.values()) / len(overall_success_factors)
                
                # Clasificar nivel de √©xito
                success_level = (
                    "exceptional" if overall_success_score >= 90 else
                    "excellent" if overall_success_score >= 80 else
                    "very_good" if overall_success_score >= 70 else
                    "good" if overall_success_score >= 60 else
                    "fair" if overall_success_score >= 50 else
                    "poor" if overall_success_score >= 40 else
                    "very_poor"
                )
                
                # Identificar fortalezas
                strengths = [
                    factor for factor, score in overall_success_factors.items()
                    if score >= 75
                ]
                
                # Identificar √°reas de mejora
                improvement_areas = [
                    factor for factor, score in overall_success_factors.items()
                    if score < 60
                ]
                
                overall_customer_success_prediction = {
                    "overall_success_score": round(overall_success_score, 2),
                    "overall_success_factors": {k: round(v, 2) for k, v in overall_success_factors.items()},
                    "success_level": success_level,
                    "strengths": strengths,
                    "improvement_areas": improvement_areas,
                    "success_recommendations": [
                        rec for rec in [
                            f"improve_{area}" for area in improvement_areas[:3]
                        ] if rec
                    ],
                    "customer_health_score": round(overall_success_score, 2),
                    "customer_health_status": (
                        "healthy" if overall_success_score >= 75 else
                        "at_risk" if overall_success_score >= 60 else
                        "critical"
                    )
                }
                
                logger.info(
                    "overall_customer_success_prediction_completed",
                    email=email,
                    overall_success_score=overall_success_score,
                    success_level=success_level,
                    customer_health_status=overall_customer_success_prediction.get("customer_health_status")
                )
            except Exception as e:
                logger.warning("overall_customer_success_prediction_error", email=email, error=str(e))
            

            # 71. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO DE ONBOARDING MEJORADO
            enhanced_onboarding_success_prediction = {}
            try:
                # Factores de √©xito de onboarding mejorado
                enhanced_onboarding_factors = {
                    "product_fit": product_fit_score,
                    "technical_readiness": ultra_enhanced_integration_fit_scoring.get("total_integration_fit_score", 0) if ultra_enhanced_integration_fit_scoring else 0,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "company_size_factor": (
                        25 if company_size in ["enterprise", "large"] else
                        20 if company_size == "medium" else
                        15
                    ),
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0,
                    "communication_quality": 70 if advanced_communication_pattern_analysis.get("communication_complexity") == "high" else 50 if advanced_communication_pattern_analysis else 50,
                    "support_quality": enhanced_data_quality_scoring.get("quality_factors", {}).get("validity", 50) if enhanced_data_quality_scoring else 50,
                    "onboarding_resources": (
                        20 if company_size in ["enterprise", "large"] else
                        15 if company_size == "medium" else
                        10
                    )
                }
                
                # Calcular score de √©xito de onboarding mejorado
                enhanced_onboarding_score = sum(enhanced_onboarding_factors.values()) / len(enhanced_onboarding_factors)
                
                # Predecir tiempo de onboarding
                onboarding_time_days = (
                    5 if enhanced_onboarding_score >= 85 else
                    7 if enhanced_onboarding_score >= 75 else
                    14 if enhanced_onboarding_score >= 65 else
                    21 if enhanced_onboarding_score >= 55 else
                    30
                )
                
                # Predecir probabilidad de √©xito
                success_probability = min(98, enhanced_onboarding_score * 1.15)
                
                # Identificar riesgos mejorados
                enhanced_onboarding_risks = []
                if enhanced_onboarding_factors["technical_readiness"] < 50:
                    enhanced_onboarding_risks.append("technical_complexity")
                if enhanced_onboarding_factors["engagement_level"] < 50:
                    enhanced_onboarding_risks.append("low_engagement")
                if enhanced_onboarding_factors["cultural_fit"] < 50:
                    enhanced_onboarding_risks.append("cultural_barriers")
                if enhanced_onboarding_factors["communication_quality"] < 50:
                    enhanced_onboarding_risks.append("communication_gaps")
                
                enhanced_onboarding_success_prediction = {
                    "enhanced_onboarding_score": round(enhanced_onboarding_score, 2),
                    "enhanced_onboarding_factors": {k: round(v, 2) for k, v in enhanced_onboarding_factors.items()},
                    "predicted_onboarding_time_days": onboarding_time_days,
                    "success_probability": round(success_probability, 2),
                    "enhanced_onboarding_risks": enhanced_onboarding_risks,
                    "onboarding_readiness_level": (
                        "very_high" if enhanced_onboarding_score >= 85 else
                        "high" if enhanced_onboarding_score >= 75 else
                        "medium" if enhanced_onboarding_score >= 65 else
                        "low" if enhanced_onboarding_score >= 55 else
                        "very_low"
                    ),
                    "enhanced_onboarding_recommendations": [
                        rec for rec in [
                            "provide_premium_onboarding" if enhanced_onboarding_score >= 75 else None,
                            "assign_dedicated_onboarding_manager" if company_size in ["enterprise", "large"] else None,
                            "create_custom_onboarding_path" if enhanced_onboarding_score >= 65 else None,
                            "offer_technical_support" if enhanced_onboarding_factors["technical_readiness"] < 60 else None,
                            "increase_engagement_programs" if enhanced_onboarding_factors["engagement_level"] < 60 else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "enhanced_onboarding_success_prediction_completed",
                    email=email,
                    enhanced_onboarding_score=enhanced_onboarding_score,
                    success_probability=success_probability,
                    predicted_onboarding_time_days=onboarding_time_days
                )
            except Exception as e:
                logger.warning("enhanced_onboarding_success_prediction_error", email=email, error=str(e))
            
            # 72. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO DE ADOPCI√ìN MEJORADO
            enhanced_adoption_success_prediction = {}
            try:
                # Factores de √©xito de adopci√≥n mejorado
                enhanced_adoption_factors = {
                    "product_fit": product_fit_score,
                    "technical_readiness": ultra_enhanced_integration_fit_scoring.get("total_integration_fit_score", 0) if ultra_enhanced_integration_fit_scoring else 0,
                    "company_size_factor": (
                        25 if company_size in ["enterprise", "large"] else
                        20 if company_size == "medium" else
                        15
                    ),
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0,
                    "support_quality": enhanced_data_quality_scoring.get("quality_factors", {}).get("validity", 50) if enhanced_data_quality_scoring else 50,
                    "onboarding_success": enhanced_onboarding_success_prediction.get("enhanced_onboarding_score", 0) if enhanced_onboarding_success_prediction else 0,
                    "tech_adoption": technology_adoption_prediction.get("tech_adoption_score", 0) if technology_adoption_prediction else 0
                }
                
                # Calcular score de √©xito de adopci√≥n mejorado
                enhanced_adoption_score = sum(enhanced_adoption_factors.values()) / len(enhanced_adoption_factors)
                
                # Predecir tiempo de adopci√≥n
                adoption_time_days = (
                    7 if enhanced_adoption_score >= 85 else
                    14 if enhanced_adoption_score >= 75 else
                    30 if enhanced_adoption_score >= 65 else
                    60 if enhanced_adoption_score >= 55 else
                    90
                )
                
                # Predecir profundidad de adopci√≥n
                adoption_depth = (
                    "full" if enhanced_adoption_score >= 85 else
                    "moderate" if enhanced_adoption_score >= 65 else
                    "limited"
                )
                
                enhanced_adoption_success_prediction = {
                    "enhanced_adoption_score": round(enhanced_adoption_score, 2),
                    "enhanced_adoption_factors": {k: round(v, 2) for k, v in enhanced_adoption_factors.items()},
                    "predicted_adoption_time_days": adoption_time_days,
                    "predicted_adoption_depth": adoption_depth,
                    "adoption_readiness_level": (
                        "very_high" if enhanced_adoption_score >= 85 else
                        "high" if enhanced_adoption_score >= 75 else
                        "medium" if enhanced_adoption_score >= 65 else
                        "low" if enhanced_adoption_score >= 55 else
                        "very_low"
                    ),
                    "enhanced_adoption_recommendations": [
                        rec for rec in [
                            "provide_comprehensive_training" if enhanced_adoption_score >= 75 else None,
                            "offer_advanced_features_early" if enhanced_adoption_score >= 85 else None,
                            "create_adoption_milestones" if enhanced_adoption_score >= 65 else None,
                            "assign_adoption_coach" if company_size in ["enterprise", "large"] else None,
                            "increase_support_resources" if enhanced_adoption_factors["support_quality"] < 60 else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "enhanced_adoption_success_prediction_completed",
                    email=email,
                    enhanced_adoption_score=enhanced_adoption_score,
                    predicted_adoption_time_days=adoption_time_days,
                    adoption_depth=adoption_depth
                )
            except Exception as e:
                logger.warning("enhanced_adoption_success_prediction_error", email=email, error=str(e))
            
            # 73. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO DE IMPLEMENTACI√ìN MEJORADO
            enhanced_implementation_success_prediction = {}
            try:
                # Factores de √©xito de implementaci√≥n mejorado
                enhanced_implementation_factors = {
                    "technical_readiness": ultra_enhanced_integration_fit_scoring.get("total_integration_fit_score", 0) if ultra_enhanced_integration_fit_scoring else 0,
                    "product_fit": product_fit_score,
                    "company_size_factor": (
                        25 if company_size in ["enterprise", "large"] else
                        20 if company_size == "medium" else
                        15
                    ),
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0,
                    "communication_quality": 70 if advanced_communication_pattern_analysis.get("communication_complexity") == "high" else 50 if advanced_communication_pattern_analysis else 50,
                    "support_quality": enhanced_data_quality_scoring.get("quality_factors", {}).get("validity", 50) if enhanced_data_quality_scoring else 50,
                    "onboarding_success": enhanced_onboarding_success_prediction.get("enhanced_onboarding_score", 0) if enhanced_onboarding_success_prediction else 0
                }
                
                # Calcular score de √©xito de implementaci√≥n mejorado
                enhanced_implementation_score = sum(enhanced_implementation_factors.values()) / len(enhanced_implementation_factors)
                
                # Predecir tiempo de implementaci√≥n
                implementation_time_days = (
                    10 if enhanced_implementation_score >= 85 else
                    14 if enhanced_implementation_score >= 75 else
                    21 if enhanced_implementation_score >= 65 else
                    30 if enhanced_implementation_score >= 55 else
                    45
                )
                
                # Predecir probabilidad de √©xito
                success_probability = min(98, enhanced_implementation_score * 1.15)
                
                # Identificar riesgos mejorados
                enhanced_implementation_risks = []
                if enhanced_implementation_factors["technical_readiness"] < 50:
                    enhanced_implementation_risks.append("technical_challenges")
                if enhanced_implementation_factors["product_fit"] < 60:
                    enhanced_implementation_risks.append("product_fit_issues")
                if enhanced_implementation_factors["engagement_level"] < 50:
                    enhanced_implementation_risks.append("low_engagement")
                if enhanced_implementation_factors["cultural_fit"] < 50:
                    enhanced_implementation_risks.append("cultural_mismatch")
                
                enhanced_implementation_success_prediction = {
                    "enhanced_implementation_score": round(enhanced_implementation_score, 2),
                    "enhanced_implementation_factors": {k: round(v, 2) for k, v in enhanced_implementation_factors.items()},
                    "predicted_implementation_time_days": implementation_time_days,
                    "success_probability": round(success_probability, 2),
                    "enhanced_implementation_risks": enhanced_implementation_risks,
                    "implementation_readiness_level": (
                        "very_high" if enhanced_implementation_score >= 85 else
                        "high" if enhanced_implementation_score >= 75 else
                        "medium" if enhanced_implementation_score >= 65 else
                        "low" if enhanced_implementation_score >= 55 else
                        "very_low"
                    ),
                    "enhanced_implementation_recommendations": [
                        rec for rec in [
                            "provide_premium_implementation" if enhanced_implementation_score >= 75 else None,
                            "assign_dedicated_implementation_manager" if company_size in ["enterprise", "large"] else None,
                            "create_custom_implementation_plan" if enhanced_implementation_score >= 65 else None,
                            "offer_technical_support" if enhanced_implementation_factors["technical_readiness"] < 60 else None,
                            "increase_engagement_programs" if enhanced_implementation_factors["engagement_level"] < 60 else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "enhanced_implementation_success_prediction_completed",
                    email=email,
                    enhanced_implementation_score=enhanced_implementation_score,
                    success_probability=success_probability,
                    predicted_implementation_time_days=implementation_time_days
                )
            except Exception as e:
                logger.warning("enhanced_implementation_success_prediction_error", email=email, error=str(e))
            
            # 74. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO DE SATISFACCI√ìN MEJORADO
            enhanced_satisfaction_success_prediction = {}
            try:
                # Factores de √©xito de satisfacci√≥n mejorado
                enhanced_satisfaction_factors = {
                    "product_fit": product_fit_score,
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0,
                    "engagement_quality": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "communication_quality": 70 if advanced_communication_pattern_analysis.get("communication_complexity") == "high" else 50 if advanced_communication_pattern_analysis else 50,
                    "support_quality": enhanced_data_quality_scoring.get("quality_factors", {}).get("validity", 50) if enhanced_data_quality_scoring else 50,
                    "onboarding_success": enhanced_onboarding_success_prediction.get("enhanced_onboarding_score", 0) if enhanced_onboarding_success_prediction else 0,
                    "adoption_success": enhanced_adoption_success_prediction.get("enhanced_adoption_score", 0) if enhanced_adoption_success_prediction else 0,
                    "value_realization": min(100, (estimated_ltv / 10000) * 10) if estimated_ltv > 0 else 0
                }
                
                # Calcular score de √©xito de satisfacci√≥n mejorado
                enhanced_satisfaction_score = sum(enhanced_satisfaction_factors.values()) / len(enhanced_satisfaction_factors)
                
                # Predecir satisfacci√≥n futura
                future_satisfaction_1m = min(100, enhanced_satisfaction_score * 1.05)
                future_satisfaction_3m = min(100, enhanced_satisfaction_score * 1.1)
                future_satisfaction_6m = min(100, enhanced_satisfaction_score * 1.15)
                future_satisfaction_12m = min(100, enhanced_satisfaction_score * 1.2)
                
                # Tendencia de satisfacci√≥n
                satisfaction_trend = (
                    "improving" if future_satisfaction_12m > enhanced_satisfaction_score + 10 else
                    "stable" if abs(future_satisfaction_12m - enhanced_satisfaction_score) <= 10 else
                    "declining"
                )
                
                enhanced_satisfaction_success_prediction = {
                    "enhanced_satisfaction_score": round(enhanced_satisfaction_score, 2),
                    "enhanced_satisfaction_factors": {k: round(v, 2) for k, v in enhanced_satisfaction_factors.items()},
                    "future_satisfaction_projection": {
                        "1_month": round(future_satisfaction_1m, 2),
                        "3_months": round(future_satisfaction_3m, 2),
                        "6_months": round(future_satisfaction_6m, 2),
                        "12_months": round(future_satisfaction_12m, 2)
                    },
                    "satisfaction_trend": satisfaction_trend,
                    "satisfaction_level": (
                        "very_high" if enhanced_satisfaction_score >= 85 else
                        "high" if enhanced_satisfaction_score >= 75 else
                        "medium" if enhanced_satisfaction_score >= 65 else
                        "low" if enhanced_satisfaction_score >= 55 else
                        "very_low"
                    ),
                    "enhanced_satisfaction_recommendations": [
                        rec for rec in [
                            "maintain_high_quality_service" if enhanced_satisfaction_score >= 85 else None,
                            "improve_product_fit" if enhanced_satisfaction_factors["product_fit"] < 70 else None,
                            "enhance_cultural_alignment" if enhanced_satisfaction_factors["cultural_fit"] < 70 else None,
                            "increase_engagement" if enhanced_satisfaction_factors["engagement_quality"] < 60 else None,
                            "improve_support_quality" if enhanced_satisfaction_factors["support_quality"] < 60 else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "enhanced_satisfaction_success_prediction_completed",
                    email=email,
                    enhanced_satisfaction_score=enhanced_satisfaction_score,
                    satisfaction_trend=satisfaction_trend
                )
            except Exception as e:
                logger.warning("enhanced_satisfaction_success_prediction_error", email=email, error=str(e))
            
            # 75. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO GENERAL MEJORADO
            enhanced_overall_success_prediction = {}
            try:
                # Factores de √©xito general mejorado
                enhanced_overall_factors = {
                    "satisfaction": enhanced_satisfaction_success_prediction.get("enhanced_satisfaction_score", 0) if enhanced_satisfaction_success_prediction else 0,
                    "adoption": enhanced_adoption_success_prediction.get("enhanced_adoption_score", 0) if enhanced_adoption_success_prediction else 0,
                    "implementation": enhanced_implementation_success_prediction.get("enhanced_implementation_score", 0) if enhanced_implementation_success_prediction else 0,
                    "onboarding": enhanced_onboarding_success_prediction.get("enhanced_onboarding_score", 0) if enhanced_onboarding_success_prediction else 0,
                    "renewal": renewal_success_prediction.get("renewal_success_score", 0) if renewal_success_prediction else 0,
                    "expansion": enhanced_expansion_prediction.get("expansion_score", 0) if enhanced_expansion_prediction else 0,
                    "retention": advanced_retention_churn_analysis.get("retention_score", 0) if advanced_retention_churn_analysis else 0,
                    "engagement": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0
                }
                
                # Calcular score de √©xito general mejorado
                enhanced_overall_score = sum(enhanced_overall_factors.values()) / len(enhanced_overall_factors)
                
                # Clasificar nivel de √©xito
                enhanced_success_level = (
                    "exceptional" if enhanced_overall_score >= 90 else
                    "excellent" if enhanced_overall_score >= 85 else
                    "very_good" if enhanced_overall_score >= 75 else
                    "good" if enhanced_overall_score >= 65 else
                    "fair" if enhanced_overall_score >= 55 else
                    "poor" if enhanced_overall_score >= 45 else
                    "very_poor"
                )
                
                # Identificar fortalezas mejoradas
                enhanced_strengths = [
                    factor for factor, score in enhanced_overall_factors.items()
                    if score >= 80
                ]
                
                # Identificar √°reas de mejora mejoradas
                enhanced_improvement_areas = [
                    factor for factor, score in enhanced_overall_factors.items()
                    if score < 60
                ]
                
                enhanced_overall_success_prediction = {
                    "enhanced_overall_success_score": round(enhanced_overall_score, 2),
                    "enhanced_overall_factors": {k: round(v, 2) for k, v in enhanced_overall_factors.items()},
                    "enhanced_success_level": enhanced_success_level,
                    "enhanced_strengths": enhanced_strengths,
                    "enhanced_improvement_areas": enhanced_improvement_areas,
                    "enhanced_success_recommendations": [
                        rec for rec in [
                            f"improve_{area}" for area in enhanced_improvement_areas[:3]
                        ] if rec
                    ],
                    "enhanced_customer_health_score": round(enhanced_overall_score, 2),
                    "enhanced_customer_health_status": (
                        "very_healthy" if enhanced_overall_score >= 85 else
                        "healthy" if enhanced_overall_score >= 75 else
                        "at_risk" if enhanced_overall_score >= 65 else
                        "critical"
                    )
                }
                
                logger.info(
                    "enhanced_overall_success_prediction_completed",
                    email=email,
                    enhanced_overall_success_score=enhanced_overall_score,
                    enhanced_success_level=enhanced_success_level,
                    enhanced_customer_health_status=enhanced_overall_success_prediction.get("enhanced_customer_health_status")
                )
            except Exception as e:
                logger.warning("enhanced_overall_success_prediction_error", email=email, error=str(e))
            

            # 76. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO DE CONVERSI√ìN MEJORADO
            enhanced_conversion_success_prediction = {}
            try:
                # Factores de √©xito de conversi√≥n mejorado
                enhanced_conversion_factors = {
                    "conversion_probability": conversion_probability,
                    "ml_intent_score": ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0,
                    "product_fit": product_fit_score,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "budget_availability": len(budget_signals) * 20 if budget_signals else 0,
                    "decision_timing": 100 - (decision_timing_prediction.get("predicted_decision_days", 90) / 90 * 100) if decision_timing_prediction else 50,
                    "competitive_advantage": 100 - (deep_competitive_analysis.get("competitive_risk_score", 0) if deep_competitive_analysis else 0),
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0,
                    "sales_success": enhanced_sales_success_prediction.get("sales_success_score", 0) if enhanced_sales_success_prediction else 0
                }
                
                # Calcular score de √©xito de conversi√≥n mejorado
                enhanced_conversion_score = sum(enhanced_conversion_factors.values()) / len(enhanced_conversion_factors)
                
                # Predecir probabilidad de conversi√≥n mejorada
                enhanced_conversion_probability = min(98, enhanced_conversion_score * 1.15)
                
                # Predecir tiempo hasta conversi√≥n
                days_to_conversion = (
                    7 if enhanced_conversion_score >= 85 else
                    14 if enhanced_conversion_score >= 75 else
                    30 if enhanced_conversion_score >= 65 else
                    60 if enhanced_conversion_score >= 55 else
                    90
                )
                
                # Identificar factores cr√≠ticos
                critical_conversion_factors = [
                    factor for factor, score in enhanced_conversion_factors.items()
                    if score >= 75
                ]
                
                # Identificar barreras mejoradas
                enhanced_conversion_barriers = []
                if enhanced_conversion_factors["budget_availability"] < 30:
                    enhanced_conversion_barriers.append("budget_constraints")
                if enhanced_conversion_factors["competitive_advantage"] < 50:
                    enhanced_conversion_barriers.append("competitive_pressure")
                if enhanced_conversion_factors["decision_timing"] < 40:
                    enhanced_conversion_barriers.append("timing_issues")
                if enhanced_conversion_factors["product_fit"] < 60:
                    enhanced_conversion_barriers.append("product_fit_concerns")
                
                enhanced_conversion_success_prediction = {
                    "enhanced_conversion_score": round(enhanced_conversion_score, 2),
                    "enhanced_conversion_factors": {k: round(v, 2) for k, v in enhanced_conversion_factors.items()},
                    "enhanced_conversion_probability": round(enhanced_conversion_probability, 2),
                    "predicted_days_to_conversion": days_to_conversion,
                    "critical_conversion_factors": critical_conversion_factors,
                    "enhanced_conversion_barriers": enhanced_conversion_barriers,
                    "conversion_success_level": (
                        "very_high" if enhanced_conversion_score >= 85 else
                        "high" if enhanced_conversion_score >= 75 else
                        "medium" if enhanced_conversion_score >= 65 else
                        "low" if enhanced_conversion_score >= 55 else
                        "very_low"
                    ),
                    "enhanced_conversion_acceleration_opportunities": [
                        opp for opp in [
                            "address_budget_concerns" if "budget_constraints" in enhanced_conversion_barriers else None,
                            "competitive_differentiation" if "competitive_pressure" in enhanced_conversion_barriers else None,
                            "optimize_timing" if "timing_issues" in enhanced_conversion_barriers else None,
                            "improve_product_fit" if "product_fit_concerns" in enhanced_conversion_barriers else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "enhanced_conversion_success_prediction_completed",
                    email=email,
                    enhanced_conversion_score=enhanced_conversion_score,
                    enhanced_conversion_probability=enhanced_conversion_probability,
                    predicted_days_to_conversion=days_to_conversion
                )
            except Exception as e:
                logger.warning("enhanced_conversion_success_prediction_error", email=email, error=str(e))
            
            # 77. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO DE RETENCI√ìN MEJORADO
            enhanced_retention_success_prediction = {}
            try:
                # Factores de √©xito de retenci√≥n mejorado
                enhanced_retention_factors = {
                    "satisfaction": enhanced_satisfaction_success_prediction.get("enhanced_satisfaction_score", 0) if enhanced_satisfaction_success_prediction else 0,
                    "product_fit": product_fit_score,
                    "engagement_quality": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0,
                    "competitive_advantage": 100 - (deep_competitive_analysis.get("competitive_risk_score", 0) if deep_competitive_analysis else 0),
                    "support_quality": enhanced_data_quality_scoring.get("quality_factors", {}).get("validity", 50) if enhanced_data_quality_scoring else 50,
                    "adoption_success": enhanced_adoption_success_prediction.get("enhanced_adoption_score", 0) if enhanced_adoption_success_prediction else 0,
                    "value_realization": min(100, (estimated_ltv / 10000) * 10) if estimated_ltv > 0 else 0
                }
                
                # Calcular score de √©xito de retenci√≥n mejorado
                enhanced_retention_score = sum(enhanced_retention_factors.values()) / len(enhanced_retention_factors)
                
                # Predecir probabilidad de retenci√≥n
                enhanced_retention_probability = min(98, enhanced_retention_score * 1.15)
                
                # Predecir tiempo de retenci√≥n
                retention_period_months = (
                    36 if enhanced_retention_score >= 85 else
                    24 if enhanced_retention_score >= 75 else
                    18 if enhanced_retention_score >= 65 else
                    12 if enhanced_retention_score >= 55 else
                    6
                )
                
                # Identificar factores cr√≠ticos
                critical_retention_factors = [
                    factor for factor, score in enhanced_retention_factors.items()
                    if score >= 75
                ]
                
                # Identificar riesgos de retenci√≥n
                enhanced_retention_risks = []
                if enhanced_retention_factors["satisfaction"] < 60:
                    enhanced_retention_risks.append("low_satisfaction")
                if enhanced_retention_factors["product_fit"] < 60:
                    enhanced_retention_risks.append("poor_product_fit")
                if enhanced_retention_factors["competitive_advantage"] < 50:
                    enhanced_retention_risks.append("competitive_pressure")
                if enhanced_retention_factors["adoption_success"] < 50:
                    enhanced_retention_risks.append("low_adoption")
                
                enhanced_retention_success_prediction = {
                    "enhanced_retention_score": round(enhanced_retention_score, 2),
                    "enhanced_retention_factors": {k: round(v, 2) for k, v in enhanced_retention_factors.items()},
                    "enhanced_retention_probability": round(enhanced_retention_probability, 2),
                    "predicted_retention_period_months": retention_period_months,
                    "critical_retention_factors": critical_retention_factors,
                    "enhanced_retention_risks": enhanced_retention_risks,
                    "retention_success_level": (
                        "very_high" if enhanced_retention_score >= 85 else
                        "high" if enhanced_retention_score >= 75 else
                        "medium" if enhanced_retention_score >= 65 else
                        "low" if enhanced_retention_score >= 55 else
                        "very_low"
                    ),
                    "enhanced_retention_strategies": [
                        strategy for strategy in [
                            "improve_satisfaction" if "low_satisfaction" in enhanced_retention_risks else None,
                            "enhance_product_fit" if "poor_product_fit" in enhanced_retention_risks else None,
                            "competitive_differentiation" if "competitive_pressure" in enhanced_retention_risks else None,
                            "increase_adoption" if "low_adoption" in enhanced_retention_risks else None
                        ] if strategy
                    ]
                }
                
                logger.info(
                    "enhanced_retention_success_prediction_completed",
                    email=email,
                    enhanced_retention_score=enhanced_retention_score,
                    enhanced_retention_probability=enhanced_retention_probability,
                    predicted_retention_period_months=retention_period_months
                )
            except Exception as e:
                logger.warning("enhanced_retention_success_prediction_error", email=email, error=str(e))
            
            # 78. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO DE EXPANSI√ìN MEJORADO
            enhanced_expansion_success_prediction = {}
            try:
                # Factores de √©xito de expansi√≥n mejorado
                enhanced_expansion_factors = {
                    "current_satisfaction": enhanced_satisfaction_success_prediction.get("enhanced_satisfaction_score", 0) if enhanced_satisfaction_success_prediction else 0,
                    "product_adoption": enhanced_adoption_success_prediction.get("enhanced_adoption_score", 0) if enhanced_adoption_success_prediction else 0,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "company_growth": market_fit_analysis.get("growth_potential_score", 0) if market_fit_analysis else 0,
                    "product_fit": product_fit_score,
                    "budget_availability": len(budget_signals) * 25 if budget_signals else 0,
                    "upselling_readiness": advanced_upselling_crossselling_prediction.get("upselling_readiness_score", 0) if advanced_upselling_crossselling_prediction else 0,
                    "cross_selling_readiness": advanced_upselling_crossselling_prediction.get("cross_selling_readiness_score", 0) if advanced_upselling_crossselling_prediction else 0
                }
                
                # Calcular score de √©xito de expansi√≥n mejorado
                enhanced_expansion_score = sum(enhanced_expansion_factors.values()) / len(enhanced_expansion_factors)
                
                # Predecir probabilidad de expansi√≥n
                enhanced_expansion_probability = min(98, enhanced_expansion_score * 1.15)
                
                # Predecir valor de expansi√≥n
                base_expansion_value = estimated_ltv * 0.35
                expansion_multiplier = enhanced_expansion_score / 100
                predicted_expansion_value = base_expansion_value * expansion_multiplier
                
                # Predecir tiempo hasta expansi√≥n
                days_to_expansion = (
                    30 if enhanced_expansion_score >= 85 else
                    60 if enhanced_expansion_score >= 75 else
                    90 if enhanced_expansion_score >= 65 else
                    180 if enhanced_expansion_score >= 55 else
                    365
                )
                
                # Tipo de expansi√≥n m√°s probable
                expansion_type = (
                    "upsell" if enhanced_expansion_factors["upselling_readiness"] >= enhanced_expansion_factors["cross_selling_readiness"] else
                    "cross_sell"
                )
                
                enhanced_expansion_success_prediction = {
                    "enhanced_expansion_score": round(enhanced_expansion_score, 2),
                    "enhanced_expansion_factors": {k: round(v, 2) for k, v in enhanced_expansion_factors.items()},
                    "enhanced_expansion_probability": round(enhanced_expansion_probability, 2),
                    "predicted_expansion_value": round(predicted_expansion_value, 2),
                    "predicted_days_to_expansion": days_to_expansion,
                    "expansion_type": expansion_type,
                    "expansion_success_level": (
                        "very_high" if enhanced_expansion_score >= 85 else
                        "high" if enhanced_expansion_score >= 75 else
                        "medium" if enhanced_expansion_score >= 65 else
                        "low" if enhanced_expansion_score >= 55 else
                        "very_low"
                    ),
                    "enhanced_expansion_opportunities": [
                        opp for opp in [
                            "upsell_enterprise_plan" if enhanced_expansion_factors["upselling_readiness"] >= 65 else None,
                            "cross_sell_addons" if enhanced_expansion_factors["cross_selling_readiness"] >= 65 else None,
                            "expand_usage" if enhanced_expansion_factors["product_adoption"] >= 75 else None,
                            "leverage_growth" if enhanced_expansion_factors["company_growth"] >= 75 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "enhanced_expansion_success_prediction_completed",
                    email=email,
                    enhanced_expansion_score=enhanced_expansion_score,
                    enhanced_expansion_probability=enhanced_expansion_probability,
                    predicted_expansion_value=predicted_expansion_value
                )
            except Exception as e:
                logger.warning("enhanced_expansion_success_prediction_error", email=email, error=str(e))
            
            # 79. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO DE REFERIDOS MEJORADO
            enhanced_referral_success_prediction = {}
            try:
                # Factores de √©xito de referidos mejorado
                enhanced_referral_factors = {
                    "satisfaction_level": enhanced_satisfaction_success_prediction.get("enhanced_satisfaction_score", 0) if enhanced_satisfaction_success_prediction else 0,
                    "social_influence": advanced_social_influence_analysis.get("total_influence_score", 0) if advanced_social_influence_analysis else 0,
                    "network_size": network_analysis.get("network_size", 0) if network_analysis else 0,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "company_size_factor": (
                        35 if company_size in ["enterprise", "large"] else
                        25 if company_size == "medium" else
                        15
                    ),
                    "industry_influence": (
                        30 if industry in ["technology", "finance", "consulting"] else
                        20 if industry in ["healthcare", "education"] else
                        15
                    ),
                    "value_realization": min(100, (estimated_ltv / 10000) * 10) if estimated_ltv > 0 else 0,
                    "referral_potential": enhanced_referral_value_prediction.get("referral_potential_score", 0) if enhanced_referral_value_prediction else 0
                }
                
                # Calcular score de √©xito de referidos mejorado
                enhanced_referral_score = sum(enhanced_referral_factors.values()) / len(enhanced_referral_factors)
                
                # Predecir probabilidad de referidos
                enhanced_referral_probability = min(98, enhanced_referral_score * 1.15)
                
                # Predecir n√∫mero de referidos
                predicted_referrals = (
                    25 if enhanced_referral_score >= 85 else
                    20 if enhanced_referral_score >= 75 else
                    15 if enhanced_referral_score >= 65 else
                    10 if enhanced_referral_score >= 55 else
                    5 if enhanced_referral_score >= 45 else
                    2
                )
                
                # Predecir tiempo hasta primer referido
                days_to_first_referral = (
                    30 if enhanced_referral_score >= 85 else
                    60 if enhanced_referral_score >= 75 else
                    90 if enhanced_referral_score >= 65 else
                    180 if enhanced_referral_score >= 55 else
                    365
                )
                
                enhanced_referral_success_prediction = {
                    "enhanced_referral_score": round(enhanced_referral_score, 2),
                    "enhanced_referral_factors": {k: round(v, 2) for k, v in enhanced_referral_factors.items()},
                    "enhanced_referral_probability": round(enhanced_referral_probability, 2),
                    "predicted_referrals": predicted_referrals,
                    "predicted_days_to_first_referral": days_to_first_referral,
                    "referral_success_level": (
                        "very_high" if enhanced_referral_score >= 85 else
                        "high" if enhanced_referral_score >= 75 else
                        "medium" if enhanced_referral_score >= 65 else
                        "low" if enhanced_referral_score >= 55 else
                        "very_low"
                    ),
                    "enhanced_referral_program_recommendations": [
                        rec for rec in [
                            "implement_aggressive_referral_program" if enhanced_referral_score >= 75 else None,
                            "offer_premium_referral_incentives" if enhanced_referral_score >= 65 else None,
                            "create_referral_content" if enhanced_referral_score >= 55 else None,
                            "leverage_social_influence" if enhanced_referral_factors["social_influence"] >= 65 else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "enhanced_referral_success_prediction_completed",
                    email=email,
                    enhanced_referral_score=enhanced_referral_score,
                    enhanced_referral_probability=enhanced_referral_probability,
                    predicted_referrals=predicted_referrals
                )
            except Exception as e:
                logger.warning("enhanced_referral_success_prediction_error", email=email, error=str(e))
            
            # 80. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO ULTRA MEJORADO
            ultra_enhanced_success_prediction = {}
            try:
                # Factores de √©xito ultra mejorado
                ultra_enhanced_factors = {
                    "conversion": enhanced_conversion_success_prediction.get("enhanced_conversion_score", 0) if enhanced_conversion_success_prediction else 0,
                    "retention": enhanced_retention_success_prediction.get("enhanced_retention_score", 0) if enhanced_retention_success_prediction else 0,
                    "expansion": enhanced_expansion_success_prediction.get("enhanced_expansion_score", 0) if enhanced_expansion_success_prediction else 0,
                    "referral": enhanced_referral_success_prediction.get("enhanced_referral_score", 0) if enhanced_referral_success_prediction else 0,
                    "satisfaction": enhanced_satisfaction_success_prediction.get("enhanced_satisfaction_score", 0) if enhanced_satisfaction_success_prediction else 0,
                    "adoption": enhanced_adoption_success_prediction.get("enhanced_adoption_score", 0) if enhanced_adoption_success_prediction else 0,
                    "implementation": enhanced_implementation_success_prediction.get("enhanced_implementation_score", 0) if enhanced_implementation_success_prediction else 0,
                    "onboarding": enhanced_onboarding_success_prediction.get("enhanced_onboarding_score", 0) if enhanced_onboarding_success_prediction else 0
                }
                
                # Calcular score de √©xito ultra mejorado
                ultra_enhanced_score = sum(ultra_enhanced_factors.values()) / len(ultra_enhanced_factors)
                
                # Clasificar nivel de √©xito ultra
                ultra_success_level = (
                    "exceptional" if ultra_enhanced_score >= 95 else
                    "excellent" if ultra_enhanced_score >= 90 else
                    "very_good" if ultra_enhanced_score >= 85 else
                    "good" if ultra_enhanced_score >= 75 else
                    "fair" if ultra_enhanced_score >= 65 else
                    "poor" if ultra_enhanced_score >= 55 else
                    "very_poor"
                )
                
                # Identificar fortalezas ultra
                ultra_strengths = [
                    factor for factor, score in ultra_enhanced_factors.items()
                    if score >= 85
                ]
                
                # Identificar √°reas de mejora ultra
                ultra_improvement_areas = [
                    factor for factor, score in ultra_enhanced_factors.items()
                    if score < 65
                ]
                
                ultra_enhanced_success_prediction = {
                    "ultra_enhanced_success_score": round(ultra_enhanced_score, 2),
                    "ultra_enhanced_factors": {k: round(v, 2) for k, v in ultra_enhanced_factors.items()},
                    "ultra_success_level": ultra_success_level,
                    "ultra_strengths": ultra_strengths,
                    "ultra_improvement_areas": ultra_improvement_areas,
                    "ultra_success_recommendations": [
                        rec for rec in [
                            f"improve_{area}" for area in ultra_improvement_areas[:3]
                        ] if rec
                    ],
                    "ultra_customer_health_score": round(ultra_enhanced_score, 2),
                    "ultra_customer_health_status": (
                        "very_healthy" if ultra_enhanced_score >= 90 else
                        "healthy" if ultra_enhanced_score >= 85 else
                        "at_risk" if ultra_enhanced_score >= 75 else
                        "critical"
                    )
                }
                
                logger.info(
                    "ultra_enhanced_success_prediction_completed",
                    email=email,
                    ultra_enhanced_success_score=ultra_enhanced_score,
                    ultra_success_level=ultra_success_level,
                    ultra_customer_health_status=ultra_enhanced_success_prediction.get("ultra_customer_health_status")
                )
            except Exception as e:
                logger.warning("ultra_enhanced_success_prediction_error", email=email, error=str(e))
            

            # 81. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO DE VENTA ULTRA MEJORADO
            ultra_enhanced_sales_success_prediction = {}
            try:
                # Factores de √©xito de venta ultra mejorado
                ultra_sales_factors = {
                    "conversion_probability": conversion_probability,
                    "ml_intent_score": ml_purchase_intent_scoring.get("ml_intent_score", 0) if ml_purchase_intent_scoring else 0,
                    "product_fit": product_fit_score,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "budget_availability": len(budget_signals) * 25 if budget_signals else 0,
                    "decision_timing": 100 - (decision_timing_prediction.get("predicted_decision_days", 90) / 90 * 100) if decision_timing_prediction else 50,
                    "competitive_advantage": 100 - (deep_competitive_analysis.get("competitive_risk_score", 0) if deep_competitive_analysis else 0),
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0,
                    "sales_success": enhanced_sales_success_prediction.get("sales_success_score", 0) if enhanced_sales_success_prediction else 0,
                    "conversion_success": enhanced_conversion_success_prediction.get("enhanced_conversion_score", 0) if enhanced_conversion_success_prediction else 0
                }
                
                # Calcular score de √©xito de venta ultra mejorado
                ultra_sales_score = sum(ultra_sales_factors.values()) / len(ultra_sales_factors)
                
                # Predecir probabilidad de cierre ultra
                ultra_close_probability = min(99, ultra_sales_score * 1.2)
                
                # Predecir tiempo hasta cierre ultra
                ultra_days_to_close = (
                    7 if ultra_sales_score >= 90 else
                    14 if ultra_sales_score >= 85 else
                    21 if ultra_sales_score >= 75 else
                    30 if ultra_sales_score >= 65 else
                    60
                )
                
                # Identificar factores cr√≠ticos ultra
                ultra_critical_factors = [
                    factor for factor, score in ultra_sales_factors.items()
                    if score >= 80
                ]
                
                ultra_enhanced_sales_success_prediction = {
                    "ultra_sales_score": round(ultra_sales_score, 2),
                    "ultra_sales_factors": {k: round(v, 2) for k, v in ultra_sales_factors.items()},
                    "ultra_close_probability": round(ultra_close_probability, 2),
                    "ultra_predicted_days_to_close": ultra_days_to_close,
                    "ultra_critical_factors": ultra_critical_factors,
                    "ultra_sales_success_level": (
                        "exceptional" if ultra_sales_score >= 90 else
                        "excellent" if ultra_sales_score >= 85 else
                        "very_high" if ultra_sales_score >= 75 else
                        "high" if ultra_sales_score >= 65 else
                        "medium" if ultra_sales_score >= 55 else
                        "low"
                    ),
                    "ultra_sales_acceleration_opportunities": [
                        opp for opp in [
                            "aggressive_closing" if ultra_sales_score >= 85 else None,
                            "premium_offer" if ultra_sales_score >= 75 else None,
                            "executive_sponsorship" if company_size in ["enterprise", "large"] else None,
                            "competitive_differentiation" if ultra_sales_factors["competitive_advantage"] < 70 else None
                        ] if opp
                    ]
                }
                
                logger.info(
                    "ultra_enhanced_sales_success_prediction_completed",
                    email=email,
                    ultra_sales_score=ultra_sales_score,
                    ultra_close_probability=ultra_close_probability,
                    ultra_predicted_days_to_close=ultra_days_to_close
                )
            except Exception as e:
                logger.warning("ultra_enhanced_sales_success_prediction_error", email=email, error=str(e))
            
            # 82. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO DE ONBOARDING ULTRA MEJORADO
            ultra_enhanced_onboarding_success_prediction = {}
            try:
                # Factores de √©xito de onboarding ultra mejorado
                ultra_onboarding_factors = {
                    "product_fit": product_fit_score,
                    "technical_readiness": ultra_enhanced_integration_fit_scoring.get("total_integration_fit_score", 0) if ultra_enhanced_integration_fit_scoring else 0,
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "company_size_factor": (
                        30 if company_size in ["enterprise", "large"] else
                        25 if company_size == "medium" else
                        20
                    ),
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0,
                    "communication_quality": 75 if advanced_communication_pattern_analysis.get("communication_complexity") == "high" else 55 if advanced_communication_pattern_analysis else 55,
                    "support_quality": enhanced_data_quality_scoring.get("quality_factors", {}).get("validity", 55) if enhanced_data_quality_scoring else 55,
                    "onboarding_success": enhanced_onboarding_success_prediction.get("enhanced_onboarding_score", 0) if enhanced_onboarding_success_prediction else 0
                }
                
                # Calcular score de √©xito de onboarding ultra mejorado
                ultra_onboarding_score = sum(ultra_onboarding_factors.values()) / len(ultra_onboarding_factors)
                
                # Predecir tiempo de onboarding ultra
                ultra_onboarding_time_days = (
                    3 if ultra_onboarding_score >= 90 else
                    5 if ultra_onboarding_score >= 85 else
                    7 if ultra_onboarding_score >= 75 else
                    10 if ultra_onboarding_score >= 65 else
                    14
                )
                
                # Predecir probabilidad de √©xito ultra
                ultra_onboarding_success_probability = min(99, ultra_onboarding_score * 1.2)
                
                ultra_enhanced_onboarding_success_prediction = {
                    "ultra_onboarding_score": round(ultra_onboarding_score, 2),
                    "ultra_onboarding_factors": {k: round(v, 2) for k, v in ultra_onboarding_factors.items()},
                    "ultra_predicted_onboarding_time_days": ultra_onboarding_time_days,
                    "ultra_onboarding_success_probability": round(ultra_onboarding_success_probability, 2),
                    "ultra_onboarding_readiness_level": (
                        "exceptional" if ultra_onboarding_score >= 90 else
                        "excellent" if ultra_onboarding_score >= 85 else
                        "very_high" if ultra_onboarding_score >= 75 else
                        "high" if ultra_onboarding_score >= 65 else
                        "medium"
                    ),
                    "ultra_onboarding_recommendations": [
                        rec for rec in [
                            "premium_onboarding_experience" if ultra_onboarding_score >= 85 else None,
                            "dedicated_onboarding_manager" if company_size in ["enterprise", "large"] else None,
                            "custom_onboarding_path" if ultra_onboarding_score >= 75 else None,
                            "technical_support_team" if ultra_onboarding_factors["technical_readiness"] < 70 else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "ultra_enhanced_onboarding_success_prediction_completed",
                    email=email,
                    ultra_onboarding_score=ultra_onboarding_score,
                    ultra_onboarding_success_probability=ultra_onboarding_success_probability,
                    ultra_predicted_onboarding_time_days=ultra_onboarding_time_days
                )
            except Exception as e:
                logger.warning("ultra_enhanced_onboarding_success_prediction_error", email=email, error=str(e))
            
            # 83. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO DE ADOPCI√ìN ULTRA MEJORADO
            ultra_enhanced_adoption_success_prediction = {}
            try:
                # Factores de √©xito de adopci√≥n ultra mejorado
                ultra_adoption_factors = {
                    "product_fit": product_fit_score,
                    "technical_readiness": ultra_enhanced_integration_fit_scoring.get("total_integration_fit_score", 0) if ultra_enhanced_integration_fit_scoring else 0,
                    "company_size_factor": (
                        30 if company_size in ["enterprise", "large"] else
                        25 if company_size == "medium" else
                        20
                    ),
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0,
                    "support_quality": enhanced_data_quality_scoring.get("quality_factors", {}).get("validity", 55) if enhanced_data_quality_scoring else 55,
                    "onboarding_success": ultra_enhanced_onboarding_success_prediction.get("ultra_onboarding_score", 0) if ultra_enhanced_onboarding_success_prediction else 0,
                    "tech_adoption": technology_adoption_prediction.get("tech_adoption_score", 0) if technology_adoption_prediction else 0
                }
                
                # Calcular score de √©xito de adopci√≥n ultra mejorado
                ultra_adoption_score = sum(ultra_adoption_factors.values()) / len(ultra_adoption_factors)
                
                # Predecir tiempo de adopci√≥n ultra
                ultra_adoption_time_days = (
                    5 if ultra_adoption_score >= 90 else
                    7 if ultra_adoption_score >= 85 else
                    14 if ultra_adoption_score >= 75 else
                    21 if ultra_adoption_score >= 65 else
                    30
                )
                
                # Predecir profundidad de adopci√≥n ultra
                ultra_adoption_depth = (
                    "full" if ultra_adoption_score >= 90 else
                    "moderate" if ultra_adoption_score >= 75 else
                    "limited"
                )
                
                ultra_enhanced_adoption_success_prediction = {
                    "ultra_adoption_score": round(ultra_adoption_score, 2),
                    "ultra_adoption_factors": {k: round(v, 2) for k, v in ultra_adoption_factors.items()},
                    "ultra_predicted_adoption_time_days": ultra_adoption_time_days,
                    "ultra_predicted_adoption_depth": ultra_adoption_depth,
                    "ultra_adoption_readiness_level": (
                        "exceptional" if ultra_adoption_score >= 90 else
                        "excellent" if ultra_adoption_score >= 85 else
                        "very_high" if ultra_adoption_score >= 75 else
                        "high" if ultra_adoption_score >= 65 else
                        "medium"
                    ),
                    "ultra_adoption_recommendations": [
                        rec for rec in [
                            "premium_adoption_program" if ultra_adoption_score >= 85 else None,
                            "advanced_features_early_access" if ultra_adoption_score >= 90 else None,
                            "adoption_coach_assignment" if company_size in ["enterprise", "large"] else None,
                            "comprehensive_training_program" if ultra_adoption_score >= 75 else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "ultra_enhanced_adoption_success_prediction_completed",
                    email=email,
                    ultra_adoption_score=ultra_adoption_score,
                    ultra_predicted_adoption_time_days=ultra_adoption_time_days,
                    ultra_adoption_depth=ultra_adoption_depth
                )
            except Exception as e:
                logger.warning("ultra_enhanced_adoption_success_prediction_error", email=email, error=str(e))
            
            # 84. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO DE IMPLEMENTACI√ìN ULTRA MEJORADO
            ultra_enhanced_implementation_success_prediction = {}
            try:
                # Factores de √©xito de implementaci√≥n ultra mejorado
                ultra_implementation_factors = {
                    "technical_readiness": ultra_enhanced_integration_fit_scoring.get("total_integration_fit_score", 0) if ultra_enhanced_integration_fit_scoring else 0,
                    "product_fit": product_fit_score,
                    "company_size_factor": (
                        30 if company_size in ["enterprise", "large"] else
                        25 if company_size == "medium" else
                        20
                    ),
                    "engagement_level": advanced_engagement_analysis.get("advanced_engagement_score", 0) if advanced_engagement_analysis else 0,
                    "cultural_fit": advanced_cultural_fit_analysis.get("cultural_fit_score", 0) if advanced_cultural_fit_analysis else 0,
                    "communication_quality": 75 if advanced_communication_pattern_analysis.get("communication_complexity") == "high" else 55 if advanced_communication_pattern_analysis else 55,
                    "support_quality": enhanced_data_quality_scoring.get("quality_factors", {}).get("validity", 55) if enhanced_data_quality_scoring else 55,
                    "onboarding_success": ultra_enhanced_onboarding_success_prediction.get("ultra_onboarding_score", 0) if ultra_enhanced_onboarding_success_prediction else 0
                }
                
                # Calcular score de √©xito de implementaci√≥n ultra mejorado
                ultra_implementation_score = sum(ultra_implementation_factors.values()) / len(ultra_implementation_factors)
                
                # Predecir tiempo de implementaci√≥n ultra
                ultra_implementation_time_days = (
                    7 if ultra_implementation_score >= 90 else
                    10 if ultra_implementation_score >= 85 else
                    14 if ultra_implementation_score >= 75 else
                    18 if ultra_implementation_score >= 65 else
                    21
                )
                
                # Predecir probabilidad de √©xito ultra
                ultra_implementation_success_probability = min(99, ultra_implementation_score * 1.2)
                
                ultra_enhanced_implementation_success_prediction = {
                    "ultra_implementation_score": round(ultra_implementation_score, 2),
                    "ultra_implementation_factors": {k: round(v, 2) for k, v in ultra_implementation_factors.items()},
                    "ultra_predicted_implementation_time_days": ultra_implementation_time_days,
                    "ultra_implementation_success_probability": round(ultra_implementation_success_probability, 2),
                    "ultra_implementation_readiness_level": (
                        "exceptional" if ultra_implementation_score >= 90 else
                        "excellent" if ultra_implementation_score >= 85 else
                        "very_high" if ultra_implementation_score >= 75 else
                        "high" if ultra_implementation_score >= 65 else
                        "medium"
                    ),
                    "ultra_implementation_recommendations": [
                        rec for rec in [
                            "premium_implementation_service" if ultra_implementation_score >= 85 else None,
                            "dedicated_implementation_manager" if company_size in ["enterprise", "large"] else None,
                            "custom_implementation_plan" if ultra_implementation_score >= 75 else None,
                            "technical_support_team" if ultra_implementation_factors["technical_readiness"] < 70 else None
                        ] if rec
                    ]
                }
                
                logger.info(
                    "ultra_enhanced_implementation_success_prediction_completed",
                    email=email,
                    ultra_implementation_score=ultra_implementation_score,
                    ultra_implementation_success_probability=ultra_implementation_success_probability,
                    ultra_predicted_implementation_time_days=ultra_implementation_time_days
                )
            except Exception as e:
                logger.warning("ultra_enhanced_implementation_success_prediction_error", email=email, error=str(e))
            
            # 85. AN√ÅLISIS DE PREDICCI√ìN DE √âXITO ULTRA FINAL
            final_ultra_enhanced_success_prediction = {}
            try:
                # Factores de √©xito ultra final
                final_ultra_factors = {
                    "sales": ultra_enhanced_sales_success_prediction.get("ultra_sales_score", 0) if ultra_enhanced_sales_success_prediction else 0,
                    "onboarding": ultra_enhanced_onboarding_success_prediction.get("ultra_onboarding_score", 0) if ultra_enhanced_onboarding_success_prediction else 0,
                    "adoption": ultra_enhanced_adoption_success_prediction.get("ultra_adoption_score", 0) if ultra_enhanced_adoption_success_prediction else 0,
                    "implementation": ultra_enhanced_implementation_success_prediction.get("ultra_implementation_score", 0) if ultra_enhanced_implementation_success_prediction else 0,
                    "retention": enhanced_retention_success_prediction.get("enhanced_retention_score", 0) if enhanced_retention_success_prediction else 0,
                    "expansion": enhanced_expansion_success_prediction.get("enhanced_expansion_score", 0) if enhanced_expansion_success_prediction else 0,
                    "referral": enhanced_referral_success_prediction.get("enhanced_referral_score", 0) if enhanced_referral_success_prediction else 0,
                    "satisfaction": enhanced_satisfaction_success_prediction.get("enhanced_satisfaction_score", 0) if enhanced_satisfaction_success_prediction else 0
                }
                
                # Calcular score de √©xito ultra final
                final_ultra_score = sum(final_ultra_factors.values()) / len(final_ultra_factors)
                
                # Clasificar nivel de √©xito ultra final
                final_ultra_success_level = (
                    "exceptional" if final_ultra_score >= 95 else
                    "excellent" if final_ultra_score >= 90 else
                    "very_good" if final_ultra_score >= 85 else
                    "good" if final_ultra_score >= 75 else
                    "fair" if final_ultra_score >= 65 else
                    "poor" if final_ultra_score >= 55 else
                    "very_poor"
                )
                
                # Identificar fortalezas ultra final
                final_ultra_strengths = [
                    factor for factor, score in final_ultra_factors.items()
                    if score >= 90
                ]
                
                # Identificar √°reas de mejora ultra final
                final_ultra_improvement_areas = [
                    factor for factor, score in final_ultra_factors.items()
                    if score < 70
                ]
                
                final_ultra_enhanced_success_prediction = {
                    "final_ultra_success_score": round(final_ultra_score, 2),
                    "final_ultra_factors": {k: round(v, 2) for k, v in final_ultra_factors.items()},
                    "final_ultra_success_level": final_ultra_success_level,
                    "final_ultra_strengths": final_ultra_strengths,
                    "final_ultra_improvement_areas": final_ultra_improvement_areas,
                    "final_ultra_success_recommendations": [
                        rec for rec in [
                            f"improve_{area}" for area in final_ultra_improvement_areas[:3]
                        ] if rec
                    ],
                    "final_ultra_customer_health_score": round(final_ultra_score, 2),
                    "final_ultra_customer_health_status": (
                        "very_healthy" if final_ultra_score >= 90 else
                        "healthy" if final_ultra_score >= 85 else
                        "at_risk" if fina
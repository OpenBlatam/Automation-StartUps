# üë®‚Äçüíª Gu√≠a de Desarrollo

> **Versi√≥n**: 2.0 | **√öltima actualizaci√≥n**: 2024 | **Estado**: Producci√≥n Ready ‚úÖ

Gu√≠a completa para desarrolladores que trabajan en la plataforma.

## üìã Tabla de Contenidos

- [Configuraci√≥n del Entorno](#-configuraci√≥n-del-entorno)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [Crear un Nuevo DAG de Airflow](#-crear-un-nuevo-dag-de-airflow)
- [Crear un Nuevo Workflow en Kestra](#-crear-un-nuevo-workflow-en-kestra)
- [Crear un Nuevo Worker](#-crear-un-nuevo-worker)
- [Testing](#-testing)
- [Code Review](#-code-review)
- [Mejores Pr√°cticas](#-mejores-pr√°cticas)
- [Debugging](#-debugging)
- [Troubleshooting Com√∫n](#-troubleshooting-com√∫n)

---

## üöÄ Configuraci√≥n del Entorno

### Requisitos Previos

- Python 3.11+
- Docker y Docker Compose
- Kubernetes CLI (kubectl)
- Helm 3.13+
- Terraform 1.6+
- Git

### Setup Inicial

```bash
# 1. Clonar el repositorio
git clone <repository-url>
cd IA

# 2. Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# 3. Instalar dependencias
pip install -r data/airflow/requirements.txt

# 4. Configurar variables de entorno
cp data/airflow/ENV_EXAMPLE .env
# Editar .env con tus configuraciones

# 5. Iniciar servicios locales (opcional)
cd data/airflow
docker-compose up -d
```

### Configuraci√≥n de IDE

#### VS Code

```json
{
  "python.linting.enabled": true,
  "python.linting.pylintEnabled": false,
  "python.linting.ruffEnabled": true,
  "python.formatting.provider": "black",
  "editor.formatOnSave": true,
  "python.analysis.typeCheckingMode": "basic"
}
```

#### PyCharm

1. Configurar Python interpreter: `File > Settings > Project > Python Interpreter`
2. Habilitar Ruff: `File > Settings > Tools > Ruff`
3. Configurar Black: `File > Settings > Tools > Black`

---

## üìÅ Estructura del Proyecto

```
IA/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dags/           # DAGs de Airflow
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ plugins/        # Plugins y utilidades
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scripts/         # Scripts de utilidad
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tests/          # Tests unitarios
‚îÇ   ‚îú‚îÄ‚îÄ db/                 # Schemas de base de datos
‚îÇ   ‚îî‚îÄ‚îÄ integrations/       # Integraciones externas
‚îú‚îÄ‚îÄ workflow/
‚îÇ   ‚îú‚îÄ‚îÄ kestra/             # Workflows de Kestra
‚îÇ   ‚îú‚îÄ‚îÄ flowable/           # Procesos BPMN Flowable
‚îÇ   ‚îî‚îÄ‚îÄ camunda/            # Procesos BPMN Camunda
‚îú‚îÄ‚îÄ ml/                     # Machine Learning
‚îú‚îÄ‚îÄ infra/                  # Infraestructura (Terraform, Ansible)
‚îú‚îÄ‚îÄ kubernetes/             # Manifiestos de Kubernetes
‚îú‚îÄ‚îÄ observability/          # Observabilidad (Prometheus, Grafana)
‚îú‚îÄ‚îÄ security/                # Seguridad (Vault, OPA)
‚îî‚îÄ‚îÄ docs/                   # Documentaci√≥n
```

---

## ‚úàÔ∏è Crear un Nuevo DAG de Airflow

### Plantilla B√°sica

```python
"""
DAG para [descripci√≥n del prop√≥sito].
"""
from __future__ import annotations

from datetime import timedelta
import logging

import pendulum
from airflow.decorators import dag, task
from airflow.providers.postgres.hooks.postgres import PostgresHook

logger = logging.getLogger(__name__)

# Configuraci√≥n del DAG
@dag(
    dag_id="mi_nuevo_dag",
    description="Descripci√≥n del DAG",
    schedule_interval="@daily",  # o cron expression
    start_date=pendulum.datetime(2024, 1, 1, tz="UTC"),
    catchup=False,
    tags=["etl", "datos"],
    default_args={
        "retries": 3,
        "retry_delay": timedelta(minutes=5),
        "on_failure_callback": on_task_failure,
    },
)
def mi_dag():
    """Definici√≥n del DAG."""
    
    @task
    def extraer_datos():
        """Extrae datos de la fuente."""
        logger.info("Extrayendo datos...")
        # Tu l√≥gica aqu√≠
        return {"status": "success", "count": 100}
    
    @task
    def transformar_datos(extraccion_result):
        """Transforma los datos."""
        logger.info(f"Transformando {extraccion_result['count']} registros...")
        # Tu l√≥gica aqu√≠
        return {"status": "success"}
    
    @task
    def cargar_datos(transformacion_result):
        """Carga los datos al destino."""
        logger.info("Cargando datos...")
        # Tu l√≥gica aqu√≠
        return {"status": "success"}
    
    # Flujo del DAG
    datos_extraidos = extraer_datos()
    datos_transformados = transformar_datos(datos_extraidos)
    cargar_datos(datos_transformados)

# Instanciar el DAG
mi_dag()
```

### Mejores Pr√°cticas

1. **Usar plugins existentes**: Reutilizar c√≥digo de `data/airflow/plugins/`
2. **Configuraci√≥n centralizada**: Usar `etl_config_constants.py` para constantes
3. **Manejo de errores**: Implementar retry logic y logging
4. **Idempotencia**: Asegurar que las tareas sean idempotentes
5. **Documentaci√≥n**: Documentar cada tarea y par√°metros

### Ejemplo con Plugins

```python
from data.airflow.plugins.approval_cleanup_config import get_config
from data.airflow.plugins.approval_cleanup_ops import get_pg_hook, execute_query_with_timeout
from data.airflow.plugins.approval_cleanup_utils import log_with_context

@task
def mi_tarea():
    """Tarea usando plugins."""
    config = get_config()
    pg_hook = get_pg_hook()
    
    log_with_context('info', 'Iniciando tarea...')
    
    result = execute_query_with_timeout(
        pg_hook,
        "SELECT * FROM tabla WHERE fecha > %s",
        parameters=(pendulum.now().subtract(days=1),)
    )
    
    return {"count": len(result)}
```

Ver [`data/airflow/dags/INDEX_ETL_IMPROVED.md`](../data/airflow/dags/INDEX_ETL_IMPROVED.md) para m√°s ejemplos.

---

## üéØ Crear un Nuevo Workflow en Kestra

### Estructura B√°sica

```yaml
id: mi-workflow
namespace: company
description: Descripci√≥n del workflow

tasks:
  - id: extraer
    type: io.kestra.plugin.fs.http.Download
    uri: https://api.example.com/data
    outputFile: /tmp/data.json

  - id: transformar
    type: io.kestra.plugin.scripts.python.Script
    script: |
      import json
      with open('/tmp/data.json') as f:
          data = json.load(f)
      # Transformaci√≥n aqu√≠
      with open('/tmp/transformed.json', 'w') as f:
          json.dump(data, f)

  - id: cargar
    type: io.kestra.plugin.jdbc.postgresql.CopyIn
    connectionString: "jdbc:postgresql://db:5432/mydb"
    from: /tmp/transformed.json
    table: mi_tabla

triggers:
  - id: schedule
    type: io.kestra.core.models.triggers.types.Schedule
    cron: "0 8 * * *"  # Diario a las 8 AM
```

### Caracter√≠sticas Avanzadas

- **Error Handling**: Manejo de errores con `onFailure`
- **Parallel Execution**: Tareas paralelas con `parallel`
- **Conditional Logic**: Flujos condicionales con `if`
- **Variables**: Variables din√°micas con `{{ variables.var_name }}`

Ver [`workflow/kestra/README.md`](../workflow/kestra/README.md) para m√°s ejemplos.

---

## üîß Crear un Nuevo Worker

### Worker de Python (Camunda)

```python
"""
Worker para procesar tareas externas de Camunda.
"""
import os
import logging
from camunda.external_task.external_task import ExternalTask, TaskResult
from camunda.external_task.external_task_worker import ExternalTaskWorker

logger = logging.getLogger(__name__)

# Configuraci√≥n
CAMUNDA_URL = os.getenv("CAMUNDA_URL", "http://camunda:8080/engine-rest")
TOPIC_NAME = "mi-topic"

def process_task(task: ExternalTask) -> TaskResult:
    """Procesa una tarea externa."""
    try:
        # Obtener variables
        variables = task.get_variables()
        data = variables.get("data")
        
        logger.info(f"Procesando tarea {task.get_task_id()} con datos: {data}")
        
        # Tu l√≥gica aqu√≠
        result = process_data(data)
        
        return task.complete({
            "result": result,
            "status": "success"
        })
        
    except Exception as e:
        logger.error(f"Error procesando tarea: {e}")
        return task.failure(
            error_message=str(e),
            error_details=str(e),
            max_retries=3,
            retry_timeout=5000
        )

if __name__ == "__main__":
    # Configurar worker
    worker = ExternalTaskWorker(
        worker_id="mi-worker",
        base_url=CAMUNDA_URL,
        config={
            "maxTasks": 1,
            "lockDuration": 10000,
            "asyncResponseTimeout": 5000
        }
    )
    
    # Suscribirse al topic
    worker.subscribe(TOPIC_NAME, process_task)
```

Ver [`workflow/camunda/README_worker.md`](../workflow/camunda/README_worker.md) para m√°s detalles.

---

## üß™ Testing

### Tests Unitarios

```python
"""
Tests unitarios para mi m√≥dulo.
"""
import pytest
from unittest.mock import Mock, patch
from data.airflow.plugins.approval_cleanup_ops import get_pg_hook

def test_get_pg_hook():
    """Test para obtener hook de PostgreSQL."""
    with patch('data.airflow.plugins.approval_cleanup_ops.PostgresHook') as mock_hook:
        hook = get_pg_hook()
        assert hook is not None
        mock_hook.assert_called_once()

def test_execute_query():
    """Test para ejecutar query."""
    # Tu test aqu√≠
    pass
```

### Ejecutar Tests

```bash
# Ejecutar todos los tests
pytest

# Ejecutar tests espec√≠ficos
pytest tests/test_approval_cleanup_ops.py

# Con cobertura
pytest --cov=data.airflow.plugins --cov-report=html

# Tests en paralelo
pytest -n auto
```

### Tests de Integraci√≥n

```python
"""
Tests de integraci√≥n con base de datos real.
"""
import pytest
from airflow.providers.postgres.hooks.postgres import PostgresHook

@pytest.fixture
def pg_hook():
    """Fixture para hook de PostgreSQL."""
    return PostgresHook(postgres_conn_id="test_db")

def test_integration_query(pg_hook):
    """Test de integraci√≥n con BD."""
    result = pg_hook.get_records("SELECT 1")
    assert result == [(1,)]
```

---

## üìù Code Review

### Checklist de Code Review

- [ ] **Funcionalidad**: ¬øEl c√≥digo hace lo que se espera?
- [ ] **Testing**: ¬øHay tests adecuados?
- [ ] **Documentaci√≥n**: ¬øEst√° documentado el c√≥digo?
- [ ] **Performance**: ¬øHay problemas de performance?
- [ ] **Seguridad**: ¬øHay vulnerabilidades de seguridad?
- [ ] **Estilo**: ¬øSigue las convenciones del proyecto?
- [ ] **Error Handling**: ¬øManeja errores correctamente?
- [ ] **Logging**: ¬øTiene logging apropiado?

### Convenciones de C√≥digo

#### Python

- **PEP 8**: Seguir gu√≠a de estilo PEP 8
- **Type Hints**: Usar type hints donde sea posible
- **Docstrings**: Documentar funciones y clases
- **Line Length**: M√°ximo 100 caracteres

```python
def procesar_datos(
    datos: List[Dict[str, Any]],
    config: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Procesa los datos seg√∫n la configuraci√≥n.
    
    Args:
        datos: Lista de diccionarios con los datos
        config: Configuraci√≥n opcional
        
    Returns:
        Diccionario con resultados del procesamiento
        
    Raises:
        ValueError: Si los datos son inv√°lidos
    """
    # Implementaci√≥n
    pass
```

#### YAML (Kestra)

- **Indentaci√≥n**: 2 espacios
- **Comentarios**: Explicar secciones complejas
- **Nombres**: Usar nombres descriptivos

---

## ‚úÖ Mejores Pr√°cticas

### 1. Manejo de Errores

```python
# ‚úÖ Bueno
try:
    result = process_data(data)
    logger.info(f"Procesado exitosamente: {result}")
except ValueError as e:
    logger.error(f"Error de validaci√≥n: {e}")
    raise
except Exception as e:
    logger.exception(f"Error inesperado: {e}")
    raise AirflowFailException(f"Error procesando datos: {e}")

# ‚ùå Malo
result = process_data(data)  # Sin manejo de errores
```

### 2. Logging

```python
# ‚úÖ Bueno
logger.info("Iniciando procesamiento", extra={
    "task_id": task_id,
    "record_count": len(records)
})

# ‚ùå Malo
print(f"Processing {len(records)} records")  # No usar print
```

### 3. Configuraci√≥n

```python
# ‚úÖ Bueno - Usar variables de entorno
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "1000"))

# ‚ùå Malo - Hardcodear valores
BATCH_SIZE = 1000
```

### 4. Idempotencia

```python
# ‚úÖ Bueno - Idempotente
@task
def cargar_datos():
    """Carga datos de forma idempotente."""
    # Verificar si ya existe
    if datos_ya_existen():
        logger.info("Datos ya cargados, saltando...")
        return
    
    # Cargar datos
    cargar()

# ‚ùå Malo - No idempotente
@task
def cargar_datos():
    """Carga datos (siempre inserta)."""
    insertar_datos()  # Puede duplicar
```

### 5. Performance

```python
# ‚úÖ Bueno - Procesamiento en lotes
def procesar_registros(registros: List[Dict]) -> None:
    """Procesa registros en lotes."""
    batch_size = 1000
    for i in range(0, len(registros), batch_size):
        batch = registros[i:i + batch_size]
        procesar_lote(batch)

# ‚ùå Malo - Procesamiento uno por uno
def procesar_registros(registros: List[Dict]) -> None:
    """Procesa registros uno por uno."""
    for registro in registros:
        procesar(registro)  # Muy lento
```

---

## üêõ Debugging

### Debugging Local

```bash
# Ejecutar DAG localmente
airflow dags test mi_dag 2024-01-01

# Ejecutar tarea espec√≠fica
airflow tasks test mi_dag mi_tarea 2024-01-01

# Con Python debugger
python -m pdb -c continue script.py
```

### Debugging en Kubernetes

```bash
# Ver logs de un pod
kubectl logs -f <pod-name> -n <namespace>

# Ejecutar shell en pod
kubectl exec -it <pod-name> -n <namespace> -- /bin/bash

# Ver eventos
kubectl get events -n <namespace> --sort-by='.lastTimestamp'
```

### Debugging con Logs

```python
# Logging estructurado
logger.info("Procesando datos", extra={
    "task_id": task_id,
    "record_count": len(records),
    "duration_ms": duration
})

# Logging con niveles
logger.debug("Detalles de depuraci√≥n")
logger.info("Informaci√≥n general")
logger.warning("Advertencia")
logger.error("Error")
logger.exception("Excepci√≥n con traceback")
```

---

## üîç Troubleshooting Com√∫n

### Error: "Module not found"

```bash
# Verificar que el m√≥dulo est√° en PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:/path/to/project"

# Verificar imports
python -c "from data.airflow.plugins.approval_cleanup_config import get_config; print('OK')"
```

### Error: "Database connection failed"

```bash
# Verificar connection ID en Airflow
airflow connections list | grep <connection_id>

# Probar conexi√≥n
python -c "from airflow.providers.postgres.hooks.postgres import PostgresHook; h = PostgresHook(postgres_conn_id='<id>'); print(h.get_conn())"
```

### Error: "Task timeout"

```python
# Aumentar timeout en la tarea
@task(
    execution_timeout=timedelta(hours=2)  # Aumentar timeout
)
def mi_tarea_lenta():
    # Tu c√≥digo
    pass
```

---

## üìö Recursos Adicionales

- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)
- [Python Style Guide (PEP 8)](https://pep8.org/)
- [Kestra Documentation](https://kestra.io/docs/)
- [Camunda External Task Pattern](https://docs.camunda.org/manual/latest/user-guide/process-engine/external-tasks/)

---

**Versi√≥n**: 2.0 | **Estado**: Producci√≥n Ready ‚úÖ  
**Mantenido por**: platform-team  
**√öltima actualizaci√≥n**: 2024

